{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to FastVideo","text":"FastVideo is a unified inference and post-training framework for accelerated video generation. Star Watch Fork <p>FastVideo is an inference and post-training framework for diffusion models. It features an end-to-end unified pipeline for accelerating diffusion models, starting from data preprocessing to model training, finetuning, distillation, and inference. FastVideo is designed to be modular and extensible, allowing users to easily add new optimizations and techniques. Whether it is training-free optimizations or post-training optimizations, FastVideo has you covered.</p>"},{"location":"#key-features","title":"Key Features","text":"<p>FastVideo has the following features: - State-of-the-art performance optimizations for inference   - Sliding Tile Attention   - TeaCache   - Sage Attention - E2E post-training support   - Data preprocessing pipeline for video data.   - Sparse distillation for Wan2.1 and Wan2.2 using Video Sparse Attention and Distribution Matching Distillation   - Support full finetuning and LoRA finetuning for state-of-the-art open video DiTs.   - Scalable training with FSDP2, sequence parallelism, and selective activation checkpointing, with near linear scaling to 64 GPUs.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>Welcome to FastVideo! This documentation will help you get started with our unified inference and post-training framework for accelerated video generation.</p> <p>Use the navigation menu on the left to explore different sections:</p> <ul> <li>Getting Started: Installation and quick start guides</li> <li>Inference: Learn how to use FastVideo for video generation</li> <li>Training: Data preprocessing and fine-tuning workflows  </li> <li>Distillation: Post-training optimization techniques</li> <li>Sliding Tile Attention: Advanced attention mechanisms</li> <li>Video Sparse Attention: Efficient attention for video models</li> <li>Design: Framework architecture and design principles</li> <li>Developer Guide: Contributing and development setup</li> <li>API Reference: Complete API documentation</li> </ul>"},{"location":"seed_parameter_behavior/","title":"Seed Parameter Behavior in vLLM","text":""},{"location":"seed_parameter_behavior/#overview","title":"Overview","text":"<p>The <code>seed</code> parameter in vLLM is used to control the random states for various random number generators. This parameter can affect the behavior of random operations in user code, especially when working with models in vLLM.</p>"},{"location":"seed_parameter_behavior/#default-behavior","title":"Default Behavior","text":"<p>By default, the <code>seed</code> parameter is set to <code>None</code>. When the <code>seed</code> parameter is <code>None</code>, the global random states for <code>random</code>, <code>np.random</code>, and <code>torch.manual_seed</code> are not set. This means that the random operations will behave as expected, without any fixed random states.</p>"},{"location":"seed_parameter_behavior/#specifying-a-seed","title":"Specifying a Seed","text":"<p>If a specific seed value is provided, the global random states for <code>random</code>, <code>np.random</code>, and <code>torch.manual_seed</code> will be set accordingly. This can be useful for reproducibility, as it ensures that the random operations produce the same results across multiple runs.</p>"},{"location":"seed_parameter_behavior/#example-usage","title":"Example Usage","text":""},{"location":"seed_parameter_behavior/#without-specifying-a-seed","title":"Without Specifying a Seed","text":"<pre><code>import random\nfrom vllm import LLM\n\n# Initialize a vLLM model without specifying a seed\nmodel = LLM(model=\"Qwen/Qwen2.5-0.5B-Instruct\")\n\n# Try generating random numbers\nprint(random.randint(0, 100))  # Outputs different numbers across runs\n</code></pre>"},{"location":"seed_parameter_behavior/#specifying-a-seed_1","title":"Specifying a Seed","text":"<pre><code>import random\nfrom vllm import LLM\n\n# Initialize a vLLM model with a specific seed\nmodel = LLM(model=\"Qwen/Qwen2.5-0.5B-Instruct\", seed=42)\n\n# Try generating random numbers\nprint(random.randint(0, 100))  # Outputs the same number across runs\n</code></pre>"},{"location":"seed_parameter_behavior/#important-notes","title":"Important Notes","text":"<ul> <li>If the <code>seed</code> parameter is not specified, the behavior of global random states remains unaffected.</li> <li>If a specific seed value is provided, the global random states for <code>random</code>, <code>np.random</code>, and <code>torch.manual_seed</code> will be set to that value.</li> <li>This behavior can be useful for reproducibility but may lead to non-intuitive behavior if the user is not explicitly aware of it.</li> </ul>"},{"location":"seed_parameter_behavior/#conclusion","title":"Conclusion","text":"<p>Understanding the behavior of the <code>seed</code> parameter in vLLM is crucial for ensuring the expected behavior of random operations in your code. By default, the <code>seed</code> parameter is set to <code>None</code>, which means that the global random states are not affected. However, specifying a seed value can help achieve reproducibility in your experiments.</p>"},{"location":"api/fastvideo/","title":"FastVideo API Reference","text":"<p>This page contains the complete API reference for the FastVideo library.</p>"},{"location":"api/fastvideo/#fastvideo","title":"fastvideo","text":""},{"location":"api/fastvideo/#modules","title":"Modules","text":"Name Description attention Attention mechanisms and backends for video generation configs Configuration classes for pipelines, models, and sampling distributed Distributed execution and communication utilities entrypoints Main API entry points for video generation models Model implementations (transformers, VAEs, schedulers) pipelines Core pipeline classes for video diffusion training Training utilities and helpers workflow Workflow management and orchestration dataset Dataset handling and preprocessing layers Custom neural network layers platforms Platform-specific implementations utils Utility functions and helpers worker Execution workers for video generation"},{"location":"api/fastvideo/#fastvideoattention","title":"fastvideo.attention","text":""},{"location":"api/fastvideo/#fastvideo.attention","title":"attention","text":""},{"location":"api/fastvideo/#fastvideo.attention-classes","title":"Classes","text":""},{"location":"api/fastvideo/#fastvideo.attention.AttentionBackend","title":"fastvideo.attention.AttentionBackend","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class for attention backends.</p>"},{"location":"api/fastvideo/#fastvideo.attention.AttentionMetadata","title":"fastvideo.attention.AttentionMetadata  <code>dataclass</code>","text":"<pre><code>AttentionMetadata(current_timestep: int)\n</code></pre> <p>Attention metadata for prefill and decode batched together.</p>"},{"location":"api/fastvideo/#fastvideo.attention.AttentionMetadata-functions","title":"Functions","text":"fastvideo.attention.AttentionMetadata.asdict_zerocopy \u00b6 <pre><code>asdict_zerocopy(\n    skip_fields: set[str] | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Similar to dataclasses.asdict, but avoids deepcopying.</p> Source code in <code>fastvideo/attention/backends/abstract.py</code> <pre><code>def asdict_zerocopy(self,\n                    skip_fields: set[str] | None = None) -&gt; dict[str, Any]:\n    \"\"\"Similar to dataclasses.asdict, but avoids deepcopying.\"\"\"\n    if skip_fields is None:\n        skip_fields = set()\n    # Note that if we add dataclasses as fields, they will need\n    # similar handling.\n    return {\n        field.name: getattr(self, field.name)\n        for field in fields(self) if field.name not in skip_fields\n    }\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.attention.AttentionMetadataBuilder","title":"fastvideo.attention.AttentionMetadataBuilder","text":"<pre><code>AttentionMetadataBuilder()\n</code></pre> <p>               Bases: <code>ABC</code>, <code>Generic[T]</code></p> <p>Abstract class for attention metadata builders.</p> <p>Create the builder, remember some configuration and parameters.</p> Source code in <code>fastvideo/attention/backends/abstract.py</code> <pre><code>@abstractmethod\ndef __init__(self) -&gt; None:\n    \"\"\"Create the builder, remember some configuration and parameters.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.attention.AttentionMetadataBuilder-functions","title":"Functions","text":"fastvideo.attention.AttentionMetadataBuilder.build <code>abstractmethod</code> \u00b6 <pre><code>build(**kwargs: dict[str, Any]) -&gt; AttentionMetadata\n</code></pre> <p>Build attention metadata with on-device tensors.</p> Source code in <code>fastvideo/attention/backends/abstract.py</code> <pre><code>@abstractmethod\ndef build(\n    self,\n    **kwargs: dict[str, Any],\n) -&gt; AttentionMetadata:\n    \"\"\"Build attention metadata with on-device tensors.\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.attention.AttentionMetadataBuilder.prepare <code>abstractmethod</code> \u00b6 <pre><code>prepare() -&gt; None\n</code></pre> <p>Prepare for one batch.</p> Source code in <code>fastvideo/attention/backends/abstract.py</code> <pre><code>@abstractmethod\ndef prepare(self) -&gt; None:\n    \"\"\"Prepare for one batch.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.attention.DistributedAttention","title":"fastvideo.attention.DistributedAttention","text":"<pre><code>DistributedAttention(\n    num_heads: int,\n    head_size: int,\n    num_kv_heads: int | None = None,\n    softmax_scale: float | None = None,\n    causal: bool = False,\n    supported_attention_backends: tuple[\n        AttentionBackendEnum, ...\n    ]\n    | None = None,\n    prefix: str = \"\",\n    **extra_impl_args\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Distributed attention layer.</p> Source code in <code>fastvideo/attention/layer.py</code> <pre><code>def __init__(self,\n             num_heads: int,\n             head_size: int,\n             num_kv_heads: int | None = None,\n             softmax_scale: float | None = None,\n             causal: bool = False,\n             supported_attention_backends: tuple[AttentionBackendEnum, ...]\n             | None = None,\n             prefix: str = \"\",\n             **extra_impl_args) -&gt; None:\n    super().__init__()\n    if softmax_scale is None:\n        self.softmax_scale = head_size**-0.5\n    else:\n        self.softmax_scale = softmax_scale\n\n    if num_kv_heads is None:\n        num_kv_heads = num_heads\n\n    dtype = get_compute_dtype()\n    attn_backend = get_attn_backend(\n        head_size,\n        dtype,\n        supported_attention_backends=supported_attention_backends)\n    impl_cls = attn_backend.get_impl_cls()\n    self.attn_impl = impl_cls(num_heads=num_heads,\n                              head_size=head_size,\n                              causal=causal,\n                              softmax_scale=self.softmax_scale,\n                              num_kv_heads=num_kv_heads,\n                              prefix=f\"{prefix}.impl\",\n                              **extra_impl_args)\n    self.num_heads = num_heads\n    self.head_size = head_size\n    self.num_kv_heads = num_kv_heads\n    self.backend = backend_name_to_enum(attn_backend.get_name())\n    self.dtype = dtype\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.attention.DistributedAttention-functions","title":"Functions","text":"fastvideo.attention.DistributedAttention.forward \u00b6 <pre><code>forward(\n    q: Tensor,\n    k: Tensor,\n    v: Tensor,\n    replicated_q: Tensor | None = None,\n    replicated_k: Tensor | None = None,\n    replicated_v: Tensor | None = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor | None]\n</code></pre> <p>Forward pass for distributed attention.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>Tensor</code> <p>Query tensor [batch_size, seq_len, num_heads, head_dim]</p> required <code>k</code> <code>Tensor</code> <p>Key tensor [batch_size, seq_len, num_heads, head_dim]</p> required <code>v</code> <code>Tensor</code> <p>Value tensor [batch_size, seq_len, num_heads, head_dim]</p> required <code>replicated_q</code> <code>Optional[Tensor]</code> <p>Replicated query tensor, typically for text tokens</p> <code>None</code> <code>replicated_k</code> <code>Optional[Tensor]</code> <p>Replicated key tensor</p> <code>None</code> <code>replicated_v</code> <code>Optional[Tensor]</code> <p>Replicated value tensor</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor | None]</code> <p>Tuple[torch.Tensor, Optional[torch.Tensor]]: A tuple containing: - o (torch.Tensor): Output tensor after attention for the main sequence - replicated_o (Optional[torch.Tensor]): Output tensor for replicated tokens, if provided</p> Source code in <code>fastvideo/attention/layer.py</code> <pre><code>@torch.compiler.disable\ndef forward(\n    self,\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    replicated_q: torch.Tensor | None = None,\n    replicated_k: torch.Tensor | None = None,\n    replicated_v: torch.Tensor | None = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor | None]:\n    \"\"\"Forward pass for distributed attention.\n\n    Args:\n        q (torch.Tensor): Query tensor [batch_size, seq_len, num_heads, head_dim]\n        k (torch.Tensor): Key tensor [batch_size, seq_len, num_heads, head_dim]\n        v (torch.Tensor): Value tensor [batch_size, seq_len, num_heads, head_dim]\n        replicated_q (Optional[torch.Tensor]): Replicated query tensor, typically for text tokens\n        replicated_k (Optional[torch.Tensor]): Replicated key tensor\n        replicated_v (Optional[torch.Tensor]): Replicated value tensor\n\n    Returns:\n        Tuple[torch.Tensor, Optional[torch.Tensor]]: A tuple containing:\n            - o (torch.Tensor): Output tensor after attention for the main sequence\n            - replicated_o (Optional[torch.Tensor]): Output tensor for replicated tokens, if provided\n    \"\"\"\n    # Check input shapes\n    assert q.dim() == 4 and k.dim() == 4 and v.dim(\n    ) == 4, \"Expected 4D tensors\"\n    batch_size, seq_len, num_heads, head_dim = q.shape\n    local_rank = get_sp_parallel_rank()\n    world_size = get_sp_world_size()\n\n    forward_context: ForwardContext = get_forward_context()\n    ctx_attn_metadata = forward_context.attn_metadata\n\n    # Stack QKV\n    qkv = torch.cat([q, k, v], dim=0)  # [3, seq_len, num_heads, head_dim]\n\n    # Redistribute heads across sequence dimension\n    qkv = sequence_model_parallel_all_to_all_4D(qkv,\n                                                scatter_dim=2,\n                                                gather_dim=1)\n    # Apply backend-specific preprocess_qkv\n    qkv = self.attn_impl.preprocess_qkv(qkv, ctx_attn_metadata)\n\n    # Concatenate with replicated QKV if provided\n    if replicated_q is not None:\n        assert replicated_k is not None and replicated_v is not None\n        replicated_qkv = torch.cat(\n            [replicated_q, replicated_k, replicated_v],\n            dim=0)  # [3, seq_len, num_heads, head_dim]\n        heads_per_rank = num_heads // world_size\n        replicated_qkv = replicated_qkv[:, :, local_rank *\n                                        heads_per_rank:(local_rank + 1) *\n                                        heads_per_rank]\n        qkv = torch.cat([qkv, replicated_qkv], dim=1)\n\n    q, k, v = qkv.chunk(3, dim=0)\n\n    output = self.attn_impl.forward(q, k, v, ctx_attn_metadata)\n\n    # Redistribute back if using sequence parallelism\n    replicated_output = None\n    if replicated_q is not None:\n        replicated_output = output[:, seq_len * world_size:]\n        output = output[:, :seq_len * world_size]\n        # TODO: make this asynchronous\n        replicated_output = sequence_model_parallel_all_gather(\n            replicated_output.contiguous(), dim=2)\n    # Apply backend-specific postprocess_output\n    output = self.attn_impl.postprocess_output(output, ctx_attn_metadata)\n\n    output = sequence_model_parallel_all_to_all_4D(output,\n                                                   scatter_dim=1,\n                                                   gather_dim=2)\n    return output, replicated_output\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.attention.DistributedAttention_VSA","title":"fastvideo.attention.DistributedAttention_VSA","text":"<pre><code>DistributedAttention_VSA(\n    num_heads: int,\n    head_size: int,\n    num_kv_heads: int | None = None,\n    softmax_scale: float | None = None,\n    causal: bool = False,\n    supported_attention_backends: tuple[\n        AttentionBackendEnum, ...\n    ]\n    | None = None,\n    prefix: str = \"\",\n    **extra_impl_args\n)\n</code></pre> <p>               Bases: <code>DistributedAttention</code></p> <p>Distributed attention layer with VSA support.</p> Source code in <code>fastvideo/attention/layer.py</code> <pre><code>def __init__(self,\n             num_heads: int,\n             head_size: int,\n             num_kv_heads: int | None = None,\n             softmax_scale: float | None = None,\n             causal: bool = False,\n             supported_attention_backends: tuple[AttentionBackendEnum, ...]\n             | None = None,\n             prefix: str = \"\",\n             **extra_impl_args) -&gt; None:\n    super().__init__()\n    if softmax_scale is None:\n        self.softmax_scale = head_size**-0.5\n    else:\n        self.softmax_scale = softmax_scale\n\n    if num_kv_heads is None:\n        num_kv_heads = num_heads\n\n    dtype = get_compute_dtype()\n    attn_backend = get_attn_backend(\n        head_size,\n        dtype,\n        supported_attention_backends=supported_attention_backends)\n    impl_cls = attn_backend.get_impl_cls()\n    self.attn_impl = impl_cls(num_heads=num_heads,\n                              head_size=head_size,\n                              causal=causal,\n                              softmax_scale=self.softmax_scale,\n                              num_kv_heads=num_kv_heads,\n                              prefix=f\"{prefix}.impl\",\n                              **extra_impl_args)\n    self.num_heads = num_heads\n    self.head_size = head_size\n    self.num_kv_heads = num_kv_heads\n    self.backend = backend_name_to_enum(attn_backend.get_name())\n    self.dtype = dtype\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.attention.DistributedAttention_VSA-functions","title":"Functions","text":"fastvideo.attention.DistributedAttention_VSA.forward \u00b6 <pre><code>forward(\n    q: Tensor,\n    k: Tensor,\n    v: Tensor,\n    replicated_q: Tensor | None = None,\n    replicated_k: Tensor | None = None,\n    replicated_v: Tensor | None = None,\n    gate_compress: Tensor | None = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor | None]\n</code></pre> <p>Forward pass for distributed attention.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>Tensor</code> <p>Query tensor [batch_size, seq_len, num_heads, head_dim]</p> required <code>k</code> <code>Tensor</code> <p>Key tensor [batch_size, seq_len, num_heads, head_dim]</p> required <code>v</code> <code>Tensor</code> <p>Value tensor [batch_size, seq_len, num_heads, head_dim]</p> required <code>gate_compress</code> <code>Tensor</code> <p>Gate compress tensor [batch_size, seq_len, num_heads, head_dim]</p> <code>None</code> <code>replicated_q</code> <code>Optional[Tensor]</code> <p>Replicated query tensor, typically for text tokens</p> <code>None</code> <code>replicated_k</code> <code>Optional[Tensor]</code> <p>Replicated key tensor</p> <code>None</code> <code>replicated_v</code> <code>Optional[Tensor]</code> <p>Replicated value tensor</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor | None]</code> <p>Tuple[torch.Tensor, Optional[torch.Tensor]]: A tuple containing: - o (torch.Tensor): Output tensor after attention for the main sequence - replicated_o (Optional[torch.Tensor]): Output tensor for replicated tokens, if provided</p> Source code in <code>fastvideo/attention/layer.py</code> <pre><code>@torch.compiler.disable\ndef forward(\n    self,\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    replicated_q: torch.Tensor | None = None,\n    replicated_k: torch.Tensor | None = None,\n    replicated_v: torch.Tensor | None = None,\n    gate_compress: torch.Tensor | None = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor | None]:\n    \"\"\"Forward pass for distributed attention.\n\n    Args:\n        q (torch.Tensor): Query tensor [batch_size, seq_len, num_heads, head_dim]\n        k (torch.Tensor): Key tensor [batch_size, seq_len, num_heads, head_dim]\n        v (torch.Tensor): Value tensor [batch_size, seq_len, num_heads, head_dim]\n        gate_compress (torch.Tensor): Gate compress tensor [batch_size, seq_len, num_heads, head_dim]\n        replicated_q (Optional[torch.Tensor]): Replicated query tensor, typically for text tokens\n        replicated_k (Optional[torch.Tensor]): Replicated key tensor\n        replicated_v (Optional[torch.Tensor]): Replicated value tensor\n\n    Returns:\n        Tuple[torch.Tensor, Optional[torch.Tensor]]: A tuple containing:\n            - o (torch.Tensor): Output tensor after attention for the main sequence\n            - replicated_o (Optional[torch.Tensor]): Output tensor for replicated tokens, if provided\n    \"\"\"\n    # Check text tokens are not supported for VSA now\n    assert replicated_q is None and replicated_k is None and replicated_v is None, \"Replicated QKV is not supported for VSA now\"\n    # Check input shapes\n    assert q.dim() == 4 and k.dim() == 4 and v.dim(\n    ) == 4, \"Expected 4D tensors\"\n\n    forward_context: ForwardContext = get_forward_context()\n    ctx_attn_metadata = forward_context.attn_metadata\n\n    # Stack QKV\n    qkvg = torch.cat([q, k, v, gate_compress],\n                     dim=0)  # [3, seq_len, num_heads, head_dim]\n\n    # Redistribute heads across sequence dimension\n    qkvg = sequence_model_parallel_all_to_all_4D(qkvg,\n                                                 scatter_dim=2,\n                                                 gather_dim=1)\n\n    qkvg = self.attn_impl.preprocess_qkv(qkvg, ctx_attn_metadata)\n\n    q, k, v, gate_compress = qkvg.chunk(4, dim=0)\n    output = self.attn_impl.forward(\n        q, k, v, gate_compress, ctx_attn_metadata)  # type: ignore[call-arg]\n\n    # Redistribute back if using sequence parallelism\n    replicated_output = None\n\n    # Apply backend-specific postprocess_output\n    output = self.attn_impl.postprocess_output(output, ctx_attn_metadata)\n\n    output = sequence_model_parallel_all_to_all_4D(output,\n                                                   scatter_dim=1,\n                                                   gather_dim=2)\n    return output, replicated_output\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.attention.LocalAttention","title":"fastvideo.attention.LocalAttention","text":"<pre><code>LocalAttention(\n    num_heads: int,\n    head_size: int,\n    num_kv_heads: int | None = None,\n    softmax_scale: float | None = None,\n    causal: bool = False,\n    supported_attention_backends: tuple[\n        AttentionBackendEnum, ...\n    ]\n    | None = None,\n    **extra_impl_args\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Attention layer.</p> Source code in <code>fastvideo/attention/layer.py</code> <pre><code>def __init__(self,\n             num_heads: int,\n             head_size: int,\n             num_kv_heads: int | None = None,\n             softmax_scale: float | None = None,\n             causal: bool = False,\n             supported_attention_backends: tuple[AttentionBackendEnum, ...]\n             | None = None,\n             **extra_impl_args) -&gt; None:\n    super().__init__()\n    if softmax_scale is None:\n        self.softmax_scale = head_size**-0.5\n    else:\n        self.softmax_scale = softmax_scale\n    if num_kv_heads is None:\n        num_kv_heads = num_heads\n\n    dtype = get_compute_dtype()\n    attn_backend = get_attn_backend(\n        head_size,\n        dtype,\n        supported_attention_backends=supported_attention_backends)\n    impl_cls = attn_backend.get_impl_cls()\n    self.attn_impl = impl_cls(num_heads=num_heads,\n                              head_size=head_size,\n                              softmax_scale=self.softmax_scale,\n                              num_kv_heads=num_kv_heads,\n                              causal=causal,\n                              **extra_impl_args)\n    self.num_heads = num_heads\n    self.head_size = head_size\n    self.num_kv_heads = num_kv_heads\n    self.backend = backend_name_to_enum(attn_backend.get_name())\n    self.dtype = dtype\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.attention.LocalAttention-functions","title":"Functions","text":"fastvideo.attention.LocalAttention.forward \u00b6 <pre><code>forward(q: Tensor, k: Tensor, v: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Apply local attention between query, key and value tensors.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>Tensor</code> <p>Query tensor of shape [batch_size, seq_len, num_heads, head_dim]</p> required <code>k</code> <code>Tensor</code> <p>Key tensor of shape [batch_size, seq_len, num_heads, head_dim] </p> required <code>v</code> <code>Tensor</code> <p>Value tensor of shape [batch_size, seq_len, num_heads, head_dim]</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output tensor after local attention</p> Source code in <code>fastvideo/attention/layer.py</code> <pre><code>def forward(\n    self,\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Apply local attention between query, key and value tensors.\n\n    Args:\n        q (torch.Tensor): Query tensor of shape [batch_size, seq_len, num_heads, head_dim]\n        k (torch.Tensor): Key tensor of shape [batch_size, seq_len, num_heads, head_dim] \n        v (torch.Tensor): Value tensor of shape [batch_size, seq_len, num_heads, head_dim]\n\n    Returns:\n        torch.Tensor: Output tensor after local attention\n    \"\"\"\n    # Check input shapes\n    assert q.dim() == 4 and k.dim() == 4 and v.dim(\n    ) == 4, \"Expected 4D tensors\"\n\n    forward_context: ForwardContext = get_forward_context()\n    ctx_attn_metadata = forward_context.attn_metadata\n\n    output = self.attn_impl.forward(q, k, v, ctx_attn_metadata)\n    return output\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.attention-modules","title":"Modules","text":""},{"location":"api/fastvideo/#fastvideo.attention.backends","title":"fastvideo.attention.backends","text":""},{"location":"api/fastvideo/#fastvideo.attention.backends-modules","title":"Modules","text":"fastvideo.attention.backends.abstract \u00b6 Classes\u00b6 fastvideo.attention.backends.abstract.AttentionBackend \u00b6 <p>               Bases: <code>ABC</code></p> <p>Abstract class for attention backends.</p> fastvideo.attention.backends.abstract.AttentionImpl \u00b6 <pre><code>AttentionImpl(\n    num_heads: int,\n    head_size: int,\n    softmax_scale: float,\n    causal: bool = False,\n    num_kv_heads: int | None = None,\n    prefix: str = \"\",\n    **extra_impl_args\n)\n</code></pre> <p>               Bases: <code>ABC</code>, <code>Generic[T]</code></p> Source code in <code>fastvideo/attention/backends/abstract.py</code> <pre><code>@abstractmethod\ndef __init__(\n    self,\n    num_heads: int,\n    head_size: int,\n    softmax_scale: float,\n    causal: bool = False,\n    num_kv_heads: int | None = None,\n    prefix: str = \"\",\n    **extra_impl_args,\n) -&gt; None:\n    raise NotImplementedError\n</code></pre> Functions\u00b6 fastvideo.attention.backends.abstract.AttentionImpl.postprocess_output \u00b6 <pre><code>postprocess_output(\n    output: Tensor, attn_metadata: T\n) -&gt; torch.Tensor\n</code></pre> <p>Postprocess the output tensor after the attention operation.</p> <p>Default implementation returns the tensor unchanged. Subclasses can override this to implement custom postprocessing like untiling, scaling, or other transformations.</p> <p>Called BEFORE all_to_all for distributed attention</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Tensor</code> <p>The output tensor from the attention operation</p> required <code>attn_metadata</code> <code>T</code> <p>Metadata for the attention operation</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Postprocessed output tensor</p> Source code in <code>fastvideo/attention/backends/abstract.py</code> <pre><code>def postprocess_output(\n    self,\n    output: torch.Tensor,\n    attn_metadata: T,\n) -&gt; torch.Tensor:\n    \"\"\"Postprocess the output tensor after the attention operation.\n\n    Default implementation returns the tensor unchanged.\n    Subclasses can override this to implement custom postprocessing\n    like untiling, scaling, or other transformations.\n\n    Called BEFORE all_to_all for distributed attention\n\n    Args:\n        output: The output tensor from the attention operation\n        attn_metadata: Metadata for the attention operation\n\n    Returns:\n        Postprocessed output tensor\n    \"\"\"\n\n    return output\n</code></pre> fastvideo.attention.backends.abstract.AttentionImpl.preprocess_qkv \u00b6 <pre><code>preprocess_qkv(\n    qkv: Tensor, attn_metadata: T\n) -&gt; torch.Tensor\n</code></pre> <p>Preprocess QKV tensor before performing attention operation.</p> <p>Default implementation returns the tensor unchanged. Subclasses can override this to implement custom preprocessing like reshaping, tiling, scaling, or other transformations.</p> <p>Called AFTER all_to_all for distributed attention</p> <p>Parameters:</p> Name Type Description Default <code>qkv</code> <code>Tensor</code> <p>The query-key-value tensor</p> required <code>attn_metadata</code> <code>T</code> <p>Metadata for the attention operation</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Processed QKV tensor</p> Source code in <code>fastvideo/attention/backends/abstract.py</code> <pre><code>def preprocess_qkv(self, qkv: torch.Tensor,\n                   attn_metadata: T) -&gt; torch.Tensor:\n    \"\"\"Preprocess QKV tensor before performing attention operation.\n\n    Default implementation returns the tensor unchanged.\n    Subclasses can override this to implement custom preprocessing\n    like reshaping, tiling, scaling, or other transformations.\n\n    Called AFTER all_to_all for distributed attention\n\n    Args:\n        qkv: The query-key-value tensor\n        attn_metadata: Metadata for the attention operation\n\n    Returns:\n        Processed QKV tensor\n    \"\"\"\n    return qkv\n</code></pre> fastvideo.attention.backends.abstract.AttentionMetadata <code>dataclass</code> \u00b6 <pre><code>AttentionMetadata(current_timestep: int)\n</code></pre> <p>Attention metadata for prefill and decode batched together.</p> Functions\u00b6 fastvideo.attention.backends.abstract.AttentionMetadata.asdict_zerocopy \u00b6 <pre><code>asdict_zerocopy(\n    skip_fields: set[str] | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Similar to dataclasses.asdict, but avoids deepcopying.</p> Source code in <code>fastvideo/attention/backends/abstract.py</code> <pre><code>def asdict_zerocopy(self,\n                    skip_fields: set[str] | None = None) -&gt; dict[str, Any]:\n    \"\"\"Similar to dataclasses.asdict, but avoids deepcopying.\"\"\"\n    if skip_fields is None:\n        skip_fields = set()\n    # Note that if we add dataclasses as fields, they will need\n    # similar handling.\n    return {\n        field.name: getattr(self, field.name)\n        for field in fields(self) if field.name not in skip_fields\n    }\n</code></pre> fastvideo.attention.backends.abstract.AttentionMetadataBuilder \u00b6 <pre><code>AttentionMetadataBuilder()\n</code></pre> <p>               Bases: <code>ABC</code>, <code>Generic[T]</code></p> <p>Abstract class for attention metadata builders.</p> <p>Create the builder, remember some configuration and parameters.</p> Source code in <code>fastvideo/attention/backends/abstract.py</code> <pre><code>@abstractmethod\ndef __init__(self) -&gt; None:\n    \"\"\"Create the builder, remember some configuration and parameters.\"\"\"\n    raise NotImplementedError\n</code></pre> Functions\u00b6 fastvideo.attention.backends.abstract.AttentionMetadataBuilder.build <code>abstractmethod</code> \u00b6 <pre><code>build(**kwargs: dict[str, Any]) -&gt; AttentionMetadata\n</code></pre> <p>Build attention metadata with on-device tensors.</p> Source code in <code>fastvideo/attention/backends/abstract.py</code> <pre><code>@abstractmethod\ndef build(\n    self,\n    **kwargs: dict[str, Any],\n) -&gt; AttentionMetadata:\n    \"\"\"Build attention metadata with on-device tensors.\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.attention.backends.abstract.AttentionMetadataBuilder.prepare <code>abstractmethod</code> \u00b6 <pre><code>prepare() -&gt; None\n</code></pre> <p>Prepare for one batch.</p> Source code in <code>fastvideo/attention/backends/abstract.py</code> <pre><code>@abstractmethod\ndef prepare(self) -&gt; None:\n    \"\"\"Prepare for one batch.\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.attention.backends.video_sparse_attn \u00b6 Classes\u00b6 Functions\u00b6 fastvideo.attention.backends.video_sparse_attn.construct_variable_block_sizes <code>cached</code> \u00b6 <pre><code>construct_variable_block_sizes(\n    dit_seq_shape: tuple[int, int, int],\n    num_tiles: tuple[int, int, int],\n    device: device,\n) -&gt; torch.LongTensor\n</code></pre> <p>Compute the number of valid (non\u2011padded) tokens inside every (ts_t\u00a0\u00d7\u00a0ts_h\u00a0\u00d7\u00a0ts_w) tile after padding \u2011\u2011 flattened in the order (t\u2011tile, h\u2011tile, w\u2011tile) that <code>rearrange</code> uses.</p> fastvideo.attention.backends.vmoba \u00b6 Classes\u00b6 fastvideo.attention.backends.vmoba.VMOBAAttentionImpl \u00b6 <pre><code>VMOBAAttentionImpl(\n    num_heads,\n    head_size,\n    softmax_scale,\n    causal=False,\n    num_kv_heads=None,\n    prefix=\"\",\n    **extra_impl_args\n)\n</code></pre> <p>               Bases: <code>AttentionImpl</code></p> Source code in <code>fastvideo/attention/backends/vmoba.py</code> <pre><code>def __init__(self,\n             num_heads,\n             head_size,\n             softmax_scale,\n             causal=False,\n             num_kv_heads=None,\n             prefix=\"\",\n             **extra_impl_args) -&gt; None:\n    self.prefix = prefix\n    self.layer_idx = self._get_layer_idx(prefix)\n    from flash_attn.bert_padding import pad_input\n    self.pad_input = pad_input\n</code></pre> Functions\u00b6 fastvideo.attention.backends.vmoba.VMOBAAttentionImpl.forward \u00b6 <pre><code>forward(\n    query: Tensor,\n    key: Tensor,\n    value: Tensor,\n    attn_metadata: AttentionMetadata,\n) -&gt; torch.Tensor\n</code></pre> <p>query: [B, L, H, D] key:   [B, L, H, D] value: [B, L, H, D] attn_metadata: AttentionMetadata</p> Source code in <code>fastvideo/attention/backends/vmoba.py</code> <pre><code>def forward(\n    self,\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    attn_metadata: AttentionMetadata,\n) -&gt; torch.Tensor:\n    \"\"\"\n    query: [B, L, H, D]\n    key:   [B, L, H, D]\n    value: [B, L, H, D]\n    attn_metadata: AttentionMetadata\n    \"\"\"\n    batch_size, sequence_length, num_heads, head_dim = query.shape\n\n    # select chunk type according to layer idx:\n    loop_layer_num = attn_metadata.temporal_layer + attn_metadata.spatial_layer + attn_metadata.st_layer\n    moba_layer = self.layer_idx - attn_metadata.first_full_layer\n    if moba_layer % loop_layer_num &lt; attn_metadata.temporal_layer:\n        moba_chunk_size = attn_metadata.temporal_chunk_size\n        moba_topk = attn_metadata.temporal_topk\n    elif moba_layer % loop_layer_num &lt; attn_metadata.temporal_layer + attn_metadata.spatial_layer:\n        moba_chunk_size = attn_metadata.spatial_chunk_size\n        moba_topk = attn_metadata.spatial_topk\n    elif moba_layer % loop_layer_num &lt; attn_metadata.temporal_layer + attn_metadata.spatial_layer + attn_metadata.st_layer:\n        moba_chunk_size = attn_metadata.st_chunk_size\n        moba_topk = attn_metadata.st_topk\n\n    query, chunk_size = process_moba_input(query,\n                                           attn_metadata.patch_resolution,\n                                           moba_chunk_size)\n    key, chunk_size = process_moba_input(key,\n                                         attn_metadata.patch_resolution,\n                                         moba_chunk_size)\n    value, chunk_size = process_moba_input(value,\n                                           attn_metadata.patch_resolution,\n                                           moba_chunk_size)\n    max_seqlen = query.shape[1]\n    indices_q = torch.arange(0,\n                             query.shape[0] * query.shape[1],\n                             device=query.device)\n    cu_seqlens = torch.arange(0,\n                              query.shape[0] * query.shape[1] + 1,\n                              query.shape[1],\n                              dtype=torch.int32,\n                              device=query.device)\n    query = rearrange(query, \"b s ... -&gt; (b s) ...\")\n    key = rearrange(key, \"b s ... -&gt; (b s) ...\")\n    value = rearrange(value, \"b s ... -&gt; (b s) ...\")\n\n    # current_timestep=attn_metadata.current_timestep\n    hidden_states = moba_attn_varlen(\n        query,\n        key,\n        value,\n        cu_seqlens=cu_seqlens,\n        max_seqlen=max_seqlen,\n        moba_chunk_size=chunk_size,\n        moba_topk=moba_topk,\n        select_mode=attn_metadata.moba_select_mode,\n        simsum_threshold=attn_metadata.moba_threshold,\n        threshold_type=attn_metadata.moba_threshold_type,\n    )\n    hidden_states = self.pad_input(hidden_states, indices_q, batch_size,\n                                   sequence_length)\n    hidden_states = process_moba_output(hidden_states,\n                                        attn_metadata.patch_resolution,\n                                        moba_chunk_size)\n\n    return hidden_states\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/#fastvideo.attention.backends.video_sparse_attn.construct_variable_block_sizes--returns","title":"Returns","text":"<p>torch.LongTensor  # shape: [\u220f full_window_size]</p> Source code in <code>fastvideo/attention/backends/video_sparse_attn.py</code> <pre><code>@functools.lru_cache(maxsize=10)\ndef construct_variable_block_sizes(\n    dit_seq_shape: tuple[int, int, int],\n    num_tiles: tuple[int, int, int],\n    device: torch.device,\n) -&gt; torch.LongTensor:\n    \"\"\"\n    Compute the number of valid (non\u2011padded) tokens inside every\n    (ts_t\u00a0\u00d7\u00a0ts_h\u00a0\u00d7\u00a0ts_w) tile after padding \u2011\u2011 flattened in the order\n    (t\u2011tile, h\u2011tile, w\u2011tile) that `rearrange` uses.\n\n    Returns\n    -------\n    torch.LongTensor  # shape: [\u220f full_window_size]\n    \"\"\"\n    # unpack\n    t, h, w = dit_seq_shape\n    ts_t, ts_h, ts_w = VSA_TILE_SIZE\n    n_t, n_h, n_w = num_tiles\n\n    def _sizes(dim_len: int, tile: int, n_tiles: int) -&gt; torch.LongTensor:\n        \"\"\"Vector with the size of each tile along one dimension.\"\"\"\n        sizes = torch.full((n_tiles, ), tile, dtype=torch.int, device=device)\n        # size of last (possibly partial) tile\n        remainder = dim_len - (n_tiles - 1) * tile\n        sizes[-1] = remainder if remainder &gt; 0 else tile\n        return sizes\n\n    t_sizes = _sizes(t, ts_t, n_t)  # [n_t]\n    h_sizes = _sizes(h, ts_h, n_h)  # [n_h]\n    w_sizes = _sizes(w, ts_w, n_w)  # [n_w]\n\n    # broadcast\u2011multiply to get voxels per tile, then flatten\n    block_sizes = (\n        t_sizes[:, None, None]  # [n_t, 1,   1]\n        * h_sizes[None, :, None]  # [1,   n_h, 1]\n        * w_sizes[None, None, :]  # [1,   1,   n_w]\n    ).reshape(-1)  # [n_t * n_h * n_w]\n\n    return block_sizes\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.attention.layer","title":"fastvideo.attention.layer","text":""},{"location":"api/fastvideo/#fastvideo.attention.layer-classes","title":"Classes","text":"fastvideo.attention.layer.DistributedAttention \u00b6 <pre><code>DistributedAttention(\n    num_heads: int,\n    head_size: int,\n    num_kv_heads: int | None = None,\n    softmax_scale: float | None = None,\n    causal: bool = False,\n    supported_attention_backends: tuple[\n        AttentionBackendEnum, ...\n    ]\n    | None = None,\n    prefix: str = \"\",\n    **extra_impl_args\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Distributed attention layer.</p> Source code in <code>fastvideo/attention/layer.py</code> <pre><code>def __init__(self,\n             num_heads: int,\n             head_size: int,\n             num_kv_heads: int | None = None,\n             softmax_scale: float | None = None,\n             causal: bool = False,\n             supported_attention_backends: tuple[AttentionBackendEnum, ...]\n             | None = None,\n             prefix: str = \"\",\n             **extra_impl_args) -&gt; None:\n    super().__init__()\n    if softmax_scale is None:\n        self.softmax_scale = head_size**-0.5\n    else:\n        self.softmax_scale = softmax_scale\n\n    if num_kv_heads is None:\n        num_kv_heads = num_heads\n\n    dtype = get_compute_dtype()\n    attn_backend = get_attn_backend(\n        head_size,\n        dtype,\n        supported_attention_backends=supported_attention_backends)\n    impl_cls = attn_backend.get_impl_cls()\n    self.attn_impl = impl_cls(num_heads=num_heads,\n                              head_size=head_size,\n                              causal=causal,\n                              softmax_scale=self.softmax_scale,\n                              num_kv_heads=num_kv_heads,\n                              prefix=f\"{prefix}.impl\",\n                              **extra_impl_args)\n    self.num_heads = num_heads\n    self.head_size = head_size\n    self.num_kv_heads = num_kv_heads\n    self.backend = backend_name_to_enum(attn_backend.get_name())\n    self.dtype = dtype\n</code></pre> Functions\u00b6 fastvideo.attention.layer.DistributedAttention.forward \u00b6 <pre><code>forward(\n    q: Tensor,\n    k: Tensor,\n    v: Tensor,\n    replicated_q: Tensor | None = None,\n    replicated_k: Tensor | None = None,\n    replicated_v: Tensor | None = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor | None]\n</code></pre> <p>Forward pass for distributed attention.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>Tensor</code> <p>Query tensor [batch_size, seq_len, num_heads, head_dim]</p> required <code>k</code> <code>Tensor</code> <p>Key tensor [batch_size, seq_len, num_heads, head_dim]</p> required <code>v</code> <code>Tensor</code> <p>Value tensor [batch_size, seq_len, num_heads, head_dim]</p> required <code>replicated_q</code> <code>Optional[Tensor]</code> <p>Replicated query tensor, typically for text tokens</p> <code>None</code> <code>replicated_k</code> <code>Optional[Tensor]</code> <p>Replicated key tensor</p> <code>None</code> <code>replicated_v</code> <code>Optional[Tensor]</code> <p>Replicated value tensor</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor | None]</code> <p>Tuple[torch.Tensor, Optional[torch.Tensor]]: A tuple containing: - o (torch.Tensor): Output tensor after attention for the main sequence - replicated_o (Optional[torch.Tensor]): Output tensor for replicated tokens, if provided</p> Source code in <code>fastvideo/attention/layer.py</code> <pre><code>@torch.compiler.disable\ndef forward(\n    self,\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    replicated_q: torch.Tensor | None = None,\n    replicated_k: torch.Tensor | None = None,\n    replicated_v: torch.Tensor | None = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor | None]:\n    \"\"\"Forward pass for distributed attention.\n\n    Args:\n        q (torch.Tensor): Query tensor [batch_size, seq_len, num_heads, head_dim]\n        k (torch.Tensor): Key tensor [batch_size, seq_len, num_heads, head_dim]\n        v (torch.Tensor): Value tensor [batch_size, seq_len, num_heads, head_dim]\n        replicated_q (Optional[torch.Tensor]): Replicated query tensor, typically for text tokens\n        replicated_k (Optional[torch.Tensor]): Replicated key tensor\n        replicated_v (Optional[torch.Tensor]): Replicated value tensor\n\n    Returns:\n        Tuple[torch.Tensor, Optional[torch.Tensor]]: A tuple containing:\n            - o (torch.Tensor): Output tensor after attention for the main sequence\n            - replicated_o (Optional[torch.Tensor]): Output tensor for replicated tokens, if provided\n    \"\"\"\n    # Check input shapes\n    assert q.dim() == 4 and k.dim() == 4 and v.dim(\n    ) == 4, \"Expected 4D tensors\"\n    batch_size, seq_len, num_heads, head_dim = q.shape\n    local_rank = get_sp_parallel_rank()\n    world_size = get_sp_world_size()\n\n    forward_context: ForwardContext = get_forward_context()\n    ctx_attn_metadata = forward_context.attn_metadata\n\n    # Stack QKV\n    qkv = torch.cat([q, k, v], dim=0)  # [3, seq_len, num_heads, head_dim]\n\n    # Redistribute heads across sequence dimension\n    qkv = sequence_model_parallel_all_to_all_4D(qkv,\n                                                scatter_dim=2,\n                                                gather_dim=1)\n    # Apply backend-specific preprocess_qkv\n    qkv = self.attn_impl.preprocess_qkv(qkv, ctx_attn_metadata)\n\n    # Concatenate with replicated QKV if provided\n    if replicated_q is not None:\n        assert replicated_k is not None and replicated_v is not None\n        replicated_qkv = torch.cat(\n            [replicated_q, replicated_k, replicated_v],\n            dim=0)  # [3, seq_len, num_heads, head_dim]\n        heads_per_rank = num_heads // world_size\n        replicated_qkv = replicated_qkv[:, :, local_rank *\n                                        heads_per_rank:(local_rank + 1) *\n                                        heads_per_rank]\n        qkv = torch.cat([qkv, replicated_qkv], dim=1)\n\n    q, k, v = qkv.chunk(3, dim=0)\n\n    output = self.attn_impl.forward(q, k, v, ctx_attn_metadata)\n\n    # Redistribute back if using sequence parallelism\n    replicated_output = None\n    if replicated_q is not None:\n        replicated_output = output[:, seq_len * world_size:]\n        output = output[:, :seq_len * world_size]\n        # TODO: make this asynchronous\n        replicated_output = sequence_model_parallel_all_gather(\n            replicated_output.contiguous(), dim=2)\n    # Apply backend-specific postprocess_output\n    output = self.attn_impl.postprocess_output(output, ctx_attn_metadata)\n\n    output = sequence_model_parallel_all_to_all_4D(output,\n                                                   scatter_dim=1,\n                                                   gather_dim=2)\n    return output, replicated_output\n</code></pre> fastvideo.attention.layer.DistributedAttention_VSA \u00b6 <pre><code>DistributedAttention_VSA(\n    num_heads: int,\n    head_size: int,\n    num_kv_heads: int | None = None,\n    softmax_scale: float | None = None,\n    causal: bool = False,\n    supported_attention_backends: tuple[\n        AttentionBackendEnum, ...\n    ]\n    | None = None,\n    prefix: str = \"\",\n    **extra_impl_args\n)\n</code></pre> <p>               Bases: <code>DistributedAttention</code></p> <p>Distributed attention layer with VSA support.</p> Source code in <code>fastvideo/attention/layer.py</code> <pre><code>def __init__(self,\n             num_heads: int,\n             head_size: int,\n             num_kv_heads: int | None = None,\n             softmax_scale: float | None = None,\n             causal: bool = False,\n             supported_attention_backends: tuple[AttentionBackendEnum, ...]\n             | None = None,\n             prefix: str = \"\",\n             **extra_impl_args) -&gt; None:\n    super().__init__()\n    if softmax_scale is None:\n        self.softmax_scale = head_size**-0.5\n    else:\n        self.softmax_scale = softmax_scale\n\n    if num_kv_heads is None:\n        num_kv_heads = num_heads\n\n    dtype = get_compute_dtype()\n    attn_backend = get_attn_backend(\n        head_size,\n        dtype,\n        supported_attention_backends=supported_attention_backends)\n    impl_cls = attn_backend.get_impl_cls()\n    self.attn_impl = impl_cls(num_heads=num_heads,\n                              head_size=head_size,\n                              causal=causal,\n                              softmax_scale=self.softmax_scale,\n                              num_kv_heads=num_kv_heads,\n                              prefix=f\"{prefix}.impl\",\n                              **extra_impl_args)\n    self.num_heads = num_heads\n    self.head_size = head_size\n    self.num_kv_heads = num_kv_heads\n    self.backend = backend_name_to_enum(attn_backend.get_name())\n    self.dtype = dtype\n</code></pre> Functions\u00b6 fastvideo.attention.layer.DistributedAttention_VSA.forward \u00b6 <pre><code>forward(\n    q: Tensor,\n    k: Tensor,\n    v: Tensor,\n    replicated_q: Tensor | None = None,\n    replicated_k: Tensor | None = None,\n    replicated_v: Tensor | None = None,\n    gate_compress: Tensor | None = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor | None]\n</code></pre> <p>Forward pass for distributed attention.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>Tensor</code> <p>Query tensor [batch_size, seq_len, num_heads, head_dim]</p> required <code>k</code> <code>Tensor</code> <p>Key tensor [batch_size, seq_len, num_heads, head_dim]</p> required <code>v</code> <code>Tensor</code> <p>Value tensor [batch_size, seq_len, num_heads, head_dim]</p> required <code>gate_compress</code> <code>Tensor</code> <p>Gate compress tensor [batch_size, seq_len, num_heads, head_dim]</p> <code>None</code> <code>replicated_q</code> <code>Optional[Tensor]</code> <p>Replicated query tensor, typically for text tokens</p> <code>None</code> <code>replicated_k</code> <code>Optional[Tensor]</code> <p>Replicated key tensor</p> <code>None</code> <code>replicated_v</code> <code>Optional[Tensor]</code> <p>Replicated value tensor</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor | None]</code> <p>Tuple[torch.Tensor, Optional[torch.Tensor]]: A tuple containing: - o (torch.Tensor): Output tensor after attention for the main sequence - replicated_o (Optional[torch.Tensor]): Output tensor for replicated tokens, if provided</p> Source code in <code>fastvideo/attention/layer.py</code> <pre><code>@torch.compiler.disable\ndef forward(\n    self,\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    replicated_q: torch.Tensor | None = None,\n    replicated_k: torch.Tensor | None = None,\n    replicated_v: torch.Tensor | None = None,\n    gate_compress: torch.Tensor | None = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor | None]:\n    \"\"\"Forward pass for distributed attention.\n\n    Args:\n        q (torch.Tensor): Query tensor [batch_size, seq_len, num_heads, head_dim]\n        k (torch.Tensor): Key tensor [batch_size, seq_len, num_heads, head_dim]\n        v (torch.Tensor): Value tensor [batch_size, seq_len, num_heads, head_dim]\n        gate_compress (torch.Tensor): Gate compress tensor [batch_size, seq_len, num_heads, head_dim]\n        replicated_q (Optional[torch.Tensor]): Replicated query tensor, typically for text tokens\n        replicated_k (Optional[torch.Tensor]): Replicated key tensor\n        replicated_v (Optional[torch.Tensor]): Replicated value tensor\n\n    Returns:\n        Tuple[torch.Tensor, Optional[torch.Tensor]]: A tuple containing:\n            - o (torch.Tensor): Output tensor after attention for the main sequence\n            - replicated_o (Optional[torch.Tensor]): Output tensor for replicated tokens, if provided\n    \"\"\"\n    # Check text tokens are not supported for VSA now\n    assert replicated_q is None and replicated_k is None and replicated_v is None, \"Replicated QKV is not supported for VSA now\"\n    # Check input shapes\n    assert q.dim() == 4 and k.dim() == 4 and v.dim(\n    ) == 4, \"Expected 4D tensors\"\n\n    forward_context: ForwardContext = get_forward_context()\n    ctx_attn_metadata = forward_context.attn_metadata\n\n    # Stack QKV\n    qkvg = torch.cat([q, k, v, gate_compress],\n                     dim=0)  # [3, seq_len, num_heads, head_dim]\n\n    # Redistribute heads across sequence dimension\n    qkvg = sequence_model_parallel_all_to_all_4D(qkvg,\n                                                 scatter_dim=2,\n                                                 gather_dim=1)\n\n    qkvg = self.attn_impl.preprocess_qkv(qkvg, ctx_attn_metadata)\n\n    q, k, v, gate_compress = qkvg.chunk(4, dim=0)\n    output = self.attn_impl.forward(\n        q, k, v, gate_compress, ctx_attn_metadata)  # type: ignore[call-arg]\n\n    # Redistribute back if using sequence parallelism\n    replicated_output = None\n\n    # Apply backend-specific postprocess_output\n    output = self.attn_impl.postprocess_output(output, ctx_attn_metadata)\n\n    output = sequence_model_parallel_all_to_all_4D(output,\n                                                   scatter_dim=1,\n                                                   gather_dim=2)\n    return output, replicated_output\n</code></pre> fastvideo.attention.layer.LocalAttention \u00b6 <pre><code>LocalAttention(\n    num_heads: int,\n    head_size: int,\n    num_kv_heads: int | None = None,\n    softmax_scale: float | None = None,\n    causal: bool = False,\n    supported_attention_backends: tuple[\n        AttentionBackendEnum, ...\n    ]\n    | None = None,\n    **extra_impl_args\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Attention layer.</p> Source code in <code>fastvideo/attention/layer.py</code> <pre><code>def __init__(self,\n             num_heads: int,\n             head_size: int,\n             num_kv_heads: int | None = None,\n             softmax_scale: float | None = None,\n             causal: bool = False,\n             supported_attention_backends: tuple[AttentionBackendEnum, ...]\n             | None = None,\n             **extra_impl_args) -&gt; None:\n    super().__init__()\n    if softmax_scale is None:\n        self.softmax_scale = head_size**-0.5\n    else:\n        self.softmax_scale = softmax_scale\n    if num_kv_heads is None:\n        num_kv_heads = num_heads\n\n    dtype = get_compute_dtype()\n    attn_backend = get_attn_backend(\n        head_size,\n        dtype,\n        supported_attention_backends=supported_attention_backends)\n    impl_cls = attn_backend.get_impl_cls()\n    self.attn_impl = impl_cls(num_heads=num_heads,\n                              head_size=head_size,\n                              softmax_scale=self.softmax_scale,\n                              num_kv_heads=num_kv_heads,\n                              causal=causal,\n                              **extra_impl_args)\n    self.num_heads = num_heads\n    self.head_size = head_size\n    self.num_kv_heads = num_kv_heads\n    self.backend = backend_name_to_enum(attn_backend.get_name())\n    self.dtype = dtype\n</code></pre> Functions\u00b6 fastvideo.attention.layer.LocalAttention.forward \u00b6 <pre><code>forward(q: Tensor, k: Tensor, v: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Apply local attention between query, key and value tensors.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>Tensor</code> <p>Query tensor of shape [batch_size, seq_len, num_heads, head_dim]</p> required <code>k</code> <code>Tensor</code> <p>Key tensor of shape [batch_size, seq_len, num_heads, head_dim] </p> required <code>v</code> <code>Tensor</code> <p>Value tensor of shape [batch_size, seq_len, num_heads, head_dim]</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output tensor after local attention</p> Source code in <code>fastvideo/attention/layer.py</code> <pre><code>def forward(\n    self,\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Apply local attention between query, key and value tensors.\n\n    Args:\n        q (torch.Tensor): Query tensor of shape [batch_size, seq_len, num_heads, head_dim]\n        k (torch.Tensor): Key tensor of shape [batch_size, seq_len, num_heads, head_dim] \n        v (torch.Tensor): Value tensor of shape [batch_size, seq_len, num_heads, head_dim]\n\n    Returns:\n        torch.Tensor: Output tensor after local attention\n    \"\"\"\n    # Check input shapes\n    assert q.dim() == 4 and k.dim() == 4 and v.dim(\n    ) == 4, \"Expected 4D tensors\"\n\n    forward_context: ForwardContext = get_forward_context()\n    ctx_attn_metadata = forward_context.attn_metadata\n\n    output = self.attn_impl.forward(q, k, v, ctx_attn_metadata)\n    return output\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.attention.layer-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideo.attention.selector","title":"fastvideo.attention.selector","text":""},{"location":"api/fastvideo/#fastvideo.attention.selector-classes","title":"Classes","text":""},{"location":"api/fastvideo/#fastvideo.attention.selector-functions","title":"Functions","text":"fastvideo.attention.selector.backend_name_to_enum \u00b6 <pre><code>backend_name_to_enum(\n    backend_name: str,\n) -&gt; AttentionBackendEnum | None\n</code></pre> <p>Convert a string backend name to a _Backend enum value.</p> <p>Returns: * _Backend: enum value if backend_name is a valid in-tree type * None: otherwise it's an invalid in-tree type or an out-of-tree platform is         loaded.</p> Source code in <code>fastvideo/attention/selector.py</code> <pre><code>def backend_name_to_enum(backend_name: str) -&gt; AttentionBackendEnum | None:\n    \"\"\"\n    Convert a string backend name to a _Backend enum value.\n\n    Returns:\n    * _Backend: enum value if backend_name is a valid in-tree type\n    * None: otherwise it's an invalid in-tree type or an out-of-tree platform is\n            loaded.\n    \"\"\"\n    assert backend_name is not None\n    return AttentionBackendEnum[backend_name] if backend_name in AttentionBackendEnum.__members__ else \\\n          None\n</code></pre> fastvideo.attention.selector.get_env_variable_attn_backend \u00b6 <pre><code>get_env_variable_attn_backend() -&gt; AttentionBackendEnum | None\n</code></pre> <p>Get the backend override specified by the FastVideo attention backend environment variable, if one is specified.</p> <p>Returns:</p> <ul> <li>_Backend enum value if an override is specified</li> <li>None otherwise</li> </ul> Source code in <code>fastvideo/attention/selector.py</code> <pre><code>def get_env_variable_attn_backend() -&gt; AttentionBackendEnum | None:\n    '''\n    Get the backend override specified by the FastVideo attention\n    backend environment variable, if one is specified.\n\n    Returns:\n\n    * _Backend enum value if an override is specified\n    * None otherwise\n    '''\n    backend_name = os.environ.get(STR_BACKEND_ENV_VAR)\n    return (None\n            if backend_name is None else backend_name_to_enum(backend_name))\n</code></pre> fastvideo.attention.selector.get_global_forced_attn_backend \u00b6 <pre><code>get_global_forced_attn_backend() -&gt; AttentionBackendEnum | None\n</code></pre> <p>Get the currently-forced choice of attention backend, or None if auto-selection is currently enabled.</p> Source code in <code>fastvideo/attention/selector.py</code> <pre><code>def get_global_forced_attn_backend() -&gt; AttentionBackendEnum | None:\n    '''\n    Get the currently-forced choice of attention backend,\n    or None if auto-selection is currently enabled.\n    '''\n    return forced_attn_backend\n</code></pre> fastvideo.attention.selector.global_force_attn_backend \u00b6 <pre><code>global_force_attn_backend(\n    attn_backend: AttentionBackendEnum | None,\n) -&gt; None\n</code></pre> <p>Force all attention operations to use a specified backend.</p> <p>Passing <code>None</code> for the argument re-enables automatic backend selection.,</p> <p>Arguments:</p> <ul> <li>attn_backend: backend selection (None to revert to auto)</li> </ul> Source code in <code>fastvideo/attention/selector.py</code> <pre><code>def global_force_attn_backend(\n        attn_backend: AttentionBackendEnum | None) -&gt; None:\n    '''\n    Force all attention operations to use a specified backend.\n\n    Passing `None` for the argument re-enables automatic\n    backend selection.,\n\n    Arguments:\n\n    * attn_backend: backend selection (None to revert to auto)\n    '''\n    global forced_attn_backend\n    forced_attn_backend = attn_backend\n</code></pre> fastvideo.attention.selector.global_force_attn_backend_context_manager \u00b6 <pre><code>global_force_attn_backend_context_manager(\n    attn_backend: AttentionBackendEnum,\n) -&gt; Generator[None, None, None]\n</code></pre> <p>Globally force a FastVideo attention backend override within a context manager, reverting the global attention backend override to its prior state upon exiting the context manager.</p> <p>Arguments:</p> <ul> <li>attn_backend: attention backend to force</li> </ul> <p>Returns:</p> <ul> <li>Generator</li> </ul> Source code in <code>fastvideo/attention/selector.py</code> <pre><code>@contextmanager\ndef global_force_attn_backend_context_manager(\n        attn_backend: AttentionBackendEnum) -&gt; Generator[None, None, None]:\n    '''\n    Globally force a FastVideo attention backend override within a\n    context manager, reverting the global attention backend\n    override to its prior state upon exiting the context\n    manager.\n\n    Arguments:\n\n    * attn_backend: attention backend to force\n\n    Returns:\n\n    * Generator\n    '''\n\n    # Save the current state of the global backend override (if any)\n    original_value = get_global_forced_attn_backend()\n\n    # Globally force the new backend override\n    global_force_attn_backend(attn_backend)\n\n    # Yield control back to the enclosed code block\n    try:\n        yield\n    finally:\n        # Revert the original global backend override, if any\n        global_force_attn_backend(original_value)\n</code></pre>"},{"location":"api/fastvideo/#fastvideoconfigs","title":"fastvideo.configs","text":""},{"location":"api/fastvideo/#fastvideo.configs","title":"configs","text":""},{"location":"api/fastvideo/#fastvideo.configs-modules","title":"Modules","text":""},{"location":"api/fastvideo/#fastvideo.configs.configs","title":"fastvideo.configs.configs","text":""},{"location":"api/fastvideo/#fastvideo.configs.configs-classes","title":"Classes","text":"fastvideo.configs.configs.DatasetType \u00b6 <p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration for different dataset types.</p> Functions\u00b6 fastvideo.configs.configs.DatasetType.choices <code>classmethod</code> \u00b6 <pre><code>choices() -&gt; list[str]\n</code></pre> <p>Get all available choices as strings for argparse.</p> Source code in <code>fastvideo/configs/configs.py</code> <pre><code>@classmethod\ndef choices(cls) -&gt; list[str]:\n    \"\"\"Get all available choices as strings for argparse.\"\"\"\n    return [dataset_type.value for dataset_type in cls]\n</code></pre> fastvideo.configs.configs.DatasetType.from_string <code>classmethod</code> \u00b6 <pre><code>from_string(value: str) -&gt; DatasetType\n</code></pre> <p>Convert string to DatasetType enum.</p> Source code in <code>fastvideo/configs/configs.py</code> <pre><code>@classmethod\ndef from_string(cls, value: str) -&gt; \"DatasetType\":\n    \"\"\"Convert string to DatasetType enum.\"\"\"\n    try:\n        return cls(value.lower())\n    except ValueError:\n        raise ValueError(\n            f\"Invalid dataset type: {value}. Must be one of: {', '.join([m.value for m in cls])}\"\n        ) from None\n</code></pre> fastvideo.configs.configs.PreprocessConfig <code>dataclass</code> \u00b6 <pre><code>PreprocessConfig(\n    model_path: str = \"\",\n    dataset_path: str = \"\",\n    dataset_type: DatasetType = DatasetType.HF,\n    dataset_output_dir: str = \"./output\",\n    dataloader_num_workers: int = 1,\n    preprocess_video_batch_size: int = 2,\n    samples_per_file: int = 64,\n    flush_frequency: int = 256,\n    video_loader_type: VideoLoaderType = VideoLoaderType.TORCHCODEC,\n    max_height: int = 480,\n    max_width: int = 848,\n    num_frames: int = 163,\n    video_length_tolerance_range: float = 2.0,\n    train_fps: int = 30,\n    speed_factor: float = 1.0,\n    drop_short_ratio: float = 1.0,\n    do_temporal_sample: bool = False,\n    training_cfg_rate: float = 0.0,\n    seed: int = 42,\n)\n</code></pre> <p>Configuration for preprocessing operations.</p> Functions\u00b6 fastvideo.configs.configs.PreprocessConfig.add_cli_args <code>staticmethod</code> \u00b6 <pre><code>add_cli_args(\n    parser: FlexibleArgumentParser,\n    prefix: str = \"preprocess\",\n) -&gt; FlexibleArgumentParser\n</code></pre> <p>Add preprocessing configuration arguments to the parser.</p> Source code in <code>fastvideo/configs/configs.py</code> <pre><code>@staticmethod\ndef add_cli_args(parser: FlexibleArgumentParser,\n                 prefix: str = \"preprocess\") -&gt; FlexibleArgumentParser:\n    \"\"\"Add preprocessing configuration arguments to the parser.\"\"\"\n    prefix_with_dot = f\"{prefix}.\" if (prefix.strip() != \"\") else \"\"\n\n    preprocess_args = parser.add_argument_group(\"Preprocessing Arguments\")\n    # Model &amp; Dataset\n    preprocess_args.add_argument(f\"--{prefix_with_dot}model-path\",\n                                 type=str,\n                                 default=PreprocessConfig.model_path,\n                                 help=\"Path to the model for preprocessing\")\n    preprocess_args.add_argument(\n        f\"--{prefix_with_dot}dataset-path\",\n        type=str,\n        default=PreprocessConfig.dataset_path,\n        help=\"Path to the dataset directory for preprocessing\")\n    preprocess_args.add_argument(\n        f\"--{prefix_with_dot}dataset-type\",\n        type=str,\n        choices=DatasetType.choices(),\n        default=PreprocessConfig.dataset_type.value,\n        help=\"Type of the dataset\")\n    preprocess_args.add_argument(\n        f\"--{prefix_with_dot}dataset-output-dir\",\n        type=str,\n        default=PreprocessConfig.dataset_output_dir,\n        help=\"The output directory where the dataset will be written.\")\n\n    # Dataloader\n    preprocess_args.add_argument(\n        f\"--{prefix_with_dot}dataloader-num-workers\",\n        type=int,\n        default=PreprocessConfig.dataloader_num_workers,\n        help=\n        \"Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\"\n    )\n    preprocess_args.add_argument(\n        f\"--{prefix_with_dot}preprocess-video-batch-size\",\n        type=int,\n        default=PreprocessConfig.preprocess_video_batch_size,\n        help=\"Batch size (per device) for the training dataloader.\")\n\n    # Saver\n    preprocess_args.add_argument(f\"--{prefix_with_dot}samples-per-file\",\n                                 type=int,\n                                 default=PreprocessConfig.samples_per_file,\n                                 help=\"Number of samples per output file\")\n    preprocess_args.add_argument(f\"--{prefix_with_dot}flush-frequency\",\n                                 type=int,\n                                 default=PreprocessConfig.flush_frequency,\n                                 help=\"How often to save to parquet files\")\n\n    # Video processing parameters\n    preprocess_args.add_argument(\n        f\"--{prefix_with_dot}video-loader-type\",\n        type=str,\n        choices=VideoLoaderType.choices(),\n        default=PreprocessConfig.video_loader_type.value,\n        help=\"Type of the video loader\")\n    preprocess_args.add_argument(f\"--{prefix_with_dot}max-height\",\n                                 type=int,\n                                 default=PreprocessConfig.max_height,\n                                 help=\"Maximum height for video processing\")\n    preprocess_args.add_argument(f\"--{prefix_with_dot}max-width\",\n                                 type=int,\n                                 default=PreprocessConfig.max_width,\n                                 help=\"Maximum width for video processing\")\n    preprocess_args.add_argument(f\"--{prefix_with_dot}num-frames\",\n                                 type=int,\n                                 default=PreprocessConfig.num_frames,\n                                 help=\"Number of frames to process\")\n    preprocess_args.add_argument(\n        f\"--{prefix_with_dot}video-length-tolerance-range\",\n        type=float,\n        default=PreprocessConfig.video_length_tolerance_range,\n        help=\"Video length tolerance range\")\n    preprocess_args.add_argument(f\"--{prefix_with_dot}train-fps\",\n                                 type=int,\n                                 default=PreprocessConfig.train_fps,\n                                 help=\"Training FPS\")\n    preprocess_args.add_argument(f\"--{prefix_with_dot}speed-factor\",\n                                 type=float,\n                                 default=PreprocessConfig.speed_factor,\n                                 help=\"Speed factor for video processing\")\n    preprocess_args.add_argument(f\"--{prefix_with_dot}drop-short-ratio\",\n                                 type=float,\n                                 default=PreprocessConfig.drop_short_ratio,\n                                 help=\"Ratio for dropping short videos\")\n    preprocess_args.add_argument(\n        f\"--{prefix_with_dot}do-temporal-sample\",\n        action=StoreBoolean,\n        default=PreprocessConfig.do_temporal_sample,\n        help=\"Whether to do temporal sampling\")\n\n    # Model Training configuration\n    preprocess_args.add_argument(f\"--{prefix_with_dot}training-cfg-rate\",\n                                 type=float,\n                                 default=PreprocessConfig.training_cfg_rate,\n                                 help=\"Training CFG rate\")\n    preprocess_args.add_argument(f\"--{prefix_with_dot}seed\",\n                                 type=int,\n                                 default=PreprocessConfig.seed,\n                                 help=\"Seed for random number generator\")\n\n    return parser\n</code></pre> fastvideo.configs.configs.PreprocessConfig.from_kwargs <code>classmethod</code> \u00b6 <pre><code>from_kwargs(\n    kwargs: dict[str, Any]\n) -&gt; Optional[PreprocessConfig]\n</code></pre> <p>Create PreprocessConfig from keyword arguments.</p> Source code in <code>fastvideo/configs/configs.py</code> <pre><code>@classmethod\ndef from_kwargs(cls, kwargs: dict[str,\n                                  Any]) -&gt; Optional[\"PreprocessConfig\"]:\n    \"\"\"Create PreprocessConfig from keyword arguments.\"\"\"\n    if 'dataset_type' in kwargs and isinstance(kwargs['dataset_type'], str):\n        kwargs['dataset_type'] = DatasetType.from_string(\n            kwargs['dataset_type'])\n    if 'video_loader_type' in kwargs and isinstance(\n            kwargs['video_loader_type'], str):\n        kwargs['video_loader_type'] = VideoLoaderType.from_string(\n            kwargs['video_loader_type'])\n\n    preprocess_config = cls()\n    if not update_config_from_args(\n            preprocess_config, kwargs, prefix=\"preprocess\", pop_args=True):\n        return None\n    return preprocess_config\n</code></pre> fastvideo.configs.configs.VideoLoaderType \u00b6 <p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration for different video loaders.</p> Functions\u00b6 fastvideo.configs.configs.VideoLoaderType.choices <code>classmethod</code> \u00b6 <pre><code>choices() -&gt; list[str]\n</code></pre> <p>Get all available choices as strings for argparse.</p> Source code in <code>fastvideo/configs/configs.py</code> <pre><code>@classmethod\ndef choices(cls) -&gt; list[str]:\n    \"\"\"Get all available choices as strings for argparse.\"\"\"\n    return [video_loader.value for video_loader in cls]\n</code></pre> fastvideo.configs.configs.VideoLoaderType.from_string <code>classmethod</code> \u00b6 <pre><code>from_string(value: str) -&gt; VideoLoaderType\n</code></pre> <p>Convert string to VideoLoader enum.</p> Source code in <code>fastvideo/configs/configs.py</code> <pre><code>@classmethod\ndef from_string(cls, value: str) -&gt; \"VideoLoaderType\":\n    \"\"\"Convert string to VideoLoader enum.\"\"\"\n    try:\n        return cls(value.lower())\n    except ValueError:\n        raise ValueError(\n            f\"Invalid video loader: {value}. Must be one of: {', '.join([m.value for m in cls])}\"\n        ) from None\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.configs.configs-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideo.configs.models","title":"fastvideo.configs.models","text":""},{"location":"api/fastvideo/#fastvideo.configs.models-classes","title":"Classes","text":"fastvideo.configs.models.DiTConfig <code>dataclass</code> \u00b6 <pre><code>DiTConfig(\n    arch_config: DiTArchConfig = DiTArchConfig(),\n    prefix: str = \"\",\n    quant_config: QuantizationConfig | None = None,\n)\n</code></pre> <p>               Bases: <code>ModelConfig</code></p> Functions\u00b6 fastvideo.configs.models.DiTConfig.add_cli_args <code>staticmethod</code> \u00b6 <pre><code>add_cli_args(\n    parser: Any, prefix: str = \"dit-config\"\n) -&gt; Any\n</code></pre> <p>Add CLI arguments for DiTConfig fields</p> Source code in <code>fastvideo/configs/models/dits/base.py</code> <pre><code>@staticmethod\ndef add_cli_args(parser: Any, prefix: str = \"dit-config\") -&gt; Any:\n    \"\"\"Add CLI arguments for DiTConfig fields\"\"\"\n    parser.add_argument(\n        f\"--{prefix}.prefix\",\n        type=str,\n        dest=f\"{prefix.replace('-', '_')}.prefix\",\n        default=DiTConfig.prefix,\n        help=\"Prefix for the DiT model\",\n    )\n\n    parser.add_argument(\n        f\"--{prefix}.quant-config\",\n        type=str,\n        dest=f\"{prefix.replace('-', '_')}.quant_config\",\n        default=None,\n        help=\"Quantization configuration for the DiT model\",\n    )\n\n    return parser\n</code></pre> fastvideo.configs.models.VAEConfig <code>dataclass</code> \u00b6 <pre><code>VAEConfig(\n    arch_config: VAEArchConfig = VAEArchConfig(),\n    load_encoder: bool = True,\n    load_decoder: bool = True,\n    tile_sample_min_height: int = 256,\n    tile_sample_min_width: int = 256,\n    tile_sample_min_num_frames: int = 16,\n    tile_sample_stride_height: int = 192,\n    tile_sample_stride_width: int = 192,\n    tile_sample_stride_num_frames: int = 12,\n    blend_num_frames: int = 0,\n    use_tiling: bool = True,\n    use_temporal_tiling: bool = True,\n    use_parallel_tiling: bool = True,\n    use_temporal_scaling_frames: bool = True,\n)\n</code></pre> <p>               Bases: <code>ModelConfig</code></p> Functions\u00b6 fastvideo.configs.models.VAEConfig.add_cli_args <code>staticmethod</code> \u00b6 <pre><code>add_cli_args(\n    parser: Any, prefix: str = \"vae-config\"\n) -&gt; Any\n</code></pre> <p>Add CLI arguments for VAEConfig fields</p> Source code in <code>fastvideo/configs/models/vaes/base.py</code> <pre><code>@staticmethod\ndef add_cli_args(parser: Any, prefix: str = \"vae-config\") -&gt; Any:\n    \"\"\"Add CLI arguments for VAEConfig fields\"\"\"\n    parser.add_argument(\n        f\"--{prefix}.load-encoder\",\n        action=StoreBoolean,\n        dest=f\"{prefix.replace('-', '_')}.load_encoder\",\n        default=VAEConfig.load_encoder,\n        help=\"Whether to load the VAE encoder\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.load-decoder\",\n        action=StoreBoolean,\n        dest=f\"{prefix.replace('-', '_')}.load_decoder\",\n        default=VAEConfig.load_decoder,\n        help=\"Whether to load the VAE decoder\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-min-height\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_min_height\",\n        default=VAEConfig.tile_sample_min_height,\n        help=\"Minimum height for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-min-width\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_min_width\",\n        default=VAEConfig.tile_sample_min_width,\n        help=\"Minimum width for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-min-num-frames\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_min_num_frames\",\n        default=VAEConfig.tile_sample_min_num_frames,\n        help=\"Minimum number of frames for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-stride-height\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_stride_height\",\n        default=VAEConfig.tile_sample_stride_height,\n        help=\"Stride height for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-stride-width\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_stride_width\",\n        default=VAEConfig.tile_sample_stride_width,\n        help=\"Stride width for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-stride-num-frames\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_stride_num_frames\",\n        default=VAEConfig.tile_sample_stride_num_frames,\n        help=\"Stride number of frames for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.blend-num-frames\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.blend_num_frames\",\n        default=VAEConfig.blend_num_frames,\n        help=\"Number of frames to blend for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.use-tiling\",\n        action=StoreBoolean,\n        dest=f\"{prefix.replace('-', '_')}.use_tiling\",\n        default=VAEConfig.use_tiling,\n        help=\"Whether to use tiling for VAE\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.use-temporal-tiling\",\n        action=StoreBoolean,\n        dest=f\"{prefix.replace('-', '_')}.use_temporal_tiling\",\n        default=VAEConfig.use_temporal_tiling,\n        help=\"Whether to use temporal tiling for VAE\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.use-parallel-tiling\",\n        action=StoreBoolean,\n        dest=f\"{prefix.replace('-', '_')}.use_parallel_tiling\",\n        default=VAEConfig.use_parallel_tiling,\n        help=\"Whether to use parallel tiling for VAE\",\n    )\n\n    return parser\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.configs.models-modules","title":"Modules","text":"fastvideo.configs.models.dits \u00b6 Modules\u00b6 fastvideo.configs.models.dits.base \u00b6 Classes\u00b6 fastvideo.configs.models.dits.base.DiTConfig <code>dataclass</code> \u00b6 <pre><code>DiTConfig(\n    arch_config: DiTArchConfig = DiTArchConfig(),\n    prefix: str = \"\",\n    quant_config: QuantizationConfig | None = None,\n)\n</code></pre> <p>               Bases: <code>ModelConfig</code></p> Functions\u00b6 fastvideo.configs.models.dits.base.DiTConfig.add_cli_args <code>staticmethod</code> \u00b6 <pre><code>add_cli_args(\n    parser: Any, prefix: str = \"dit-config\"\n) -&gt; Any\n</code></pre> <p>Add CLI arguments for DiTConfig fields</p> Source code in <code>fastvideo/configs/models/dits/base.py</code> <pre><code>@staticmethod\ndef add_cli_args(parser: Any, prefix: str = \"dit-config\") -&gt; Any:\n    \"\"\"Add CLI arguments for DiTConfig fields\"\"\"\n    parser.add_argument(\n        f\"--{prefix}.prefix\",\n        type=str,\n        dest=f\"{prefix.replace('-', '_')}.prefix\",\n        default=DiTConfig.prefix,\n        help=\"Prefix for the DiT model\",\n    )\n\n    parser.add_argument(\n        f\"--{prefix}.quant-config\",\n        type=str,\n        dest=f\"{prefix.replace('-', '_')}.quant_config\",\n        default=None,\n        help=\"Quantization configuration for the DiT model\",\n    )\n\n    return parser\n</code></pre> fastvideo.configs.models.vaes \u00b6 Modules\u00b6 fastvideo.configs.models.vaes.base \u00b6 Classes\u00b6 fastvideo.configs.models.vaes.base.VAEConfig <code>dataclass</code> \u00b6 <pre><code>VAEConfig(\n    arch_config: VAEArchConfig = VAEArchConfig(),\n    load_encoder: bool = True,\n    load_decoder: bool = True,\n    tile_sample_min_height: int = 256,\n    tile_sample_min_width: int = 256,\n    tile_sample_min_num_frames: int = 16,\n    tile_sample_stride_height: int = 192,\n    tile_sample_stride_width: int = 192,\n    tile_sample_stride_num_frames: int = 12,\n    blend_num_frames: int = 0,\n    use_tiling: bool = True,\n    use_temporal_tiling: bool = True,\n    use_parallel_tiling: bool = True,\n    use_temporal_scaling_frames: bool = True,\n)\n</code></pre> <p>               Bases: <code>ModelConfig</code></p> Functions\u00b6 fastvideo.configs.models.vaes.base.VAEConfig.add_cli_args <code>staticmethod</code> \u00b6 <pre><code>add_cli_args(\n    parser: Any, prefix: str = \"vae-config\"\n) -&gt; Any\n</code></pre> <p>Add CLI arguments for VAEConfig fields</p> Source code in <code>fastvideo/configs/models/vaes/base.py</code> <pre><code>@staticmethod\ndef add_cli_args(parser: Any, prefix: str = \"vae-config\") -&gt; Any:\n    \"\"\"Add CLI arguments for VAEConfig fields\"\"\"\n    parser.add_argument(\n        f\"--{prefix}.load-encoder\",\n        action=StoreBoolean,\n        dest=f\"{prefix.replace('-', '_')}.load_encoder\",\n        default=VAEConfig.load_encoder,\n        help=\"Whether to load the VAE encoder\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.load-decoder\",\n        action=StoreBoolean,\n        dest=f\"{prefix.replace('-', '_')}.load_decoder\",\n        default=VAEConfig.load_decoder,\n        help=\"Whether to load the VAE decoder\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-min-height\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_min_height\",\n        default=VAEConfig.tile_sample_min_height,\n        help=\"Minimum height for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-min-width\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_min_width\",\n        default=VAEConfig.tile_sample_min_width,\n        help=\"Minimum width for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-min-num-frames\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_min_num_frames\",\n        default=VAEConfig.tile_sample_min_num_frames,\n        help=\"Minimum number of frames for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-stride-height\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_stride_height\",\n        default=VAEConfig.tile_sample_stride_height,\n        help=\"Stride height for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-stride-width\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_stride_width\",\n        default=VAEConfig.tile_sample_stride_width,\n        help=\"Stride width for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-stride-num-frames\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_stride_num_frames\",\n        default=VAEConfig.tile_sample_stride_num_frames,\n        help=\"Stride number of frames for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.blend-num-frames\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.blend_num_frames\",\n        default=VAEConfig.blend_num_frames,\n        help=\"Number of frames to blend for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.use-tiling\",\n        action=StoreBoolean,\n        dest=f\"{prefix.replace('-', '_')}.use_tiling\",\n        default=VAEConfig.use_tiling,\n        help=\"Whether to use tiling for VAE\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.use-temporal-tiling\",\n        action=StoreBoolean,\n        dest=f\"{prefix.replace('-', '_')}.use_temporal_tiling\",\n        default=VAEConfig.use_temporal_tiling,\n        help=\"Whether to use temporal tiling for VAE\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.use-parallel-tiling\",\n        action=StoreBoolean,\n        dest=f\"{prefix.replace('-', '_')}.use_parallel_tiling\",\n        default=VAEConfig.use_parallel_tiling,\n        help=\"Whether to use parallel tiling for VAE\",\n    )\n\n    return parser\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.configs.pipelines","title":"fastvideo.configs.pipelines","text":""},{"location":"api/fastvideo/#fastvideo.configs.pipelines-classes","title":"Classes","text":"fastvideo.configs.pipelines.FastHunyuanConfig <code>dataclass</code> \u00b6 <pre><code>FastHunyuanConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: int = 6,\n    flow_shift: int = 17,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = HunyuanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = HunyuanVAEConfig(),\n    vae_precision: str = \"fp16\",\n    vae_tiling: bool = True,\n    vae_sp: bool = True,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (LlamaConfig(), CLIPTextConfig())\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp16\", \"fp16\")\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (\n        lambda: (\n            llama_preprocess_text,\n            clip_preprocess_text,\n        )\n    )(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], tensor], ...\n    ] = (\n        lambda: (\n            llama_postprocess_text,\n            clip_postprocess_text,\n        )\n    )(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n)\n</code></pre> <p>               Bases: <code>HunyuanConfig</code></p> <p>Configuration specifically optimized for FastHunyuan weights.</p> fastvideo.configs.pipelines.HunyuanConfig <code>dataclass</code> \u00b6 <pre><code>HunyuanConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: int = 6,\n    flow_shift: int = 7,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = HunyuanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = HunyuanVAEConfig(),\n    vae_precision: str = \"fp16\",\n    vae_tiling: bool = True,\n    vae_sp: bool = True,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (LlamaConfig(), CLIPTextConfig())\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp16\", \"fp16\")\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (\n        lambda: (\n            llama_preprocess_text,\n            clip_preprocess_text,\n        )\n    )(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], tensor], ...\n    ] = (\n        lambda: (\n            llama_postprocess_text,\n            clip_postprocess_text,\n        )\n    )(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n)\n</code></pre> <p>               Bases: <code>PipelineConfig</code></p> <p>Base configuration for HunYuan pipeline architecture.</p> fastvideo.configs.pipelines.PipelineConfig <code>dataclass</code> \u00b6 <pre><code>PipelineConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = None,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = DiTConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = VAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = True,\n    vae_sp: bool = True,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (EncoderConfig(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], tensor], ...\n    ] = (lambda: (postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n)\n</code></pre> <p>Base configuration for all pipeline architectures.</p> Functions\u00b6 fastvideo.configs.pipelines.PipelineConfig.from_kwargs <code>classmethod</code> \u00b6 <pre><code>from_kwargs(\n    kwargs: dict[str, Any], config_cli_prefix: str = \"\"\n) -&gt; PipelineConfig\n</code></pre> <p>Load PipelineConfig from kwargs Dictionary. kwargs: dictionary of kwargs config_cli_prefix: prefix of CLI arguments for this PipelineConfig instance</p> Source code in <code>fastvideo/configs/pipelines/base.py</code> <pre><code>@classmethod\ndef from_kwargs(cls,\n                kwargs: dict[str, Any],\n                config_cli_prefix: str = \"\") -&gt; \"PipelineConfig\":\n    \"\"\"\n    Load PipelineConfig from kwargs Dictionary.\n    kwargs: dictionary of kwargs\n    config_cli_prefix: prefix of CLI arguments for this PipelineConfig instance\n    \"\"\"\n    from fastvideo.configs.pipelines.registry import (\n        get_pipeline_config_cls_from_name)\n\n    prefix_with_dot = f\"{config_cli_prefix}.\" if (config_cli_prefix.strip()\n                                                  != \"\") else \"\"\n    model_path: str | None = kwargs.get(prefix_with_dot + 'model_path',\n                                        None) or kwargs.get('model_path')\n    pipeline_config_or_path: str | PipelineConfig | dict[\n        str, Any] | None = kwargs.get(prefix_with_dot + 'pipeline_config',\n                                      None) or kwargs.get('pipeline_config')\n    if model_path is None:\n        raise ValueError(\"model_path is required in kwargs\")\n\n    # 1. Get the pipeline config class from the registry\n    pipeline_config_cls = get_pipeline_config_cls_from_name(model_path)\n\n    # 2. Instantiate PipelineConfig\n    if pipeline_config_cls is None:\n        logger.warning(\n            \"Couldn't find pipeline config for %s. Using the default pipeline config.\",\n            model_path)\n        pipeline_config = cls()\n    else:\n        pipeline_config = pipeline_config_cls()\n\n    # 3. Load PipelineConfig from a json file or a PipelineConfig object if provided\n    if isinstance(pipeline_config_or_path, str):\n        pipeline_config.load_from_json(pipeline_config_or_path)\n        kwargs[prefix_with_dot +\n               'pipeline_config_path'] = pipeline_config_or_path\n    elif isinstance(pipeline_config_or_path, PipelineConfig):\n        pipeline_config = pipeline_config_or_path\n    elif isinstance(pipeline_config_or_path, dict):\n        pipeline_config.update_pipeline_config(pipeline_config_or_path)\n\n    # 4. Update PipelineConfig from CLI arguments if provided\n    kwargs[prefix_with_dot + 'model_path'] = model_path\n    pipeline_config.update_config_from_dict(kwargs, config_cli_prefix)\n    return pipeline_config\n</code></pre> fastvideo.configs.pipelines.PipelineConfig.from_pretrained <code>classmethod</code> \u00b6 <pre><code>from_pretrained(model_path: str) -&gt; PipelineConfig\n</code></pre> <p>use the pipeline class setting from model_path to match the pipeline config</p> Source code in <code>fastvideo/configs/pipelines/base.py</code> <pre><code>@classmethod\ndef from_pretrained(cls, model_path: str) -&gt; \"PipelineConfig\":\n    \"\"\"\n    use the pipeline class setting from model_path to match the pipeline config\n    \"\"\"\n    from fastvideo.configs.pipelines.registry import (\n        get_pipeline_config_cls_from_name)\n    pipeline_config_cls = get_pipeline_config_cls_from_name(model_path)\n\n    return cast(PipelineConfig, pipeline_config_cls(model_path=model_path))\n</code></pre> fastvideo.configs.pipelines.SlidingTileAttnConfig <code>dataclass</code> \u00b6 <pre><code>SlidingTileAttnConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = None,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = DiTConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = VAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = True,\n    vae_sp: bool = True,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (EncoderConfig(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], tensor], ...\n    ] = (lambda: (postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    window_size: int = 16,\n    stride: int = 8,\n    height: int = 576,\n    width: int = 1024,\n    pad_to_square: bool = False,\n    use_overlap_optimization: bool = True,\n)\n</code></pre> <p>               Bases: <code>PipelineConfig</code></p> <p>Configuration for sliding tile attention.</p> fastvideo.configs.pipelines.StepVideoT2VConfig <code>dataclass</code> \u00b6 <pre><code>StepVideoT2VConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: int = 13,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = StepVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = StepVideoVAEConfig(),\n    vae_precision: str = \"bf16\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (EncoderConfig(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], tensor], ...\n    ] = (lambda: (postprocess_text,))(),\n    pos_magic: str = \"\u200b\u8d85\u9ad8\u200b\u6e05\u200b\u3001HDR \u200b\u89c6\u9891\u200b\u3001\u200b\u73af\u5883\u5149\u200b\u3001\u200b\u675c\u6bd4\u200b\u5168\u666f\u200b\u58f0\u200b\u3001\u200b\u753b\u9762\u200b\u7a33\u5b9a\u200b\u3001\u200b\u6d41\u7545\u200b\u52a8\u4f5c\u200b\u3001\u200b\u903c\u771f\u200b\u7684\u200b\u7ec6\u8282\u200b\u3001\u200b\u4e13\u4e1a\u7ea7\u200b\u6784\u56fe\u200b\u3001\u200b\u8d85\u73b0\u5b9e\u4e3b\u4e49\u200b\u3001\u200b\u81ea\u7136\u200b\u3001\u200b\u751f\u52a8\u200b\u3001\u200b\u8d85\u200b\u7ec6\u8282\u200b\u3001\u200b\u6e05\u6670\u200b\u3002\",\n    neg_magic: str = \"\u200b\u753b\u9762\u200b\u6697\u200b\u3001\u200b\u4f4e\u5206\u8fa8\u7387\u200b\u3001\u200b\u4e0d\u826f\u200b\u624b\u200b\u3001\u200b\u6587\u672c\u200b\u3001\u200b\u7f3a\u5c11\u200b\u624b\u6307\u200b\u3001\u200b\u591a\u4f59\u200b\u7684\u200b\u624b\u6307\u200b\u3001\u200b\u88c1\u526a\u200b\u3001\u200b\u4f4e\u8d28\u91cf\u200b\u3001\u200b\u9897\u7c92\u72b6\u200b\u3001\u200b\u7b7e\u540d\u200b\u3001\u200b\u6c34\u5370\u200b\u3001\u200b\u7528\u6237\u540d\u200b\u3001\u200b\u6a21\u7cca\u200b\u3002\",\n    timesteps_scale: bool = False,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n)\n</code></pre> <p>               Bases: <code>PipelineConfig</code></p> <p>Base configuration for StepVideo pipeline architecture.</p> fastvideo.configs.pipelines.WanI2V480PConfig <code>dataclass</code> \u00b6 <pre><code>WanI2V480PConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = 3.0,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = WanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = WanVAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = CLIPVisionConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (T5Config(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], Tensor], ...\n    ] = (lambda: (t5_postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n    warp_denoising_step: bool = True,\n)\n</code></pre> <p>               Bases: <code>WanT2V480PConfig</code></p> <p>Base configuration for Wan I2V 14B 480P pipeline architecture.</p> fastvideo.configs.pipelines.WanI2V720PConfig <code>dataclass</code> \u00b6 <pre><code>WanI2V720PConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = 5.0,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = WanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = WanVAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = CLIPVisionConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (T5Config(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], Tensor], ...\n    ] = (lambda: (t5_postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n    warp_denoising_step: bool = True,\n)\n</code></pre> <p>               Bases: <code>WanI2V480PConfig</code></p> <p>Base configuration for Wan I2V 14B 720P pipeline architecture.</p> fastvideo.configs.pipelines.WanT2V480PConfig <code>dataclass</code> \u00b6 <pre><code>WanT2V480PConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = 3.0,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = WanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = WanVAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (T5Config(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], Tensor], ...\n    ] = (lambda: (t5_postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n    warp_denoising_step: bool = True,\n)\n</code></pre> <p>               Bases: <code>PipelineConfig</code></p> <p>Base configuration for Wan T2V 1.3B pipeline architecture.</p> fastvideo.configs.pipelines.WanT2V720PConfig <code>dataclass</code> \u00b6 <pre><code>WanT2V720PConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = 5.0,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = WanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = WanVAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (T5Config(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], Tensor], ...\n    ] = (lambda: (t5_postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n    warp_denoising_step: bool = True,\n)\n</code></pre> <p>               Bases: <code>WanT2V480PConfig</code></p> <p>Base configuration for Wan T2V 14B 720P pipeline architecture.</p>"},{"location":"api/fastvideo/#fastvideo.configs.pipelines-functions","title":"Functions","text":"fastvideo.configs.pipelines.get_pipeline_config_cls_from_name \u00b6 <pre><code>get_pipeline_config_cls_from_name(\n    pipeline_name_or_path: str,\n) -&gt; type[PipelineConfig]\n</code></pre> <p>Get the appropriate configuration class for a given pipeline name or path.</p> <p>This function implements a multi-step lookup process to find the most suitable configuration class for a given pipeline. It follows this order: 1. Exact match in the PIPE_NAME_TO_CONFIG 2. Partial match in the PIPE_NAME_TO_CONFIG 3. Fallback to class name in the model_index.json 4. else raise an error</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_name_or_path</code> <code>str</code> <p>The name or path of the pipeline. This can be: - A registered model ID (e.g., \"FastVideo/FastHunyuan-diffusers\") - A local path to a model directory - A model ID that will be downloaded</p> required <p>Returns:</p> Type Description <code>type[PipelineConfig]</code> <p>Type[PipelineConfig]: The configuration class that best matches the pipeline. This will be one of: - A specific weight configuration class if an exact match is found - A fallback configuration class based on the pipeline architecture - The base PipelineConfig class if no matches are found</p> Note <ul> <li>For local paths, the function will verify the model configuration</li> <li>For remote models, it will attempt to download the model index</li> <li>Warning messages are logged when falling back to less specific configurations</li> </ul> Source code in <code>fastvideo/configs/pipelines/registry.py</code> <pre><code>def get_pipeline_config_cls_from_name(\n        pipeline_name_or_path: str) -&gt; type[PipelineConfig]:\n    \"\"\"Get the appropriate configuration class for a given pipeline name or path.\n\n    This function implements a multi-step lookup process to find the most suitable\n    configuration class for a given pipeline. It follows this order:\n    1. Exact match in the PIPE_NAME_TO_CONFIG\n    2. Partial match in the PIPE_NAME_TO_CONFIG\n    3. Fallback to class name in the model_index.json\n    4. else raise an error\n\n    Args:\n        pipeline_name_or_path (str): The name or path of the pipeline. This can be:\n            - A registered model ID (e.g., \"FastVideo/FastHunyuan-diffusers\")\n            - A local path to a model directory\n            - A model ID that will be downloaded\n\n    Returns:\n        Type[PipelineConfig]: The configuration class that best matches the pipeline.\n            This will be one of:\n            - A specific weight configuration class if an exact match is found\n            - A fallback configuration class based on the pipeline architecture\n            - The base PipelineConfig class if no matches are found\n\n    Note:\n        - For local paths, the function will verify the model configuration\n        - For remote models, it will attempt to download the model index\n        - Warning messages are logged when falling back to less specific configurations\n    \"\"\"\n\n    pipeline_config_cls: type[PipelineConfig] | None = None\n\n    # First try exact match for specific weights\n    if pipeline_name_or_path in PIPE_NAME_TO_CONFIG:\n        pipeline_config_cls = PIPE_NAME_TO_CONFIG[pipeline_name_or_path]\n\n    # Try partial matches (for local paths that might include the weight ID)\n    for registered_id, config_class in PIPE_NAME_TO_CONFIG.items():\n        if registered_id in pipeline_name_or_path:\n            pipeline_config_cls = config_class\n            break\n\n    # If no match, try to use the fallback config\n    if pipeline_config_cls is None:\n        if os.path.exists(pipeline_name_or_path):\n            config = verify_model_config_and_directory(pipeline_name_or_path)\n        else:\n            config = maybe_download_model_index(pipeline_name_or_path)\n        logger.warning(\n            \"Trying to use the config from the model_index.json. FastVideo may not correctly identify the optimal config for this model in this situation.\"\n        )\n\n        pipeline_name = config[\"_class_name\"]\n        # Try to determine pipeline architecture for fallback\n        for pipeline_type, detector in PIPELINE_DETECTOR.items():\n            if detector(pipeline_name.lower()):\n                pipeline_config_cls = PIPELINE_FALLBACK_CONFIG.get(\n                    pipeline_type)\n                break\n\n        if pipeline_config_cls is not None:\n            logger.warning(\n                \"No match found for pipeline %s, using fallback config %s.\",\n                pipeline_name_or_path, pipeline_config_cls)\n\n    if pipeline_config_cls is None:\n        raise ValueError(\n            f\"No match found for pipeline {pipeline_name_or_path}, please check the pipeline name or path.\"\n        )\n\n    return pipeline_config_cls\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.configs.pipelines-modules","title":"Modules","text":"fastvideo.configs.pipelines.base \u00b6 Classes\u00b6 fastvideo.configs.pipelines.base.PipelineConfig <code>dataclass</code> \u00b6 <pre><code>PipelineConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = None,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = DiTConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = VAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = True,\n    vae_sp: bool = True,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (EncoderConfig(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], tensor], ...\n    ] = (lambda: (postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n)\n</code></pre> <p>Base configuration for all pipeline architectures.</p> Functions\u00b6 fastvideo.configs.pipelines.base.PipelineConfig.from_kwargs <code>classmethod</code> \u00b6 <pre><code>from_kwargs(\n    kwargs: dict[str, Any], config_cli_prefix: str = \"\"\n) -&gt; PipelineConfig\n</code></pre> <p>Load PipelineConfig from kwargs Dictionary. kwargs: dictionary of kwargs config_cli_prefix: prefix of CLI arguments for this PipelineConfig instance</p> Source code in <code>fastvideo/configs/pipelines/base.py</code> <pre><code>@classmethod\ndef from_kwargs(cls,\n                kwargs: dict[str, Any],\n                config_cli_prefix: str = \"\") -&gt; \"PipelineConfig\":\n    \"\"\"\n    Load PipelineConfig from kwargs Dictionary.\n    kwargs: dictionary of kwargs\n    config_cli_prefix: prefix of CLI arguments for this PipelineConfig instance\n    \"\"\"\n    from fastvideo.configs.pipelines.registry import (\n        get_pipeline_config_cls_from_name)\n\n    prefix_with_dot = f\"{config_cli_prefix}.\" if (config_cli_prefix.strip()\n                                                  != \"\") else \"\"\n    model_path: str | None = kwargs.get(prefix_with_dot + 'model_path',\n                                        None) or kwargs.get('model_path')\n    pipeline_config_or_path: str | PipelineConfig | dict[\n        str, Any] | None = kwargs.get(prefix_with_dot + 'pipeline_config',\n                                      None) or kwargs.get('pipeline_config')\n    if model_path is None:\n        raise ValueError(\"model_path is required in kwargs\")\n\n    # 1. Get the pipeline config class from the registry\n    pipeline_config_cls = get_pipeline_config_cls_from_name(model_path)\n\n    # 2. Instantiate PipelineConfig\n    if pipeline_config_cls is None:\n        logger.warning(\n            \"Couldn't find pipeline config for %s. Using the default pipeline config.\",\n            model_path)\n        pipeline_config = cls()\n    else:\n        pipeline_config = pipeline_config_cls()\n\n    # 3. Load PipelineConfig from a json file or a PipelineConfig object if provided\n    if isinstance(pipeline_config_or_path, str):\n        pipeline_config.load_from_json(pipeline_config_or_path)\n        kwargs[prefix_with_dot +\n               'pipeline_config_path'] = pipeline_config_or_path\n    elif isinstance(pipeline_config_or_path, PipelineConfig):\n        pipeline_config = pipeline_config_or_path\n    elif isinstance(pipeline_config_or_path, dict):\n        pipeline_config.update_pipeline_config(pipeline_config_or_path)\n\n    # 4. Update PipelineConfig from CLI arguments if provided\n    kwargs[prefix_with_dot + 'model_path'] = model_path\n    pipeline_config.update_config_from_dict(kwargs, config_cli_prefix)\n    return pipeline_config\n</code></pre> fastvideo.configs.pipelines.base.PipelineConfig.from_pretrained <code>classmethod</code> \u00b6 <pre><code>from_pretrained(model_path: str) -&gt; PipelineConfig\n</code></pre> <p>use the pipeline class setting from model_path to match the pipeline config</p> Source code in <code>fastvideo/configs/pipelines/base.py</code> <pre><code>@classmethod\ndef from_pretrained(cls, model_path: str) -&gt; \"PipelineConfig\":\n    \"\"\"\n    use the pipeline class setting from model_path to match the pipeline config\n    \"\"\"\n    from fastvideo.configs.pipelines.registry import (\n        get_pipeline_config_cls_from_name)\n    pipeline_config_cls = get_pipeline_config_cls_from_name(model_path)\n\n    return cast(PipelineConfig, pipeline_config_cls(model_path=model_path))\n</code></pre> fastvideo.configs.pipelines.base.STA_Mode \u00b6 <p>               Bases: <code>str</code>, <code>Enum</code></p> <p>STA (Sliding Tile Attention) modes.</p> fastvideo.configs.pipelines.base.SlidingTileAttnConfig <code>dataclass</code> \u00b6 <pre><code>SlidingTileAttnConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = None,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = DiTConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = VAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = True,\n    vae_sp: bool = True,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (EncoderConfig(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], tensor], ...\n    ] = (lambda: (postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    window_size: int = 16,\n    stride: int = 8,\n    height: int = 576,\n    width: int = 1024,\n    pad_to_square: bool = False,\n    use_overlap_optimization: bool = True,\n)\n</code></pre> <p>               Bases: <code>PipelineConfig</code></p> <p>Configuration for sliding tile attention.</p> Functions\u00b6 fastvideo.configs.pipelines.base.parse_int_list \u00b6 <pre><code>parse_int_list(value: str) -&gt; list[int]\n</code></pre> <p>Parse a comma-separated string of integers into a list.</p> Source code in <code>fastvideo/configs/pipelines/base.py</code> <pre><code>def parse_int_list(value: str) -&gt; list[int]:\n    \"\"\"Parse a comma-separated string of integers into a list.\"\"\"\n    if not value:\n        return []\n    return [int(x.strip()) for x in value.split(\",\")]\n</code></pre> fastvideo.configs.pipelines.hunyuan \u00b6 Classes\u00b6 fastvideo.configs.pipelines.hunyuan.FastHunyuanConfig <code>dataclass</code> \u00b6 <pre><code>FastHunyuanConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: int = 6,\n    flow_shift: int = 17,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = HunyuanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = HunyuanVAEConfig(),\n    vae_precision: str = \"fp16\",\n    vae_tiling: bool = True,\n    vae_sp: bool = True,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (LlamaConfig(), CLIPTextConfig())\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp16\", \"fp16\")\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (\n        lambda: (\n            llama_preprocess_text,\n            clip_preprocess_text,\n        )\n    )(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], tensor], ...\n    ] = (\n        lambda: (\n            llama_postprocess_text,\n            clip_postprocess_text,\n        )\n    )(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n)\n</code></pre> <p>               Bases: <code>HunyuanConfig</code></p> <p>Configuration specifically optimized for FastHunyuan weights.</p> fastvideo.configs.pipelines.hunyuan.HunyuanConfig <code>dataclass</code> \u00b6 <pre><code>HunyuanConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: int = 6,\n    flow_shift: int = 7,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = HunyuanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = HunyuanVAEConfig(),\n    vae_precision: str = \"fp16\",\n    vae_tiling: bool = True,\n    vae_sp: bool = True,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (LlamaConfig(), CLIPTextConfig())\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp16\", \"fp16\")\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (\n        lambda: (\n            llama_preprocess_text,\n            clip_preprocess_text,\n        )\n    )(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], tensor], ...\n    ] = (\n        lambda: (\n            llama_postprocess_text,\n            clip_postprocess_text,\n        )\n    )(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n)\n</code></pre> <p>               Bases: <code>PipelineConfig</code></p> <p>Base configuration for HunYuan pipeline architecture.</p> fastvideo.configs.pipelines.registry \u00b6 <p>Registry for pipeline weight-specific configurations.</p> Classes\u00b6 Functions\u00b6 fastvideo.configs.pipelines.registry.get_pipeline_config_cls_from_name \u00b6 <pre><code>get_pipeline_config_cls_from_name(\n    pipeline_name_or_path: str,\n) -&gt; type[PipelineConfig]\n</code></pre> <p>Get the appropriate configuration class for a given pipeline name or path.</p> <p>This function implements a multi-step lookup process to find the most suitable configuration class for a given pipeline. It follows this order: 1. Exact match in the PIPE_NAME_TO_CONFIG 2. Partial match in the PIPE_NAME_TO_CONFIG 3. Fallback to class name in the model_index.json 4. else raise an error</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_name_or_path</code> <code>str</code> <p>The name or path of the pipeline. This can be: - A registered model ID (e.g., \"FastVideo/FastHunyuan-diffusers\") - A local path to a model directory - A model ID that will be downloaded</p> required <p>Returns:</p> Type Description <code>type[PipelineConfig]</code> <p>Type[PipelineConfig]: The configuration class that best matches the pipeline. This will be one of: - A specific weight configuration class if an exact match is found - A fallback configuration class based on the pipeline architecture - The base PipelineConfig class if no matches are found</p> Note <ul> <li>For local paths, the function will verify the model configuration</li> <li>For remote models, it will attempt to download the model index</li> <li>Warning messages are logged when falling back to less specific configurations</li> </ul> Source code in <code>fastvideo/configs/pipelines/registry.py</code> <pre><code>def get_pipeline_config_cls_from_name(\n        pipeline_name_or_path: str) -&gt; type[PipelineConfig]:\n    \"\"\"Get the appropriate configuration class for a given pipeline name or path.\n\n    This function implements a multi-step lookup process to find the most suitable\n    configuration class for a given pipeline. It follows this order:\n    1. Exact match in the PIPE_NAME_TO_CONFIG\n    2. Partial match in the PIPE_NAME_TO_CONFIG\n    3. Fallback to class name in the model_index.json\n    4. else raise an error\n\n    Args:\n        pipeline_name_or_path (str): The name or path of the pipeline. This can be:\n            - A registered model ID (e.g., \"FastVideo/FastHunyuan-diffusers\")\n            - A local path to a model directory\n            - A model ID that will be downloaded\n\n    Returns:\n        Type[PipelineConfig]: The configuration class that best matches the pipeline.\n            This will be one of:\n            - A specific weight configuration class if an exact match is found\n            - A fallback configuration class based on the pipeline architecture\n            - The base PipelineConfig class if no matches are found\n\n    Note:\n        - For local paths, the function will verify the model configuration\n        - For remote models, it will attempt to download the model index\n        - Warning messages are logged when falling back to less specific configurations\n    \"\"\"\n\n    pipeline_config_cls: type[PipelineConfig] | None = None\n\n    # First try exact match for specific weights\n    if pipeline_name_or_path in PIPE_NAME_TO_CONFIG:\n        pipeline_config_cls = PIPE_NAME_TO_CONFIG[pipeline_name_or_path]\n\n    # Try partial matches (for local paths that might include the weight ID)\n    for registered_id, config_class in PIPE_NAME_TO_CONFIG.items():\n        if registered_id in pipeline_name_or_path:\n            pipeline_config_cls = config_class\n            break\n\n    # If no match, try to use the fallback config\n    if pipeline_config_cls is None:\n        if os.path.exists(pipeline_name_or_path):\n            config = verify_model_config_and_directory(pipeline_name_or_path)\n        else:\n            config = maybe_download_model_index(pipeline_name_or_path)\n        logger.warning(\n            \"Trying to use the config from the model_index.json. FastVideo may not correctly identify the optimal config for this model in this situation.\"\n        )\n\n        pipeline_name = config[\"_class_name\"]\n        # Try to determine pipeline architecture for fallback\n        for pipeline_type, detector in PIPELINE_DETECTOR.items():\n            if detector(pipeline_name.lower()):\n                pipeline_config_cls = PIPELINE_FALLBACK_CONFIG.get(\n                    pipeline_type)\n                break\n\n        if pipeline_config_cls is not None:\n            logger.warning(\n                \"No match found for pipeline %s, using fallback config %s.\",\n                pipeline_name_or_path, pipeline_config_cls)\n\n    if pipeline_config_cls is None:\n        raise ValueError(\n            f\"No match found for pipeline {pipeline_name_or_path}, please check the pipeline name or path.\"\n        )\n\n    return pipeline_config_cls\n</code></pre> fastvideo.configs.pipelines.stepvideo \u00b6 Classes\u00b6 fastvideo.configs.pipelines.stepvideo.StepVideoT2VConfig <code>dataclass</code> \u00b6 <pre><code>StepVideoT2VConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: int = 13,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = StepVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = StepVideoVAEConfig(),\n    vae_precision: str = \"bf16\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (EncoderConfig(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], tensor], ...\n    ] = (lambda: (postprocess_text,))(),\n    pos_magic: str = \"\u200b\u8d85\u9ad8\u200b\u6e05\u200b\u3001HDR \u200b\u89c6\u9891\u200b\u3001\u200b\u73af\u5883\u5149\u200b\u3001\u200b\u675c\u6bd4\u200b\u5168\u666f\u200b\u58f0\u200b\u3001\u200b\u753b\u9762\u200b\u7a33\u5b9a\u200b\u3001\u200b\u6d41\u7545\u200b\u52a8\u4f5c\u200b\u3001\u200b\u903c\u771f\u200b\u7684\u200b\u7ec6\u8282\u200b\u3001\u200b\u4e13\u4e1a\u7ea7\u200b\u6784\u56fe\u200b\u3001\u200b\u8d85\u73b0\u5b9e\u4e3b\u4e49\u200b\u3001\u200b\u81ea\u7136\u200b\u3001\u200b\u751f\u52a8\u200b\u3001\u200b\u8d85\u200b\u7ec6\u8282\u200b\u3001\u200b\u6e05\u6670\u200b\u3002\",\n    neg_magic: str = \"\u200b\u753b\u9762\u200b\u6697\u200b\u3001\u200b\u4f4e\u5206\u8fa8\u7387\u200b\u3001\u200b\u4e0d\u826f\u200b\u624b\u200b\u3001\u200b\u6587\u672c\u200b\u3001\u200b\u7f3a\u5c11\u200b\u624b\u6307\u200b\u3001\u200b\u591a\u4f59\u200b\u7684\u200b\u624b\u6307\u200b\u3001\u200b\u88c1\u526a\u200b\u3001\u200b\u4f4e\u8d28\u91cf\u200b\u3001\u200b\u9897\u7c92\u72b6\u200b\u3001\u200b\u7b7e\u540d\u200b\u3001\u200b\u6c34\u5370\u200b\u3001\u200b\u7528\u6237\u540d\u200b\u3001\u200b\u6a21\u7cca\u200b\u3002\",\n    timesteps_scale: bool = False,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n)\n</code></pre> <p>               Bases: <code>PipelineConfig</code></p> <p>Base configuration for StepVideo pipeline architecture.</p> fastvideo.configs.pipelines.wan \u00b6 Classes\u00b6 fastvideo.configs.pipelines.wan.FastWan2_1_T2V_480P_Config <code>dataclass</code> \u00b6 <pre><code>FastWan2_1_T2V_480P_Config(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = 8.0,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = WanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = WanVAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (T5Config(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], Tensor], ...\n    ] = (lambda: (t5_postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int]\n    | None = (lambda: [1000, 757, 522])(),\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n    warp_denoising_step: bool = True,\n)\n</code></pre> <p>               Bases: <code>WanT2V480PConfig</code></p> <p>Base configuration for FastWan T2V 1.3B 480P pipeline architecture with DMD</p> fastvideo.configs.pipelines.wan.WANV2VConfig <code>dataclass</code> \u00b6 <pre><code>WANV2VConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = 3.0,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = WanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = WanVAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = WAN2_1ControlCLIPVisionConfig(),\n    image_encoder_precision: str = \"bf16\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (T5Config(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], Tensor], ...\n    ] = (lambda: (t5_postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n    warp_denoising_step: bool = True,\n)\n</code></pre> <p>               Bases: <code>WanI2V480PConfig</code></p> <p>Configuration for WAN2.1 1.3B Control pipeline.</p> fastvideo.configs.pipelines.wan.WanI2V480PConfig <code>dataclass</code> \u00b6 <pre><code>WanI2V480PConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = 3.0,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = WanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = WanVAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = CLIPVisionConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (T5Config(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], Tensor], ...\n    ] = (lambda: (t5_postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n    warp_denoising_step: bool = True,\n)\n</code></pre> <p>               Bases: <code>WanT2V480PConfig</code></p> <p>Base configuration for Wan I2V 14B 480P pipeline architecture.</p> fastvideo.configs.pipelines.wan.WanI2V720PConfig <code>dataclass</code> \u00b6 <pre><code>WanI2V720PConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = 5.0,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = WanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = WanVAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = CLIPVisionConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (T5Config(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], Tensor], ...\n    ] = (lambda: (t5_postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n    warp_denoising_step: bool = True,\n)\n</code></pre> <p>               Bases: <code>WanI2V480PConfig</code></p> <p>Base configuration for Wan I2V 14B 720P pipeline architecture.</p> fastvideo.configs.pipelines.wan.WanT2V480PConfig <code>dataclass</code> \u00b6 <pre><code>WanT2V480PConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = 3.0,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = WanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = WanVAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (T5Config(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], Tensor], ...\n    ] = (lambda: (t5_postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n    warp_denoising_step: bool = True,\n)\n</code></pre> <p>               Bases: <code>PipelineConfig</code></p> <p>Base configuration for Wan T2V 1.3B pipeline architecture.</p> fastvideo.configs.pipelines.wan.WanT2V720PConfig <code>dataclass</code> \u00b6 <pre><code>WanT2V720PConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = 5.0,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = WanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = WanVAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (T5Config(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], Tensor], ...\n    ] = (lambda: (t5_postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n    warp_denoising_step: bool = True,\n)\n</code></pre> <p>               Bases: <code>WanT2V480PConfig</code></p> <p>Base configuration for Wan T2V 14B 720P pipeline architecture.</p>"},{"location":"api/fastvideo/#fastvideo.configs.sample","title":"fastvideo.configs.sample","text":""},{"location":"api/fastvideo/#fastvideo.configs.sample-classes","title":"Classes","text":"fastvideo.configs.sample.SamplingParam <code>dataclass</code> \u00b6 <pre><code>SamplingParam(\n    data_type: str = \"video\",\n    image_path: str | None = None,\n    video_path: str | None = None,\n    prompt: str | list[str] | None = None,\n    negative_prompt: str = \"Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\",\n    prompt_path: str | None = None,\n    output_path: str = \"outputs/\",\n    output_video_name: str | None = None,\n    num_videos_per_prompt: int = 1,\n    seed: int = 1024,\n    num_frames: int = 125,\n    num_frames_round_down: bool = False,\n    height: int = 720,\n    width: int = 1280,\n    fps: int = 24,\n    num_inference_steps: int = 50,\n    guidance_scale: float = 1.0,\n    guidance_rescale: float = 0.0,\n    boundary_ratio: float | None = None,\n    enable_teacache: bool = False,\n    save_video: bool = True,\n    return_frames: bool = False,\n    return_trajectory_latents: bool = False,\n    return_trajectory_decoded: bool = False,\n)\n</code></pre> <p>Sampling parameters for video generation.</p> Functions\u00b6 fastvideo.configs.sample.SamplingParam.add_cli_args <code>staticmethod</code> \u00b6 <pre><code>add_cli_args(parser: Any) -&gt; Any\n</code></pre> <p>Add CLI arguments for SamplingParam fields</p> Source code in <code>fastvideo/configs/sample/base.py</code> <pre><code>@staticmethod\ndef add_cli_args(parser: Any) -&gt; Any:\n    \"\"\"Add CLI arguments for SamplingParam fields\"\"\"\n    parser.add_argument(\n        \"--prompt\",\n        type=str,\n        default=SamplingParam.prompt,\n        help=\"Text prompt for video generation\",\n    )\n    parser.add_argument(\n        \"--negative-prompt\",\n        type=str,\n        default=SamplingParam.negative_prompt,\n        help=\"Negative text prompt for video generation\",\n    )\n    parser.add_argument(\n        \"--prompt-path\",\n        type=str,\n        default=SamplingParam.prompt_path,\n        help=\"Path to a text file containing the prompt\",\n    )\n    parser.add_argument(\n        \"--output-path\",\n        type=str,\n        default=SamplingParam.output_path,\n        help=\"Path to save the generated video\",\n    )\n    parser.add_argument(\n        \"--output-video-name\",\n        type=str,\n        default=SamplingParam.output_video_name,\n        help=\"Name of the output video\",\n    )\n    parser.add_argument(\n        \"--num-videos-per-prompt\",\n        type=int,\n        default=SamplingParam.num_videos_per_prompt,\n        help=\"Number of videos to generate per prompt\",\n    )\n    parser.add_argument(\n        \"--seed\",\n        type=int,\n        default=SamplingParam.seed,\n        help=\"Random seed for generation\",\n    )\n    parser.add_argument(\n        \"--num-frames\",\n        type=int,\n        default=SamplingParam.num_frames,\n        help=\"Number of frames to generate\",\n    )\n    parser.add_argument(\n        \"--height\",\n        type=int,\n        default=SamplingParam.height,\n        help=\"Height of generated video\",\n    )\n    parser.add_argument(\n        \"--width\",\n        type=int,\n        default=SamplingParam.width,\n        help=\"Width of generated video\",\n    )\n    parser.add_argument(\n        \"--fps\",\n        type=int,\n        default=SamplingParam.fps,\n        help=\"Frames per second for saved video\",\n    )\n    parser.add_argument(\n        \"--num-inference-steps\",\n        type=int,\n        default=SamplingParam.num_inference_steps,\n        help=\"Number of denoising steps\",\n    )\n    parser.add_argument(\n        \"--guidance-scale\",\n        type=float,\n        default=SamplingParam.guidance_scale,\n        help=\"Classifier-free guidance scale\",\n    )\n    parser.add_argument(\n        \"--guidance-rescale\",\n        type=float,\n        default=SamplingParam.guidance_rescale,\n        help=\"Guidance rescale factor\",\n    )\n    parser.add_argument(\n        \"--boundary-ratio\",\n        type=float,\n        default=SamplingParam.boundary_ratio,\n        help=\"Boundary timestep ratio\",\n    )\n    parser.add_argument(\n        \"--save-video\",\n        action=\"store_true\",\n        default=SamplingParam.save_video,\n        help=\"Whether to save the video to disk\",\n    )\n    parser.add_argument(\n        \"--no-save-video\",\n        action=\"store_false\",\n        dest=\"save_video\",\n        help=\"Don't save the video to disk\",\n    )\n    parser.add_argument(\n        \"--return-frames\",\n        action=\"store_true\",\n        default=SamplingParam.return_frames,\n        help=\"Whether to return the raw frames\",\n    )\n    parser.add_argument(\n        \"--image-path\",\n        type=str,\n        default=SamplingParam.image_path,\n        help=\"Path to input image for image-to-video generation\",\n    )\n    parser.add_argument(\n        \"--video_path\",\n        type=str,\n        default=SamplingParam.video_path,\n        help=\"Path to input video for video-to-video generation\",\n    )\n    parser.add_argument(\n        \"--moba-config-path\",\n        type=str,\n        default=None,\n        help=\n        \"Path to a JSON file containing V-MoBA specific configurations.\",\n    )\n    parser.add_argument(\n        \"--return-trajectory-latents\",\n        action=\"store_true\",\n        default=SamplingParam.return_trajectory_latents,\n        help=\"Whether to return the trajectory\",\n    )\n    parser.add_argument(\n        \"--return-trajectory-decoded\",\n        action=\"store_true\",\n        default=SamplingParam.return_trajectory_decoded,\n        help=\"Whether to return the decoded trajectory\",\n    )\n    return parser\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.configs.sample-modules","title":"Modules","text":"fastvideo.configs.sample.base \u00b6 Classes\u00b6 fastvideo.configs.sample.base.SamplingParam <code>dataclass</code> \u00b6 <pre><code>SamplingParam(\n    data_type: str = \"video\",\n    image_path: str | None = None,\n    video_path: str | None = None,\n    prompt: str | list[str] | None = None,\n    negative_prompt: str = \"Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\",\n    prompt_path: str | None = None,\n    output_path: str = \"outputs/\",\n    output_video_name: str | None = None,\n    num_videos_per_prompt: int = 1,\n    seed: int = 1024,\n    num_frames: int = 125,\n    num_frames_round_down: bool = False,\n    height: int = 720,\n    width: int = 1280,\n    fps: int = 24,\n    num_inference_steps: int = 50,\n    guidance_scale: float = 1.0,\n    guidance_rescale: float = 0.0,\n    boundary_ratio: float | None = None,\n    enable_teacache: bool = False,\n    save_video: bool = True,\n    return_frames: bool = False,\n    return_trajectory_latents: bool = False,\n    return_trajectory_decoded: bool = False,\n)\n</code></pre> <p>Sampling parameters for video generation.</p> Functions\u00b6 fastvideo.configs.sample.base.SamplingParam.add_cli_args <code>staticmethod</code> \u00b6 <pre><code>add_cli_args(parser: Any) -&gt; Any\n</code></pre> <p>Add CLI arguments for SamplingParam fields</p> Source code in <code>fastvideo/configs/sample/base.py</code> <pre><code>@staticmethod\ndef add_cli_args(parser: Any) -&gt; Any:\n    \"\"\"Add CLI arguments for SamplingParam fields\"\"\"\n    parser.add_argument(\n        \"--prompt\",\n        type=str,\n        default=SamplingParam.prompt,\n        help=\"Text prompt for video generation\",\n    )\n    parser.add_argument(\n        \"--negative-prompt\",\n        type=str,\n        default=SamplingParam.negative_prompt,\n        help=\"Negative text prompt for video generation\",\n    )\n    parser.add_argument(\n        \"--prompt-path\",\n        type=str,\n        default=SamplingParam.prompt_path,\n        help=\"Path to a text file containing the prompt\",\n    )\n    parser.add_argument(\n        \"--output-path\",\n        type=str,\n        default=SamplingParam.output_path,\n        help=\"Path to save the generated video\",\n    )\n    parser.add_argument(\n        \"--output-video-name\",\n        type=str,\n        default=SamplingParam.output_video_name,\n        help=\"Name of the output video\",\n    )\n    parser.add_argument(\n        \"--num-videos-per-prompt\",\n        type=int,\n        default=SamplingParam.num_videos_per_prompt,\n        help=\"Number of videos to generate per prompt\",\n    )\n    parser.add_argument(\n        \"--seed\",\n        type=int,\n        default=SamplingParam.seed,\n        help=\"Random seed for generation\",\n    )\n    parser.add_argument(\n        \"--num-frames\",\n        type=int,\n        default=SamplingParam.num_frames,\n        help=\"Number of frames to generate\",\n    )\n    parser.add_argument(\n        \"--height\",\n        type=int,\n        default=SamplingParam.height,\n        help=\"Height of generated video\",\n    )\n    parser.add_argument(\n        \"--width\",\n        type=int,\n        default=SamplingParam.width,\n        help=\"Width of generated video\",\n    )\n    parser.add_argument(\n        \"--fps\",\n        type=int,\n        default=SamplingParam.fps,\n        help=\"Frames per second for saved video\",\n    )\n    parser.add_argument(\n        \"--num-inference-steps\",\n        type=int,\n        default=SamplingParam.num_inference_steps,\n        help=\"Number of denoising steps\",\n    )\n    parser.add_argument(\n        \"--guidance-scale\",\n        type=float,\n        default=SamplingParam.guidance_scale,\n        help=\"Classifier-free guidance scale\",\n    )\n    parser.add_argument(\n        \"--guidance-rescale\",\n        type=float,\n        default=SamplingParam.guidance_rescale,\n        help=\"Guidance rescale factor\",\n    )\n    parser.add_argument(\n        \"--boundary-ratio\",\n        type=float,\n        default=SamplingParam.boundary_ratio,\n        help=\"Boundary timestep ratio\",\n    )\n    parser.add_argument(\n        \"--save-video\",\n        action=\"store_true\",\n        default=SamplingParam.save_video,\n        help=\"Whether to save the video to disk\",\n    )\n    parser.add_argument(\n        \"--no-save-video\",\n        action=\"store_false\",\n        dest=\"save_video\",\n        help=\"Don't save the video to disk\",\n    )\n    parser.add_argument(\n        \"--return-frames\",\n        action=\"store_true\",\n        default=SamplingParam.return_frames,\n        help=\"Whether to return the raw frames\",\n    )\n    parser.add_argument(\n        \"--image-path\",\n        type=str,\n        default=SamplingParam.image_path,\n        help=\"Path to input image for image-to-video generation\",\n    )\n    parser.add_argument(\n        \"--video_path\",\n        type=str,\n        default=SamplingParam.video_path,\n        help=\"Path to input video for video-to-video generation\",\n    )\n    parser.add_argument(\n        \"--moba-config-path\",\n        type=str,\n        default=None,\n        help=\n        \"Path to a JSON file containing V-MoBA specific configurations.\",\n    )\n    parser.add_argument(\n        \"--return-trajectory-latents\",\n        action=\"store_true\",\n        default=SamplingParam.return_trajectory_latents,\n        help=\"Whether to return the trajectory\",\n    )\n    parser.add_argument(\n        \"--return-trajectory-decoded\",\n        action=\"store_true\",\n        default=SamplingParam.return_trajectory_decoded,\n        help=\"Whether to return the decoded trajectory\",\n    )\n    return parser\n</code></pre> Functions\u00b6 fastvideo.configs.sample.registry \u00b6 Classes\u00b6 Functions\u00b6 fastvideo.configs.sample.registry.get_sampling_param_cls_for_name \u00b6 <pre><code>get_sampling_param_cls_for_name(\n    pipeline_name_or_path: str,\n) -&gt; Any | None\n</code></pre> <p>Get the appropriate sampling param for specific pretrained weights.</p> Source code in <code>fastvideo/configs/sample/registry.py</code> <pre><code>def get_sampling_param_cls_for_name(pipeline_name_or_path: str) -&gt; Any | None:\n    \"\"\"Get the appropriate sampling param for specific pretrained weights.\"\"\"\n\n    if os.path.exists(pipeline_name_or_path):\n        config = verify_model_config_and_directory(pipeline_name_or_path)\n        logger.warning(\n            \"FastVideo may not correctly identify the optimal sampling param for this model, as the local directory may have been renamed.\"\n        )\n    else:\n        config = maybe_download_model_index(pipeline_name_or_path)\n\n    pipeline_name = config[\"_class_name\"]\n\n    # First try exact match for specific weights\n    if pipeline_name_or_path in SAMPLING_PARAM_REGISTRY:\n        return SAMPLING_PARAM_REGISTRY[pipeline_name_or_path]\n\n    # Try partial matches (for local paths that might include the weight ID)\n    for registered_id, config_class in SAMPLING_PARAM_REGISTRY.items():\n        if registered_id in pipeline_name_or_path:\n            return config_class\n\n    # If no match, try to use the fallback config\n    fallback_config = None\n    # Try to determine pipeline architecture for fallback\n    for pipeline_type, detector in SAMPLING_PARAM_DETECTOR.items():\n        if detector(pipeline_name.lower()):\n            fallback_config = SAMPLING_FALLBACK_PARAM.get(pipeline_type)\n            break\n\n    logger.warning(\n        \"No match found for pipeline %s, using fallback sampling param %s.\",\n        pipeline_name_or_path, fallback_config)\n    return fallback_config\n</code></pre> fastvideo.configs.sample.wan \u00b6 Classes\u00b6 fastvideo.configs.sample.wan.Wan2_1_Fun_1_3B_InP_SamplingParam <code>dataclass</code> \u00b6 <pre><code>Wan2_1_Fun_1_3B_InP_SamplingParam(\n    data_type: str = \"video\",\n    image_path: str | None = None,\n    video_path: str | None = None,\n    prompt: str | list[str] | None = None,\n    negative_prompt: str\n    | None = \"\u200b\u8272\u8c03\u200b\u8273\u4e3d\u200b\uff0c\u200b\u8fc7\u200b\u66dd\u200b\uff0c\u200b\u9759\u6001\u200b\uff0c\u200b\u7ec6\u8282\u200b\u6a21\u7cca\u4e0d\u6e05\u200b\uff0c\u200b\u5b57\u5e55\u200b\uff0c\u200b\u98ce\u683c\u200b\uff0c\u200b\u4f5c\u54c1\u200b\uff0c\u200b\u753b\u4f5c\u200b\uff0c\u200b\u753b\u9762\u200b\uff0c\u200b\u9759\u6b62\u200b\uff0c\u200b\u6574\u4f53\u200b\u53d1\u7070\u200b\uff0c\u200b\u6700\u5dee\u200b\u8d28\u91cf\u200b\uff0c\u200b\u4f4e\u8d28\u91cf\u200b\uff0cJPEG\u200b\u538b\u7f29\u200b\u6b8b\u7559\u200b\uff0c\u200b\u4e11\u964b\u200b\u7684\u200b\uff0c\u200b\u6b8b\u7f3a\u200b\u7684\u200b\uff0c\u200b\u591a\u4f59\u200b\u7684\u200b\u624b\u6307\u200b\uff0c\u200b\u753b\u5f97\u200b\u4e0d\u597d\u200b\u7684\u200b\u624b\u90e8\u200b\uff0c\u200b\u753b\u5f97\u200b\u4e0d\u597d\u200b\u7684\u200b\u8138\u90e8\u200b\uff0c\u200b\u7578\u5f62\u200b\u7684\u200b\uff0c\u200b\u6bc1\u5bb9\u200b\u7684\u200b\uff0c\u200b\u5f62\u6001\u200b\u7578\u5f62\u200b\u7684\u200b\u80a2\u4f53\u200b\uff0c\u200b\u624b\u6307\u200b\u878d\u5408\u200b\uff0c\u200b\u9759\u6b62\u4e0d\u52a8\u200b\u7684\u200b\u753b\u9762\u200b\uff0c\u200b\u6742\u4e71\u200b\u7684\u200b\u80cc\u666f\u200b\uff0c\u200b\u4e09\u6761\u200b\u817f\u200b\uff0c\u200b\u80cc\u666f\u200b\u4eba\u200b\u5f88\u591a\u200b\uff0c\u200b\u5012\u200b\u7740\u200b\u8d70\u200b\",\n    prompt_path: str | None = None,\n    output_path: str = \"outputs/\",\n    output_video_name: str | None = None,\n    num_videos_per_prompt: int = 1,\n    seed: int = 1024,\n    num_frames: int = 81,\n    num_frames_round_down: bool = False,\n    height: int = 480,\n    width: int = 832,\n    fps: int = 16,\n    num_inference_steps: int = 50,\n    guidance_scale: float = 6.0,\n    guidance_rescale: float = 0.0,\n    boundary_ratio: float | None = None,\n    enable_teacache: bool = False,\n    save_video: bool = True,\n    return_frames: bool = False,\n    return_trajectory_latents: bool = False,\n    return_trajectory_decoded: bool = False,\n)\n</code></pre> <p>               Bases: <code>SamplingParam</code></p> <p>Sampling parameters for Wan2.1 Fun 1.3B InP model.</p> fastvideo.configs.sample.wan.Wan2_2_Base_SamplingParam <code>dataclass</code> \u00b6 <pre><code>Wan2_2_Base_SamplingParam(\n    data_type: str = \"video\",\n    image_path: str | None = None,\n    video_path: str | None = None,\n    prompt: str | list[str] | None = None,\n    negative_prompt: str\n    | None = \"\u200b\u8272\u8c03\u200b\u8273\u4e3d\u200b\uff0c\u200b\u8fc7\u200b\u66dd\u200b\uff0c\u200b\u9759\u6001\u200b\uff0c\u200b\u7ec6\u8282\u200b\u6a21\u7cca\u4e0d\u6e05\u200b\uff0c\u200b\u5b57\u5e55\u200b\uff0c\u200b\u98ce\u683c\u200b\uff0c\u200b\u4f5c\u54c1\u200b\uff0c\u200b\u753b\u4f5c\u200b\uff0c\u200b\u753b\u9762\u200b\uff0c\u200b\u9759\u6b62\u200b\uff0c\u200b\u6574\u4f53\u200b\u53d1\u7070\u200b\uff0c\u200b\u6700\u5dee\u200b\u8d28\u91cf\u200b\uff0c\u200b\u4f4e\u8d28\u91cf\u200b\uff0cJPEG\u200b\u538b\u7f29\u200b\u6b8b\u7559\u200b\uff0c\u200b\u4e11\u964b\u200b\u7684\u200b\uff0c\u200b\u6b8b\u7f3a\u200b\u7684\u200b\uff0c\u200b\u591a\u4f59\u200b\u7684\u200b\u624b\u6307\u200b\uff0c\u200b\u753b\u5f97\u200b\u4e0d\u597d\u200b\u7684\u200b\u624b\u90e8\u200b\uff0c\u200b\u753b\u5f97\u200b\u4e0d\u597d\u200b\u7684\u200b\u8138\u90e8\u200b\uff0c\u200b\u7578\u5f62\u200b\u7684\u200b\uff0c\u200b\u6bc1\u5bb9\u200b\u7684\u200b\uff0c\u200b\u5f62\u6001\u200b\u7578\u5f62\u200b\u7684\u200b\u80a2\u4f53\u200b\uff0c\u200b\u624b\u6307\u200b\u878d\u5408\u200b\uff0c\u200b\u9759\u6b62\u4e0d\u52a8\u200b\u7684\u200b\u753b\u9762\u200b\uff0c\u200b\u6742\u4e71\u200b\u7684\u200b\u80cc\u666f\u200b\uff0c\u200b\u4e09\u6761\u200b\u817f\u200b\uff0c\u200b\u80cc\u666f\u200b\u4eba\u200b\u5f88\u591a\u200b\uff0c\u200b\u5012\u200b\u7740\u200b\u8d70\u200b\",\n    prompt_path: str | None = None,\n    output_path: str = \"outputs/\",\n    output_video_name: str | None = None,\n    num_videos_per_prompt: int = 1,\n    seed: int = 1024,\n    num_frames: int = 125,\n    num_frames_round_down: bool = False,\n    height: int = 720,\n    width: int = 1280,\n    fps: int = 24,\n    num_inference_steps: int = 50,\n    guidance_scale: float = 1.0,\n    guidance_rescale: float = 0.0,\n    boundary_ratio: float | None = None,\n    enable_teacache: bool = False,\n    save_video: bool = True,\n    return_frames: bool = False,\n    return_trajectory_latents: bool = False,\n    return_trajectory_decoded: bool = False,\n)\n</code></pre> <p>               Bases: <code>SamplingParam</code></p> <p>Sampling parameters for Wan2.2 TI2V 5B model.</p> fastvideo.configs.sample.wan.Wan2_2_TI2V_5B_SamplingParam <code>dataclass</code> \u00b6 <pre><code>Wan2_2_TI2V_5B_SamplingParam(\n    data_type: str = \"video\",\n    image_path: str | None = None,\n    video_path: str | None = None,\n    prompt: str | list[str] | None = None,\n    negative_prompt: str\n    | None = \"\u200b\u8272\u8c03\u200b\u8273\u4e3d\u200b\uff0c\u200b\u8fc7\u200b\u66dd\u200b\uff0c\u200b\u9759\u6001\u200b\uff0c\u200b\u7ec6\u8282\u200b\u6a21\u7cca\u4e0d\u6e05\u200b\uff0c\u200b\u5b57\u5e55\u200b\uff0c\u200b\u98ce\u683c\u200b\uff0c\u200b\u4f5c\u54c1\u200b\uff0c\u200b\u753b\u4f5c\u200b\uff0c\u200b\u753b\u9762\u200b\uff0c\u200b\u9759\u6b62\u200b\uff0c\u200b\u6574\u4f53\u200b\u53d1\u7070\u200b\uff0c\u200b\u6700\u5dee\u200b\u8d28\u91cf\u200b\uff0c\u200b\u4f4e\u8d28\u91cf\u200b\uff0cJPEG\u200b\u538b\u7f29\u200b\u6b8b\u7559\u200b\uff0c\u200b\u4e11\u964b\u200b\u7684\u200b\uff0c\u200b\u6b8b\u7f3a\u200b\u7684\u200b\uff0c\u200b\u591a\u4f59\u200b\u7684\u200b\u624b\u6307\u200b\uff0c\u200b\u753b\u5f97\u200b\u4e0d\u597d\u200b\u7684\u200b\u624b\u90e8\u200b\uff0c\u200b\u753b\u5f97\u200b\u4e0d\u597d\u200b\u7684\u200b\u8138\u90e8\u200b\uff0c\u200b\u7578\u5f62\u200b\u7684\u200b\uff0c\u200b\u6bc1\u5bb9\u200b\u7684\u200b\uff0c\u200b\u5f62\u6001\u200b\u7578\u5f62\u200b\u7684\u200b\u80a2\u4f53\u200b\uff0c\u200b\u624b\u6307\u200b\u878d\u5408\u200b\uff0c\u200b\u9759\u6b62\u4e0d\u52a8\u200b\u7684\u200b\u753b\u9762\u200b\uff0c\u200b\u6742\u4e71\u200b\u7684\u200b\u80cc\u666f\u200b\uff0c\u200b\u4e09\u6761\u200b\u817f\u200b\uff0c\u200b\u80cc\u666f\u200b\u4eba\u200b\u5f88\u591a\u200b\uff0c\u200b\u5012\u200b\u7740\u200b\u8d70\u200b\",\n    prompt_path: str | None = None,\n    output_path: str = \"outputs/\",\n    output_video_name: str | None = None,\n    num_videos_per_prompt: int = 1,\n    seed: int = 1024,\n    num_frames: int = 121,\n    num_frames_round_down: bool = False,\n    height: int = 704,\n    width: int = 1280,\n    fps: int = 24,\n    num_inference_steps: int = 50,\n    guidance_scale: float = 5.0,\n    guidance_rescale: float = 0.0,\n    boundary_ratio: float | None = None,\n    enable_teacache: bool = False,\n    save_video: bool = True,\n    return_frames: bool = False,\n    return_trajectory_latents: bool = False,\n    return_trajectory_decoded: bool = False,\n)\n</code></pre> <p>               Bases: <code>Wan2_2_Base_SamplingParam</code></p> <p>Sampling parameters for Wan2.2 TI2V 5B model.</p>"},{"location":"api/fastvideo/#fastvideo.configs.utils","title":"fastvideo.configs.utils","text":""},{"location":"api/fastvideo/#fastvideo.configs.utils-functions","title":"Functions","text":"fastvideo.configs.utils.clean_cli_args \u00b6 <pre><code>clean_cli_args(args: Namespace) -&gt; dict[str, Any]\n</code></pre> <p>Clean the arguments by removing the ones that not explicitly provided by the user.</p> Source code in <code>fastvideo/configs/utils.py</code> <pre><code>def clean_cli_args(args: argparse.Namespace) -&gt; dict[str, Any]:\n    \"\"\"\n    Clean the arguments by removing the ones that not explicitly provided by the user.\n    \"\"\"\n    provided_args = {}\n    for k, v in vars(args).items():\n        if (v is not None and hasattr(args, '_provided')\n                and k in args._provided):\n            provided_args[k] = v\n\n    return provided_args\n</code></pre> fastvideo.configs.utils.update_config_from_args \u00b6 <pre><code>update_config_from_args(\n    config: Any,\n    args_dict: dict[str, Any],\n    prefix: str = \"\",\n    pop_args: bool = False,\n) -&gt; bool\n</code></pre> <p>Update configuration object from arguments dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Any</code> <p>The configuration object to update</p> required <code>args_dict</code> <code>dict[str, Any]</code> <p>Dictionary containing arguments</p> required <code>prefix</code> <code>str</code> <p>Prefix for the configuration parameters in the args_dict.    If None, assumes direct attribute mapping without prefix.</p> <code>''</code> Source code in <code>fastvideo/configs/utils.py</code> <pre><code>def update_config_from_args(config: Any,\n                            args_dict: dict[str, Any],\n                            prefix: str = \"\",\n                            pop_args: bool = False) -&gt; bool:\n    \"\"\"\n    Update configuration object from arguments dictionary.\n\n    Args:\n        config: The configuration object to update\n        args_dict: Dictionary containing arguments\n        prefix: Prefix for the configuration parameters in the args_dict.\n               If None, assumes direct attribute mapping without prefix.\n    \"\"\"\n    # Handle top-level attributes (no prefix)\n    args_not_to_remove = [\n        'model_path',\n    ]\n    args_to_remove = []\n    if prefix.strip() == \"\":\n        for key, value in args_dict.items():\n            if hasattr(config, key) and value is not None:\n                if key == \"text_encoder_precisions\" and isinstance(value, list):\n                    setattr(config, key, tuple(value))\n                else:\n                    setattr(config, key, value)\n                if pop_args:\n                    args_to_remove.append(key)\n    else:\n        # Handle nested attributes with prefix\n        prefix_with_dot = f\"{prefix}.\"\n        for key, value in args_dict.items():\n            if key.startswith(prefix_with_dot) and value is not None:\n                attr_name = key[len(prefix_with_dot):]\n                if hasattr(config, attr_name):\n                    setattr(config, attr_name, value)\n                if pop_args:\n                    args_to_remove.append(key)\n\n    if pop_args:\n        for key in args_to_remove:\n            if key not in args_not_to_remove:\n                args_dict.pop(key)\n\n    return len(args_to_remove) &gt; 0\n</code></pre>"},{"location":"api/fastvideo/#submodules","title":"Submodules","text":""},{"location":"api/fastvideo/#fastvideoconfigspipelines","title":"fastvideo.configs.pipelines","text":""},{"location":"api/fastvideo/#fastvideo.configs.pipelines","title":"pipelines","text":""},{"location":"api/fastvideo/#fastvideo.configs.pipelines-classes","title":"Classes","text":""},{"location":"api/fastvideo/#fastvideo.configs.pipelines.FastHunyuanConfig","title":"fastvideo.configs.pipelines.FastHunyuanConfig  <code>dataclass</code>","text":"<pre><code>FastHunyuanConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: int = 6,\n    flow_shift: int = 17,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = HunyuanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = HunyuanVAEConfig(),\n    vae_precision: str = \"fp16\",\n    vae_tiling: bool = True,\n    vae_sp: bool = True,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (LlamaConfig(), CLIPTextConfig())\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp16\", \"fp16\")\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (\n        lambda: (\n            llama_preprocess_text,\n            clip_preprocess_text,\n        )\n    )(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], tensor], ...\n    ] = (\n        lambda: (\n            llama_postprocess_text,\n            clip_postprocess_text,\n        )\n    )(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n)\n</code></pre> <p>               Bases: <code>HunyuanConfig</code></p> <p>Configuration specifically optimized for FastHunyuan weights.</p>"},{"location":"api/fastvideo/#fastvideo.configs.pipelines.HunyuanConfig","title":"fastvideo.configs.pipelines.HunyuanConfig  <code>dataclass</code>","text":"<pre><code>HunyuanConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: int = 6,\n    flow_shift: int = 7,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = HunyuanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = HunyuanVAEConfig(),\n    vae_precision: str = \"fp16\",\n    vae_tiling: bool = True,\n    vae_sp: bool = True,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (LlamaConfig(), CLIPTextConfig())\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp16\", \"fp16\")\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (\n        lambda: (\n            llama_preprocess_text,\n            clip_preprocess_text,\n        )\n    )(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], tensor], ...\n    ] = (\n        lambda: (\n            llama_postprocess_text,\n            clip_postprocess_text,\n        )\n    )(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n)\n</code></pre> <p>               Bases: <code>PipelineConfig</code></p> <p>Base configuration for HunYuan pipeline architecture.</p>"},{"location":"api/fastvideo/#fastvideo.configs.pipelines.PipelineConfig","title":"fastvideo.configs.pipelines.PipelineConfig  <code>dataclass</code>","text":"<pre><code>PipelineConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = None,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = DiTConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = VAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = True,\n    vae_sp: bool = True,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (EncoderConfig(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], tensor], ...\n    ] = (lambda: (postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n)\n</code></pre> <p>Base configuration for all pipeline architectures.</p> Functions\u00b6 fastvideo.configs.pipelines.PipelineConfig.from_kwargs <code>classmethod</code> \u00b6 <pre><code>from_kwargs(\n    kwargs: dict[str, Any], config_cli_prefix: str = \"\"\n) -&gt; PipelineConfig\n</code></pre> <p>Load PipelineConfig from kwargs Dictionary. kwargs: dictionary of kwargs config_cli_prefix: prefix of CLI arguments for this PipelineConfig instance</p> Source code in <code>fastvideo/configs/pipelines/base.py</code> <pre><code>@classmethod\ndef from_kwargs(cls,\n                kwargs: dict[str, Any],\n                config_cli_prefix: str = \"\") -&gt; \"PipelineConfig\":\n    \"\"\"\n    Load PipelineConfig from kwargs Dictionary.\n    kwargs: dictionary of kwargs\n    config_cli_prefix: prefix of CLI arguments for this PipelineConfig instance\n    \"\"\"\n    from fastvideo.configs.pipelines.registry import (\n        get_pipeline_config_cls_from_name)\n\n    prefix_with_dot = f\"{config_cli_prefix}.\" if (config_cli_prefix.strip()\n                                                  != \"\") else \"\"\n    model_path: str | None = kwargs.get(prefix_with_dot + 'model_path',\n                                        None) or kwargs.get('model_path')\n    pipeline_config_or_path: str | PipelineConfig | dict[\n        str, Any] | None = kwargs.get(prefix_with_dot + 'pipeline_config',\n                                      None) or kwargs.get('pipeline_config')\n    if model_path is None:\n        raise ValueError(\"model_path is required in kwargs\")\n\n    # 1. Get the pipeline config class from the registry\n    pipeline_config_cls = get_pipeline_config_cls_from_name(model_path)\n\n    # 2. Instantiate PipelineConfig\n    if pipeline_config_cls is None:\n        logger.warning(\n            \"Couldn't find pipeline config for %s. Using the default pipeline config.\",\n            model_path)\n        pipeline_config = cls()\n    else:\n        pipeline_config = pipeline_config_cls()\n\n    # 3. Load PipelineConfig from a json file or a PipelineConfig object if provided\n    if isinstance(pipeline_config_or_path, str):\n        pipeline_config.load_from_json(pipeline_config_or_path)\n        kwargs[prefix_with_dot +\n               'pipeline_config_path'] = pipeline_config_or_path\n    elif isinstance(pipeline_config_or_path, PipelineConfig):\n        pipeline_config = pipeline_config_or_path\n    elif isinstance(pipeline_config_or_path, dict):\n        pipeline_config.update_pipeline_config(pipeline_config_or_path)\n\n    # 4. Update PipelineConfig from CLI arguments if provided\n    kwargs[prefix_with_dot + 'model_path'] = model_path\n    pipeline_config.update_config_from_dict(kwargs, config_cli_prefix)\n    return pipeline_config\n</code></pre> fastvideo.configs.pipelines.PipelineConfig.from_pretrained <code>classmethod</code> \u00b6 <pre><code>from_pretrained(model_path: str) -&gt; PipelineConfig\n</code></pre> <p>use the pipeline class setting from model_path to match the pipeline config</p> Source code in <code>fastvideo/configs/pipelines/base.py</code> <pre><code>@classmethod\ndef from_pretrained(cls, model_path: str) -&gt; \"PipelineConfig\":\n    \"\"\"\n    use the pipeline class setting from model_path to match the pipeline config\n    \"\"\"\n    from fastvideo.configs.pipelines.registry import (\n        get_pipeline_config_cls_from_name)\n    pipeline_config_cls = get_pipeline_config_cls_from_name(model_path)\n\n    return cast(PipelineConfig, pipeline_config_cls(model_path=model_path))\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.configs.pipelines.SlidingTileAttnConfig","title":"fastvideo.configs.pipelines.SlidingTileAttnConfig  <code>dataclass</code>","text":"<pre><code>SlidingTileAttnConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = None,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = DiTConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = VAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = True,\n    vae_sp: bool = True,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (EncoderConfig(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], tensor], ...\n    ] = (lambda: (postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    window_size: int = 16,\n    stride: int = 8,\n    height: int = 576,\n    width: int = 1024,\n    pad_to_square: bool = False,\n    use_overlap_optimization: bool = True,\n)\n</code></pre> <p>               Bases: <code>PipelineConfig</code></p> <p>Configuration for sliding tile attention.</p>"},{"location":"api/fastvideo/#fastvideo.configs.pipelines.StepVideoT2VConfig","title":"fastvideo.configs.pipelines.StepVideoT2VConfig  <code>dataclass</code>","text":"<pre><code>StepVideoT2VConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: int = 13,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = StepVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = StepVideoVAEConfig(),\n    vae_precision: str = \"bf16\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (EncoderConfig(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], tensor], ...\n    ] = (lambda: (postprocess_text,))(),\n    pos_magic: str = \"\u200b\u8d85\u9ad8\u200b\u6e05\u200b\u3001HDR \u200b\u89c6\u9891\u200b\u3001\u200b\u73af\u5883\u5149\u200b\u3001\u200b\u675c\u6bd4\u200b\u5168\u666f\u200b\u58f0\u200b\u3001\u200b\u753b\u9762\u200b\u7a33\u5b9a\u200b\u3001\u200b\u6d41\u7545\u200b\u52a8\u4f5c\u200b\u3001\u200b\u903c\u771f\u200b\u7684\u200b\u7ec6\u8282\u200b\u3001\u200b\u4e13\u4e1a\u7ea7\u200b\u6784\u56fe\u200b\u3001\u200b\u8d85\u73b0\u5b9e\u4e3b\u4e49\u200b\u3001\u200b\u81ea\u7136\u200b\u3001\u200b\u751f\u52a8\u200b\u3001\u200b\u8d85\u200b\u7ec6\u8282\u200b\u3001\u200b\u6e05\u6670\u200b\u3002\",\n    neg_magic: str = \"\u200b\u753b\u9762\u200b\u6697\u200b\u3001\u200b\u4f4e\u5206\u8fa8\u7387\u200b\u3001\u200b\u4e0d\u826f\u200b\u624b\u200b\u3001\u200b\u6587\u672c\u200b\u3001\u200b\u7f3a\u5c11\u200b\u624b\u6307\u200b\u3001\u200b\u591a\u4f59\u200b\u7684\u200b\u624b\u6307\u200b\u3001\u200b\u88c1\u526a\u200b\u3001\u200b\u4f4e\u8d28\u91cf\u200b\u3001\u200b\u9897\u7c92\u72b6\u200b\u3001\u200b\u7b7e\u540d\u200b\u3001\u200b\u6c34\u5370\u200b\u3001\u200b\u7528\u6237\u540d\u200b\u3001\u200b\u6a21\u7cca\u200b\u3002\",\n    timesteps_scale: bool = False,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n)\n</code></pre> <p>               Bases: <code>PipelineConfig</code></p> <p>Base configuration for StepVideo pipeline architecture.</p>"},{"location":"api/fastvideo/#fastvideo.configs.pipelines.WanI2V480PConfig","title":"fastvideo.configs.pipelines.WanI2V480PConfig  <code>dataclass</code>","text":"<pre><code>WanI2V480PConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = 3.0,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = WanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = WanVAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = CLIPVisionConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (T5Config(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], Tensor], ...\n    ] = (lambda: (t5_postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n    warp_denoising_step: bool = True,\n)\n</code></pre> <p>               Bases: <code>WanT2V480PConfig</code></p> <p>Base configuration for Wan I2V 14B 480P pipeline architecture.</p>"},{"location":"api/fastvideo/#fastvideo.configs.pipelines.WanI2V720PConfig","title":"fastvideo.configs.pipelines.WanI2V720PConfig  <code>dataclass</code>","text":"<pre><code>WanI2V720PConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = 5.0,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = WanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = WanVAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = CLIPVisionConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (T5Config(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], Tensor], ...\n    ] = (lambda: (t5_postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n    warp_denoising_step: bool = True,\n)\n</code></pre> <p>               Bases: <code>WanI2V480PConfig</code></p> <p>Base configuration for Wan I2V 14B 720P pipeline architecture.</p>"},{"location":"api/fastvideo/#fastvideo.configs.pipelines.WanT2V480PConfig","title":"fastvideo.configs.pipelines.WanT2V480PConfig  <code>dataclass</code>","text":"<pre><code>WanT2V480PConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = 3.0,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = WanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = WanVAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (T5Config(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], Tensor], ...\n    ] = (lambda: (t5_postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n    warp_denoising_step: bool = True,\n)\n</code></pre> <p>               Bases: <code>PipelineConfig</code></p> <p>Base configuration for Wan T2V 1.3B pipeline architecture.</p>"},{"location":"api/fastvideo/#fastvideo.configs.pipelines.WanT2V720PConfig","title":"fastvideo.configs.pipelines.WanT2V720PConfig  <code>dataclass</code>","text":"<pre><code>WanT2V720PConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = 5.0,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = WanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = WanVAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (T5Config(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], Tensor], ...\n    ] = (lambda: (t5_postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n    warp_denoising_step: bool = True,\n)\n</code></pre> <p>               Bases: <code>WanT2V480PConfig</code></p> <p>Base configuration for Wan T2V 14B 720P pipeline architecture.</p>"},{"location":"api/fastvideo/#fastvideo.configs.pipelines-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideo.configs.pipelines.get_pipeline_config_cls_from_name","title":"fastvideo.configs.pipelines.get_pipeline_config_cls_from_name","text":"<pre><code>get_pipeline_config_cls_from_name(\n    pipeline_name_or_path: str,\n) -&gt; type[PipelineConfig]\n</code></pre> <p>Get the appropriate configuration class for a given pipeline name or path.</p> <p>This function implements a multi-step lookup process to find the most suitable configuration class for a given pipeline. It follows this order: 1. Exact match in the PIPE_NAME_TO_CONFIG 2. Partial match in the PIPE_NAME_TO_CONFIG 3. Fallback to class name in the model_index.json 4. else raise an error</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_name_or_path</code> <code>str</code> <p>The name or path of the pipeline. This can be: - A registered model ID (e.g., \"FastVideo/FastHunyuan-diffusers\") - A local path to a model directory - A model ID that will be downloaded</p> required <p>Returns:</p> Type Description <code>type[PipelineConfig]</code> <p>Type[PipelineConfig]: The configuration class that best matches the pipeline. This will be one of: - A specific weight configuration class if an exact match is found - A fallback configuration class based on the pipeline architecture - The base PipelineConfig class if no matches are found</p> Note <ul> <li>For local paths, the function will verify the model configuration</li> <li>For remote models, it will attempt to download the model index</li> <li>Warning messages are logged when falling back to less specific configurations</li> </ul> Source code in <code>fastvideo/configs/pipelines/registry.py</code> <pre><code>def get_pipeline_config_cls_from_name(\n        pipeline_name_or_path: str) -&gt; type[PipelineConfig]:\n    \"\"\"Get the appropriate configuration class for a given pipeline name or path.\n\n    This function implements a multi-step lookup process to find the most suitable\n    configuration class for a given pipeline. It follows this order:\n    1. Exact match in the PIPE_NAME_TO_CONFIG\n    2. Partial match in the PIPE_NAME_TO_CONFIG\n    3. Fallback to class name in the model_index.json\n    4. else raise an error\n\n    Args:\n        pipeline_name_or_path (str): The name or path of the pipeline. This can be:\n            - A registered model ID (e.g., \"FastVideo/FastHunyuan-diffusers\")\n            - A local path to a model directory\n            - A model ID that will be downloaded\n\n    Returns:\n        Type[PipelineConfig]: The configuration class that best matches the pipeline.\n            This will be one of:\n            - A specific weight configuration class if an exact match is found\n            - A fallback configuration class based on the pipeline architecture\n            - The base PipelineConfig class if no matches are found\n\n    Note:\n        - For local paths, the function will verify the model configuration\n        - For remote models, it will attempt to download the model index\n        - Warning messages are logged when falling back to less specific configurations\n    \"\"\"\n\n    pipeline_config_cls: type[PipelineConfig] | None = None\n\n    # First try exact match for specific weights\n    if pipeline_name_or_path in PIPE_NAME_TO_CONFIG:\n        pipeline_config_cls = PIPE_NAME_TO_CONFIG[pipeline_name_or_path]\n\n    # Try partial matches (for local paths that might include the weight ID)\n    for registered_id, config_class in PIPE_NAME_TO_CONFIG.items():\n        if registered_id in pipeline_name_or_path:\n            pipeline_config_cls = config_class\n            break\n\n    # If no match, try to use the fallback config\n    if pipeline_config_cls is None:\n        if os.path.exists(pipeline_name_or_path):\n            config = verify_model_config_and_directory(pipeline_name_or_path)\n        else:\n            config = maybe_download_model_index(pipeline_name_or_path)\n        logger.warning(\n            \"Trying to use the config from the model_index.json. FastVideo may not correctly identify the optimal config for this model in this situation.\"\n        )\n\n        pipeline_name = config[\"_class_name\"]\n        # Try to determine pipeline architecture for fallback\n        for pipeline_type, detector in PIPELINE_DETECTOR.items():\n            if detector(pipeline_name.lower()):\n                pipeline_config_cls = PIPELINE_FALLBACK_CONFIG.get(\n                    pipeline_type)\n                break\n\n        if pipeline_config_cls is not None:\n            logger.warning(\n                \"No match found for pipeline %s, using fallback config %s.\",\n                pipeline_name_or_path, pipeline_config_cls)\n\n    if pipeline_config_cls is None:\n        raise ValueError(\n            f\"No match found for pipeline {pipeline_name_or_path}, please check the pipeline name or path.\"\n        )\n\n    return pipeline_config_cls\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.configs.pipelines-modules","title":"Modules","text":""},{"location":"api/fastvideo/#fastvideo.configs.pipelines.base","title":"fastvideo.configs.pipelines.base","text":"Classes\u00b6 fastvideo.configs.pipelines.base.PipelineConfig <code>dataclass</code> \u00b6 <pre><code>PipelineConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = None,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = DiTConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = VAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = True,\n    vae_sp: bool = True,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (EncoderConfig(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], tensor], ...\n    ] = (lambda: (postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n)\n</code></pre> <p>Base configuration for all pipeline architectures.</p> Functions\u00b6 fastvideo.configs.pipelines.base.PipelineConfig.from_kwargs <code>classmethod</code> \u00b6 <pre><code>from_kwargs(\n    kwargs: dict[str, Any], config_cli_prefix: str = \"\"\n) -&gt; PipelineConfig\n</code></pre> <p>Load PipelineConfig from kwargs Dictionary. kwargs: dictionary of kwargs config_cli_prefix: prefix of CLI arguments for this PipelineConfig instance</p> Source code in <code>fastvideo/configs/pipelines/base.py</code> <pre><code>@classmethod\ndef from_kwargs(cls,\n                kwargs: dict[str, Any],\n                config_cli_prefix: str = \"\") -&gt; \"PipelineConfig\":\n    \"\"\"\n    Load PipelineConfig from kwargs Dictionary.\n    kwargs: dictionary of kwargs\n    config_cli_prefix: prefix of CLI arguments for this PipelineConfig instance\n    \"\"\"\n    from fastvideo.configs.pipelines.registry import (\n        get_pipeline_config_cls_from_name)\n\n    prefix_with_dot = f\"{config_cli_prefix}.\" if (config_cli_prefix.strip()\n                                                  != \"\") else \"\"\n    model_path: str | None = kwargs.get(prefix_with_dot + 'model_path',\n                                        None) or kwargs.get('model_path')\n    pipeline_config_or_path: str | PipelineConfig | dict[\n        str, Any] | None = kwargs.get(prefix_with_dot + 'pipeline_config',\n                                      None) or kwargs.get('pipeline_config')\n    if model_path is None:\n        raise ValueError(\"model_path is required in kwargs\")\n\n    # 1. Get the pipeline config class from the registry\n    pipeline_config_cls = get_pipeline_config_cls_from_name(model_path)\n\n    # 2. Instantiate PipelineConfig\n    if pipeline_config_cls is None:\n        logger.warning(\n            \"Couldn't find pipeline config for %s. Using the default pipeline config.\",\n            model_path)\n        pipeline_config = cls()\n    else:\n        pipeline_config = pipeline_config_cls()\n\n    # 3. Load PipelineConfig from a json file or a PipelineConfig object if provided\n    if isinstance(pipeline_config_or_path, str):\n        pipeline_config.load_from_json(pipeline_config_or_path)\n        kwargs[prefix_with_dot +\n               'pipeline_config_path'] = pipeline_config_or_path\n    elif isinstance(pipeline_config_or_path, PipelineConfig):\n        pipeline_config = pipeline_config_or_path\n    elif isinstance(pipeline_config_or_path, dict):\n        pipeline_config.update_pipeline_config(pipeline_config_or_path)\n\n    # 4. Update PipelineConfig from CLI arguments if provided\n    kwargs[prefix_with_dot + 'model_path'] = model_path\n    pipeline_config.update_config_from_dict(kwargs, config_cli_prefix)\n    return pipeline_config\n</code></pre> fastvideo.configs.pipelines.base.PipelineConfig.from_pretrained <code>classmethod</code> \u00b6 <pre><code>from_pretrained(model_path: str) -&gt; PipelineConfig\n</code></pre> <p>use the pipeline class setting from model_path to match the pipeline config</p> Source code in <code>fastvideo/configs/pipelines/base.py</code> <pre><code>@classmethod\ndef from_pretrained(cls, model_path: str) -&gt; \"PipelineConfig\":\n    \"\"\"\n    use the pipeline class setting from model_path to match the pipeline config\n    \"\"\"\n    from fastvideo.configs.pipelines.registry import (\n        get_pipeline_config_cls_from_name)\n    pipeline_config_cls = get_pipeline_config_cls_from_name(model_path)\n\n    return cast(PipelineConfig, pipeline_config_cls(model_path=model_path))\n</code></pre> fastvideo.configs.pipelines.base.STA_Mode \u00b6 <p>               Bases: <code>str</code>, <code>Enum</code></p> <p>STA (Sliding Tile Attention) modes.</p> fastvideo.configs.pipelines.base.SlidingTileAttnConfig <code>dataclass</code> \u00b6 <pre><code>SlidingTileAttnConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = None,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = DiTConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = VAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = True,\n    vae_sp: bool = True,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (EncoderConfig(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], tensor], ...\n    ] = (lambda: (postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    window_size: int = 16,\n    stride: int = 8,\n    height: int = 576,\n    width: int = 1024,\n    pad_to_square: bool = False,\n    use_overlap_optimization: bool = True,\n)\n</code></pre> <p>               Bases: <code>PipelineConfig</code></p> <p>Configuration for sliding tile attention.</p> Functions\u00b6 fastvideo.configs.pipelines.base.parse_int_list \u00b6 <pre><code>parse_int_list(value: str) -&gt; list[int]\n</code></pre> <p>Parse a comma-separated string of integers into a list.</p> Source code in <code>fastvideo/configs/pipelines/base.py</code> <pre><code>def parse_int_list(value: str) -&gt; list[int]:\n    \"\"\"Parse a comma-separated string of integers into a list.\"\"\"\n    if not value:\n        return []\n    return [int(x.strip()) for x in value.split(\",\")]\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.configs.pipelines.hunyuan","title":"fastvideo.configs.pipelines.hunyuan","text":"Classes\u00b6 fastvideo.configs.pipelines.hunyuan.FastHunyuanConfig <code>dataclass</code> \u00b6 <pre><code>FastHunyuanConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: int = 6,\n    flow_shift: int = 17,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = HunyuanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = HunyuanVAEConfig(),\n    vae_precision: str = \"fp16\",\n    vae_tiling: bool = True,\n    vae_sp: bool = True,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (LlamaConfig(), CLIPTextConfig())\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp16\", \"fp16\")\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (\n        lambda: (\n            llama_preprocess_text,\n            clip_preprocess_text,\n        )\n    )(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], tensor], ...\n    ] = (\n        lambda: (\n            llama_postprocess_text,\n            clip_postprocess_text,\n        )\n    )(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n)\n</code></pre> <p>               Bases: <code>HunyuanConfig</code></p> <p>Configuration specifically optimized for FastHunyuan weights.</p> fastvideo.configs.pipelines.hunyuan.HunyuanConfig <code>dataclass</code> \u00b6 <pre><code>HunyuanConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: int = 6,\n    flow_shift: int = 7,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = HunyuanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = HunyuanVAEConfig(),\n    vae_precision: str = \"fp16\",\n    vae_tiling: bool = True,\n    vae_sp: bool = True,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (LlamaConfig(), CLIPTextConfig())\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp16\", \"fp16\")\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (\n        lambda: (\n            llama_preprocess_text,\n            clip_preprocess_text,\n        )\n    )(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], tensor], ...\n    ] = (\n        lambda: (\n            llama_postprocess_text,\n            clip_postprocess_text,\n        )\n    )(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n)\n</code></pre> <p>               Bases: <code>PipelineConfig</code></p> <p>Base configuration for HunYuan pipeline architecture.</p>"},{"location":"api/fastvideo/#fastvideo.configs.pipelines.registry","title":"fastvideo.configs.pipelines.registry","text":"<p>Registry for pipeline weight-specific configurations.</p> Classes\u00b6 Functions\u00b6 fastvideo.configs.pipelines.registry.get_pipeline_config_cls_from_name \u00b6 <pre><code>get_pipeline_config_cls_from_name(\n    pipeline_name_or_path: str,\n) -&gt; type[PipelineConfig]\n</code></pre> <p>Get the appropriate configuration class for a given pipeline name or path.</p> <p>This function implements a multi-step lookup process to find the most suitable configuration class for a given pipeline. It follows this order: 1. Exact match in the PIPE_NAME_TO_CONFIG 2. Partial match in the PIPE_NAME_TO_CONFIG 3. Fallback to class name in the model_index.json 4. else raise an error</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_name_or_path</code> <code>str</code> <p>The name or path of the pipeline. This can be: - A registered model ID (e.g., \"FastVideo/FastHunyuan-diffusers\") - A local path to a model directory - A model ID that will be downloaded</p> required <p>Returns:</p> Type Description <code>type[PipelineConfig]</code> <p>Type[PipelineConfig]: The configuration class that best matches the pipeline. This will be one of: - A specific weight configuration class if an exact match is found - A fallback configuration class based on the pipeline architecture - The base PipelineConfig class if no matches are found</p> Note <ul> <li>For local paths, the function will verify the model configuration</li> <li>For remote models, it will attempt to download the model index</li> <li>Warning messages are logged when falling back to less specific configurations</li> </ul> Source code in <code>fastvideo/configs/pipelines/registry.py</code> <pre><code>def get_pipeline_config_cls_from_name(\n        pipeline_name_or_path: str) -&gt; type[PipelineConfig]:\n    \"\"\"Get the appropriate configuration class for a given pipeline name or path.\n\n    This function implements a multi-step lookup process to find the most suitable\n    configuration class for a given pipeline. It follows this order:\n    1. Exact match in the PIPE_NAME_TO_CONFIG\n    2. Partial match in the PIPE_NAME_TO_CONFIG\n    3. Fallback to class name in the model_index.json\n    4. else raise an error\n\n    Args:\n        pipeline_name_or_path (str): The name or path of the pipeline. This can be:\n            - A registered model ID (e.g., \"FastVideo/FastHunyuan-diffusers\")\n            - A local path to a model directory\n            - A model ID that will be downloaded\n\n    Returns:\n        Type[PipelineConfig]: The configuration class that best matches the pipeline.\n            This will be one of:\n            - A specific weight configuration class if an exact match is found\n            - A fallback configuration class based on the pipeline architecture\n            - The base PipelineConfig class if no matches are found\n\n    Note:\n        - For local paths, the function will verify the model configuration\n        - For remote models, it will attempt to download the model index\n        - Warning messages are logged when falling back to less specific configurations\n    \"\"\"\n\n    pipeline_config_cls: type[PipelineConfig] | None = None\n\n    # First try exact match for specific weights\n    if pipeline_name_or_path in PIPE_NAME_TO_CONFIG:\n        pipeline_config_cls = PIPE_NAME_TO_CONFIG[pipeline_name_or_path]\n\n    # Try partial matches (for local paths that might include the weight ID)\n    for registered_id, config_class in PIPE_NAME_TO_CONFIG.items():\n        if registered_id in pipeline_name_or_path:\n            pipeline_config_cls = config_class\n            break\n\n    # If no match, try to use the fallback config\n    if pipeline_config_cls is None:\n        if os.path.exists(pipeline_name_or_path):\n            config = verify_model_config_and_directory(pipeline_name_or_path)\n        else:\n            config = maybe_download_model_index(pipeline_name_or_path)\n        logger.warning(\n            \"Trying to use the config from the model_index.json. FastVideo may not correctly identify the optimal config for this model in this situation.\"\n        )\n\n        pipeline_name = config[\"_class_name\"]\n        # Try to determine pipeline architecture for fallback\n        for pipeline_type, detector in PIPELINE_DETECTOR.items():\n            if detector(pipeline_name.lower()):\n                pipeline_config_cls = PIPELINE_FALLBACK_CONFIG.get(\n                    pipeline_type)\n                break\n\n        if pipeline_config_cls is not None:\n            logger.warning(\n                \"No match found for pipeline %s, using fallback config %s.\",\n                pipeline_name_or_path, pipeline_config_cls)\n\n    if pipeline_config_cls is None:\n        raise ValueError(\n            f\"No match found for pipeline {pipeline_name_or_path}, please check the pipeline name or path.\"\n        )\n\n    return pipeline_config_cls\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.configs.pipelines.stepvideo","title":"fastvideo.configs.pipelines.stepvideo","text":"Classes\u00b6 fastvideo.configs.pipelines.stepvideo.StepVideoT2VConfig <code>dataclass</code> \u00b6 <pre><code>StepVideoT2VConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: int = 13,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = StepVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = StepVideoVAEConfig(),\n    vae_precision: str = \"bf16\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (EncoderConfig(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], tensor], ...\n    ] = (lambda: (postprocess_text,))(),\n    pos_magic: str = \"\u200b\u8d85\u9ad8\u200b\u6e05\u200b\u3001HDR \u200b\u89c6\u9891\u200b\u3001\u200b\u73af\u5883\u5149\u200b\u3001\u200b\u675c\u6bd4\u200b\u5168\u666f\u200b\u58f0\u200b\u3001\u200b\u753b\u9762\u200b\u7a33\u5b9a\u200b\u3001\u200b\u6d41\u7545\u200b\u52a8\u4f5c\u200b\u3001\u200b\u903c\u771f\u200b\u7684\u200b\u7ec6\u8282\u200b\u3001\u200b\u4e13\u4e1a\u7ea7\u200b\u6784\u56fe\u200b\u3001\u200b\u8d85\u73b0\u5b9e\u4e3b\u4e49\u200b\u3001\u200b\u81ea\u7136\u200b\u3001\u200b\u751f\u52a8\u200b\u3001\u200b\u8d85\u200b\u7ec6\u8282\u200b\u3001\u200b\u6e05\u6670\u200b\u3002\",\n    neg_magic: str = \"\u200b\u753b\u9762\u200b\u6697\u200b\u3001\u200b\u4f4e\u5206\u8fa8\u7387\u200b\u3001\u200b\u4e0d\u826f\u200b\u624b\u200b\u3001\u200b\u6587\u672c\u200b\u3001\u200b\u7f3a\u5c11\u200b\u624b\u6307\u200b\u3001\u200b\u591a\u4f59\u200b\u7684\u200b\u624b\u6307\u200b\u3001\u200b\u88c1\u526a\u200b\u3001\u200b\u4f4e\u8d28\u91cf\u200b\u3001\u200b\u9897\u7c92\u72b6\u200b\u3001\u200b\u7b7e\u540d\u200b\u3001\u200b\u6c34\u5370\u200b\u3001\u200b\u7528\u6237\u540d\u200b\u3001\u200b\u6a21\u7cca\u200b\u3002\",\n    timesteps_scale: bool = False,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n)\n</code></pre> <p>               Bases: <code>PipelineConfig</code></p> <p>Base configuration for StepVideo pipeline architecture.</p>"},{"location":"api/fastvideo/#fastvideo.configs.pipelines.wan","title":"fastvideo.configs.pipelines.wan","text":"Classes\u00b6 fastvideo.configs.pipelines.wan.FastWan2_1_T2V_480P_Config <code>dataclass</code> \u00b6 <pre><code>FastWan2_1_T2V_480P_Config(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = 8.0,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = WanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = WanVAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (T5Config(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], Tensor], ...\n    ] = (lambda: (t5_postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int]\n    | None = (lambda: [1000, 757, 522])(),\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n    warp_denoising_step: bool = True,\n)\n</code></pre> <p>               Bases: <code>WanT2V480PConfig</code></p> <p>Base configuration for FastWan T2V 1.3B 480P pipeline architecture with DMD</p> fastvideo.configs.pipelines.wan.WANV2VConfig <code>dataclass</code> \u00b6 <pre><code>WANV2VConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = 3.0,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = WanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = WanVAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = WAN2_1ControlCLIPVisionConfig(),\n    image_encoder_precision: str = \"bf16\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (T5Config(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], Tensor], ...\n    ] = (lambda: (t5_postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n    warp_denoising_step: bool = True,\n)\n</code></pre> <p>               Bases: <code>WanI2V480PConfig</code></p> <p>Configuration for WAN2.1 1.3B Control pipeline.</p> fastvideo.configs.pipelines.wan.WanI2V480PConfig <code>dataclass</code> \u00b6 <pre><code>WanI2V480PConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = 3.0,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = WanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = WanVAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = CLIPVisionConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (T5Config(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], Tensor], ...\n    ] = (lambda: (t5_postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n    warp_denoising_step: bool = True,\n)\n</code></pre> <p>               Bases: <code>WanT2V480PConfig</code></p> <p>Base configuration for Wan I2V 14B 480P pipeline architecture.</p> fastvideo.configs.pipelines.wan.WanI2V720PConfig <code>dataclass</code> \u00b6 <pre><code>WanI2V720PConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = 5.0,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = WanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = WanVAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = CLIPVisionConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (T5Config(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], Tensor], ...\n    ] = (lambda: (t5_postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n    warp_denoising_step: bool = True,\n)\n</code></pre> <p>               Bases: <code>WanI2V480PConfig</code></p> <p>Base configuration for Wan I2V 14B 720P pipeline architecture.</p> fastvideo.configs.pipelines.wan.WanT2V480PConfig <code>dataclass</code> \u00b6 <pre><code>WanT2V480PConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = 3.0,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = WanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = WanVAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (T5Config(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], Tensor], ...\n    ] = (lambda: (t5_postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n    warp_denoising_step: bool = True,\n)\n</code></pre> <p>               Bases: <code>PipelineConfig</code></p> <p>Base configuration for Wan T2V 1.3B pipeline architecture.</p> fastvideo.configs.pipelines.wan.WanT2V720PConfig <code>dataclass</code> \u00b6 <pre><code>WanT2V720PConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = 5.0,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = WanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = WanVAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (T5Config(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], Tensor], ...\n    ] = (lambda: (t5_postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n    warp_denoising_step: bool = True,\n)\n</code></pre> <p>               Bases: <code>WanT2V480PConfig</code></p> <p>Base configuration for Wan T2V 14B 720P pipeline architecture.</p>"},{"location":"api/fastvideo/#fastvideoconfigsmodels","title":"fastvideo.configs.models","text":""},{"location":"api/fastvideo/#fastvideo.configs.models","title":"models","text":""},{"location":"api/fastvideo/#fastvideo.configs.models-classes","title":"Classes","text":""},{"location":"api/fastvideo/#fastvideo.configs.models.DiTConfig","title":"fastvideo.configs.models.DiTConfig  <code>dataclass</code>","text":"<pre><code>DiTConfig(\n    arch_config: DiTArchConfig = DiTArchConfig(),\n    prefix: str = \"\",\n    quant_config: QuantizationConfig | None = None,\n)\n</code></pre> <p>               Bases: <code>ModelConfig</code></p> Functions\u00b6 fastvideo.configs.models.DiTConfig.add_cli_args <code>staticmethod</code> \u00b6 <pre><code>add_cli_args(\n    parser: Any, prefix: str = \"dit-config\"\n) -&gt; Any\n</code></pre> <p>Add CLI arguments for DiTConfig fields</p> Source code in <code>fastvideo/configs/models/dits/base.py</code> <pre><code>@staticmethod\ndef add_cli_args(parser: Any, prefix: str = \"dit-config\") -&gt; Any:\n    \"\"\"Add CLI arguments for DiTConfig fields\"\"\"\n    parser.add_argument(\n        f\"--{prefix}.prefix\",\n        type=str,\n        dest=f\"{prefix.replace('-', '_')}.prefix\",\n        default=DiTConfig.prefix,\n        help=\"Prefix for the DiT model\",\n    )\n\n    parser.add_argument(\n        f\"--{prefix}.quant-config\",\n        type=str,\n        dest=f\"{prefix.replace('-', '_')}.quant_config\",\n        default=None,\n        help=\"Quantization configuration for the DiT model\",\n    )\n\n    return parser\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.configs.models.VAEConfig","title":"fastvideo.configs.models.VAEConfig  <code>dataclass</code>","text":"<pre><code>VAEConfig(\n    arch_config: VAEArchConfig = VAEArchConfig(),\n    load_encoder: bool = True,\n    load_decoder: bool = True,\n    tile_sample_min_height: int = 256,\n    tile_sample_min_width: int = 256,\n    tile_sample_min_num_frames: int = 16,\n    tile_sample_stride_height: int = 192,\n    tile_sample_stride_width: int = 192,\n    tile_sample_stride_num_frames: int = 12,\n    blend_num_frames: int = 0,\n    use_tiling: bool = True,\n    use_temporal_tiling: bool = True,\n    use_parallel_tiling: bool = True,\n    use_temporal_scaling_frames: bool = True,\n)\n</code></pre> <p>               Bases: <code>ModelConfig</code></p> Functions\u00b6 fastvideo.configs.models.VAEConfig.add_cli_args <code>staticmethod</code> \u00b6 <pre><code>add_cli_args(\n    parser: Any, prefix: str = \"vae-config\"\n) -&gt; Any\n</code></pre> <p>Add CLI arguments for VAEConfig fields</p> Source code in <code>fastvideo/configs/models/vaes/base.py</code> <pre><code>@staticmethod\ndef add_cli_args(parser: Any, prefix: str = \"vae-config\") -&gt; Any:\n    \"\"\"Add CLI arguments for VAEConfig fields\"\"\"\n    parser.add_argument(\n        f\"--{prefix}.load-encoder\",\n        action=StoreBoolean,\n        dest=f\"{prefix.replace('-', '_')}.load_encoder\",\n        default=VAEConfig.load_encoder,\n        help=\"Whether to load the VAE encoder\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.load-decoder\",\n        action=StoreBoolean,\n        dest=f\"{prefix.replace('-', '_')}.load_decoder\",\n        default=VAEConfig.load_decoder,\n        help=\"Whether to load the VAE decoder\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-min-height\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_min_height\",\n        default=VAEConfig.tile_sample_min_height,\n        help=\"Minimum height for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-min-width\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_min_width\",\n        default=VAEConfig.tile_sample_min_width,\n        help=\"Minimum width for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-min-num-frames\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_min_num_frames\",\n        default=VAEConfig.tile_sample_min_num_frames,\n        help=\"Minimum number of frames for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-stride-height\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_stride_height\",\n        default=VAEConfig.tile_sample_stride_height,\n        help=\"Stride height for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-stride-width\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_stride_width\",\n        default=VAEConfig.tile_sample_stride_width,\n        help=\"Stride width for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-stride-num-frames\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_stride_num_frames\",\n        default=VAEConfig.tile_sample_stride_num_frames,\n        help=\"Stride number of frames for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.blend-num-frames\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.blend_num_frames\",\n        default=VAEConfig.blend_num_frames,\n        help=\"Number of frames to blend for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.use-tiling\",\n        action=StoreBoolean,\n        dest=f\"{prefix.replace('-', '_')}.use_tiling\",\n        default=VAEConfig.use_tiling,\n        help=\"Whether to use tiling for VAE\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.use-temporal-tiling\",\n        action=StoreBoolean,\n        dest=f\"{prefix.replace('-', '_')}.use_temporal_tiling\",\n        default=VAEConfig.use_temporal_tiling,\n        help=\"Whether to use temporal tiling for VAE\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.use-parallel-tiling\",\n        action=StoreBoolean,\n        dest=f\"{prefix.replace('-', '_')}.use_parallel_tiling\",\n        default=VAEConfig.use_parallel_tiling,\n        help=\"Whether to use parallel tiling for VAE\",\n    )\n\n    return parser\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.configs.models-modules","title":"Modules","text":""},{"location":"api/fastvideo/#fastvideo.configs.models.dits","title":"fastvideo.configs.models.dits","text":"Modules\u00b6 fastvideo.configs.models.dits.base \u00b6 Classes\u00b6 fastvideo.configs.models.dits.base.DiTConfig <code>dataclass</code> \u00b6 <pre><code>DiTConfig(\n    arch_config: DiTArchConfig = DiTArchConfig(),\n    prefix: str = \"\",\n    quant_config: QuantizationConfig | None = None,\n)\n</code></pre> <p>               Bases: <code>ModelConfig</code></p> Functions\u00b6 fastvideo.configs.models.dits.base.DiTConfig.add_cli_args <code>staticmethod</code> \u00b6 <pre><code>add_cli_args(\n    parser: Any, prefix: str = \"dit-config\"\n) -&gt; Any\n</code></pre> <p>Add CLI arguments for DiTConfig fields</p> Source code in <code>fastvideo/configs/models/dits/base.py</code> <pre><code>@staticmethod\ndef add_cli_args(parser: Any, prefix: str = \"dit-config\") -&gt; Any:\n    \"\"\"Add CLI arguments for DiTConfig fields\"\"\"\n    parser.add_argument(\n        f\"--{prefix}.prefix\",\n        type=str,\n        dest=f\"{prefix.replace('-', '_')}.prefix\",\n        default=DiTConfig.prefix,\n        help=\"Prefix for the DiT model\",\n    )\n\n    parser.add_argument(\n        f\"--{prefix}.quant-config\",\n        type=str,\n        dest=f\"{prefix.replace('-', '_')}.quant_config\",\n        default=None,\n        help=\"Quantization configuration for the DiT model\",\n    )\n\n    return parser\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.configs.models.vaes","title":"fastvideo.configs.models.vaes","text":"Modules\u00b6 fastvideo.configs.models.vaes.base \u00b6 Classes\u00b6 fastvideo.configs.models.vaes.base.VAEConfig <code>dataclass</code> \u00b6 <pre><code>VAEConfig(\n    arch_config: VAEArchConfig = VAEArchConfig(),\n    load_encoder: bool = True,\n    load_decoder: bool = True,\n    tile_sample_min_height: int = 256,\n    tile_sample_min_width: int = 256,\n    tile_sample_min_num_frames: int = 16,\n    tile_sample_stride_height: int = 192,\n    tile_sample_stride_width: int = 192,\n    tile_sample_stride_num_frames: int = 12,\n    blend_num_frames: int = 0,\n    use_tiling: bool = True,\n    use_temporal_tiling: bool = True,\n    use_parallel_tiling: bool = True,\n    use_temporal_scaling_frames: bool = True,\n)\n</code></pre> <p>               Bases: <code>ModelConfig</code></p> Functions\u00b6 fastvideo.configs.models.vaes.base.VAEConfig.add_cli_args <code>staticmethod</code> \u00b6 <pre><code>add_cli_args(\n    parser: Any, prefix: str = \"vae-config\"\n) -&gt; Any\n</code></pre> <p>Add CLI arguments for VAEConfig fields</p> Source code in <code>fastvideo/configs/models/vaes/base.py</code> <pre><code>@staticmethod\ndef add_cli_args(parser: Any, prefix: str = \"vae-config\") -&gt; Any:\n    \"\"\"Add CLI arguments for VAEConfig fields\"\"\"\n    parser.add_argument(\n        f\"--{prefix}.load-encoder\",\n        action=StoreBoolean,\n        dest=f\"{prefix.replace('-', '_')}.load_encoder\",\n        default=VAEConfig.load_encoder,\n        help=\"Whether to load the VAE encoder\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.load-decoder\",\n        action=StoreBoolean,\n        dest=f\"{prefix.replace('-', '_')}.load_decoder\",\n        default=VAEConfig.load_decoder,\n        help=\"Whether to load the VAE decoder\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-min-height\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_min_height\",\n        default=VAEConfig.tile_sample_min_height,\n        help=\"Minimum height for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-min-width\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_min_width\",\n        default=VAEConfig.tile_sample_min_width,\n        help=\"Minimum width for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-min-num-frames\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_min_num_frames\",\n        default=VAEConfig.tile_sample_min_num_frames,\n        help=\"Minimum number of frames for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-stride-height\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_stride_height\",\n        default=VAEConfig.tile_sample_stride_height,\n        help=\"Stride height for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-stride-width\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_stride_width\",\n        default=VAEConfig.tile_sample_stride_width,\n        help=\"Stride width for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-stride-num-frames\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_stride_num_frames\",\n        default=VAEConfig.tile_sample_stride_num_frames,\n        help=\"Stride number of frames for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.blend-num-frames\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.blend_num_frames\",\n        default=VAEConfig.blend_num_frames,\n        help=\"Number of frames to blend for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.use-tiling\",\n        action=StoreBoolean,\n        dest=f\"{prefix.replace('-', '_')}.use_tiling\",\n        default=VAEConfig.use_tiling,\n        help=\"Whether to use tiling for VAE\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.use-temporal-tiling\",\n        action=StoreBoolean,\n        dest=f\"{prefix.replace('-', '_')}.use_temporal_tiling\",\n        default=VAEConfig.use_temporal_tiling,\n        help=\"Whether to use temporal tiling for VAE\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.use-parallel-tiling\",\n        action=StoreBoolean,\n        dest=f\"{prefix.replace('-', '_')}.use_parallel_tiling\",\n        default=VAEConfig.use_parallel_tiling,\n        help=\"Whether to use parallel tiling for VAE\",\n    )\n\n    return parser\n</code></pre>"},{"location":"api/fastvideo/#fastvideoconfigssample","title":"fastvideo.configs.sample","text":""},{"location":"api/fastvideo/#fastvideo.configs.sample","title":"sample","text":""},{"location":"api/fastvideo/#fastvideo.configs.sample-classes","title":"Classes","text":""},{"location":"api/fastvideo/#fastvideo.configs.sample.SamplingParam","title":"fastvideo.configs.sample.SamplingParam  <code>dataclass</code>","text":"<pre><code>SamplingParam(\n    data_type: str = \"video\",\n    image_path: str | None = None,\n    video_path: str | None = None,\n    prompt: str | list[str] | None = None,\n    negative_prompt: str = \"Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\",\n    prompt_path: str | None = None,\n    output_path: str = \"outputs/\",\n    output_video_name: str | None = None,\n    num_videos_per_prompt: int = 1,\n    seed: int = 1024,\n    num_frames: int = 125,\n    num_frames_round_down: bool = False,\n    height: int = 720,\n    width: int = 1280,\n    fps: int = 24,\n    num_inference_steps: int = 50,\n    guidance_scale: float = 1.0,\n    guidance_rescale: float = 0.0,\n    boundary_ratio: float | None = None,\n    enable_teacache: bool = False,\n    save_video: bool = True,\n    return_frames: bool = False,\n    return_trajectory_latents: bool = False,\n    return_trajectory_decoded: bool = False,\n)\n</code></pre> <p>Sampling parameters for video generation.</p> Functions\u00b6 fastvideo.configs.sample.SamplingParam.add_cli_args <code>staticmethod</code> \u00b6 <pre><code>add_cli_args(parser: Any) -&gt; Any\n</code></pre> <p>Add CLI arguments for SamplingParam fields</p> Source code in <code>fastvideo/configs/sample/base.py</code> <pre><code>@staticmethod\ndef add_cli_args(parser: Any) -&gt; Any:\n    \"\"\"Add CLI arguments for SamplingParam fields\"\"\"\n    parser.add_argument(\n        \"--prompt\",\n        type=str,\n        default=SamplingParam.prompt,\n        help=\"Text prompt for video generation\",\n    )\n    parser.add_argument(\n        \"--negative-prompt\",\n        type=str,\n        default=SamplingParam.negative_prompt,\n        help=\"Negative text prompt for video generation\",\n    )\n    parser.add_argument(\n        \"--prompt-path\",\n        type=str,\n        default=SamplingParam.prompt_path,\n        help=\"Path to a text file containing the prompt\",\n    )\n    parser.add_argument(\n        \"--output-path\",\n        type=str,\n        default=SamplingParam.output_path,\n        help=\"Path to save the generated video\",\n    )\n    parser.add_argument(\n        \"--output-video-name\",\n        type=str,\n        default=SamplingParam.output_video_name,\n        help=\"Name of the output video\",\n    )\n    parser.add_argument(\n        \"--num-videos-per-prompt\",\n        type=int,\n        default=SamplingParam.num_videos_per_prompt,\n        help=\"Number of videos to generate per prompt\",\n    )\n    parser.add_argument(\n        \"--seed\",\n        type=int,\n        default=SamplingParam.seed,\n        help=\"Random seed for generation\",\n    )\n    parser.add_argument(\n        \"--num-frames\",\n        type=int,\n        default=SamplingParam.num_frames,\n        help=\"Number of frames to generate\",\n    )\n    parser.add_argument(\n        \"--height\",\n        type=int,\n        default=SamplingParam.height,\n        help=\"Height of generated video\",\n    )\n    parser.add_argument(\n        \"--width\",\n        type=int,\n        default=SamplingParam.width,\n        help=\"Width of generated video\",\n    )\n    parser.add_argument(\n        \"--fps\",\n        type=int,\n        default=SamplingParam.fps,\n        help=\"Frames per second for saved video\",\n    )\n    parser.add_argument(\n        \"--num-inference-steps\",\n        type=int,\n        default=SamplingParam.num_inference_steps,\n        help=\"Number of denoising steps\",\n    )\n    parser.add_argument(\n        \"--guidance-scale\",\n        type=float,\n        default=SamplingParam.guidance_scale,\n        help=\"Classifier-free guidance scale\",\n    )\n    parser.add_argument(\n        \"--guidance-rescale\",\n        type=float,\n        default=SamplingParam.guidance_rescale,\n        help=\"Guidance rescale factor\",\n    )\n    parser.add_argument(\n        \"--boundary-ratio\",\n        type=float,\n        default=SamplingParam.boundary_ratio,\n        help=\"Boundary timestep ratio\",\n    )\n    parser.add_argument(\n        \"--save-video\",\n        action=\"store_true\",\n        default=SamplingParam.save_video,\n        help=\"Whether to save the video to disk\",\n    )\n    parser.add_argument(\n        \"--no-save-video\",\n        action=\"store_false\",\n        dest=\"save_video\",\n        help=\"Don't save the video to disk\",\n    )\n    parser.add_argument(\n        \"--return-frames\",\n        action=\"store_true\",\n        default=SamplingParam.return_frames,\n        help=\"Whether to return the raw frames\",\n    )\n    parser.add_argument(\n        \"--image-path\",\n        type=str,\n        default=SamplingParam.image_path,\n        help=\"Path to input image for image-to-video generation\",\n    )\n    parser.add_argument(\n        \"--video_path\",\n        type=str,\n        default=SamplingParam.video_path,\n        help=\"Path to input video for video-to-video generation\",\n    )\n    parser.add_argument(\n        \"--moba-config-path\",\n        type=str,\n        default=None,\n        help=\n        \"Path to a JSON file containing V-MoBA specific configurations.\",\n    )\n    parser.add_argument(\n        \"--return-trajectory-latents\",\n        action=\"store_true\",\n        default=SamplingParam.return_trajectory_latents,\n        help=\"Whether to return the trajectory\",\n    )\n    parser.add_argument(\n        \"--return-trajectory-decoded\",\n        action=\"store_true\",\n        default=SamplingParam.return_trajectory_decoded,\n        help=\"Whether to return the decoded trajectory\",\n    )\n    return parser\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.configs.sample-modules","title":"Modules","text":""},{"location":"api/fastvideo/#fastvideo.configs.sample.base","title":"fastvideo.configs.sample.base","text":"Classes\u00b6 fastvideo.configs.sample.base.SamplingParam <code>dataclass</code> \u00b6 <pre><code>SamplingParam(\n    data_type: str = \"video\",\n    image_path: str | None = None,\n    video_path: str | None = None,\n    prompt: str | list[str] | None = None,\n    negative_prompt: str = \"Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\",\n    prompt_path: str | None = None,\n    output_path: str = \"outputs/\",\n    output_video_name: str | None = None,\n    num_videos_per_prompt: int = 1,\n    seed: int = 1024,\n    num_frames: int = 125,\n    num_frames_round_down: bool = False,\n    height: int = 720,\n    width: int = 1280,\n    fps: int = 24,\n    num_inference_steps: int = 50,\n    guidance_scale: float = 1.0,\n    guidance_rescale: float = 0.0,\n    boundary_ratio: float | None = None,\n    enable_teacache: bool = False,\n    save_video: bool = True,\n    return_frames: bool = False,\n    return_trajectory_latents: bool = False,\n    return_trajectory_decoded: bool = False,\n)\n</code></pre> <p>Sampling parameters for video generation.</p> Functions\u00b6 fastvideo.configs.sample.base.SamplingParam.add_cli_args <code>staticmethod</code> \u00b6 <pre><code>add_cli_args(parser: Any) -&gt; Any\n</code></pre> <p>Add CLI arguments for SamplingParam fields</p> Source code in <code>fastvideo/configs/sample/base.py</code> <pre><code>@staticmethod\ndef add_cli_args(parser: Any) -&gt; Any:\n    \"\"\"Add CLI arguments for SamplingParam fields\"\"\"\n    parser.add_argument(\n        \"--prompt\",\n        type=str,\n        default=SamplingParam.prompt,\n        help=\"Text prompt for video generation\",\n    )\n    parser.add_argument(\n        \"--negative-prompt\",\n        type=str,\n        default=SamplingParam.negative_prompt,\n        help=\"Negative text prompt for video generation\",\n    )\n    parser.add_argument(\n        \"--prompt-path\",\n        type=str,\n        default=SamplingParam.prompt_path,\n        help=\"Path to a text file containing the prompt\",\n    )\n    parser.add_argument(\n        \"--output-path\",\n        type=str,\n        default=SamplingParam.output_path,\n        help=\"Path to save the generated video\",\n    )\n    parser.add_argument(\n        \"--output-video-name\",\n        type=str,\n        default=SamplingParam.output_video_name,\n        help=\"Name of the output video\",\n    )\n    parser.add_argument(\n        \"--num-videos-per-prompt\",\n        type=int,\n        default=SamplingParam.num_videos_per_prompt,\n        help=\"Number of videos to generate per prompt\",\n    )\n    parser.add_argument(\n        \"--seed\",\n        type=int,\n        default=SamplingParam.seed,\n        help=\"Random seed for generation\",\n    )\n    parser.add_argument(\n        \"--num-frames\",\n        type=int,\n        default=SamplingParam.num_frames,\n        help=\"Number of frames to generate\",\n    )\n    parser.add_argument(\n        \"--height\",\n        type=int,\n        default=SamplingParam.height,\n        help=\"Height of generated video\",\n    )\n    parser.add_argument(\n        \"--width\",\n        type=int,\n        default=SamplingParam.width,\n        help=\"Width of generated video\",\n    )\n    parser.add_argument(\n        \"--fps\",\n        type=int,\n        default=SamplingParam.fps,\n        help=\"Frames per second for saved video\",\n    )\n    parser.add_argument(\n        \"--num-inference-steps\",\n        type=int,\n        default=SamplingParam.num_inference_steps,\n        help=\"Number of denoising steps\",\n    )\n    parser.add_argument(\n        \"--guidance-scale\",\n        type=float,\n        default=SamplingParam.guidance_scale,\n        help=\"Classifier-free guidance scale\",\n    )\n    parser.add_argument(\n        \"--guidance-rescale\",\n        type=float,\n        default=SamplingParam.guidance_rescale,\n        help=\"Guidance rescale factor\",\n    )\n    parser.add_argument(\n        \"--boundary-ratio\",\n        type=float,\n        default=SamplingParam.boundary_ratio,\n        help=\"Boundary timestep ratio\",\n    )\n    parser.add_argument(\n        \"--save-video\",\n        action=\"store_true\",\n        default=SamplingParam.save_video,\n        help=\"Whether to save the video to disk\",\n    )\n    parser.add_argument(\n        \"--no-save-video\",\n        action=\"store_false\",\n        dest=\"save_video\",\n        help=\"Don't save the video to disk\",\n    )\n    parser.add_argument(\n        \"--return-frames\",\n        action=\"store_true\",\n        default=SamplingParam.return_frames,\n        help=\"Whether to return the raw frames\",\n    )\n    parser.add_argument(\n        \"--image-path\",\n        type=str,\n        default=SamplingParam.image_path,\n        help=\"Path to input image for image-to-video generation\",\n    )\n    parser.add_argument(\n        \"--video_path\",\n        type=str,\n        default=SamplingParam.video_path,\n        help=\"Path to input video for video-to-video generation\",\n    )\n    parser.add_argument(\n        \"--moba-config-path\",\n        type=str,\n        default=None,\n        help=\n        \"Path to a JSON file containing V-MoBA specific configurations.\",\n    )\n    parser.add_argument(\n        \"--return-trajectory-latents\",\n        action=\"store_true\",\n        default=SamplingParam.return_trajectory_latents,\n        help=\"Whether to return the trajectory\",\n    )\n    parser.add_argument(\n        \"--return-trajectory-decoded\",\n        action=\"store_true\",\n        default=SamplingParam.return_trajectory_decoded,\n        help=\"Whether to return the decoded trajectory\",\n    )\n    return parser\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/#fastvideo.configs.sample.registry","title":"fastvideo.configs.sample.registry","text":"Classes\u00b6 Functions\u00b6 fastvideo.configs.sample.registry.get_sampling_param_cls_for_name \u00b6 <pre><code>get_sampling_param_cls_for_name(\n    pipeline_name_or_path: str,\n) -&gt; Any | None\n</code></pre> <p>Get the appropriate sampling param for specific pretrained weights.</p> Source code in <code>fastvideo/configs/sample/registry.py</code> <pre><code>def get_sampling_param_cls_for_name(pipeline_name_or_path: str) -&gt; Any | None:\n    \"\"\"Get the appropriate sampling param for specific pretrained weights.\"\"\"\n\n    if os.path.exists(pipeline_name_or_path):\n        config = verify_model_config_and_directory(pipeline_name_or_path)\n        logger.warning(\n            \"FastVideo may not correctly identify the optimal sampling param for this model, as the local directory may have been renamed.\"\n        )\n    else:\n        config = maybe_download_model_index(pipeline_name_or_path)\n\n    pipeline_name = config[\"_class_name\"]\n\n    # First try exact match for specific weights\n    if pipeline_name_or_path in SAMPLING_PARAM_REGISTRY:\n        return SAMPLING_PARAM_REGISTRY[pipeline_name_or_path]\n\n    # Try partial matches (for local paths that might include the weight ID)\n    for registered_id, config_class in SAMPLING_PARAM_REGISTRY.items():\n        if registered_id in pipeline_name_or_path:\n            return config_class\n\n    # If no match, try to use the fallback config\n    fallback_config = None\n    # Try to determine pipeline architecture for fallback\n    for pipeline_type, detector in SAMPLING_PARAM_DETECTOR.items():\n        if detector(pipeline_name.lower()):\n            fallback_config = SAMPLING_FALLBACK_PARAM.get(pipeline_type)\n            break\n\n    logger.warning(\n        \"No match found for pipeline %s, using fallback sampling param %s.\",\n        pipeline_name_or_path, fallback_config)\n    return fallback_config\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.configs.sample.wan","title":"fastvideo.configs.sample.wan","text":"Classes\u00b6 fastvideo.configs.sample.wan.Wan2_1_Fun_1_3B_InP_SamplingParam <code>dataclass</code> \u00b6 <pre><code>Wan2_1_Fun_1_3B_InP_SamplingParam(\n    data_type: str = \"video\",\n    image_path: str | None = None,\n    video_path: str | None = None,\n    prompt: str | list[str] | None = None,\n    negative_prompt: str\n    | None = \"\u200b\u8272\u8c03\u200b\u8273\u4e3d\u200b\uff0c\u200b\u8fc7\u200b\u66dd\u200b\uff0c\u200b\u9759\u6001\u200b\uff0c\u200b\u7ec6\u8282\u200b\u6a21\u7cca\u4e0d\u6e05\u200b\uff0c\u200b\u5b57\u5e55\u200b\uff0c\u200b\u98ce\u683c\u200b\uff0c\u200b\u4f5c\u54c1\u200b\uff0c\u200b\u753b\u4f5c\u200b\uff0c\u200b\u753b\u9762\u200b\uff0c\u200b\u9759\u6b62\u200b\uff0c\u200b\u6574\u4f53\u200b\u53d1\u7070\u200b\uff0c\u200b\u6700\u5dee\u200b\u8d28\u91cf\u200b\uff0c\u200b\u4f4e\u8d28\u91cf\u200b\uff0cJPEG\u200b\u538b\u7f29\u200b\u6b8b\u7559\u200b\uff0c\u200b\u4e11\u964b\u200b\u7684\u200b\uff0c\u200b\u6b8b\u7f3a\u200b\u7684\u200b\uff0c\u200b\u591a\u4f59\u200b\u7684\u200b\u624b\u6307\u200b\uff0c\u200b\u753b\u5f97\u200b\u4e0d\u597d\u200b\u7684\u200b\u624b\u90e8\u200b\uff0c\u200b\u753b\u5f97\u200b\u4e0d\u597d\u200b\u7684\u200b\u8138\u90e8\u200b\uff0c\u200b\u7578\u5f62\u200b\u7684\u200b\uff0c\u200b\u6bc1\u5bb9\u200b\u7684\u200b\uff0c\u200b\u5f62\u6001\u200b\u7578\u5f62\u200b\u7684\u200b\u80a2\u4f53\u200b\uff0c\u200b\u624b\u6307\u200b\u878d\u5408\u200b\uff0c\u200b\u9759\u6b62\u4e0d\u52a8\u200b\u7684\u200b\u753b\u9762\u200b\uff0c\u200b\u6742\u4e71\u200b\u7684\u200b\u80cc\u666f\u200b\uff0c\u200b\u4e09\u6761\u200b\u817f\u200b\uff0c\u200b\u80cc\u666f\u200b\u4eba\u200b\u5f88\u591a\u200b\uff0c\u200b\u5012\u200b\u7740\u200b\u8d70\u200b\",\n    prompt_path: str | None = None,\n    output_path: str = \"outputs/\",\n    output_video_name: str | None = None,\n    num_videos_per_prompt: int = 1,\n    seed: int = 1024,\n    num_frames: int = 81,\n    num_frames_round_down: bool = False,\n    height: int = 480,\n    width: int = 832,\n    fps: int = 16,\n    num_inference_steps: int = 50,\n    guidance_scale: float = 6.0,\n    guidance_rescale: float = 0.0,\n    boundary_ratio: float | None = None,\n    enable_teacache: bool = False,\n    save_video: bool = True,\n    return_frames: bool = False,\n    return_trajectory_latents: bool = False,\n    return_trajectory_decoded: bool = False,\n)\n</code></pre> <p>               Bases: <code>SamplingParam</code></p> <p>Sampling parameters for Wan2.1 Fun 1.3B InP model.</p> fastvideo.configs.sample.wan.Wan2_2_Base_SamplingParam <code>dataclass</code> \u00b6 <pre><code>Wan2_2_Base_SamplingParam(\n    data_type: str = \"video\",\n    image_path: str | None = None,\n    video_path: str | None = None,\n    prompt: str | list[str] | None = None,\n    negative_prompt: str\n    | None = \"\u200b\u8272\u8c03\u200b\u8273\u4e3d\u200b\uff0c\u200b\u8fc7\u200b\u66dd\u200b\uff0c\u200b\u9759\u6001\u200b\uff0c\u200b\u7ec6\u8282\u200b\u6a21\u7cca\u4e0d\u6e05\u200b\uff0c\u200b\u5b57\u5e55\u200b\uff0c\u200b\u98ce\u683c\u200b\uff0c\u200b\u4f5c\u54c1\u200b\uff0c\u200b\u753b\u4f5c\u200b\uff0c\u200b\u753b\u9762\u200b\uff0c\u200b\u9759\u6b62\u200b\uff0c\u200b\u6574\u4f53\u200b\u53d1\u7070\u200b\uff0c\u200b\u6700\u5dee\u200b\u8d28\u91cf\u200b\uff0c\u200b\u4f4e\u8d28\u91cf\u200b\uff0cJPEG\u200b\u538b\u7f29\u200b\u6b8b\u7559\u200b\uff0c\u200b\u4e11\u964b\u200b\u7684\u200b\uff0c\u200b\u6b8b\u7f3a\u200b\u7684\u200b\uff0c\u200b\u591a\u4f59\u200b\u7684\u200b\u624b\u6307\u200b\uff0c\u200b\u753b\u5f97\u200b\u4e0d\u597d\u200b\u7684\u200b\u624b\u90e8\u200b\uff0c\u200b\u753b\u5f97\u200b\u4e0d\u597d\u200b\u7684\u200b\u8138\u90e8\u200b\uff0c\u200b\u7578\u5f62\u200b\u7684\u200b\uff0c\u200b\u6bc1\u5bb9\u200b\u7684\u200b\uff0c\u200b\u5f62\u6001\u200b\u7578\u5f62\u200b\u7684\u200b\u80a2\u4f53\u200b\uff0c\u200b\u624b\u6307\u200b\u878d\u5408\u200b\uff0c\u200b\u9759\u6b62\u4e0d\u52a8\u200b\u7684\u200b\u753b\u9762\u200b\uff0c\u200b\u6742\u4e71\u200b\u7684\u200b\u80cc\u666f\u200b\uff0c\u200b\u4e09\u6761\u200b\u817f\u200b\uff0c\u200b\u80cc\u666f\u200b\u4eba\u200b\u5f88\u591a\u200b\uff0c\u200b\u5012\u200b\u7740\u200b\u8d70\u200b\",\n    prompt_path: str | None = None,\n    output_path: str = \"outputs/\",\n    output_video_name: str | None = None,\n    num_videos_per_prompt: int = 1,\n    seed: int = 1024,\n    num_frames: int = 125,\n    num_frames_round_down: bool = False,\n    height: int = 720,\n    width: int = 1280,\n    fps: int = 24,\n    num_inference_steps: int = 50,\n    guidance_scale: float = 1.0,\n    guidance_rescale: float = 0.0,\n    boundary_ratio: float | None = None,\n    enable_teacache: bool = False,\n    save_video: bool = True,\n    return_frames: bool = False,\n    return_trajectory_latents: bool = False,\n    return_trajectory_decoded: bool = False,\n)\n</code></pre> <p>               Bases: <code>SamplingParam</code></p> <p>Sampling parameters for Wan2.2 TI2V 5B model.</p> fastvideo.configs.sample.wan.Wan2_2_TI2V_5B_SamplingParam <code>dataclass</code> \u00b6 <pre><code>Wan2_2_TI2V_5B_SamplingParam(\n    data_type: str = \"video\",\n    image_path: str | None = None,\n    video_path: str | None = None,\n    prompt: str | list[str] | None = None,\n    negative_prompt: str\n    | None = \"\u200b\u8272\u8c03\u200b\u8273\u4e3d\u200b\uff0c\u200b\u8fc7\u200b\u66dd\u200b\uff0c\u200b\u9759\u6001\u200b\uff0c\u200b\u7ec6\u8282\u200b\u6a21\u7cca\u4e0d\u6e05\u200b\uff0c\u200b\u5b57\u5e55\u200b\uff0c\u200b\u98ce\u683c\u200b\uff0c\u200b\u4f5c\u54c1\u200b\uff0c\u200b\u753b\u4f5c\u200b\uff0c\u200b\u753b\u9762\u200b\uff0c\u200b\u9759\u6b62\u200b\uff0c\u200b\u6574\u4f53\u200b\u53d1\u7070\u200b\uff0c\u200b\u6700\u5dee\u200b\u8d28\u91cf\u200b\uff0c\u200b\u4f4e\u8d28\u91cf\u200b\uff0cJPEG\u200b\u538b\u7f29\u200b\u6b8b\u7559\u200b\uff0c\u200b\u4e11\u964b\u200b\u7684\u200b\uff0c\u200b\u6b8b\u7f3a\u200b\u7684\u200b\uff0c\u200b\u591a\u4f59\u200b\u7684\u200b\u624b\u6307\u200b\uff0c\u200b\u753b\u5f97\u200b\u4e0d\u597d\u200b\u7684\u200b\u624b\u90e8\u200b\uff0c\u200b\u753b\u5f97\u200b\u4e0d\u597d\u200b\u7684\u200b\u8138\u90e8\u200b\uff0c\u200b\u7578\u5f62\u200b\u7684\u200b\uff0c\u200b\u6bc1\u5bb9\u200b\u7684\u200b\uff0c\u200b\u5f62\u6001\u200b\u7578\u5f62\u200b\u7684\u200b\u80a2\u4f53\u200b\uff0c\u200b\u624b\u6307\u200b\u878d\u5408\u200b\uff0c\u200b\u9759\u6b62\u4e0d\u52a8\u200b\u7684\u200b\u753b\u9762\u200b\uff0c\u200b\u6742\u4e71\u200b\u7684\u200b\u80cc\u666f\u200b\uff0c\u200b\u4e09\u6761\u200b\u817f\u200b\uff0c\u200b\u80cc\u666f\u200b\u4eba\u200b\u5f88\u591a\u200b\uff0c\u200b\u5012\u200b\u7740\u200b\u8d70\u200b\",\n    prompt_path: str | None = None,\n    output_path: str = \"outputs/\",\n    output_video_name: str | None = None,\n    num_videos_per_prompt: int = 1,\n    seed: int = 1024,\n    num_frames: int = 121,\n    num_frames_round_down: bool = False,\n    height: int = 704,\n    width: int = 1280,\n    fps: int = 24,\n    num_inference_steps: int = 50,\n    guidance_scale: float = 5.0,\n    guidance_rescale: float = 0.0,\n    boundary_ratio: float | None = None,\n    enable_teacache: bool = False,\n    save_video: bool = True,\n    return_frames: bool = False,\n    return_trajectory_latents: bool = False,\n    return_trajectory_decoded: bool = False,\n)\n</code></pre> <p>               Bases: <code>Wan2_2_Base_SamplingParam</code></p> <p>Sampling parameters for Wan2.2 TI2V 5B model.</p>"},{"location":"api/fastvideo/#fastvideodistributed","title":"fastvideo.distributed","text":""},{"location":"api/fastvideo/#fastvideo.distributed","title":"distributed","text":""},{"location":"api/fastvideo/#fastvideo.distributed-classes","title":"Classes","text":""},{"location":"api/fastvideo/#fastvideo.distributed.StatelessProcessGroup","title":"fastvideo.distributed.StatelessProcessGroup  <code>dataclass</code>","text":"<pre><code>StatelessProcessGroup(\n    rank: int,\n    world_size: int,\n    store: Store,\n    data_expiration_seconds: int = 3600,\n    send_dst_counter: dict[int, int] = dict(),\n    recv_src_counter: dict[int, int] = dict(),\n    broadcast_send_counter: int = 0,\n    broadcast_recv_src_counter: dict[int, int] = dict(),\n    entries: deque[tuple[str, float]] = deque(),\n)\n</code></pre> <p>A dataclass to hold a metadata store, and the rank, world_size of the group. Only use it to communicate metadata between processes. For data-plane communication, create NCCL-related objects.</p>"},{"location":"api/fastvideo/#fastvideo.distributed.StatelessProcessGroup-functions","title":"Functions","text":"fastvideo.distributed.StatelessProcessGroup.all_gather_obj \u00b6 <pre><code>all_gather_obj(obj: Any) -&gt; list[Any]\n</code></pre> <p>All gather an object from all ranks.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def all_gather_obj(self, obj: Any) -&gt; list[Any]:\n    \"\"\"All gather an object from all ranks.\"\"\"\n    gathered_objs = []\n    for i in range(self.world_size):\n        if i == self.rank:\n            gathered_objs.append(obj)\n            self.broadcast_obj(obj, src=self.rank)\n        else:\n            recv_obj = self.broadcast_obj(None, src=i)\n            gathered_objs.append(recv_obj)\n    return gathered_objs\n</code></pre> fastvideo.distributed.StatelessProcessGroup.barrier \u00b6 <pre><code>barrier()\n</code></pre> <p>A barrier to synchronize all ranks.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def barrier(self):\n    \"\"\"A barrier to synchronize all ranks.\"\"\"\n    for i in range(self.world_size):\n        if i == self.rank:\n            self.broadcast_obj(None, src=self.rank)\n        else:\n            self.broadcast_obj(None, src=i)\n</code></pre> fastvideo.distributed.StatelessProcessGroup.broadcast_obj \u00b6 <pre><code>broadcast_obj(obj: Any | None, src: int) -&gt; Any\n</code></pre> <p>Broadcast an object from a source rank to all other ranks. It does not clean up after all ranks have received the object. Use it for limited times, e.g., for initialization.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def broadcast_obj(self, obj: Any | None, src: int) -&gt; Any:\n    \"\"\"Broadcast an object from a source rank to all other ranks.\n    It does not clean up after all ranks have received the object.\n    Use it for limited times, e.g., for initialization.\n    \"\"\"\n    if self.rank == src:\n        self.expire_data()\n        key = (f\"broadcast_from/{src}/\"\n               f\"{self.broadcast_send_counter}\")\n        self.store.set(key, pickle.dumps(obj))\n        self.broadcast_send_counter += 1\n        self.entries.append((key, time.perf_counter()))\n        return obj\n    else:\n        key = (f\"broadcast_from/{src}/\"\n               f\"{self.broadcast_recv_src_counter[src]}\")\n        recv_obj = pickle.loads(self.store.get(key))\n        self.broadcast_recv_src_counter[src] += 1\n        return recv_obj\n</code></pre> fastvideo.distributed.StatelessProcessGroup.create <code>staticmethod</code> \u00b6 <pre><code>create(\n    host: str,\n    port: int,\n    rank: int,\n    world_size: int,\n    data_expiration_seconds: int = 3600,\n) -&gt; StatelessProcessGroup\n</code></pre> <p>A replacement for <code>torch.distributed.init_process_group</code> that does not pollute the global state.</p> <p>If we have process A and process B called <code>torch.distributed.init_process_group</code> to form a group, and then we want to form another group with process A, B, C, D, it is not possible in PyTorch, because process A and process B have already formed a group, and process C and process D cannot join that group. This function is a workaround for this issue.</p> <p><code>torch.distributed.init_process_group</code> is a global call, while this function is a stateless call. It will return a <code>StatelessProcessGroup</code> object that can be used for exchanging metadata. With this function, process A and process B can call <code>StatelessProcessGroup.create</code> to form a group, and then process A, B, C, and D can call <code>StatelessProcessGroup.create</code> to form another group.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>@staticmethod\ndef create(\n    host: str,\n    port: int,\n    rank: int,\n    world_size: int,\n    data_expiration_seconds: int = 3600,\n) -&gt; \"StatelessProcessGroup\":\n    \"\"\"A replacement for `torch.distributed.init_process_group` that does not\n    pollute the global state.\n\n    If we have process A and process B called `torch.distributed.init_process_group`\n    to form a group, and then we want to form another group with process A, B, C,\n    D, it is not possible in PyTorch, because process A and process B have already\n    formed a group, and process C and process D cannot join that group. This\n    function is a workaround for this issue.\n\n    `torch.distributed.init_process_group` is a global call, while this function\n    is a stateless call. It will return a `StatelessProcessGroup` object that can be\n    used for exchanging metadata. With this function, process A and process B\n    can call `StatelessProcessGroup.create` to form a group, and then process A, B,\n    C, and D can call `StatelessProcessGroup.create` to form another group.\n    \"\"\" # noqa\n    store = TCPStore(\n        host_name=host,\n        port=port,\n        world_size=world_size,\n        is_master=(rank == 0),\n    )\n\n    return StatelessProcessGroup(\n        rank=rank,\n        world_size=world_size,\n        store=store,\n        data_expiration_seconds=data_expiration_seconds)\n</code></pre> fastvideo.distributed.StatelessProcessGroup.expire_data \u00b6 <pre><code>expire_data() -&gt; None\n</code></pre> <p>Expire data that is older than <code>data_expiration_seconds</code> seconds.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def expire_data(self) -&gt; None:\n    \"\"\"Expire data that is older than `data_expiration_seconds` seconds.\"\"\"\n    while self.entries:\n        # check the oldest entry\n        key, timestamp = self.entries[0]\n        if time.perf_counter() - timestamp &gt; self.data_expiration_seconds:\n            self.store.delete_key(key)\n            self.entries.popleft()\n        else:\n            break\n</code></pre> fastvideo.distributed.StatelessProcessGroup.recv_obj \u00b6 <pre><code>recv_obj(src: int) -&gt; Any\n</code></pre> <p>Receive an object from a source rank.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def recv_obj(self, src: int) -&gt; Any:\n    \"\"\"Receive an object from a source rank.\"\"\"\n    obj = pickle.loads(\n        self.store.get(f\"send_to/{self.rank}/{self.recv_src_counter[src]}\"))\n    self.recv_src_counter[src] += 1\n    return obj\n</code></pre> fastvideo.distributed.StatelessProcessGroup.send_obj \u00b6 <pre><code>send_obj(obj: Any, dst: int)\n</code></pre> <p>Send an object to a destination rank.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def send_obj(self, obj: Any, dst: int):\n    \"\"\"Send an object to a destination rank.\"\"\"\n    self.expire_data()\n    key = f\"send_to/{dst}/{self.send_dst_counter[dst]}\"\n    self.store.set(key, pickle.dumps(obj))\n    self.send_dst_counter[dst] += 1\n    self.entries.append((key, time.perf_counter()))\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.distributed-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideo.distributed.divide","title":"fastvideo.distributed.divide","text":"<pre><code>divide(numerator: int, denominator: int) -&gt; int\n</code></pre> <p>Ensure that numerator is divisible by the denominator and return the division value.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def divide(numerator: int, denominator: int) -&gt; int:\n    \"\"\"Ensure that numerator is divisible by the denominator and return\n    the division value.\"\"\"\n    ensure_divisibility(numerator, denominator)\n    return numerator // denominator\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.distributed.ensure_divisibility","title":"fastvideo.distributed.ensure_divisibility","text":"<pre><code>ensure_divisibility(numerator, denominator) -&gt; None\n</code></pre> <p>Ensure that numerator is divisible by the denominator.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def ensure_divisibility(numerator, denominator) -&gt; None:\n    \"\"\"Ensure that numerator is divisible by the denominator.\"\"\"\n    assert numerator % denominator == 0, \"{} is not divisible by {}\".format(\n        numerator, denominator)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.distributed.get_dp_rank","title":"fastvideo.distributed.get_dp_rank","text":"<pre><code>get_dp_rank() -&gt; int\n</code></pre> <p>Return my rank for the data parallel group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_dp_rank() -&gt; int:\n    \"\"\"Return my rank for the data parallel group.\"\"\"\n    return get_dp_group().rank_in_group\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.distributed.get_dp_world_size","title":"fastvideo.distributed.get_dp_world_size","text":"<pre><code>get_dp_world_size() -&gt; int\n</code></pre> <p>Return world size for the data parallel group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_dp_world_size() -&gt; int:\n    \"\"\"Return world size for the data parallel group.\"\"\"\n    return get_dp_group().world_size\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.distributed.get_local_torch_device","title":"fastvideo.distributed.get_local_torch_device","text":"<pre><code>get_local_torch_device() -&gt; torch.device\n</code></pre> <p>Return the torch device for the current rank.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_local_torch_device() -&gt; torch.device:\n    \"\"\"Return the torch device for the current rank.\"\"\"\n    from fastvideo.platforms import current_platform\n    if current_platform.is_npu():\n        device = torch.device(f\"npu:{envs.LOCAL_RANK}\")\n    elif current_platform.is_cuda_alike() or current_platform.is_cuda():\n        device = torch.device(f\"cuda:{envs.LOCAL_RANK}\")\n    else:\n        device = torch.device(\"mps\")\n    return device\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.distributed.get_sp_parallel_rank","title":"fastvideo.distributed.get_sp_parallel_rank","text":"<pre><code>get_sp_parallel_rank() -&gt; int\n</code></pre> <p>Return my rank for the sequence model parallel group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_sp_parallel_rank() -&gt; int:\n    \"\"\"Return my rank for the sequence model parallel group.\"\"\"\n    return get_sp_group().rank_in_group\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.distributed.get_sp_world_size","title":"fastvideo.distributed.get_sp_world_size","text":"<pre><code>get_sp_world_size() -&gt; int\n</code></pre> <p>Return world size for the sequence model parallel group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_sp_world_size() -&gt; int:\n    \"\"\"Return world size for the sequence model parallel group.\"\"\"\n    return get_sp_group().world_size\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.distributed.get_tp_rank","title":"fastvideo.distributed.get_tp_rank","text":"<pre><code>get_tp_rank() -&gt; int\n</code></pre> <p>Return my rank for the tensor model parallel group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_tp_rank() -&gt; int:\n    \"\"\"Return my rank for the tensor model parallel group.\"\"\"\n    return get_tp_group().rank_in_group\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.distributed.get_tp_world_size","title":"fastvideo.distributed.get_tp_world_size","text":"<pre><code>get_tp_world_size() -&gt; int\n</code></pre> <p>Return world size for the tensor model parallel group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_tp_world_size() -&gt; int:\n    \"\"\"Return world size for the tensor model parallel group.\"\"\"\n    return get_tp_group().world_size\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.distributed.get_world_rank","title":"fastvideo.distributed.get_world_rank","text":"<pre><code>get_world_rank() -&gt; int\n</code></pre> <p>Return my rank for the world group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_world_rank() -&gt; int:\n    \"\"\"Return my rank for the world group.\"\"\"\n    return get_world_group().rank\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.distributed.get_world_size","title":"fastvideo.distributed.get_world_size","text":"<pre><code>get_world_size() -&gt; int\n</code></pre> <p>Return world size for the world group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_world_size() -&gt; int:\n    \"\"\"Return world size for the world group.\"\"\"\n    return get_world_group().world_size\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.distributed.init_logger","title":"fastvideo.distributed.init_logger","text":"<pre><code>init_logger(name: str) -&gt; _FastvideoLogger\n</code></pre> <p>The main purpose of this function is to ensure that loggers are retrieved in such a way that we can be sure the root fastvideo logger has already been configured.</p> Source code in <code>fastvideo/logger.py</code> <pre><code>def init_logger(name: str) -&gt; _FastvideoLogger:\n    \"\"\"The main purpose of this function is to ensure that loggers are\n    retrieved in such a way that we can be sure the root fastvideo logger has\n    already been configured.\"\"\"\n\n    logger = logging.getLogger(name)\n\n    methods_to_patch = {\n        \"info_once\": _print_info_once,\n        \"warning_once\": _print_warning_once,\n        \"info\": _info,\n    }\n\n    for method_name, method in methods_to_patch.items():\n        setattr(logger, method_name,\n                MethodType(method, logger))  # type: ignore[arg-type]\n\n    return cast(_FastvideoLogger, logger)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.distributed.initialize_model_parallel","title":"fastvideo.distributed.initialize_model_parallel","text":"<pre><code>initialize_model_parallel(\n    tensor_model_parallel_size: int = 1,\n    sequence_model_parallel_size: int = 1,\n    data_parallel_size: int = 1,\n    backend: str | None = None,\n) -&gt; None\n</code></pre> <p>Initialize model parallel groups.</p> <p>Parameters:</p> Name Type Description Default <code>tensor_model_parallel_size</code> <code>int</code> <p>number of GPUs used for tensor model parallelism (used for language encoder).</p> <code>1</code> <code>sequence_model_parallel_size</code> <code>int</code> <p>number of GPUs used for sequence model parallelism (used for DiT).</p> <code>1</code> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def initialize_model_parallel(\n    tensor_model_parallel_size: int = 1,\n    sequence_model_parallel_size: int = 1,\n    data_parallel_size: int = 1,\n    backend: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Initialize model parallel groups.\n\n    Arguments:\n        tensor_model_parallel_size: number of GPUs used for tensor model\n            parallelism (used for language encoder).\n        sequence_model_parallel_size: number of GPUs used for sequence model\n            parallelism (used for DiT).\n    \"\"\"\n    # Get world size and rank. Ensure some consistencies.\n    assert _WORLD is not None, \"world group is not initialized, please call init_distributed_environment first\"\n    world_size: int = get_world_size()\n    backend = backend or torch.distributed.get_backend(\n        get_world_group().device_group)\n    assert world_size &gt;= tensor_model_parallel_size, f\"world_size({world_size}) must be greater than or equal to tensor_model_parallel_size({tensor_model_parallel_size})\"\n    num_tensor_model_parallel_groups: int = (world_size //\n                                             tensor_model_parallel_size)\n    global _TP\n    assert _TP is None, (\"tensor model parallel group is already initialized\")\n    group_ranks = []\n    for i in range(num_tensor_model_parallel_groups):\n        ranks = list(\n            range(i * tensor_model_parallel_size,\n                  (i + 1) * tensor_model_parallel_size))\n        group_ranks.append(ranks)\n\n    # message queue broadcaster is only used in tensor model parallel group\n    _TP = init_model_parallel_group(group_ranks,\n                                    get_world_group().local_rank,\n                                    backend,\n                                    use_message_queue_broadcaster=True,\n                                    group_name=\"tp\")\n\n    # Build the sequence model-parallel groups.\n    num_sequence_model_parallel_groups: int = (world_size //\n                                               sequence_model_parallel_size)\n    global _SP\n    assert _SP is None, (\"sequence model parallel group is already initialized\")\n    group_ranks = []\n\n    # Since SP is incompatible with TP and PP, we can use a simpler group creation logic\n    for i in range(num_sequence_model_parallel_groups):\n        # Create groups of consecutive ranks\n        ranks = list(\n            range(i * sequence_model_parallel_size,\n                  (i + 1) * sequence_model_parallel_size))\n        group_ranks.append(ranks)\n\n    _SP = init_model_parallel_group(group_ranks,\n                                    get_world_group().local_rank,\n                                    backend,\n                                    group_name=\"sp\")\n\n    # Build the data parallel groups.\n    num_data_parallel_groups: int = sequence_model_parallel_size\n    global _DP\n    assert _DP is None, (\"data parallel group is already initialized\")\n    group_ranks = []\n\n    for i in range(num_data_parallel_groups):\n        ranks = list(range(i, world_size, num_data_parallel_groups))\n        group_ranks.append(ranks)\n\n    _DP = init_model_parallel_group(group_ranks,\n                                    get_world_group().local_rank,\n                                    backend,\n                                    group_name=\"dp\")\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.distributed.model_parallel_is_initialized","title":"fastvideo.distributed.model_parallel_is_initialized","text":"<pre><code>model_parallel_is_initialized() -&gt; bool\n</code></pre> <p>Check if tensor, sequence parallel groups are initialized.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def model_parallel_is_initialized() -&gt; bool:\n    \"\"\"Check if tensor, sequence parallel groups are initialized.\"\"\"\n    return _TP is not None and _SP is not None and _DP is not None\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.distributed.sequence_model_parallel_all_gather","title":"fastvideo.distributed.sequence_model_parallel_all_gather","text":"<pre><code>sequence_model_parallel_all_gather(\n    input_: Tensor, dim: int = -1\n) -&gt; torch.Tensor\n</code></pre> <p>All-gather the input tensor across model parallel group.</p> Source code in <code>fastvideo/distributed/communication_op.py</code> <pre><code>def sequence_model_parallel_all_gather(input_: torch.Tensor,\n                                       dim: int = -1) -&gt; torch.Tensor:\n    \"\"\"All-gather the input tensor across model parallel group.\"\"\"\n    return get_sp_group().all_gather(input_, dim)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.distributed.sequence_model_parallel_all_to_all_4D","title":"fastvideo.distributed.sequence_model_parallel_all_to_all_4D","text":"<pre><code>sequence_model_parallel_all_to_all_4D(\n    input_: Tensor,\n    scatter_dim: int = 2,\n    gather_dim: int = 1,\n) -&gt; torch.Tensor\n</code></pre> <p>All-to-all communication of 4D tensors (e.g. QKV matrices) across sequence parallel group.</p> Source code in <code>fastvideo/distributed/communication_op.py</code> <pre><code>def sequence_model_parallel_all_to_all_4D(input_: torch.Tensor,\n                                          scatter_dim: int = 2,\n                                          gather_dim: int = 1) -&gt; torch.Tensor:\n    \"\"\"All-to-all communication of 4D tensors (e.g. QKV matrices) across sequence parallel group.\"\"\"\n    return get_sp_group().all_to_all_4D(input_, scatter_dim, gather_dim)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.distributed.split_tensor_along_last_dim","title":"fastvideo.distributed.split_tensor_along_last_dim","text":"<pre><code>split_tensor_along_last_dim(\n    tensor: Tensor,\n    num_partitions: int,\n    contiguous_split_chunks: bool = False,\n) -&gt; Sequence[torch.Tensor]\n</code></pre> <p>Split a tensor along its last dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>input tensor.</p> required <code>num_partitions</code> <code>int</code> <p>number of partitions to split the tensor</p> required <code>contiguous_split_chunks</code> <code>bool</code> <p>If True, make each chunk contiguous                      in memory.</p> <code>False</code> <p>Returns:</p> Type Description <code>Sequence[Tensor]</code> <p>A list of Tensors</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def split_tensor_along_last_dim(\n    tensor: torch.Tensor,\n    num_partitions: int,\n    contiguous_split_chunks: bool = False,\n) -&gt; Sequence[torch.Tensor]:\n    \"\"\" Split a tensor along its last dimension.\n\n        Arguments:\n            tensor: input tensor.\n            num_partitions: number of partitions to split the tensor\n            contiguous_split_chunks: If True, make each chunk contiguous\n                                     in memory.\n\n        Returns:\n            A list of Tensors\n    \"\"\"\n    # Get the size and dimension.\n    last_dim = tensor.dim() - 1\n    last_dim_size = divide(tensor.size()[last_dim], num_partitions)\n    # Split.\n    tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)\n    # NOTE: torch.split does not create contiguous tensors by default.\n    if contiguous_split_chunks:\n        return tuple(chunk.contiguous() for chunk in tensor_list)\n\n    return tuple(tensor_list)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.distributed.tensor_model_parallel_all_gather","title":"fastvideo.distributed.tensor_model_parallel_all_gather","text":"<pre><code>tensor_model_parallel_all_gather(\n    input_: Tensor, dim: int = -1\n) -&gt; torch.Tensor\n</code></pre> <p>All-gather the input tensor across model parallel group.</p> Source code in <code>fastvideo/distributed/communication_op.py</code> <pre><code>def tensor_model_parallel_all_gather(input_: torch.Tensor,\n                                     dim: int = -1) -&gt; torch.Tensor:\n    \"\"\"All-gather the input tensor across model parallel group.\"\"\"\n    return get_tp_group().all_gather(input_, dim)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.distributed.tensor_model_parallel_all_reduce","title":"fastvideo.distributed.tensor_model_parallel_all_reduce","text":"<pre><code>tensor_model_parallel_all_reduce(\n    input_: Tensor,\n) -&gt; torch.Tensor\n</code></pre> <p>All-reduce the input tensor across model parallel group.</p> Source code in <code>fastvideo/distributed/communication_op.py</code> <pre><code>def tensor_model_parallel_all_reduce(input_: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"All-reduce the input tensor across model parallel group.\"\"\"\n    return get_tp_group().all_reduce(input_)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.distributed-modules","title":"Modules","text":""},{"location":"api/fastvideo/#fastvideo.distributed.communication_op","title":"fastvideo.distributed.communication_op","text":""},{"location":"api/fastvideo/#fastvideo.distributed.communication_op-functions","title":"Functions","text":"fastvideo.distributed.communication_op.sequence_model_parallel_all_gather \u00b6 <pre><code>sequence_model_parallel_all_gather(\n    input_: Tensor, dim: int = -1\n) -&gt; torch.Tensor\n</code></pre> <p>All-gather the input tensor across model parallel group.</p> Source code in <code>fastvideo/distributed/communication_op.py</code> <pre><code>def sequence_model_parallel_all_gather(input_: torch.Tensor,\n                                       dim: int = -1) -&gt; torch.Tensor:\n    \"\"\"All-gather the input tensor across model parallel group.\"\"\"\n    return get_sp_group().all_gather(input_, dim)\n</code></pre> fastvideo.distributed.communication_op.sequence_model_parallel_all_to_all_4D \u00b6 <pre><code>sequence_model_parallel_all_to_all_4D(\n    input_: Tensor,\n    scatter_dim: int = 2,\n    gather_dim: int = 1,\n) -&gt; torch.Tensor\n</code></pre> <p>All-to-all communication of 4D tensors (e.g. QKV matrices) across sequence parallel group.</p> Source code in <code>fastvideo/distributed/communication_op.py</code> <pre><code>def sequence_model_parallel_all_to_all_4D(input_: torch.Tensor,\n                                          scatter_dim: int = 2,\n                                          gather_dim: int = 1) -&gt; torch.Tensor:\n    \"\"\"All-to-all communication of 4D tensors (e.g. QKV matrices) across sequence parallel group.\"\"\"\n    return get_sp_group().all_to_all_4D(input_, scatter_dim, gather_dim)\n</code></pre> fastvideo.distributed.communication_op.tensor_model_parallel_all_gather \u00b6 <pre><code>tensor_model_parallel_all_gather(\n    input_: Tensor, dim: int = -1\n) -&gt; torch.Tensor\n</code></pre> <p>All-gather the input tensor across model parallel group.</p> Source code in <code>fastvideo/distributed/communication_op.py</code> <pre><code>def tensor_model_parallel_all_gather(input_: torch.Tensor,\n                                     dim: int = -1) -&gt; torch.Tensor:\n    \"\"\"All-gather the input tensor across model parallel group.\"\"\"\n    return get_tp_group().all_gather(input_, dim)\n</code></pre> fastvideo.distributed.communication_op.tensor_model_parallel_all_reduce \u00b6 <pre><code>tensor_model_parallel_all_reduce(\n    input_: Tensor,\n) -&gt; torch.Tensor\n</code></pre> <p>All-reduce the input tensor across model parallel group.</p> Source code in <code>fastvideo/distributed/communication_op.py</code> <pre><code>def tensor_model_parallel_all_reduce(input_: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"All-reduce the input tensor across model parallel group.\"\"\"\n    return get_tp_group().all_reduce(input_)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.distributed.device_communicators","title":"fastvideo.distributed.device_communicators","text":""},{"location":"api/fastvideo/#fastvideo.distributed.device_communicators-modules","title":"Modules","text":"fastvideo.distributed.device_communicators.base_device_communicator \u00b6 Classes\u00b6 fastvideo.distributed.device_communicators.base_device_communicator.DeviceCommunicatorBase \u00b6 <pre><code>DeviceCommunicatorBase(\n    cpu_group: ProcessGroup,\n    device: device | None = None,\n    device_group: ProcessGroup | None = None,\n    unique_name: str = \"\",\n)\n</code></pre> <p>Base class for device-specific communicator with autograd support. It can use the <code>cpu_group</code> to initialize the communicator. If the device has PyTorch integration (PyTorch can recognize its communication backend), the <code>device_group</code> will also be given.</p> Source code in <code>fastvideo/distributed/device_communicators/base_device_communicator.py</code> <pre><code>def __init__(self,\n             cpu_group: ProcessGroup,\n             device: torch.device | None = None,\n             device_group: ProcessGroup | None = None,\n             unique_name: str = \"\"):\n    self.device = device or torch.device(\"cpu\")\n    self.cpu_group = cpu_group\n    self.device_group = device_group\n    self.unique_name = unique_name\n    self.rank = dist.get_rank(cpu_group)\n    self.world_size = dist.get_world_size(cpu_group)\n    self.ranks = dist.get_process_group_ranks(cpu_group)\n    self.global_rank = dist.get_rank()\n    self.global_world_size = dist.get_world_size()\n    self.rank_in_group = dist.get_group_rank(self.cpu_group,\n                                             self.global_rank)\n</code></pre> Functions\u00b6 fastvideo.distributed.device_communicators.base_device_communicator.DeviceCommunicatorBase.all_gather \u00b6 <pre><code>all_gather(input_: Tensor, dim: int = -1) -&gt; torch.Tensor\n</code></pre> <p>Performs an all_gather operation with gradient support.</p> Source code in <code>fastvideo/distributed/device_communicators/base_device_communicator.py</code> <pre><code>def all_gather(self, input_: torch.Tensor, dim: int = -1) -&gt; torch.Tensor:\n    \"\"\"Performs an all_gather operation with gradient support.\"\"\"\n    if dim &lt; 0:\n        dim += input_.dim()\n    return DistributedAutograd.AllGather.apply(self.device_group, input_,\n                                               self.world_size, dim)\n</code></pre> fastvideo.distributed.device_communicators.base_device_communicator.DeviceCommunicatorBase.all_reduce \u00b6 <pre><code>all_reduce(\n    input_: Tensor, op: ReduceOp | None = ReduceOp.SUM\n) -&gt; torch.Tensor\n</code></pre> <p>Performs an all_reduce operation with gradient support.</p> Source code in <code>fastvideo/distributed/device_communicators/base_device_communicator.py</code> <pre><code>def all_reduce(self,\n               input_: torch.Tensor,\n               op: dist.ReduceOp | None = ReduceOp.SUM) -&gt; torch.Tensor:\n    \"\"\"Performs an all_reduce operation with gradient support.\"\"\"\n    return DistributedAutograd.AllReduce.apply(self.device_group, input_,\n                                               op)\n</code></pre> fastvideo.distributed.device_communicators.base_device_communicator.DeviceCommunicatorBase.all_to_all_4D \u00b6 <pre><code>all_to_all_4D(\n    input_: Tensor,\n    scatter_dim: int = 2,\n    gather_dim: int = 1,\n) -&gt; torch.Tensor\n</code></pre> <p>Performs a 4D all-to-all operation with gradient support.</p> Source code in <code>fastvideo/distributed/device_communicators/base_device_communicator.py</code> <pre><code>def all_to_all_4D(self,\n                  input_: torch.Tensor,\n                  scatter_dim: int = 2,\n                  gather_dim: int = 1) -&gt; torch.Tensor:\n    \"\"\"Performs a 4D all-to-all operation with gradient support.\"\"\"\n    return DistributedAutograd.AllToAll4D.apply(self.device_group, input_,\n                                                self.world_size,\n                                                scatter_dim, gather_dim)\n</code></pre> fastvideo.distributed.device_communicators.base_device_communicator.DeviceCommunicatorBase.gather \u00b6 <pre><code>gather(\n    input_: Tensor, dst: int = 0, dim: int = -1\n) -&gt; torch.Tensor | None\n</code></pre> <p>NOTE: We assume that the input tensor is on the same device across all the ranks. NOTE: <code>dst</code> is the local rank of the destination rank.</p> Source code in <code>fastvideo/distributed/device_communicators/base_device_communicator.py</code> <pre><code>def gather(self,\n           input_: torch.Tensor,\n           dst: int = 0,\n           dim: int = -1) -&gt; torch.Tensor | None:\n    \"\"\"\n    NOTE: We assume that the input tensor is on the same device across\n    all the ranks.\n    NOTE: `dst` is the local rank of the destination rank.\n    \"\"\"\n    world_size = self.world_size\n    assert -input_.dim() &lt;= dim &lt; input_.dim(), (\n        f\"Invalid dim ({dim}) for input tensor with shape {input_.size()}\")\n    if dim &lt; 0:\n        # Convert negative dim to positive.\n        dim += input_.dim()\n\n    # Allocate output tensor.\n    if self.rank_in_group == dst:\n        gather_list = [torch.empty_like(input_) for _ in range(world_size)]\n    else:\n        gather_list = None\n    # Gather.\n    torch.distributed.gather(input_,\n                             gather_list,\n                             dst=self.ranks[dst],\n                             group=self.device_group)\n    if self.rank_in_group == dst:\n        output_tensor = torch.cat(gather_list, dim=dim)\n    else:\n        output_tensor = None\n    return output_tensor\n</code></pre> fastvideo.distributed.device_communicators.base_device_communicator.DeviceCommunicatorBase.recv \u00b6 <pre><code>recv(\n    size: Size, dtype: dtype, src: int | None = None\n) -&gt; torch.Tensor\n</code></pre> <p>Receives a tensor from the source rank.</p> Source code in <code>fastvideo/distributed/device_communicators/base_device_communicator.py</code> <pre><code>def recv(self,\n         size: torch.Size,\n         dtype: torch.dtype,\n         src: int | None = None) -&gt; torch.Tensor:\n    \"\"\"Receives a tensor from the source rank.\"\"\"\n    \"\"\"NOTE: `src` is the local rank of the source rank.\"\"\"\n    if src is None:\n        src = (self.rank_in_group - 1) % self.world_size\n\n    tensor = torch.empty(size, dtype=dtype, device=self.device)\n    torch.distributed.recv(tensor, self.ranks[src], self.device_group)\n    return tensor\n</code></pre> fastvideo.distributed.device_communicators.base_device_communicator.DeviceCommunicatorBase.send \u00b6 <pre><code>send(tensor: Tensor, dst: int | None = None) -&gt; None\n</code></pre> <p>Sends a tensor to the destination rank in a non-blocking way</p> Source code in <code>fastvideo/distributed/device_communicators/base_device_communicator.py</code> <pre><code>def send(self, tensor: torch.Tensor, dst: int | None = None) -&gt; None:\n    \"\"\"Sends a tensor to the destination rank in a non-blocking way\"\"\"\n    \"\"\"NOTE: `dst` is the local rank of the destination rank.\"\"\"\n    if dst is None:\n        dst = (self.rank_in_group + 1) % self.world_size\n    torch.distributed.send(tensor, self.ranks[dst], self.device_group)\n</code></pre> fastvideo.distributed.device_communicators.base_device_communicator.DistributedAutograd \u00b6 <p>Collection of autograd functions for distributed operations.</p> <p>This class provides custom autograd functions for distributed operations like all_reduce, all_gather, and all_to_all. Each operation is implemented as a static inner class with proper forward and backward implementations.</p> Classes\u00b6 fastvideo.distributed.device_communicators.base_device_communicator.DistributedAutograd.AllGather \u00b6 <p>               Bases: <code>Function</code></p> <p>Differentiable all_gather operation.</p> <p>The operation gathers tensors from all ranks and concatenates them along a specified dimension. The backward pass uses reduce_scatter to efficiently distribute gradients back to source ranks.</p> fastvideo.distributed.device_communicators.base_device_communicator.DistributedAutograd.AllReduce \u00b6 <p>               Bases: <code>Function</code></p> <p>Differentiable all_reduce operation.</p> <p>The gradient of all_reduce is another all_reduce operation since the operation combines values from all ranks equally.</p> fastvideo.distributed.device_communicators.base_device_communicator.DistributedAutograd.AllToAll4D \u00b6 <p>               Bases: <code>Function</code></p> <p>Differentiable all_to_all operation specialized for 4D tensors.</p> <p>This operation is particularly useful for attention operations where we need to redistribute data across ranks for efficient parallel processing.</p> <p>The operation supports two modes: 1. scatter_dim=2, gather_dim=1: Used for redistributing attention heads 2. scatter_dim=1, gather_dim=2: Used for redistributing sequence dimensions</p> fastvideo.distributed.device_communicators.cpu_communicator \u00b6 Classes\u00b6 fastvideo.distributed.device_communicators.cpu_communicator.CpuCommunicator \u00b6 <pre><code>CpuCommunicator(\n    cpu_group: ProcessGroup,\n    device: device | None = None,\n    device_group: ProcessGroup | None = None,\n    unique_name: str = \"\",\n)\n</code></pre> <p>               Bases: <code>DeviceCommunicatorBase</code></p> Source code in <code>fastvideo/distributed/device_communicators/cpu_communicator.py</code> <pre><code>def __init__(self,\n             cpu_group: ProcessGroup,\n             device: torch.device | None = None,\n             device_group: ProcessGroup | None = None,\n             unique_name: str = \"\"):\n    super().__init__(cpu_group, device, device_group, unique_name)\n    self.dist_module = torch.distributed\n\n    from fastvideo.platforms import current_platform\n\n    if (current_platform.get_cpu_architecture()\n            == CpuArchEnum.X86) and hasattr(\n                torch.ops._C,\n                \"init_shm_manager\") and unique_name.startswith(\"tp\"):\n        self.dist_module = _CPUSHMDistributed(self)\n</code></pre> Functions\u00b6 fastvideo.distributed.device_communicators.cpu_communicator.CpuCommunicator.gather \u00b6 <pre><code>gather(\n    input_: Tensor, dst: int = 0, dim: int = -1\n) -&gt; torch.Tensor | None\n</code></pre> <p>NOTE: We assume that the input tensor is on the same device across all the ranks. NOTE: <code>dst</code> is the local rank of the destination rank.</p> Source code in <code>fastvideo/distributed/device_communicators/cpu_communicator.py</code> <pre><code>def gather(self,\n           input_: torch.Tensor,\n           dst: int = 0,\n           dim: int = -1) -&gt; torch.Tensor | None:\n    \"\"\"\n    NOTE: We assume that the input tensor is on the same device across\n    all the ranks.\n    NOTE: `dst` is the local rank of the destination rank.\n    \"\"\"\n    world_size = self.world_size\n    assert -input_.dim() &lt;= dim &lt; input_.dim(), (\n        f\"Invalid dim ({dim}) for input tensor with shape {input_.size()}\")\n    if dim &lt; 0:\n        # Convert negative dim to positive.\n        dim += input_.dim()\n\n    # Allocate output tensor.\n    if self.rank_in_group == dst:\n        gather_list = [torch.empty_like(input_) for _ in range(world_size)]\n    else:\n        gather_list = None\n\n    # Gather.\n    self.dist_module.gather(input_,\n                            gather_list,\n                            dst=self.ranks[dst],\n                            group=self.device_group)\n\n    if self.rank_in_group == dst:\n        output_tensor = torch.cat(gather_list, dim=dim)\n    else:\n        output_tensor = None\n    return output_tensor\n</code></pre> fastvideo.distributed.device_communicators.cuda_communicator \u00b6 Classes\u00b6 fastvideo.distributed.device_communicators.cuda_communicator.CudaCommunicator \u00b6 <pre><code>CudaCommunicator(\n    cpu_group: ProcessGroup,\n    device: device | None = None,\n    device_group: ProcessGroup | None = None,\n    unique_name: str = \"\",\n)\n</code></pre> <p>               Bases: <code>DeviceCommunicatorBase</code></p> Source code in <code>fastvideo/distributed/device_communicators/cuda_communicator.py</code> <pre><code>def __init__(self,\n             cpu_group: ProcessGroup,\n             device: torch.device | None = None,\n             device_group: ProcessGroup | None = None,\n             unique_name: str = \"\"):\n    super().__init__(cpu_group, device, device_group, unique_name)\n\n    from fastvideo.distributed.device_communicators.pynccl import (\n        PyNcclCommunicator)\n\n    self.pynccl_comm: PyNcclCommunicator | None = None\n    if self.world_size &gt; 1:\n        self.pynccl_comm = PyNcclCommunicator(\n            group=self.cpu_group,\n            device=self.device,\n        )\n</code></pre> Functions\u00b6 fastvideo.distributed.device_communicators.cuda_communicator.CudaCommunicator.recv \u00b6 <pre><code>recv(\n    size: Size, dtype: dtype, src: int | None = None\n) -&gt; torch.Tensor\n</code></pre> <p>Receives a tensor from the source rank.</p> Source code in <code>fastvideo/distributed/device_communicators/cuda_communicator.py</code> <pre><code>def recv(self,\n         size: torch.Size,\n         dtype: torch.dtype,\n         src: int | None = None) -&gt; torch.Tensor:\n    \"\"\"Receives a tensor from the source rank.\"\"\"\n    \"\"\"NOTE: `src` is the local rank of the source rank.\"\"\"\n    if src is None:\n        src = (self.rank_in_group - 1) % self.world_size\n\n    tensor = torch.empty(size, dtype=dtype, device=self.device)\n    pynccl_comm = self.pynccl_comm\n    if pynccl_comm is not None and not pynccl_comm.disabled:\n        pynccl_comm.recv(tensor, src)\n    else:\n        torch.distributed.recv(tensor, self.ranks[src], self.device_group)\n    return tensor\n</code></pre> fastvideo.distributed.device_communicators.cuda_communicator.CudaCommunicator.send \u00b6 <pre><code>send(tensor: Tensor, dst: int | None = None) -&gt; None\n</code></pre> <p>Sends a tensor to the destination rank in a non-blocking way</p> Source code in <code>fastvideo/distributed/device_communicators/cuda_communicator.py</code> <pre><code>def send(self, tensor: torch.Tensor, dst: int | None = None) -&gt; None:\n    \"\"\"Sends a tensor to the destination rank in a non-blocking way\"\"\"\n    \"\"\"NOTE: `dst` is the local rank of the destination rank.\"\"\"\n    if dst is None:\n        dst = (self.rank_in_group + 1) % self.world_size\n\n    pynccl_comm = self.pynccl_comm\n    if pynccl_comm is not None and not pynccl_comm.disabled:\n        pynccl_comm.send(tensor, dst)\n    else:\n        torch.distributed.send(tensor, self.ranks[dst], self.device_group)\n</code></pre> fastvideo.distributed.device_communicators.npu_communicator \u00b6 Classes\u00b6 fastvideo.distributed.device_communicators.npu_communicator.NpuCommunicator \u00b6 <pre><code>NpuCommunicator(\n    cpu_group: ProcessGroup,\n    device: device | None = None,\n    device_group: ProcessGroup | None = None,\n    unique_name: str = \"\",\n)\n</code></pre> <p>               Bases: <code>DeviceCommunicatorBase</code></p> Source code in <code>fastvideo/distributed/device_communicators/npu_communicator.py</code> <pre><code>def __init__(self,\n             cpu_group: ProcessGroup,\n             device: torch.device | None = None,\n             device_group: ProcessGroup | None = None,\n             unique_name: str = \"\"):\n    super().__init__(cpu_group, device, device_group, unique_name)\n\n    from fastvideo.distributed.device_communicators.pyhccl import (\n        PyHcclCommunicator)\n\n    self.pyhccl_comm: PyHcclCommunicator | None = None\n    if self.world_size &gt; 1:\n        self.pyhccl_comm = PyHcclCommunicator(\n            group=self.cpu_group,\n            device=self.device,\n        )\n</code></pre> Functions\u00b6 fastvideo.distributed.device_communicators.npu_communicator.NpuCommunicator.recv \u00b6 <pre><code>recv(\n    size: Size, dtype: dtype, src: int | None = None\n) -&gt; torch.Tensor\n</code></pre> <p>Receives a tensor from the source rank.</p> Source code in <code>fastvideo/distributed/device_communicators/npu_communicator.py</code> <pre><code>def recv(self,\n         size: torch.Size,\n         dtype: torch.dtype,\n         src: int | None = None) -&gt; torch.Tensor:\n    \"\"\"Receives a tensor from the source rank.\"\"\"\n    \"\"\"NOTE: `src` is the local rank of the source rank.\"\"\"\n    if src is None:\n        src = (self.rank_in_group - 1) % self.world_size\n\n    tensor = torch.empty(size, dtype=dtype, device=self.device)\n    pyhccl_comm = self.pyhccl_comm\n    if pyhccl_comm is not None and not pyhccl_comm.disabled:\n        pyhccl_comm.recv(tensor, src)\n    else:\n        torch.distributed.recv(tensor, self.ranks[src], self.device_group)\n    return tensor\n</code></pre> fastvideo.distributed.device_communicators.npu_communicator.NpuCommunicator.send \u00b6 <pre><code>send(tensor: Tensor, dst: int | None = None) -&gt; None\n</code></pre> <p>Sends a tensor to the destination rank in a non-blocking way</p> Source code in <code>fastvideo/distributed/device_communicators/npu_communicator.py</code> <pre><code>def send(self, tensor: torch.Tensor, dst: int | None = None) -&gt; None:\n    \"\"\"Sends a tensor to the destination rank in a non-blocking way\"\"\"\n    \"\"\"NOTE: `dst` is the local rank of the destination rank.\"\"\"\n    if dst is None:\n        dst = (self.rank_in_group + 1) % self.world_size\n\n    pyhccl_comm = self.pyhccl_comm\n    if pyhccl_comm is not None and not pyhccl_comm.disabled:\n        pyhccl_comm.send(tensor, dst)\n    else:\n        torch.distributed.send(tensor, self.ranks[dst], self.device_group)\n</code></pre> fastvideo.distributed.device_communicators.pyhccl \u00b6 Classes\u00b6 fastvideo.distributed.device_communicators.pyhccl.PyHcclCommunicator \u00b6 <pre><code>PyHcclCommunicator(\n    group: ProcessGroup | StatelessProcessGroup,\n    device: int | str | device,\n    library_path: str | None = None,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>ProcessGroup | StatelessProcessGroup</code> <p>the process group to work on. If None, it will use the default process group.</p> required <code>device</code> <code>int | str | device</code> <p>the device to bind the PyHcclCommunicator to. If None, it will be bind to f\"npu:{local_rank}\".</p> required <code>library_path</code> <code>str | None</code> <p>the path to the HCCL library. If None, it will use the default library path.</p> <code>None</code> <p>It is the caller's responsibility to make sure each communicator is bind to a unique device.</p> Source code in <code>fastvideo/distributed/device_communicators/pyhccl.py</code> <pre><code>def __init__(\n    self,\n    group: ProcessGroup | StatelessProcessGroup,\n    device: int | str | torch.device,\n    library_path: str | None = None,\n):\n    \"\"\"\n    Args:\n        group: the process group to work on. If None, it will use the\n            default process group.\n        device: the device to bind the PyHcclCommunicator to. If None,\n            it will be bind to f\"npu:{local_rank}\".\n        library_path: the path to the HCCL library. If None, it will\n            use the default library path.\n    It is the caller's responsibility to make sure each communicator\n    is bind to a unique device.\n    \"\"\"\n\n    if not isinstance(group, StatelessProcessGroup):\n        assert dist.is_initialized()\n        assert dist.get_backend(group) != dist.Backend.HCCL, (\n            \"PyHcclCommunicator should be attached to a non-HCCL group.\")\n        # note: this rank is the rank in the group\n        self.rank = dist.get_rank(group)\n        self.world_size = dist.get_world_size(group)\n    else:\n        self.rank = group.rank\n        self.world_size = group.world_size\n\n    self.group = group\n\n    # if world_size == 1, no need to create communicator\n    if self.world_size == 1:\n        self.available = False\n        self.disabled = True\n        return\n\n    try:\n        self.hccl = HCCLLibrary(library_path)\n    except Exception:\n        logger.warning(\"disable hccl because of missing HCCL library\")\n        # disable because of missing HCCL library\n        # e.g. in a non-NPU environment\n        self.available = False\n        self.disabled = True\n        return\n\n    self.available = True\n    self.disabled = False\n\n    logger.info(\"FastVideo is using pyhccl\")\n\n    if isinstance(device, int):\n        device = torch.device(f\"npu:{device}\")\n    elif isinstance(device, str):\n        device = torch.device(device)\n    # now `device` is a `torch.device` object\n    assert isinstance(device, torch.device)\n    self.device = device\n\n    if self.rank == 0:\n        # get the unique id from HCCL\n        with torch.npu.device(device):\n            self.unique_id = self.hccl.hcclGetUniqueId()\n    else:\n        # construct an empty unique id\n        self.unique_id = hcclUniqueId()\n\n    if not isinstance(group, StatelessProcessGroup):\n        tensor = torch.ByteTensor(list(self.unique_id.internal))\n        ranks = dist.get_process_group_ranks(group)\n        # arg `src` in `broadcast` is the global rank\n        dist.broadcast(tensor, src=ranks[0], group=group)\n        byte_list = tensor.tolist()\n        for i, byte in enumerate(byte_list):\n            self.unique_id.internal[i] = byte\n    else:\n        self.unique_id = group.broadcast_obj(self.unique_id, src=0)\n\n    # hccl communicator and stream will use this device\n    # `torch.npu.device` is a context manager that changes the\n    # current npu device to the specified one\n    with torch.npu.device(device):\n        self.comm: hcclComm_t = self.hccl.hcclCommInitRank(\n            self.world_size, self.unique_id, self.rank)\n\n        stream = current_stream()\n        # A small all_reduce for warmup.\n        data = torch.zeros(1, device=device)\n        self.all_reduce(data)\n        stream.synchronize()\n        del data\n</code></pre> Functions\u00b6 Functions\u00b6 fastvideo.distributed.device_communicators.pynccl \u00b6 Classes\u00b6 fastvideo.distributed.device_communicators.pynccl.PyNcclCommunicator \u00b6 <pre><code>PyNcclCommunicator(\n    group: ProcessGroup | StatelessProcessGroup,\n    device: int | str | device,\n    library_path: str | None = None,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>ProcessGroup | StatelessProcessGroup</code> <p>the process group to work on. If None, it will use the default process group.</p> required <code>device</code> <code>int | str | device</code> <p>the device to bind the PyNcclCommunicator to. If None, it will be bind to f\"cuda:{local_rank}\".</p> required <code>library_path</code> <code>str | None</code> <p>the path to the NCCL library. If None, it will use the default library path.</p> <code>None</code> <p>It is the caller's responsibility to make sure each communicator is bind to a unique device.</p> Source code in <code>fastvideo/distributed/device_communicators/pynccl.py</code> <pre><code>def __init__(\n    self,\n    group: ProcessGroup | StatelessProcessGroup,\n    device: int | str | torch.device,\n    library_path: str | None = None,\n):\n    \"\"\"\n    Args:\n        group: the process group to work on. If None, it will use the\n            default process group.\n        device: the device to bind the PyNcclCommunicator to. If None,\n            it will be bind to f\"cuda:{local_rank}\".\n        library_path: the path to the NCCL library. If None, it will\n            use the default library path.\n    It is the caller's responsibility to make sure each communicator\n    is bind to a unique device.\n    \"\"\"\n    if not isinstance(group, StatelessProcessGroup):\n        assert dist.is_initialized()\n        assert dist.get_backend(group) != dist.Backend.NCCL, (\n            \"PyNcclCommunicator should be attached to a non-NCCL group.\")\n        # note: this rank is the rank in the group\n        self.rank = dist.get_rank(group)\n        self.world_size = dist.get_world_size(group)\n    else:\n        self.rank = group.rank\n        self.world_size = group.world_size\n\n    self.group = group\n\n    # if world_size == 1, no need to create communicator\n    if self.world_size == 1:\n        self.available = False\n        self.disabled = True\n        return\n    try:\n        self.nccl = NCCLLibrary(library_path)\n    except Exception:\n        # disable because of missing NCCL library\n        # e.g. in a non-GPU environment\n        self.available = False\n        self.disabled = True\n        return\n\n    self.available = True\n    self.disabled = False\n\n    logger.info(\"FastVideo is using nccl==%s\", self.nccl.ncclGetVersion())\n\n    if self.rank == 0:\n        # get the unique id from NCCL\n        self.unique_id = self.nccl.ncclGetUniqueId()\n    else:\n        # construct an empty unique id\n        self.unique_id = ncclUniqueId()\n\n    if not isinstance(group, StatelessProcessGroup):\n        tensor = torch.ByteTensor(list(self.unique_id.internal))\n        ranks = dist.get_process_group_ranks(group)\n        # arg `src` in `broadcast` is the global rank\n        dist.broadcast(tensor, src=ranks[0], group=group)\n        byte_list = tensor.tolist()\n        for i, byte in enumerate(byte_list):\n            self.unique_id.internal[i] = byte\n    else:\n        self.unique_id = group.broadcast_obj(self.unique_id, src=0)\n    if isinstance(device, int):\n        device = torch.device(f\"cuda:{device}\")\n    elif isinstance(device, str):\n        device = torch.device(device)\n    # now `device` is a `torch.device` object\n    assert isinstance(device, torch.device)\n    self.device = device\n    # nccl communicator and stream will use this device\n    # `torch.cuda.device` is a context manager that changes the\n    # current cuda device to the specified one\n    with torch.cuda.device(device):\n        self.comm: ncclComm_t = self.nccl.ncclCommInitRank(\n            self.world_size, self.unique_id, self.rank)\n\n        stream = current_stream()\n        # A small all_reduce for warmup.\n        data = torch.zeros(1, device=device)\n        self.all_reduce(data)\n        if stream is not None:\n            stream.synchronize()\n        del data\n</code></pre> Functions\u00b6 Functions\u00b6"},{"location":"api/fastvideo/#fastvideo.distributed.parallel_state","title":"fastvideo.distributed.parallel_state","text":"<p>FastVideo distributed state. It takes over the control of the distributed environment from PyTorch. The typical workflow is:</p> <ul> <li>call <code>init_distributed_environment</code> to initialize the distributed environment.</li> <li> <p>call <code>initialize_model_parallel</code> or <code>ensure_model_parallel_initialized</code> to  initialize the model parallel groups.</p> </li> <li> <p>any code dealing with the distributed stuff</p> </li> <li> <p>call <code>destroy_model_parallel</code> to destroy the model parallel groups.</p> </li> <li>call <code>destroy_distributed_environment</code> to destroy the distributed environment.</li> </ul> <p>If you only need to use the distributed environment without model parallelism,  you can skip the model parallel initialization and destruction steps.</p>"},{"location":"api/fastvideo/#fastvideo.distributed.parallel_state-classes","title":"Classes","text":"fastvideo.distributed.parallel_state.GroupCoordinator \u00b6 <pre><code>GroupCoordinator(\n    group_ranks: list[list[int]],\n    local_rank: int,\n    torch_distributed_backend: str | Backend,\n    use_device_communicator: bool,\n    use_message_queue_broadcaster: bool = False,\n    group_name: str | None = None,\n)\n</code></pre> <p>PyTorch ProcessGroup wrapper for a group of processes. PyTorch ProcessGroup is bound to one specific communication backend,     e.g. NCCL, Gloo, MPI, etc. GroupCoordinator takes charge of all the communication operations among     the processes in the group. It manages both CPU and device     communication.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def __init__(\n    self,\n    group_ranks: list[list[int]],\n    local_rank: int,\n    torch_distributed_backend: str | Backend,\n    use_device_communicator: bool,\n    use_message_queue_broadcaster: bool = False,\n    group_name: str | None = None,\n):\n    group_name = group_name or \"anonymous\"\n    self.unique_name = _get_unique_name(group_name)\n    _register_group(self)\n\n    self.rank = torch.distributed.get_rank()\n    self.local_rank = local_rank\n    self.device_group = None\n    self.cpu_group = None\n\n    for ranks in group_ranks:\n        device_group = torch.distributed.new_group(\n            ranks, backend=torch_distributed_backend)\n        # a group with `gloo` backend, to allow direct coordination between\n        # processes through the CPU.\n        cpu_group = torch.distributed.new_group(ranks, backend=\"gloo\")\n        if self.rank in ranks:\n            self.ranks = ranks\n            self.world_size = len(ranks)\n            self.rank_in_group = ranks.index(self.rank)\n            self.device_group = device_group\n            self.cpu_group = cpu_group\n    try:\n        assert self.cpu_group is not None\n        assert self.device_group is not None\n    except Exception as e:\n        print(f\"rank: {self.rank} group not found\")\n        raise e\n\n    from fastvideo.platforms import current_platform\n\n    # TODO: fix it for other platforms\n    self.device = get_local_torch_device()\n\n    self.use_device_communicator = use_device_communicator\n    self.device_communicator: DeviceCommunicatorBase = None  # type: ignore\n    if use_device_communicator and self.world_size &gt; 1:\n        # Platform-aware device communicator selection\n        if current_platform.is_cuda_alike():\n            from fastvideo.distributed.device_communicators.cuda_communicator import (\n                CudaCommunicator)\n            self.device_communicator = CudaCommunicator(\n                cpu_group=self.cpu_group,\n                device=self.device,\n                device_group=self.device_group,\n                unique_name=self.unique_name,\n            )\n        elif current_platform.is_npu():\n            from fastvideo.distributed.device_communicators.npu_communicator import (\n                NpuCommunicator)\n            self.device_communicator = NpuCommunicator(\n                cpu_group=self.cpu_group,\n                device=self.device,\n                device_group=self.device_group,\n                unique_name=self.unique_name,\n            )\n        else:\n            # For MPS and CPU, use the CPU communicator\n            self.device_communicator = CpuCommunicator(\n                cpu_group=self.cpu_group,\n                device=self.device,\n                device_group=self.device_group,\n                unique_name=self.unique_name,\n            )\n\n    self.mq_broadcaster = None\n\n    from fastvideo.platforms import current_platform\n\n    # TODO(will): check if this is needed\n    # self.use_custom_op_call = current_platform.is_cuda_alike()\n    self.use_custom_op_call = False\n</code></pre> Attributes\u00b6 fastvideo.distributed.parallel_state.GroupCoordinator.first_rank <code>property</code> \u00b6 <pre><code>first_rank\n</code></pre> <p>Return the global rank of the first process in the group</p> fastvideo.distributed.parallel_state.GroupCoordinator.is_first_rank <code>property</code> \u00b6 <pre><code>is_first_rank\n</code></pre> <p>Return whether the caller is the first process in the group</p> fastvideo.distributed.parallel_state.GroupCoordinator.is_last_rank <code>property</code> \u00b6 <pre><code>is_last_rank\n</code></pre> <p>Return whether the caller is the last process in the group</p> fastvideo.distributed.parallel_state.GroupCoordinator.last_rank <code>property</code> \u00b6 <pre><code>last_rank\n</code></pre> <p>Return the global rank of the last process in the group</p> fastvideo.distributed.parallel_state.GroupCoordinator.next_rank <code>property</code> \u00b6 <pre><code>next_rank\n</code></pre> <p>Return the global rank of the process that follows the caller</p> fastvideo.distributed.parallel_state.GroupCoordinator.prev_rank <code>property</code> \u00b6 <pre><code>prev_rank\n</code></pre> <p>Return the global rank of the process that precedes the caller</p> Functions\u00b6 fastvideo.distributed.parallel_state.GroupCoordinator.all_reduce \u00b6 <pre><code>all_reduce(\n    input_: Tensor, op: ReduceOp | None = ReduceOp.SUM\n) -&gt; torch.Tensor\n</code></pre> <p>User-facing all-reduce function before we actually call the all-reduce operation.</p> <p>We need this because Dynamo does not support passing an arbitrary object (<code>self</code> in this case) to a custom op. We need to pass the  group name as a string, and then look up the group coordinator from  the group name, dispatch the all-reduce operation to the group  coordinator.</p> <p>In addition, PyTorch custom ops do not support mutation or returning a new tensor in the same op. So we always make the all-reduce operation out-of-place.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def all_reduce(\n        self,\n        input_: torch.Tensor,\n        op: torch.distributed.ReduceOp | None = ReduceOp.SUM\n) -&gt; torch.Tensor:\n    \"\"\"\n    User-facing all-reduce function before we actually call the\n    all-reduce operation.\n\n    We need this because Dynamo does not support passing an arbitrary\n    object (`self` in this case) to a custom op. We need to pass the\n     group name as a string, and then look up the group coordinator from\n     the group name, dispatch the all-reduce operation to the group\n     coordinator.\n\n    In addition, PyTorch custom ops do not support mutation or returning\n    a new tensor in the same op. So we always make the all-reduce operation\n    out-of-place.\n    \"\"\"\n    # Bypass the function if we are using only 1 GPU.\n    if self.world_size == 1:\n        return input_\n\n    if self.use_custom_op_call:\n        return torch.ops.vllm.all_reduce(input_,\n                                         group_name=self.unique_name)\n    else:\n        return self._all_reduce_out_place(input_, op=op)\n</code></pre> fastvideo.distributed.parallel_state.GroupCoordinator.barrier \u00b6 <pre><code>barrier() -&gt; None\n</code></pre> <p>Barrier synchronization among the group. NOTE: don't use <code>device_group</code> here! <code>barrier</code> in NCCL is terrible because it is internally a broadcast operation with secretly created GPU tensors. It is easy to mess up the current device. Use the CPU group instead.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def barrier(self) -&gt; None:\n    \"\"\"Barrier synchronization among the group.\n    NOTE: don't use `device_group` here! `barrier` in NCCL is\n    terrible because it is internally a broadcast operation with\n    secretly created GPU tensors. It is easy to mess up the current\n    device. Use the CPU group instead.\n    \"\"\"\n    torch.distributed.barrier(group=self.cpu_group)\n</code></pre> fastvideo.distributed.parallel_state.GroupCoordinator.broadcast \u00b6 <pre><code>broadcast(input_: Tensor, src: int = 0)\n</code></pre> <p>Broadcast the input tensor. NOTE: <code>src</code> is the local rank of the source rank.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def broadcast(self, input_: torch.Tensor, src: int = 0):\n    \"\"\"Broadcast the input tensor.\n    NOTE: `src` is the local rank of the source rank.\n    \"\"\"\n    assert src &lt; self.world_size, f\"Invalid src rank ({src})\"\n\n    # Bypass the function if we are using only 1 GPU.\n    if self.world_size == 1:\n        return input_\n    # Broadcast.\n    torch.distributed.broadcast(input_,\n                                src=self.ranks[src],\n                                group=self.device_group)\n    return input_\n</code></pre> fastvideo.distributed.parallel_state.GroupCoordinator.broadcast_object \u00b6 <pre><code>broadcast_object(obj: Any | None = None, src: int = 0)\n</code></pre> <p>Broadcast the input object. NOTE: <code>src</code> is the local rank of the source rank.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def broadcast_object(self, obj: Any | None = None, src: int = 0):\n    \"\"\"Broadcast the input object.\n    NOTE: `src` is the local rank of the source rank.\n    \"\"\"\n    assert src &lt; self.world_size, f\"Invalid src rank ({src})\"\n\n    # Bypass the function if we are using only 1 GPU.\n    if self.world_size == 1:\n        return obj\n    if self.mq_broadcaster is not None:\n        assert src == 0, \"Message queue broadcaster only supports src=0\"\n        return self.mq_broadcaster.broadcast_object(obj)\n    if self.rank_in_group == src:\n        torch.distributed.broadcast_object_list([obj],\n                                                src=self.ranks[src],\n                                                group=self.cpu_group)\n        return obj\n    else:\n        recv = [None]\n        torch.distributed.broadcast_object_list(recv,\n                                                src=self.ranks[src],\n                                                group=self.cpu_group)\n        return recv[0]\n</code></pre> fastvideo.distributed.parallel_state.GroupCoordinator.broadcast_object_list \u00b6 <pre><code>broadcast_object_list(\n    obj_list: list[Any],\n    src: int = 0,\n    group: ProcessGroup | None = None,\n)\n</code></pre> <p>Broadcast the input object list. NOTE: <code>src</code> is the local rank of the source rank.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def broadcast_object_list(self,\n                          obj_list: list[Any],\n                          src: int = 0,\n                          group: ProcessGroup | None = None):\n    \"\"\"Broadcast the input object list.\n    NOTE: `src` is the local rank of the source rank.\n    \"\"\"\n    assert src &lt; self.world_size, f\"Invalid src rank ({src})\"\n\n    # Bypass the function if we are using only 1 GPU.\n    if self.world_size == 1:\n        return obj_list\n    # Broadcast.\n    torch.distributed.broadcast_object_list(obj_list,\n                                            src=self.ranks[src],\n                                            group=self.device_group)\n    return obj_list\n</code></pre> fastvideo.distributed.parallel_state.GroupCoordinator.broadcast_tensor_dict \u00b6 <pre><code>broadcast_tensor_dict(\n    tensor_dict: dict[str, Tensor | Any] | None = None,\n    src: int = 0,\n    group: ProcessGroup | None = None,\n    metadata_group: ProcessGroup | None = None,\n) -&gt; dict[str, torch.Tensor | Any] | None\n</code></pre> <p>Broadcast the input tensor dictionary. NOTE: <code>src</code> is the local rank of the source rank.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def broadcast_tensor_dict(\n    self,\n    tensor_dict: dict[str, torch.Tensor | Any] | None = None,\n    src: int = 0,\n    group: ProcessGroup | None = None,\n    metadata_group: ProcessGroup | None = None\n) -&gt; dict[str, torch.Tensor | Any] | None:\n    \"\"\"Broadcast the input tensor dictionary.\n    NOTE: `src` is the local rank of the source rank.\n    \"\"\"\n    # Bypass the function if we are using only 1 GPU.\n    if (not torch.distributed.is_initialized() or self.world_size == 1):\n        return tensor_dict\n\n    group = self.device_group\n    metadata_group = self.cpu_group\n    assert src &lt; self.world_size, f\"Invalid src rank ({src})\"\n\n    rank_in_group = self.rank_in_group\n    if rank_in_group == src:\n        metadata_list: list[tuple[Any, Any]] = []\n        assert isinstance(\n            tensor_dict,\n            dict), (f\"Expecting a dictionary, got {type(tensor_dict)}\")\n        metadata_list, tensor_list = _split_tensor_dict(tensor_dict)\n        # `metadata_list` lives in CPU memory.\n        # `broadcast_object_list` has serialization &amp; deserialization,\n        # all happening on CPU. Therefore, we can use the CPU group.\n        self.broadcast_object(metadata_list, src=src)\n        async_handles = []\n        for tensor in tensor_list:\n            if tensor.numel() == 0:\n                # Skip broadcasting empty tensors.\n                continue\n            if tensor.is_cpu:\n                # use metadata_group for CPU tensors\n                handle = torch.distributed.broadcast(tensor,\n                                                     src=self.ranks[src],\n                                                     group=metadata_group,\n                                                     async_op=True)\n            else:\n                # use group for GPU tensors\n                handle = torch.distributed.broadcast(tensor,\n                                                     src=self.ranks[src],\n                                                     group=group,\n                                                     async_op=True)\n            async_handles.append(handle)\n        for async_handle in async_handles:\n            async_handle.wait()\n\n    else:\n        metadata_list = self.broadcast_object(None, src=src)\n        tensor_dict = {}\n        async_handles = []\n        for key, value in metadata_list:\n            if isinstance(value, TensorMetadata):\n                tensor = torch.empty(value.size,\n                                     dtype=value.dtype,\n                                     device=value.device)\n                if tensor.numel() == 0:\n                    # Skip broadcasting empty tensors.\n                    tensor_dict[key] = tensor\n                    continue\n                if tensor.is_cpu:\n                    # use metadata_group for CPU tensors\n                    handle = torch.distributed.broadcast(\n                        tensor,\n                        src=self.ranks[src],\n                        group=metadata_group,\n                        async_op=True)\n                else:\n                    # use group for GPU tensors\n                    handle = torch.distributed.broadcast(\n                        tensor,\n                        src=self.ranks[src],\n                        group=group,\n                        async_op=True)\n                async_handles.append(handle)\n                tensor_dict[key] = tensor\n            else:\n                tensor_dict[key] = value\n        for async_handle in async_handles:\n            async_handle.wait()\n    return tensor_dict\n</code></pre> fastvideo.distributed.parallel_state.GroupCoordinator.gather \u00b6 <pre><code>gather(\n    input_: Tensor, dst: int = 0, dim: int = -1\n) -&gt; torch.Tensor | None\n</code></pre> <p>NOTE: We assume that the input tensor is on the same device across all the ranks. NOTE: <code>dst</code> is the local rank of the destination rank.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def gather(self,\n           input_: torch.Tensor,\n           dst: int = 0,\n           dim: int = -1) -&gt; torch.Tensor | None:\n    \"\"\"\n    NOTE: We assume that the input tensor is on the same device across\n    all the ranks.\n    NOTE: `dst` is the local rank of the destination rank.\n    \"\"\"\n    world_size = self.world_size\n    # Bypass the function if we are using only 1 GPU.\n    if world_size == 1:\n        return input_\n    return self.device_communicator.gather(input_, dst, dim)\n</code></pre> fastvideo.distributed.parallel_state.GroupCoordinator.recv \u00b6 <pre><code>recv(\n    size: Size, dtype: dtype, src: int | None = None\n) -&gt; torch.Tensor\n</code></pre> <p>Receives a tensor from the source rank.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def recv(self,\n         size: torch.Size,\n         dtype: torch.dtype,\n         src: int | None = None) -&gt; torch.Tensor:\n    \"\"\"Receives a tensor from the source rank.\"\"\"\n    \"\"\"NOTE: `src` is the local rank of the source rank.\"\"\"\n    return self.device_communicator.recv(size, dtype, src)\n</code></pre> fastvideo.distributed.parallel_state.GroupCoordinator.recv_object \u00b6 <pre><code>recv_object(src: int) -&gt; Any\n</code></pre> <p>Receive the input object list from the source rank.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def recv_object(self, src: int) -&gt; Any:\n    \"\"\"Receive the input object list from the source rank.\"\"\"\n    \"\"\"NOTE: `src` is the local rank of the source rank.\"\"\"\n\n    assert src &lt; self.world_size, f\"Invalid src rank ({src})\"\n\n    assert src != self.rank_in_group, (\n        \"Invalid source rank. Source rank is the same as the current rank.\")\n\n    size_tensor = torch.empty(1, dtype=torch.long, device=\"cpu\")\n\n    # Receive object size\n    rank_size = torch.distributed.recv(size_tensor,\n                                       src=self.ranks[src],\n                                       group=self.cpu_group)\n\n    # Tensor to receive serialized objects into.\n    object_tensor = torch.empty(  # type: ignore[call-overload]\n        size_tensor.item(),  # type: ignore[arg-type]\n        dtype=torch.uint8,\n        device=\"cpu\")\n\n    rank_object = torch.distributed.recv(object_tensor,\n                                         src=self.ranks[src],\n                                         group=self.cpu_group)\n\n    assert rank_object == rank_size, (\n        \"Received object sender rank does not match the size sender rank.\")\n\n    obj = pickle.loads(object_tensor.numpy().tobytes())\n\n    return obj\n</code></pre> fastvideo.distributed.parallel_state.GroupCoordinator.recv_tensor_dict \u00b6 <pre><code>recv_tensor_dict(\n    src: int | None = None,\n    all_gather_group: Optional[GroupCoordinator] = None,\n) -&gt; dict[str, torch.Tensor | Any] | None\n</code></pre> <p>Recv the input tensor dictionary. NOTE: <code>src</code> is the local rank of the source rank.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def recv_tensor_dict(\n    self,\n    src: int | None = None,\n    all_gather_group: Optional[\"GroupCoordinator\"] = None,\n) -&gt; dict[str, torch.Tensor | Any] | None:\n    \"\"\"Recv the input tensor dictionary.\n    NOTE: `src` is the local rank of the source rank.\n    \"\"\"\n    # Bypass the function if we are using only 1 GPU.\n    if not torch.distributed.is_initialized() or self.world_size == 1:\n        return None\n\n    all_gather_size = (1 if all_gather_group is None else\n                       all_gather_group.world_size)\n    all_gather_rank = (0 if all_gather_group is None else\n                       all_gather_group.rank_in_group)\n\n    group = self.device_group\n    metadata_group = self.cpu_group\n\n    if src is None:\n        src = (self.rank_in_group - 1) % self.world_size\n    assert src &lt; self.world_size, f\"Invalid src rank ({src})\"\n\n    recv_metadata_list = self.recv_object(src=src)\n    tensor_dict: dict[str, Any] = {}\n    for key, value in recv_metadata_list:\n        if isinstance(value, TensorMetadata):\n            tensor = torch.empty(value.size,\n                                 dtype=value.dtype,\n                                 device=value.device)\n            if tensor.numel() == 0:\n                # Skip broadcasting empty tensors.\n                tensor_dict[key] = tensor\n                continue\n\n            # send-allgather: send only a slice, then do allgather.\n            use_all_gather = (all_gather_group is not None\n                              and tensor.numel() % all_gather_size == 0)\n\n            if use_all_gather:\n                orig_shape = tensor.shape\n                tensor = tensor.reshape(all_gather_size,\n                                        -1)[all_gather_rank]\n\n            if tensor.is_cpu:\n                # use metadata_group for CPU tensors\n                torch.distributed.recv(tensor,\n                                       src=self.ranks[src],\n                                       group=metadata_group)\n            else:\n                # use group for GPU tensors\n                torch.distributed.recv(tensor,\n                                       src=self.ranks[src],\n                                       group=group)\n            if use_all_gather:\n                # do the allgather\n                tensor = all_gather_group.all_gather(  # type: ignore\n                    tensor, dim=0)\n                tensor = tensor.reshape(orig_shape)\n\n            tensor_dict[key] = tensor\n        else:\n            tensor_dict[key] = value\n    return tensor_dict\n</code></pre> fastvideo.distributed.parallel_state.GroupCoordinator.send \u00b6 <pre><code>send(tensor: Tensor, dst: int | None = None) -&gt; None\n</code></pre> <p>Sends a tensor to the destination rank in a non-blocking way</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def send(self, tensor: torch.Tensor, dst: int | None = None) -&gt; None:\n    \"\"\"Sends a tensor to the destination rank in a non-blocking way\"\"\"\n    \"\"\"NOTE: `dst` is the local rank of the destination rank.\"\"\"\n    self.device_communicator.send(tensor, dst)\n</code></pre> fastvideo.distributed.parallel_state.GroupCoordinator.send_object \u00b6 <pre><code>send_object(obj: Any, dst: int) -&gt; None\n</code></pre> <p>Send the input object list to the destination rank.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def send_object(self, obj: Any, dst: int) -&gt; None:\n    \"\"\"Send the input object list to the destination rank.\"\"\"\n    \"\"\"NOTE: `dst` is the local rank of the destination rank.\"\"\"\n\n    assert dst &lt; self.world_size, f\"Invalid dst rank ({dst})\"\n\n    assert dst != self.rank_in_group, (\n        \"Invalid destination rank. Destination rank is the same \"\n        \"as the current rank.\")\n\n    # Serialize object to tensor and get the size as well\n    object_tensor = torch.frombuffer(pickle.dumps(obj), dtype=torch.uint8)\n\n    size_tensor = torch.tensor([object_tensor.numel()],\n                               dtype=torch.long,\n                               device=\"cpu\")\n\n    # Send object size\n\n    torch.distributed.send(size_tensor,\n                           dst=self.ranks[dst],\n                           group=self.cpu_group)\n\n    # Send object\n    torch.distributed.send(object_tensor,\n                           dst=self.ranks[dst],\n                           group=self.cpu_group)\n\n    return None\n</code></pre> fastvideo.distributed.parallel_state.GroupCoordinator.send_tensor_dict \u00b6 <pre><code>send_tensor_dict(\n    tensor_dict: dict[str, Tensor | Any],\n    dst: int | None = None,\n    all_gather_group: Optional[GroupCoordinator] = None,\n) -&gt; dict[str, torch.Tensor | Any] | None\n</code></pre> <p>Send the input tensor dictionary. NOTE: <code>dst</code> is the local rank of the source rank.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def send_tensor_dict(\n    self,\n    tensor_dict: dict[str, torch.Tensor | Any],\n    dst: int | None = None,\n    all_gather_group: Optional[\"GroupCoordinator\"] = None,\n) -&gt; dict[str, torch.Tensor | Any] | None:\n    \"\"\"Send the input tensor dictionary.\n    NOTE: `dst` is the local rank of the source rank.\n    \"\"\"\n    # Bypass the function if we are using only 1 GPU.\n    if not torch.distributed.is_initialized() or self.world_size == 1:\n        return tensor_dict\n\n    all_gather_size = (1 if all_gather_group is None else\n                       all_gather_group.world_size)\n    all_gather_rank = (0 if all_gather_group is None else\n                       all_gather_group.rank_in_group)\n\n    group = self.device_group\n    metadata_group = self.cpu_group\n\n    if dst is None:\n        dst = (self.rank_in_group + 1) % self.world_size\n    assert dst &lt; self.world_size, f\"Invalid dst rank ({dst})\"\n\n    metadata_list: list[tuple[Any, Any]] = []\n    assert isinstance(\n        tensor_dict,\n        dict), f\"Expecting a dictionary, got {type(tensor_dict)}\"\n    metadata_list, tensor_list = _split_tensor_dict(tensor_dict)\n    # `metadata_list` lives in CPU memory.\n    # `send_object_list` has serialization &amp; deserialization,\n    # all happening on CPU. Therefore, we can use the CPU group.\n    self.send_object(metadata_list, dst=dst)\n    for tensor in tensor_list:\n        if tensor.numel() == 0:\n            # Skip sending empty tensors.\n            continue\n\n        # send-allgather: send only a slice, then do allgather.\n        if (all_gather_group is not None\n                and tensor.numel() % all_gather_size == 0):\n            tensor = tensor.reshape(all_gather_size, -1)[all_gather_rank]\n\n        if tensor.is_cpu:\n            # use metadata_group for CPU tensors\n            torch.distributed.send(tensor,\n                                   dst=self.ranks[dst],\n                                   group=metadata_group)\n        else:\n            # use group for GPU tensors\n            torch.distributed.send(tensor, dst=self.ranks[dst], group=group)\n    return None\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.distributed.parallel_state-functions","title":"Functions","text":"fastvideo.distributed.parallel_state.destroy_model_parallel \u00b6 <pre><code>destroy_model_parallel() -&gt; None\n</code></pre> <p>Set the groups to none and destroy them.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def destroy_model_parallel() -&gt; None:\n    \"\"\"Set the groups to none and destroy them.\"\"\"\n    global _TP\n    if _TP:\n        _TP.destroy()\n    _TP = None\n\n    global _SP\n    if _SP:\n        _SP.destroy()\n    _SP = None\n\n    global _DP\n    if _DP:\n        _DP.destroy()\n    _DP = None\n</code></pre> fastvideo.distributed.parallel_state.get_dp_rank \u00b6 <pre><code>get_dp_rank() -&gt; int\n</code></pre> <p>Return my rank for the data parallel group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_dp_rank() -&gt; int:\n    \"\"\"Return my rank for the data parallel group.\"\"\"\n    return get_dp_group().rank_in_group\n</code></pre> fastvideo.distributed.parallel_state.get_dp_world_size \u00b6 <pre><code>get_dp_world_size() -&gt; int\n</code></pre> <p>Return world size for the data parallel group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_dp_world_size() -&gt; int:\n    \"\"\"Return world size for the data parallel group.\"\"\"\n    return get_dp_group().world_size\n</code></pre> fastvideo.distributed.parallel_state.get_local_torch_device \u00b6 <pre><code>get_local_torch_device() -&gt; torch.device\n</code></pre> <p>Return the torch device for the current rank.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_local_torch_device() -&gt; torch.device:\n    \"\"\"Return the torch device for the current rank.\"\"\"\n    from fastvideo.platforms import current_platform\n    if current_platform.is_npu():\n        device = torch.device(f\"npu:{envs.LOCAL_RANK}\")\n    elif current_platform.is_cuda_alike() or current_platform.is_cuda():\n        device = torch.device(f\"cuda:{envs.LOCAL_RANK}\")\n    else:\n        device = torch.device(\"mps\")\n    return device\n</code></pre> fastvideo.distributed.parallel_state.get_sp_parallel_rank \u00b6 <pre><code>get_sp_parallel_rank() -&gt; int\n</code></pre> <p>Return my rank for the sequence model parallel group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_sp_parallel_rank() -&gt; int:\n    \"\"\"Return my rank for the sequence model parallel group.\"\"\"\n    return get_sp_group().rank_in_group\n</code></pre> fastvideo.distributed.parallel_state.get_sp_world_size \u00b6 <pre><code>get_sp_world_size() -&gt; int\n</code></pre> <p>Return world size for the sequence model parallel group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_sp_world_size() -&gt; int:\n    \"\"\"Return world size for the sequence model parallel group.\"\"\"\n    return get_sp_group().world_size\n</code></pre> fastvideo.distributed.parallel_state.get_tp_rank \u00b6 <pre><code>get_tp_rank() -&gt; int\n</code></pre> <p>Return my rank for the tensor model parallel group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_tp_rank() -&gt; int:\n    \"\"\"Return my rank for the tensor model parallel group.\"\"\"\n    return get_tp_group().rank_in_group\n</code></pre> fastvideo.distributed.parallel_state.get_tp_world_size \u00b6 <pre><code>get_tp_world_size() -&gt; int\n</code></pre> <p>Return world size for the tensor model parallel group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_tp_world_size() -&gt; int:\n    \"\"\"Return world size for the tensor model parallel group.\"\"\"\n    return get_tp_group().world_size\n</code></pre> fastvideo.distributed.parallel_state.get_world_rank \u00b6 <pre><code>get_world_rank() -&gt; int\n</code></pre> <p>Return my rank for the world group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_world_rank() -&gt; int:\n    \"\"\"Return my rank for the world group.\"\"\"\n    return get_world_group().rank\n</code></pre> fastvideo.distributed.parallel_state.get_world_size \u00b6 <pre><code>get_world_size() -&gt; int\n</code></pre> <p>Return world size for the world group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_world_size() -&gt; int:\n    \"\"\"Return world size for the world group.\"\"\"\n    return get_world_group().world_size\n</code></pre> fastvideo.distributed.parallel_state.initialize_model_parallel \u00b6 <pre><code>initialize_model_parallel(\n    tensor_model_parallel_size: int = 1,\n    sequence_model_parallel_size: int = 1,\n    data_parallel_size: int = 1,\n    backend: str | None = None,\n) -&gt; None\n</code></pre> <p>Initialize model parallel groups.</p> <p>Parameters:</p> Name Type Description Default <code>tensor_model_parallel_size</code> <code>int</code> <p>number of GPUs used for tensor model parallelism (used for language encoder).</p> <code>1</code> <code>sequence_model_parallel_size</code> <code>int</code> <p>number of GPUs used for sequence model parallelism (used for DiT).</p> <code>1</code> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def initialize_model_parallel(\n    tensor_model_parallel_size: int = 1,\n    sequence_model_parallel_size: int = 1,\n    data_parallel_size: int = 1,\n    backend: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Initialize model parallel groups.\n\n    Arguments:\n        tensor_model_parallel_size: number of GPUs used for tensor model\n            parallelism (used for language encoder).\n        sequence_model_parallel_size: number of GPUs used for sequence model\n            parallelism (used for DiT).\n    \"\"\"\n    # Get world size and rank. Ensure some consistencies.\n    assert _WORLD is not None, \"world group is not initialized, please call init_distributed_environment first\"\n    world_size: int = get_world_size()\n    backend = backend or torch.distributed.get_backend(\n        get_world_group().device_group)\n    assert world_size &gt;= tensor_model_parallel_size, f\"world_size({world_size}) must be greater than or equal to tensor_model_parallel_size({tensor_model_parallel_size})\"\n    num_tensor_model_parallel_groups: int = (world_size //\n                                             tensor_model_parallel_size)\n    global _TP\n    assert _TP is None, (\"tensor model parallel group is already initialized\")\n    group_ranks = []\n    for i in range(num_tensor_model_parallel_groups):\n        ranks = list(\n            range(i * tensor_model_parallel_size,\n                  (i + 1) * tensor_model_parallel_size))\n        group_ranks.append(ranks)\n\n    # message queue broadcaster is only used in tensor model parallel group\n    _TP = init_model_parallel_group(group_ranks,\n                                    get_world_group().local_rank,\n                                    backend,\n                                    use_message_queue_broadcaster=True,\n                                    group_name=\"tp\")\n\n    # Build the sequence model-parallel groups.\n    num_sequence_model_parallel_groups: int = (world_size //\n                                               sequence_model_parallel_size)\n    global _SP\n    assert _SP is None, (\"sequence model parallel group is already initialized\")\n    group_ranks = []\n\n    # Since SP is incompatible with TP and PP, we can use a simpler group creation logic\n    for i in range(num_sequence_model_parallel_groups):\n        # Create groups of consecutive ranks\n        ranks = list(\n            range(i * sequence_model_parallel_size,\n                  (i + 1) * sequence_model_parallel_size))\n        group_ranks.append(ranks)\n\n    _SP = init_model_parallel_group(group_ranks,\n                                    get_world_group().local_rank,\n                                    backend,\n                                    group_name=\"sp\")\n\n    # Build the data parallel groups.\n    num_data_parallel_groups: int = sequence_model_parallel_size\n    global _DP\n    assert _DP is None, (\"data parallel group is already initialized\")\n    group_ranks = []\n\n    for i in range(num_data_parallel_groups):\n        ranks = list(range(i, world_size, num_data_parallel_groups))\n        group_ranks.append(ranks)\n\n    _DP = init_model_parallel_group(group_ranks,\n                                    get_world_group().local_rank,\n                                    backend,\n                                    group_name=\"dp\")\n</code></pre> fastvideo.distributed.parallel_state.initialize_sequence_parallel_group \u00b6 <pre><code>initialize_sequence_parallel_group(\n    sequence_model_parallel_size: int = 1,\n    backend: str | None = None,\n    group_name_suffix: str = \"\",\n) -&gt; GroupCoordinator\n</code></pre> <p>Initialize a sequence parallel group for a specific model.</p> <p>This function creates a sequence parallel group that can be used with the patch_sequence_parallel_group context manager. It allows different models to use different sequence parallelism configurations.</p> <p>Parameters:</p> Name Type Description Default <code>sequence_model_parallel_size</code> <code>int</code> <p>number of GPUs used for sequence model parallelism.</p> <code>1</code> <code>backend</code> <code>str | None</code> <p>communication backend to use.</p> <code>None</code> <code>group_name_suffix</code> <code>str</code> <p>optional suffix to make the group name unique.</p> <code>''</code> <p>Returns:</p> Type Description <code>GroupCoordinator</code> <p>A GroupCoordinator for sequence parallelism that can be used with</p> <code>GroupCoordinator</code> <p>the patch_sequence_parallel_group context manager.</p> Example usage <pre><code># Initialize sequence parallel group for model2\nsp_group_model2 = initialize_sequence_parallel_group(\n    sequence_model_parallel_size=2,\n    group_name_suffix=\"model2\"\n)\n\n# Use sequence parallelism for model2\nwith patch_sequence_parallel_group(sp_group_model2):\n    # Run model2 with sequence parallelism\n    output2 = model2(input2)\n</code></pre> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def initialize_sequence_parallel_group(\n        sequence_model_parallel_size: int = 1,\n        backend: str | None = None,\n        group_name_suffix: str = \"\") -&gt; GroupCoordinator:\n    \"\"\"Initialize a sequence parallel group for a specific model.\n\n    This function creates a sequence parallel group that can be used with the\n    patch_sequence_parallel_group context manager. It allows different models\n    to use different sequence parallelism configurations.\n\n    Arguments:\n        sequence_model_parallel_size: number of GPUs used for sequence model parallelism.\n        backend: communication backend to use.\n        group_name_suffix: optional suffix to make the group name unique.\n\n    Returns:\n        A GroupCoordinator for sequence parallelism that can be used with\n        the patch_sequence_parallel_group context manager.\n\n    Example usage:\n        ```python\n        # Initialize sequence parallel group for model2\n        sp_group_model2 = initialize_sequence_parallel_group(\n            sequence_model_parallel_size=2,\n            group_name_suffix=\"model2\"\n        )\n\n        # Use sequence parallelism for model2\n        with patch_sequence_parallel_group(sp_group_model2):\n            # Run model2 with sequence parallelism\n            output2 = model2(input2)\n        ```\n    \"\"\"\n    # Get world size and rank. Ensure some consistencies.\n    assert torch.distributed.is_initialized()\n    world_size: int = torch.distributed.get_world_size()\n    backend = backend or torch.distributed.get_backend(\n        get_world_group().device_group)\n\n    # Ensure the world size is compatible with the parallelism configuration\n    assert world_size % sequence_model_parallel_size == 0, \\\n        f\"World size ({world_size}) must be divisible by sequence_model_parallel_size ({sequence_model_parallel_size})\"\n\n    # Build the sequence model-parallel groups.\n    num_sequence_model_parallel_groups: int = (world_size //\n                                               sequence_model_parallel_size)\n    sp_group_ranks = []\n\n    for i in range(num_sequence_model_parallel_groups):\n        # Create groups of consecutive ranks\n        ranks = list(\n            range(i * sequence_model_parallel_size,\n                  (i + 1) * sequence_model_parallel_size))\n        sp_group_ranks.append(ranks)\n\n    # Create SP group coordinator with a unique name\n    group_name = f\"sp_{group_name_suffix}\" if group_name_suffix else \"sp\"\n    sp_group = init_model_parallel_group(sp_group_ranks,\n                                         get_world_group().local_rank,\n                                         backend,\n                                         group_name=group_name)\n\n    return sp_group\n</code></pre> fastvideo.distributed.parallel_state.initialize_tensor_parallel_group \u00b6 <pre><code>initialize_tensor_parallel_group(\n    tensor_model_parallel_size: int = 1,\n    backend: str | None = None,\n    group_name_suffix: str = \"\",\n) -&gt; GroupCoordinator\n</code></pre> <p>Initialize a tensor parallel group for a specific model.</p> <p>This function creates a tensor parallel group that can be used with the patch_tensor_parallel_group context manager. It allows different models to use different tensor parallelism configurations.</p> <p>Parameters:</p> Name Type Description Default <code>tensor_model_parallel_size</code> <code>int</code> <p>number of GPUs used for tensor model parallelism.</p> <code>1</code> <code>backend</code> <code>str | None</code> <p>communication backend to use.</p> <code>None</code> <code>group_name_suffix</code> <code>str</code> <p>optional suffix to make the group name unique.</p> <code>''</code> <p>Returns:</p> Type Description <code>GroupCoordinator</code> <p>A GroupCoordinator for tensor parallelism that can be used with</p> <code>GroupCoordinator</code> <p>the patch_tensor_parallel_group context manager.</p> Example usage <pre><code># Initialize tensor parallel group for model1\ntp_group_model1 = initialize_tensor_parallel_group(\n    tensor_model_parallel_size=4,\n    group_name_suffix=\"model1\"\n)\n\n# Use tensor parallelism for model1\nwith patch_tensor_parallel_group(tp_group_model1):\n    # Run model1 with tensor parallelism\n    output1 = model1(input1)\n</code></pre> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def initialize_tensor_parallel_group(\n        tensor_model_parallel_size: int = 1,\n        backend: str | None = None,\n        group_name_suffix: str = \"\") -&gt; GroupCoordinator:\n    \"\"\"Initialize a tensor parallel group for a specific model.\n\n    This function creates a tensor parallel group that can be used with the\n    patch_tensor_parallel_group context manager. It allows different models\n    to use different tensor parallelism configurations.\n\n    Arguments:\n        tensor_model_parallel_size: number of GPUs used for tensor model parallelism.\n        backend: communication backend to use.\n        group_name_suffix: optional suffix to make the group name unique.\n\n    Returns:\n        A GroupCoordinator for tensor parallelism that can be used with\n        the patch_tensor_parallel_group context manager.\n\n    Example usage:\n        ```python\n        # Initialize tensor parallel group for model1\n        tp_group_model1 = initialize_tensor_parallel_group(\n            tensor_model_parallel_size=4,\n            group_name_suffix=\"model1\"\n        )\n\n        # Use tensor parallelism for model1\n        with patch_tensor_parallel_group(tp_group_model1):\n            # Run model1 with tensor parallelism\n            output1 = model1(input1)\n        ```\n    \"\"\"\n    # Get world size and rank. Ensure some consistencies.\n    assert torch.distributed.is_initialized()\n    world_size: int = torch.distributed.get_world_size()\n    backend = backend or torch.distributed.get_backend(\n        get_world_group().device_group)\n\n    # Ensure the world size is compatible with the parallelism configuration\n    assert world_size % tensor_model_parallel_size == 0, \\\n        f\"World size ({world_size}) must be divisible by tensor_model_parallel_size ({tensor_model_parallel_size})\"\n\n    # Build the tensor model-parallel groups.\n    num_tensor_model_parallel_groups: int = (world_size //\n                                             tensor_model_parallel_size)\n    tp_group_ranks = []\n    for i in range(num_tensor_model_parallel_groups):\n        ranks = list(\n            range(i * tensor_model_parallel_size,\n                  (i + 1) * tensor_model_parallel_size))\n        tp_group_ranks.append(ranks)\n\n    # Create TP group coordinator with a unique name\n    group_name = f\"tp_{group_name_suffix}\" if group_name_suffix else \"tp\"\n    tp_group = init_model_parallel_group(tp_group_ranks,\n                                         get_world_group().local_rank,\n                                         backend,\n                                         use_message_queue_broadcaster=True,\n                                         group_name=group_name)\n\n    return tp_group\n</code></pre> fastvideo.distributed.parallel_state.is_the_same_node_as \u00b6 <pre><code>is_the_same_node_as(\n    pg: ProcessGroup | StatelessProcessGroup,\n    source_rank: int = 0,\n) -&gt; list[int]\n</code></pre> <p>This is a collective operation that returns if each rank is in the same node as the source rank. It tests if processes are attached to the same memory system (shared access to shared memory).</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def is_the_same_node_as(pg: ProcessGroup | StatelessProcessGroup,\n                        source_rank: int = 0) -&gt; list[int]:\n    \"\"\"\n    This is a collective operation that returns if each rank is in the same node\n    as the source rank. It tests if processes are attached to the same\n    memory system (shared access to shared memory).\n    \"\"\"\n    if isinstance(pg, ProcessGroup):\n        assert torch.distributed.get_backend(\n            pg) != torch.distributed.Backend.NCCL, (\n                \"in_the_same_node_as should be tested with a non-NCCL group.\")\n        # local rank inside the group\n        rank = torch.distributed.get_rank(group=pg)\n        world_size = torch.distributed.get_world_size(group=pg)\n\n        # global ranks of the processes in the group\n        ranks = torch.distributed.get_process_group_ranks(pg)\n    else:\n        rank = pg.rank\n        world_size = pg.world_size\n        ranks = list(range(world_size))\n\n    # local tensor in each process to store the result\n    is_in_the_same_node = torch.tensor([0] * world_size, dtype=torch.int32)\n\n    magic_message = b\"magic_message\"\n    shm = None\n\n    try:\n        with contextlib.suppress(OSError):\n            if rank == source_rank:\n                # create a shared memory segment\n                shm = shared_memory.SharedMemory(create=True, size=128)\n                shm.buf[:len(magic_message)] = magic_message\n                if isinstance(pg, ProcessGroup):\n                    torch.distributed.broadcast_object_list(\n                        [shm.name], src=ranks[source_rank], group=pg)\n                else:\n                    pg.broadcast_obj(shm.name, src=source_rank)\n                is_in_the_same_node[rank] = 1\n            else:\n                # try to open the shared memory segment\n                if isinstance(pg, ProcessGroup):\n                    recv = [None]\n                    torch.distributed.broadcast_object_list(\n                        recv, src=ranks[source_rank], group=pg)\n                    name = recv[0]\n                else:\n                    name = pg.broadcast_obj(None, src=source_rank)\n                # fix to https://stackoverflow.com/q/62748654/9191338\n                # Python incorrectly tracks shared memory even if it is not\n                # created by the process. The following patch is a workaround.\n                with patch(\"multiprocessing.resource_tracker.register\",\n                           lambda *args, **kwargs: None):\n                    shm = shared_memory.SharedMemory(name=name)\n                if shm.buf[:len(magic_message)] == magic_message:\n                    is_in_the_same_node[rank] = 1\n    except Exception as e:\n        logger.error(\"Error ignored in is_in_the_same_node: %s\", e)\n    finally:\n        if shm:\n            shm.close()\n\n    if isinstance(pg, ProcessGroup):\n        torch.distributed.barrier(group=pg)\n    else:\n        pg.barrier()\n\n    # clean up the shared memory segment\n    with contextlib.suppress(OSError):\n        if rank == source_rank and shm:\n            shm.unlink()\n\n    if isinstance(pg, ProcessGroup):\n        torch.distributed.all_reduce(is_in_the_same_node, group=pg)\n        aggregated_data = is_in_the_same_node\n    else:\n        aggregated_data = torch.zeros_like(is_in_the_same_node)\n        for i in range(world_size):\n            rank_data = pg.broadcast_obj(is_in_the_same_node, src=i)\n            aggregated_data += rank_data\n\n    return [x == 1 for x in aggregated_data.tolist()]\n</code></pre> fastvideo.distributed.parallel_state.model_parallel_is_initialized \u00b6 <pre><code>model_parallel_is_initialized() -&gt; bool\n</code></pre> <p>Check if tensor, sequence parallel groups are initialized.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def model_parallel_is_initialized() -&gt; bool:\n    \"\"\"Check if tensor, sequence parallel groups are initialized.\"\"\"\n    return _TP is not None and _SP is not None and _DP is not None\n</code></pre> fastvideo.distributed.parallel_state.patch_tensor_parallel_group \u00b6 <pre><code>patch_tensor_parallel_group(tp_group: GroupCoordinator)\n</code></pre> <p>Patch the tp group temporarily until this function ends.</p> <p>This method is for draft workers of speculative decoding to run draft model with different tp degree from that of target model workers.</p> <p>Parameters:</p> Name Type Description Default <code>tp_group</code> <code>GroupCoordinator</code> <p>the tp group coordinator</p> required Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>@contextmanager\ndef patch_tensor_parallel_group(tp_group: GroupCoordinator):\n    \"\"\"Patch the tp group temporarily until this function ends.\n\n    This method is for draft workers of speculative decoding to run draft model\n    with different tp degree from that of target model workers.\n\n    Args:\n        tp_group (GroupCoordinator): the tp group coordinator\n    \"\"\"\n    global _TP_STATE_PATCHED\n    assert not _TP_STATE_PATCHED, \"Should not call when it's already patched\"\n\n    _TP_STATE_PATCHED = True\n    old_tp_group = get_tp_group()\n    global _TP\n    _TP = tp_group\n    try:\n        yield\n    finally:\n        # restore the original state\n        _TP_STATE_PATCHED = False\n        _TP = old_tp_group\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.distributed.utils","title":"fastvideo.distributed.utils","text":""},{"location":"api/fastvideo/#fastvideo.distributed.utils-classes","title":"Classes","text":"fastvideo.distributed.utils.StatelessProcessGroup <code>dataclass</code> \u00b6 <pre><code>StatelessProcessGroup(\n    rank: int,\n    world_size: int,\n    store: Store,\n    data_expiration_seconds: int = 3600,\n    send_dst_counter: dict[int, int] = dict(),\n    recv_src_counter: dict[int, int] = dict(),\n    broadcast_send_counter: int = 0,\n    broadcast_recv_src_counter: dict[int, int] = dict(),\n    entries: deque[tuple[str, float]] = deque(),\n)\n</code></pre> <p>A dataclass to hold a metadata store, and the rank, world_size of the group. Only use it to communicate metadata between processes. For data-plane communication, create NCCL-related objects.</p> Functions\u00b6 fastvideo.distributed.utils.StatelessProcessGroup.all_gather_obj \u00b6 <pre><code>all_gather_obj(obj: Any) -&gt; list[Any]\n</code></pre> <p>All gather an object from all ranks.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def all_gather_obj(self, obj: Any) -&gt; list[Any]:\n    \"\"\"All gather an object from all ranks.\"\"\"\n    gathered_objs = []\n    for i in range(self.world_size):\n        if i == self.rank:\n            gathered_objs.append(obj)\n            self.broadcast_obj(obj, src=self.rank)\n        else:\n            recv_obj = self.broadcast_obj(None, src=i)\n            gathered_objs.append(recv_obj)\n    return gathered_objs\n</code></pre> fastvideo.distributed.utils.StatelessProcessGroup.barrier \u00b6 <pre><code>barrier()\n</code></pre> <p>A barrier to synchronize all ranks.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def barrier(self):\n    \"\"\"A barrier to synchronize all ranks.\"\"\"\n    for i in range(self.world_size):\n        if i == self.rank:\n            self.broadcast_obj(None, src=self.rank)\n        else:\n            self.broadcast_obj(None, src=i)\n</code></pre> fastvideo.distributed.utils.StatelessProcessGroup.broadcast_obj \u00b6 <pre><code>broadcast_obj(obj: Any | None, src: int) -&gt; Any\n</code></pre> <p>Broadcast an object from a source rank to all other ranks. It does not clean up after all ranks have received the object. Use it for limited times, e.g., for initialization.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def broadcast_obj(self, obj: Any | None, src: int) -&gt; Any:\n    \"\"\"Broadcast an object from a source rank to all other ranks.\n    It does not clean up after all ranks have received the object.\n    Use it for limited times, e.g., for initialization.\n    \"\"\"\n    if self.rank == src:\n        self.expire_data()\n        key = (f\"broadcast_from/{src}/\"\n               f\"{self.broadcast_send_counter}\")\n        self.store.set(key, pickle.dumps(obj))\n        self.broadcast_send_counter += 1\n        self.entries.append((key, time.perf_counter()))\n        return obj\n    else:\n        key = (f\"broadcast_from/{src}/\"\n               f\"{self.broadcast_recv_src_counter[src]}\")\n        recv_obj = pickle.loads(self.store.get(key))\n        self.broadcast_recv_src_counter[src] += 1\n        return recv_obj\n</code></pre> fastvideo.distributed.utils.StatelessProcessGroup.create <code>staticmethod</code> \u00b6 <pre><code>create(\n    host: str,\n    port: int,\n    rank: int,\n    world_size: int,\n    data_expiration_seconds: int = 3600,\n) -&gt; StatelessProcessGroup\n</code></pre> <p>A replacement for <code>torch.distributed.init_process_group</code> that does not pollute the global state.</p> <p>If we have process A and process B called <code>torch.distributed.init_process_group</code> to form a group, and then we want to form another group with process A, B, C, D, it is not possible in PyTorch, because process A and process B have already formed a group, and process C and process D cannot join that group. This function is a workaround for this issue.</p> <p><code>torch.distributed.init_process_group</code> is a global call, while this function is a stateless call. It will return a <code>StatelessProcessGroup</code> object that can be used for exchanging metadata. With this function, process A and process B can call <code>StatelessProcessGroup.create</code> to form a group, and then process A, B, C, and D can call <code>StatelessProcessGroup.create</code> to form another group.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>@staticmethod\ndef create(\n    host: str,\n    port: int,\n    rank: int,\n    world_size: int,\n    data_expiration_seconds: int = 3600,\n) -&gt; \"StatelessProcessGroup\":\n    \"\"\"A replacement for `torch.distributed.init_process_group` that does not\n    pollute the global state.\n\n    If we have process A and process B called `torch.distributed.init_process_group`\n    to form a group, and then we want to form another group with process A, B, C,\n    D, it is not possible in PyTorch, because process A and process B have already\n    formed a group, and process C and process D cannot join that group. This\n    function is a workaround for this issue.\n\n    `torch.distributed.init_process_group` is a global call, while this function\n    is a stateless call. It will return a `StatelessProcessGroup` object that can be\n    used for exchanging metadata. With this function, process A and process B\n    can call `StatelessProcessGroup.create` to form a group, and then process A, B,\n    C, and D can call `StatelessProcessGroup.create` to form another group.\n    \"\"\" # noqa\n    store = TCPStore(\n        host_name=host,\n        port=port,\n        world_size=world_size,\n        is_master=(rank == 0),\n    )\n\n    return StatelessProcessGroup(\n        rank=rank,\n        world_size=world_size,\n        store=store,\n        data_expiration_seconds=data_expiration_seconds)\n</code></pre> fastvideo.distributed.utils.StatelessProcessGroup.expire_data \u00b6 <pre><code>expire_data() -&gt; None\n</code></pre> <p>Expire data that is older than <code>data_expiration_seconds</code> seconds.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def expire_data(self) -&gt; None:\n    \"\"\"Expire data that is older than `data_expiration_seconds` seconds.\"\"\"\n    while self.entries:\n        # check the oldest entry\n        key, timestamp = self.entries[0]\n        if time.perf_counter() - timestamp &gt; self.data_expiration_seconds:\n            self.store.delete_key(key)\n            self.entries.popleft()\n        else:\n            break\n</code></pre> fastvideo.distributed.utils.StatelessProcessGroup.recv_obj \u00b6 <pre><code>recv_obj(src: int) -&gt; Any\n</code></pre> <p>Receive an object from a source rank.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def recv_obj(self, src: int) -&gt; Any:\n    \"\"\"Receive an object from a source rank.\"\"\"\n    obj = pickle.loads(\n        self.store.get(f\"send_to/{self.rank}/{self.recv_src_counter[src]}\"))\n    self.recv_src_counter[src] += 1\n    return obj\n</code></pre> fastvideo.distributed.utils.StatelessProcessGroup.send_obj \u00b6 <pre><code>send_obj(obj: Any, dst: int)\n</code></pre> <p>Send an object to a destination rank.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def send_obj(self, obj: Any, dst: int):\n    \"\"\"Send an object to a destination rank.\"\"\"\n    self.expire_data()\n    key = f\"send_to/{dst}/{self.send_dst_counter[dst]}\"\n    self.store.set(key, pickle.dumps(obj))\n    self.send_dst_counter[dst] += 1\n    self.entries.append((key, time.perf_counter()))\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.distributed.utils-functions","title":"Functions","text":"fastvideo.distributed.utils.divide \u00b6 <pre><code>divide(numerator: int, denominator: int) -&gt; int\n</code></pre> <p>Ensure that numerator is divisible by the denominator and return the division value.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def divide(numerator: int, denominator: int) -&gt; int:\n    \"\"\"Ensure that numerator is divisible by the denominator and return\n    the division value.\"\"\"\n    ensure_divisibility(numerator, denominator)\n    return numerator // denominator\n</code></pre> fastvideo.distributed.utils.ensure_divisibility \u00b6 <pre><code>ensure_divisibility(numerator, denominator) -&gt; None\n</code></pre> <p>Ensure that numerator is divisible by the denominator.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def ensure_divisibility(numerator, denominator) -&gt; None:\n    \"\"\"Ensure that numerator is divisible by the denominator.\"\"\"\n    assert numerator % denominator == 0, \"{} is not divisible by {}\".format(\n        numerator, denominator)\n</code></pre> fastvideo.distributed.utils.split_tensor_along_last_dim \u00b6 <pre><code>split_tensor_along_last_dim(\n    tensor: Tensor,\n    num_partitions: int,\n    contiguous_split_chunks: bool = False,\n) -&gt; Sequence[torch.Tensor]\n</code></pre> <p>Split a tensor along its last dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>input tensor.</p> required <code>num_partitions</code> <code>int</code> <p>number of partitions to split the tensor</p> required <code>contiguous_split_chunks</code> <code>bool</code> <p>If True, make each chunk contiguous                      in memory.</p> <code>False</code> <p>Returns:</p> Type Description <code>Sequence[Tensor]</code> <p>A list of Tensors</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def split_tensor_along_last_dim(\n    tensor: torch.Tensor,\n    num_partitions: int,\n    contiguous_split_chunks: bool = False,\n) -&gt; Sequence[torch.Tensor]:\n    \"\"\" Split a tensor along its last dimension.\n\n        Arguments:\n            tensor: input tensor.\n            num_partitions: number of partitions to split the tensor\n            contiguous_split_chunks: If True, make each chunk contiguous\n                                     in memory.\n\n        Returns:\n            A list of Tensors\n    \"\"\"\n    # Get the size and dimension.\n    last_dim = tensor.dim() - 1\n    last_dim_size = divide(tensor.size()[last_dim], num_partitions)\n    # Split.\n    tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)\n    # NOTE: torch.split does not create contiguous tensors by default.\n    if contiguous_split_chunks:\n        return tuple(chunk.contiguous() for chunk in tensor_list)\n\n    return tuple(tensor_list)\n</code></pre>"},{"location":"api/fastvideo/#fastvideoentrypoints","title":"fastvideo.entrypoints","text":""},{"location":"api/fastvideo/#fastvideo.entrypoints","title":"entrypoints","text":""},{"location":"api/fastvideo/#fastvideo.entrypoints-modules","title":"Modules","text":""},{"location":"api/fastvideo/#fastvideo.entrypoints.cli","title":"fastvideo.entrypoints.cli","text":""},{"location":"api/fastvideo/#fastvideo.entrypoints.cli-modules","title":"Modules","text":"fastvideo.entrypoints.cli.cli_types \u00b6 Classes\u00b6 fastvideo.entrypoints.cli.cli_types.CLISubcommand \u00b6 <p>Base class for CLI subcommands</p> Functions\u00b6 fastvideo.entrypoints.cli.cli_types.CLISubcommand.cmd \u00b6 <pre><code>cmd(args: Namespace) -&gt; None\n</code></pre> <p>Execute the command with the given arguments</p> Source code in <code>fastvideo/entrypoints/cli/cli_types.py</code> <pre><code>def cmd(self, args: argparse.Namespace) -&gt; None:\n    \"\"\"Execute the command with the given arguments\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.entrypoints.cli.cli_types.CLISubcommand.subparser_init \u00b6 <pre><code>subparser_init(\n    subparsers: _SubParsersAction,\n) -&gt; FlexibleArgumentParser\n</code></pre> <p>Initialize the subparser for this command</p> Source code in <code>fastvideo/entrypoints/cli/cli_types.py</code> <pre><code>def subparser_init(\n        self,\n        subparsers: argparse._SubParsersAction) -&gt; FlexibleArgumentParser:\n    \"\"\"Initialize the subparser for this command\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.entrypoints.cli.cli_types.CLISubcommand.validate \u00b6 <pre><code>validate(args: Namespace) -&gt; None\n</code></pre> <p>Validate the arguments for this command</p> Source code in <code>fastvideo/entrypoints/cli/cli_types.py</code> <pre><code>def validate(self, args: argparse.Namespace) -&gt; None:\n    \"\"\"Validate the arguments for this command\"\"\"\n    pass\n</code></pre> fastvideo.entrypoints.cli.generate \u00b6 Classes\u00b6 fastvideo.entrypoints.cli.generate.GenerateSubcommand \u00b6 <pre><code>GenerateSubcommand()\n</code></pre> <p>               Bases: <code>CLISubcommand</code></p> <p>The <code>generate</code> subcommand for the FastVideo CLI</p> Source code in <code>fastvideo/entrypoints/cli/generate.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.name = \"generate\"\n    super().__init__()\n    self.init_arg_names = self._get_init_arg_names()\n    self.generation_arg_names = self._get_generation_arg_names()\n</code></pre> Functions\u00b6 fastvideo.entrypoints.cli.generate.GenerateSubcommand.validate \u00b6 <pre><code>validate(args: Namespace) -&gt; None\n</code></pre> <p>Validate the arguments for this command</p> Source code in <code>fastvideo/entrypoints/cli/generate.py</code> <pre><code>def validate(self, args: argparse.Namespace) -&gt; None:\n    \"\"\"Validate the arguments for this command\"\"\"\n    if args.num_gpus is not None and args.num_gpus &lt;= 0:\n        raise ValueError(\"Number of gpus must be positive\")\n\n    if args.config and not os.path.exists(args.config):\n        raise ValueError(f\"Config file not found: {args.config}\")\n</code></pre> Functions\u00b6 fastvideo.entrypoints.cli.main \u00b6 Classes\u00b6 Functions\u00b6 fastvideo.entrypoints.cli.main.cmd_init \u00b6 <pre><code>cmd_init() -&gt; list[CLISubcommand]\n</code></pre> <p>Initialize all commands from separate modules</p> Source code in <code>fastvideo/entrypoints/cli/main.py</code> <pre><code>def cmd_init() -&gt; list[CLISubcommand]:\n    \"\"\"Initialize all commands from separate modules\"\"\"\n    commands = []\n    commands.extend(generate_cmd_init())\n    return commands\n</code></pre> fastvideo.entrypoints.cli.utils \u00b6 Functions\u00b6 fastvideo.entrypoints.cli.utils.launch_distributed \u00b6 <pre><code>launch_distributed(\n    num_gpus: int,\n    args: list[str],\n    master_port: int | None = None,\n) -&gt; int\n</code></pre> <p>Launch a distributed job with the given arguments</p> <p>Parameters:</p> Name Type Description Default <code>num_gpus</code> <code>int</code> <p>Number of GPUs to use</p> required <code>args</code> <code>list[str]</code> <p>Arguments to pass to v1_fastvideo_inference.py (defaults to sys.argv[1:])</p> required <code>master_port</code> <code>int | None</code> <p>Port for the master process (default: random)</p> <code>None</code> Source code in <code>fastvideo/entrypoints/cli/utils.py</code> <pre><code>def launch_distributed(num_gpus: int,\n                       args: list[str],\n                       master_port: int | None = None) -&gt; int:\n    \"\"\"\n    Launch a distributed job with the given arguments\n\n    Args:\n        num_gpus: Number of GPUs to use\n        args: Arguments to pass to v1_fastvideo_inference.py (defaults to sys.argv[1:])\n        master_port: Port for the master process (default: random)\n    \"\"\"\n\n    current_env = os.environ.copy()\n    python_executable = sys.executable\n    project_root = os.path.abspath(\n        os.path.join(os.path.dirname(__file__), \"../../../..\"))\n    main_script = os.path.join(project_root,\n                               \"fastvideo/sample/v1_fastvideo_inference.py\")\n\n    cmd = [\n        python_executable, \"-m\", \"torch.distributed.run\",\n        f\"--nproc_per_node={num_gpus}\"\n    ]\n\n    if master_port is not None:\n        cmd.append(f\"--master_port={master_port}\")\n\n    cmd.append(main_script)\n    cmd.extend(args)\n\n    logger.info(\"Running inference with %d GPU(s)\", num_gpus)\n    logger.info(\"Launching command: %s\", \" \".join(cmd))\n\n    current_env[\"PYTHONIOENCODING\"] = \"utf-8\"\n    process = subprocess.Popen(cmd,\n                               env=current_env,\n                               stdout=subprocess.PIPE,\n                               stderr=subprocess.STDOUT,\n                               universal_newlines=True,\n                               bufsize=1,\n                               encoding='utf-8',\n                               errors='replace')\n\n    if process.stdout:\n        for line in iter(process.stdout.readline, ''):\n            print(line.strip())\n\n    return process.wait()\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.entrypoints.video_generator","title":"fastvideo.entrypoints.video_generator","text":"<p>VideoGenerator module for FastVideo.</p> <p>This module provides a consolidated interface for generating videos using diffusion models.</p>"},{"location":"api/fastvideo/#fastvideo.entrypoints.video_generator-classes","title":"Classes","text":"fastvideo.entrypoints.video_generator.VideoGenerator \u00b6 <pre><code>VideoGenerator(\n    fastvideo_args: FastVideoArgs,\n    executor_class: type[Executor],\n    log_stats: bool,\n)\n</code></pre> <p>A unified class for generating videos using diffusion models.</p> <p>This class provides a simple interface for video generation with rich customization options, similar to popular frameworks like HF Diffusers.</p> <p>Initialize the video generator.</p> <p>Parameters:</p> Name Type Description Default <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments</p> required <code>executor_class</code> <code>type[Executor]</code> <p>The executor class to use for inference</p> required Source code in <code>fastvideo/entrypoints/video_generator.py</code> <pre><code>def __init__(self, fastvideo_args: FastVideoArgs,\n             executor_class: type[Executor], log_stats: bool):\n    \"\"\"\n    Initialize the video generator.\n\n    Args:\n        fastvideo_args: The inference arguments\n        executor_class: The executor class to use for inference\n    \"\"\"\n    self.fastvideo_args = fastvideo_args\n    self.executor = executor_class(fastvideo_args)\n</code></pre> Functions\u00b6 fastvideo.entrypoints.video_generator.VideoGenerator.from_fastvideo_args <code>classmethod</code> \u00b6 <pre><code>from_fastvideo_args(\n    fastvideo_args: FastVideoArgs,\n) -&gt; VideoGenerator\n</code></pre> <p>Create a video generator with the specified arguments.</p> <p>Parameters:</p> Name Type Description Default <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments</p> required <p>Returns:</p> Type Description <code>VideoGenerator</code> <p>The created video generator</p> Source code in <code>fastvideo/entrypoints/video_generator.py</code> <pre><code>@classmethod\ndef from_fastvideo_args(cls,\n                        fastvideo_args: FastVideoArgs) -&gt; \"VideoGenerator\":\n    \"\"\"\n    Create a video generator with the specified arguments.\n\n    Args:\n        fastvideo_args: The inference arguments\n\n    Returns:\n        The created video generator\n    \"\"\"\n    # Initialize distributed environment if needed\n    # initialize_distributed_and_parallelism(fastvideo_args)\n\n    executor_class = Executor.get_class(fastvideo_args)\n    return cls(\n        fastvideo_args=fastvideo_args,\n        executor_class=executor_class,\n        log_stats=False,  # TODO: implement\n    )\n</code></pre> fastvideo.entrypoints.video_generator.VideoGenerator.from_pretrained <code>classmethod</code> \u00b6 <pre><code>from_pretrained(\n    model_path: str,\n    device: str | None = None,\n    torch_dtype: dtype | None = None,\n    **kwargs\n) -&gt; VideoGenerator\n</code></pre> <p>Create a video generator from a pretrained model.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path or identifier for the pretrained model</p> required <code>device</code> <code>str | None</code> <p>Device to load the model on (e.g., \"cuda\", \"cuda:0\", \"cpu\")</p> <code>None</code> <code>torch_dtype</code> <code>dtype | None</code> <p>Data type for model weights (e.g., torch.float16)</p> <code>None</code> <code>pipeline_config</code> <p>Pipeline config to use for inference</p> required <code>**kwargs</code> <p>Additional arguments to customize model loading, set any FastVideoArgs or PipelineConfig attributes here.</p> <code>{}</code> <p>Returns:</p> Type Description <code>VideoGenerator</code> <p>The created video generator</p> <p>Priority level: Default pipeline config &lt; User's pipeline config &lt; User's kwargs</p> Source code in <code>fastvideo/entrypoints/video_generator.py</code> <pre><code>@classmethod\ndef from_pretrained(cls,\n                    model_path: str,\n                    device: str | None = None,\n                    torch_dtype: torch.dtype | None = None,\n                    **kwargs) -&gt; \"VideoGenerator\":\n    \"\"\"\n    Create a video generator from a pretrained model.\n\n    Args:\n        model_path: Path or identifier for the pretrained model\n        device: Device to load the model on (e.g., \"cuda\", \"cuda:0\", \"cpu\")\n        torch_dtype: Data type for model weights (e.g., torch.float16)\n        pipeline_config: Pipeline config to use for inference\n        **kwargs: Additional arguments to customize model loading, set any FastVideoArgs or PipelineConfig attributes here.\n\n    Returns:\n        The created video generator\n\n    Priority level: Default pipeline config &lt; User's pipeline config &lt; User's kwargs\n    \"\"\"\n    # If users also provide some kwargs, it will override the FastVideoArgs and PipelineConfig.\n    kwargs['model_path'] = model_path\n    fastvideo_args = FastVideoArgs.from_kwargs(**kwargs)\n\n    return cls.from_fastvideo_args(fastvideo_args)\n</code></pre> fastvideo.entrypoints.video_generator.VideoGenerator.generate_video \u00b6 <pre><code>generate_video(\n    prompt: str | None = None,\n    sampling_param: SamplingParam | None = None,\n    **kwargs\n) -&gt; dict[str, Any] | list[np.ndarray] | list[\n    dict[str, Any]\n]\n</code></pre> <p>Generate a video based on the given prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str | None</code> <p>The prompt to use for generation (optional if prompt_txt is provided)</p> <code>None</code> <code>negative_prompt</code> <p>The negative prompt to use (overrides the one in fastvideo_args)</p> required <code>output_path</code> <p>Path to save the video (overrides the one in fastvideo_args)</p> required <code>output_video_name</code> <p>Name of the video file to save. Default is the first 100 characters of the prompt.</p> required <code>save_video</code> <p>Whether to save the video to disk</p> required <code>return_frames</code> <p>Whether to return the raw frames</p> required <code>num_inference_steps</code> <p>Number of denoising steps (overrides fastvideo_args)</p> required <code>guidance_scale</code> <p>Classifier-free guidance scale (overrides fastvideo_args)</p> required <code>num_frames</code> <p>Number of frames to generate (overrides fastvideo_args)</p> required <code>height</code> <p>Height of generated video (overrides fastvideo_args)</p> required <code>width</code> <p>Width of generated video (overrides fastvideo_args)</p> required <code>fps</code> <p>Frames per second for saved video (overrides fastvideo_args)</p> required <code>seed</code> <p>Random seed for generation (overrides fastvideo_args)</p> required <code>callback</code> <p>Callback function called after each step</p> required <code>callback_steps</code> <p>Number of steps between each callback</p> required <p>Returns:</p> Type Description <code>dict[str, Any] | list[ndarray] | list[dict[str, Any]]</code> <p>Either the output dictionary, list of frames, or list of results for batch processing</p> Source code in <code>fastvideo/entrypoints/video_generator.py</code> <pre><code>def generate_video(\n    self,\n    prompt: str | None = None,\n    sampling_param: SamplingParam | None = None,\n    **kwargs,\n) -&gt; dict[str, Any] | list[np.ndarray] | list[dict[str, Any]]:\n    \"\"\"\n    Generate a video based on the given prompt.\n\n    Args:\n        prompt: The prompt to use for generation (optional if prompt_txt is provided)\n        negative_prompt: The negative prompt to use (overrides the one in fastvideo_args)\n        output_path: Path to save the video (overrides the one in fastvideo_args)\n        output_video_name: Name of the video file to save. Default is the first 100 characters of the prompt.\n        save_video: Whether to save the video to disk\n        return_frames: Whether to return the raw frames\n        num_inference_steps: Number of denoising steps (overrides fastvideo_args)\n        guidance_scale: Classifier-free guidance scale (overrides fastvideo_args)\n        num_frames: Number of frames to generate (overrides fastvideo_args)\n        height: Height of generated video (overrides fastvideo_args)\n        width: Width of generated video (overrides fastvideo_args)\n        fps: Frames per second for saved video (overrides fastvideo_args)\n        seed: Random seed for generation (overrides fastvideo_args)\n        callback: Callback function called after each step\n        callback_steps: Number of steps between each callback\n\n    Returns:\n        Either the output dictionary, list of frames, or list of results for batch processing\n    \"\"\"\n    # Handle batch processing from text file\n    if self.fastvideo_args.prompt_txt is not None:\n        prompt_txt_path = self.fastvideo_args.prompt_txt\n        if not os.path.exists(prompt_txt_path):\n            raise FileNotFoundError(\n                f\"Prompt text file not found: {prompt_txt_path}\")\n\n        # Read prompts from file\n        with open(prompt_txt_path, encoding='utf-8') as f:\n            prompts = [line.strip() for line in f if line.strip()]\n\n        if not prompts:\n            raise ValueError(f\"No prompts found in file: {prompt_txt_path}\")\n\n        logger.info(\"Found %d prompts in %s\", len(prompts), prompt_txt_path)\n\n        if sampling_param is not None:\n            original_output_video_name = sampling_param.output_video_name\n        else:\n            original_output_video_name = None\n\n        results = []\n        for i, batch_prompt in enumerate(prompts):\n            logger.info(\"Processing prompt %d/%d: %s...\", i + 1,\n                        len(prompts), batch_prompt[:100])\n\n            try:\n                # Generate video for this prompt using the same logic below\n                if sampling_param is not None and original_output_video_name is not None:\n                    sampling_param.output_video_name = original_output_video_name + f\"_{i}\"\n                result = self._generate_single_video(\n                    batch_prompt, sampling_param, **kwargs)\n\n                # Add prompt info to result\n                if isinstance(result, dict):\n                    result[\"prompt_index\"] = i\n                    result[\"prompt\"] = batch_prompt\n\n                results.append(result)\n                logger.info(\"Successfully generated video for prompt %d\",\n                            i + 1)\n\n            except Exception as e:\n                logger.error(\"Failed to generate video for prompt %d: %s\",\n                             i + 1, e)\n                continue\n\n        logger.info(\n            \"Completed batch processing. Generated %d videos successfully.\",\n            len(results))\n        return results\n\n    # Single prompt generation (original behavior)\n    if prompt is None:\n        raise ValueError(\"Either prompt or prompt_txt must be provided\")\n\n    return self._generate_single_video(prompt, sampling_param, **kwargs)\n</code></pre> fastvideo.entrypoints.video_generator.VideoGenerator.shutdown \u00b6 <pre><code>shutdown()\n</code></pre> <p>Shutdown the video generator.</p> Source code in <code>fastvideo/entrypoints/video_generator.py</code> <pre><code>def shutdown(self):\n    \"\"\"\n    Shutdown the video generator.\n    \"\"\"\n    self.executor.shutdown()\n    del self.executor\n</code></pre> fastvideo.entrypoints.video_generator.VideoGenerator.unmerge_lora_weights \u00b6 <pre><code>unmerge_lora_weights() -&gt; None\n</code></pre> <p>Use unmerged weights for inference to produce videos that align with  validation videos generated during training.</p> Source code in <code>fastvideo/entrypoints/video_generator.py</code> <pre><code>def unmerge_lora_weights(self) -&gt; None:\n    \"\"\"\n    Use unmerged weights for inference to produce videos that align with \n    validation videos generated during training.\n    \"\"\"\n    self.executor.unmerge_lora_weights()\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.entrypoints.video_generator-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideomodels","title":"fastvideo.models","text":""},{"location":"api/fastvideo/#fastvideo.models","title":"models","text":""},{"location":"api/fastvideo/#fastvideo.models-modules","title":"Modules","text":""},{"location":"api/fastvideo/#fastvideo.models.hf_transformer_utils","title":"fastvideo.models.hf_transformer_utils","text":"<p>Utilities for Huggingface Transformers.</p>"},{"location":"api/fastvideo/#fastvideo.models.hf_transformer_utils-functions","title":"Functions","text":"fastvideo.models.hf_transformer_utils.check_gguf_file \u00b6 <pre><code>check_gguf_file(model: str | PathLike) -&gt; bool\n</code></pre> <p>Check if the file is a GGUF model.</p> Source code in <code>fastvideo/models/hf_transformer_utils.py</code> <pre><code>def check_gguf_file(model: str | os.PathLike) -&gt; bool:\n    \"\"\"Check if the file is a GGUF model.\"\"\"\n    model = Path(model)\n    if not model.is_file():\n        return False\n    elif model.suffix == \".gguf\":\n        return True\n\n    with open(model, \"rb\") as f:\n        header = f.read(4)\n    return header == b\"GGUF\"\n</code></pre> fastvideo.models.hf_transformer_utils.get_diffusers_config \u00b6 <pre><code>get_diffusers_config(model: str) -&gt; dict[str, Any]\n</code></pre> <p>Gets a configuration for the given diffusers model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The model name or path.</p> required <code>fastvideo_args</code> <p>Optional inference arguments to override in the config.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The loaded configuration.</p> Source code in <code>fastvideo/models/hf_transformer_utils.py</code> <pre><code>def get_diffusers_config(model: str, ) -&gt; dict[str, Any]:\n    \"\"\"Gets a configuration for the given diffusers model.\n\n    Args:\n        model: The model name or path.\n        fastvideo_args: Optional inference arguments to override in the config.\n\n    Returns:\n        The loaded configuration.\n    \"\"\"\n    config_name = \"config.json\"\n    if \"scheduler\" in model:\n        config_name = \"scheduler_config.json\"\n    # Check if the model path exists\n    if os.path.exists(model):\n        config_file = os.path.join(model, config_name)\n        if os.path.exists(config_file):\n            try:\n                # Load the config directly from the file\n                with open(config_file) as f:\n                    config_dict: dict[str, Any] = json.load(f)\n                if \"_diffusers_version\" in config_dict:\n                    config_dict.pop(\"_diffusers_version\")\n                # TODO(will): apply any overrides from inference args\n                return config_dict\n            except Exception as e:\n                raise RuntimeError(\n                    f\"Failed to load diffusers config from {config_file}: {e}\"\n                ) from e\n        raise RuntimeError(f\"Config file not found at {config_file}\")\n    else:\n        raise RuntimeError(f\"Diffusers config file not found at {model}\")\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.models.loader","title":"fastvideo.models.loader","text":""},{"location":"api/fastvideo/#fastvideo.models.loader-modules","title":"Modules","text":"fastvideo.models.loader.component_loader \u00b6 Classes\u00b6 fastvideo.models.loader.component_loader.ComponentLoader \u00b6 <pre><code>ComponentLoader(device=None)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for loading a specific type of model component.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def __init__(self, device=None) -&gt; None:\n    self.device = device\n</code></pre> Functions\u00b6 fastvideo.models.loader.component_loader.ComponentLoader.for_module_type <code>classmethod</code> \u00b6 <pre><code>for_module_type(\n    module_type: str, transformers_or_diffusers: str\n) -&gt; ComponentLoader\n</code></pre> <p>Factory method to create a component loader for a specific module type.</p> <p>Parameters:</p> Name Type Description Default <code>module_type</code> <code>str</code> <p>Type of module (e.g., \"vae\", \"text_encoder\", \"transformer\", \"scheduler\")</p> required <code>transformers_or_diffusers</code> <code>str</code> <p>Whether the module is from transformers or diffusers</p> required <p>Returns:</p> Type Description <code>ComponentLoader</code> <p>A component loader for the specified module type</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>@classmethod\ndef for_module_type(cls, module_type: str,\n                    transformers_or_diffusers: str) -&gt; 'ComponentLoader':\n    \"\"\"\n    Factory method to create a component loader for a specific module type.\n\n    Args:\n        module_type: Type of module (e.g., \"vae\", \"text_encoder\", \"transformer\", \"scheduler\")\n        transformers_or_diffusers: Whether the module is from transformers or diffusers\n\n    Returns:\n        A component loader for the specified module type\n    \"\"\"\n    # Map of module types to their loader classes and expected library\n    module_loaders = {\n        \"scheduler\": (SchedulerLoader, \"diffusers\"),\n        \"transformer\": (TransformerLoader, \"diffusers\"),\n        \"transformer_2\": (TransformerLoader, \"diffusers\"),\n        \"vae\": (VAELoader, \"diffusers\"),\n        \"text_encoder\": (TextEncoderLoader, \"transformers\"),\n        \"text_encoder_2\": (TextEncoderLoader, \"transformers\"),\n        \"tokenizer\": (TokenizerLoader, \"transformers\"),\n        \"tokenizer_2\": (TokenizerLoader, \"transformers\"),\n        \"image_processor\": (ImageProcessorLoader, \"transformers\"),\n        \"image_encoder\": (ImageEncoderLoader, \"transformers\"),\n    }\n\n    if module_type in module_loaders:\n        loader_cls, expected_library = module_loaders[module_type]\n        # Assert that the library matches what's expected for this module type\n        assert transformers_or_diffusers == expected_library, f\"{module_type} must be loaded from {expected_library}, got {transformers_or_diffusers}\"\n        return loader_cls()\n\n    # For unknown module types, use a generic loader\n    logger.warning(\n        \"No specific loader found for module type: %s. Using generic loader.\",\n        module_type)\n    return GenericComponentLoader(transformers_or_diffusers)\n</code></pre> fastvideo.models.loader.component_loader.ComponentLoader.load <code>abstractmethod</code> \u00b6 <pre><code>load(model_path: str, fastvideo_args: FastVideoArgs)\n</code></pre> <p>Load the component based on the model path, architecture, and inference args.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the component model</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>FastVideoArgs</p> required <p>Returns:</p> Type Description <p>The loaded component</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>@abstractmethod\ndef load(self, model_path: str, fastvideo_args: FastVideoArgs):\n    \"\"\"\n    Load the component based on the model path, architecture, and inference args.\n\n    Args:\n        model_path: Path to the component model\n        fastvideo_args: FastVideoArgs\n\n    Returns:\n        The loaded component\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.models.loader.component_loader.GenericComponentLoader \u00b6 <pre><code>GenericComponentLoader(library='transformers')\n</code></pre> <p>               Bases: <code>ComponentLoader</code></p> <p>Generic loader for components that don't have a specific loader.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def __init__(self, library=\"transformers\") -&gt; None:\n    super().__init__()\n    self.library = library\n</code></pre> Functions\u00b6 fastvideo.models.loader.component_loader.GenericComponentLoader.load \u00b6 <pre><code>load(model_path: str, fastvideo_args: FastVideoArgs)\n</code></pre> <p>Load a generic component based on the model path, and inference args.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def load(self, model_path: str, fastvideo_args: FastVideoArgs):\n    \"\"\"Load a generic component based on the model path, and inference args.\"\"\"\n    logger.warning(\"Using generic loader for %s with library %s\",\n                   model_path, self.library)\n\n    if self.library == \"transformers\":\n        from transformers import AutoModel\n\n        model = AutoModel.from_pretrained(\n            model_path,\n            trust_remote_code=fastvideo_args.trust_remote_code,\n            revision=fastvideo_args.revision,\n        )\n        logger.info(\"Loaded generic transformers model: %s\",\n                    model.__class__.__name__)\n        return model\n    elif self.library == \"diffusers\":\n        logger.warning(\n            \"Generic loading for diffusers components is not fully implemented\"\n        )\n\n        model_config = get_diffusers_config(model=model_path)\n        logger.info(\"Diffusers Model config: %s\", model_config)\n        # This is a placeholder - in a real implementation, you'd need to handle this properly\n        return None\n    else:\n        raise ValueError(f\"Unsupported library: {self.library}\")\n</code></pre> fastvideo.models.loader.component_loader.ImageEncoderLoader \u00b6 <pre><code>ImageEncoderLoader(device=None)\n</code></pre> <p>               Bases: <code>TextEncoderLoader</code></p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def __init__(self, device=None) -&gt; None:\n    self.device = device\n</code></pre> Functions\u00b6 fastvideo.models.loader.component_loader.ImageEncoderLoader.load \u00b6 <pre><code>load(model_path: str, fastvideo_args: FastVideoArgs)\n</code></pre> <p>Load the text encoders based on the model path, and inference args.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def load(self, model_path: str, fastvideo_args: FastVideoArgs):\n    \"\"\"Load the text encoders based on the model path, and inference args.\"\"\"\n    # model_config: PretrainedConfig = get_hf_config(\n    #     model=model_path,\n    #     trust_remote_code=fastvideo_args.trust_remote_code,\n    #     revision=fastvideo_args.revision,\n    #     model_override_args=None,\n    # )\n    with open(os.path.join(model_path, \"config.json\")) as f:\n        model_config = json.load(f)\n    model_config.pop(\"_name_or_path\", None)\n    model_config.pop(\"transformers_version\", None)\n    model_config.pop(\"torch_dtype\", None)\n    model_config.pop(\"model_type\", None)\n    logger.info(\"HF Model config: %s\", model_config)\n\n    encoder_config = fastvideo_args.pipeline_config.image_encoder_config\n    encoder_config.update_model_arch(model_config)\n\n    from fastvideo.platforms import current_platform\n\n    if fastvideo_args.image_encoder_cpu_offload:\n        target_device = torch.device(\"mps\") if current_platform.is_mps() else torch.device(\"cpu\")\n    else:\n        target_device = get_local_torch_device()\n    # TODO(will): add support for other dtypes\n    return self.load_model(\n        model_path, encoder_config, target_device, fastvideo_args,\n        fastvideo_args.pipeline_config.image_encoder_precision)\n</code></pre> fastvideo.models.loader.component_loader.ImageProcessorLoader \u00b6 <pre><code>ImageProcessorLoader(device=None)\n</code></pre> <p>               Bases: <code>ComponentLoader</code></p> <p>Loader for image processor.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def __init__(self, device=None) -&gt; None:\n    self.device = device\n</code></pre> Functions\u00b6 fastvideo.models.loader.component_loader.ImageProcessorLoader.load \u00b6 <pre><code>load(model_path: str, fastvideo_args: FastVideoArgs)\n</code></pre> <p>Load the image processor based on the model path, and inference args.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def load(self, model_path: str, fastvideo_args: FastVideoArgs):\n    \"\"\"Load the image processor based on the model path, and inference args.\"\"\"\n    logger.info(\"Loading image processor from %s\", model_path)\n\n    image_processor = AutoImageProcessor.from_pretrained(model_path, )\n    logger.info(\"Loaded image processor: %s\",\n                image_processor.__class__.__name__)\n    return image_processor\n</code></pre> fastvideo.models.loader.component_loader.PipelineComponentLoader \u00b6 <p>Utility class for loading pipeline components. This replaces the chain of if-else statements in load_pipeline_module.</p> Functions\u00b6 fastvideo.models.loader.component_loader.PipelineComponentLoader.load_module <code>staticmethod</code> \u00b6 <pre><code>load_module(\n    module_name: str,\n    component_model_path: str,\n    transformers_or_diffusers: str,\n    fastvideo_args: FastVideoArgs,\n)\n</code></pre> <p>Load a pipeline module.</p> <p>Parameters:</p> Name Type Description Default <code>module_name</code> <code>str</code> <p>Name of the module (e.g., \"vae\", \"text_encoder\", \"transformer\", \"scheduler\")</p> required <code>component_model_path</code> <code>str</code> <p>Path to the component model</p> required <code>transformers_or_diffusers</code> <code>str</code> <p>Whether the module is from transformers or diffusers</p> required <code>pipeline_args</code> <p>Inference arguments</p> required <p>Returns:</p> Type Description <p>The loaded module</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>@staticmethod\ndef load_module(module_name: str, component_model_path: str,\n                transformers_or_diffusers: str,\n                fastvideo_args: FastVideoArgs):\n    \"\"\"\n    Load a pipeline module.\n\n    Args:\n        module_name: Name of the module (e.g., \"vae\", \"text_encoder\", \"transformer\", \"scheduler\")\n        component_model_path: Path to the component model\n        transformers_or_diffusers: Whether the module is from transformers or diffusers\n        pipeline_args: Inference arguments\n\n    Returns:\n        The loaded module\n    \"\"\"\n    logger.info(\n        \"Loading %s using %s from %s\",\n        module_name,\n        transformers_or_diffusers,\n        component_model_path,\n    )\n\n    # Get the appropriate loader for this module type\n    loader = ComponentLoader.for_module_type(module_name,\n                                             transformers_or_diffusers)\n\n    # Load the module\n    return loader.load(component_model_path, fastvideo_args)\n</code></pre> fastvideo.models.loader.component_loader.SchedulerLoader \u00b6 <pre><code>SchedulerLoader(device=None)\n</code></pre> <p>               Bases: <code>ComponentLoader</code></p> <p>Loader for scheduler.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def __init__(self, device=None) -&gt; None:\n    self.device = device\n</code></pre> Functions\u00b6 fastvideo.models.loader.component_loader.SchedulerLoader.load \u00b6 <pre><code>load(model_path: str, fastvideo_args: FastVideoArgs)\n</code></pre> <p>Load the scheduler based on the model path, and inference args.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def load(self, model_path: str, fastvideo_args: FastVideoArgs):\n    \"\"\"Load the scheduler based on the model path, and inference args.\"\"\"\n    config = get_diffusers_config(model=model_path)\n\n    class_name = config.pop(\"_class_name\")\n    assert class_name is not None, \"Model config does not contain a _class_name attribute. Only diffusers format is supported.\"\n\n    scheduler_cls, _ = ModelRegistry.resolve_model_cls(class_name)\n\n    scheduler = scheduler_cls(**config)\n    if fastvideo_args.pipeline_config.flow_shift is not None:\n        scheduler.set_shift(fastvideo_args.pipeline_config.flow_shift)\n    if fastvideo_args.pipeline_config.timesteps_scale is not None:\n        scheduler.set_timesteps_scale(\n            fastvideo_args.pipeline_config.timesteps_scale)\n    return scheduler\n</code></pre> fastvideo.models.loader.component_loader.TextEncoderLoader \u00b6 <pre><code>TextEncoderLoader(device=None)\n</code></pre> <p>               Bases: <code>ComponentLoader</code></p> <p>Loader for text encoders.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def __init__(self, device=None) -&gt; None:\n    self.device = device\n</code></pre> Classes\u00b6 fastvideo.models.loader.component_loader.TextEncoderLoader.Source <code>dataclass</code> \u00b6 <pre><code>Source(\n    model_or_path: str,\n    prefix: str = \"\",\n    fall_back_to_pt: bool = True,\n    allow_patterns_overrides: list[str] | None = None,\n)\n</code></pre> <p>A source for weights.</p> Attributes\u00b6 fastvideo.models.loader.component_loader.TextEncoderLoader.Source.allow_patterns_overrides <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>allow_patterns_overrides: list[str] | None = None\n</code></pre> <p>If defined, weights will load exclusively using these patterns.</p> fastvideo.models.loader.component_loader.TextEncoderLoader.Source.fall_back_to_pt <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>fall_back_to_pt: bool = True\n</code></pre> <p>Whether .pt weights can be used.</p> fastvideo.models.loader.component_loader.TextEncoderLoader.Source.model_or_path <code>instance-attribute</code> \u00b6 <pre><code>model_or_path: str\n</code></pre> <p>The model ID or path.</p> fastvideo.models.loader.component_loader.TextEncoderLoader.Source.prefix <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>prefix: str = ''\n</code></pre> <p>A prefix to prepend to all weights.</p> Functions\u00b6 fastvideo.models.loader.component_loader.TextEncoderLoader.load \u00b6 <pre><code>load(model_path: str, fastvideo_args: FastVideoArgs)\n</code></pre> <p>Load the text encoders based on the model path, and inference args.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def load(self, model_path: str, fastvideo_args: FastVideoArgs):\n    \"\"\"Load the text encoders based on the model path, and inference args.\"\"\"\n    # model_config: PretrainedConfig = get_hf_config(\n    #     model=model_path,\n    #     trust_remote_code=fastvideo_args.trust_remote_code,\n    #     revision=fastvideo_args.revision,\n    #     model_override_args=None,\n    # )\n    model_config = get_diffusers_config(model=model_path)\n    model_config.pop(\"_name_or_path\", None)\n    model_config.pop(\"transformers_version\", None)\n    model_config.pop(\"model_type\", None)\n    model_config.pop(\"tokenizer_class\", None)\n    model_config.pop(\"torch_dtype\", None)\n    logger.info(\"HF Model config: %s\", model_config)\n\n    # @TODO(Wei): Better way to handle this?\n    try:\n        encoder_config = fastvideo_args.pipeline_config.text_encoder_configs[\n            0]\n        encoder_config.update_model_arch(model_config)\n        encoder_precision = fastvideo_args.pipeline_config.text_encoder_precisions[\n            0]\n    except Exception:\n        encoder_config = fastvideo_args.pipeline_config.text_encoder_configs[\n            1]\n        encoder_config.update_model_arch(model_config)\n        encoder_precision = fastvideo_args.pipeline_config.text_encoder_precisions[\n            1]\n\n    target_device = get_local_torch_device()\n    # TODO(will): add support for other dtypes\n    return self.load_model(model_path, encoder_config, target_device,\n                           fastvideo_args, encoder_precision)\n</code></pre> fastvideo.models.loader.component_loader.TokenizerLoader \u00b6 <pre><code>TokenizerLoader(device=None)\n</code></pre> <p>               Bases: <code>ComponentLoader</code></p> <p>Loader for tokenizers.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def __init__(self, device=None) -&gt; None:\n    self.device = device\n</code></pre> Functions\u00b6 fastvideo.models.loader.component_loader.TokenizerLoader.load \u00b6 <pre><code>load(model_path: str, fastvideo_args: FastVideoArgs)\n</code></pre> <p>Load the tokenizer based on the model path, and inference args.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def load(self, model_path: str, fastvideo_args: FastVideoArgs):\n    \"\"\"Load the tokenizer based on the model path, and inference args.\"\"\"\n    logger.info(\"Loading tokenizer from %s\", model_path)\n\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_path,  # \"&lt;path to model&gt;/tokenizer\"\n        # in v0, this was same string as encoder_name \"ClipTextModel\"\n        # TODO(will): pass these tokenizer kwargs from inference args? Maybe\n        # other method of config?\n        padding_size='right',\n    )\n    logger.info(\"Loaded tokenizer: %s\", tokenizer.__class__.__name__)\n    return tokenizer\n</code></pre> fastvideo.models.loader.component_loader.TransformerLoader \u00b6 <pre><code>TransformerLoader(device=None)\n</code></pre> <p>               Bases: <code>ComponentLoader</code></p> <p>Loader for transformer.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def __init__(self, device=None) -&gt; None:\n    self.device = device\n</code></pre> Functions\u00b6 fastvideo.models.loader.component_loader.TransformerLoader.load \u00b6 <pre><code>load(model_path: str, fastvideo_args: FastVideoArgs)\n</code></pre> <p>Load the transformer based on the model path, and inference args.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def load(self, model_path: str, fastvideo_args: FastVideoArgs):\n    \"\"\"Load the transformer based on the model path, and inference args.\"\"\"\n    config = get_diffusers_config(model=model_path)\n    hf_config = deepcopy(config)\n    cls_name = config.pop(\"_class_name\")\n    if cls_name is None:\n        raise ValueError(\n            \"Model config does not contain a _class_name attribute. \"\n            \"Only diffusers format is supported.\")\n\n    logger.info(\"transformer cls_name: %s\", cls_name)\n    if fastvideo_args.override_transformer_cls_name is not None:\n        cls_name = fastvideo_args.override_transformer_cls_name\n        logger.info(\"Overriding transformer cls_name to %s\", cls_name)\n\n    fastvideo_args.model_paths[\"transformer\"] = model_path\n\n    # Config from Diffusers supersedes fastvideo's model config\n    dit_config = fastvideo_args.pipeline_config.dit_config\n    dit_config.update_model_arch(config)\n\n    model_cls, _ = ModelRegistry.resolve_model_cls(cls_name)\n\n    # Find all safetensors files\n    safetensors_list = glob.glob(\n        os.path.join(str(model_path), \"*.safetensors\"))\n    if not safetensors_list:\n        raise ValueError(f\"No safetensors files found in {model_path}\")\n\n    # Check if we should use custom initialization weights\n    custom_weights_path = getattr(fastvideo_args, 'init_weights_from_safetensors', None)\n    use_custom_weights = (custom_weights_path and os.path.exists(custom_weights_path) and \n                        not hasattr(fastvideo_args, '_loading_teacher_critic_model'))\n\n    if use_custom_weights:\n        if 'transformer_2' in model_path:\n            custom_weights_path = getattr(fastvideo_args, 'init_weights_from_safetensors_2', None)\n        assert custom_weights_path is not None, \"Custom initialization weights must be provided\"\n        if os.path.isdir(custom_weights_path):\n            safetensors_list = glob.glob(\n                os.path.join(str(custom_weights_path), \"*.safetensors\"))\n        else:\n            assert custom_weights_path.endswith(\".safetensors\"), \"Custom initialization weights must be a safetensors file\"\n            safetensors_list = [custom_weights_path]\n\n    logger.info(\"Loading model from %s safetensors files: %s\",\n                len(safetensors_list), safetensors_list)\n\n    default_dtype = PRECISION_TO_TYPE[\n        fastvideo_args.pipeline_config.dit_precision]\n\n    # Load the model using FSDP loader\n    logger.info(\"Loading model from %s, default_dtype: %s\", cls_name,\n                default_dtype)\n    assert fastvideo_args.hsdp_shard_dim is not None\n    model = maybe_load_fsdp_model(\n        model_cls=model_cls,\n        init_params={\n            \"config\": dit_config,\n            \"hf_config\": hf_config\n        },\n        weight_dir_list=safetensors_list,\n        device=get_local_torch_device(),\n        hsdp_replicate_dim=fastvideo_args.hsdp_replicate_dim,\n        hsdp_shard_dim=fastvideo_args.hsdp_shard_dim,\n        cpu_offload=fastvideo_args.dit_cpu_offload,\n        pin_cpu_memory=fastvideo_args.pin_cpu_memory,\n        fsdp_inference=fastvideo_args.use_fsdp_inference,\n        # TODO(will): make these configurable\n        default_dtype=default_dtype,\n        param_dtype=torch.bfloat16,\n        reduce_dtype=torch.float32,\n        output_dtype=None,\n        training_mode=fastvideo_args.training_mode,\n        enable_torch_compile=fastvideo_args.enable_torch_compile,\n        torch_compile_kwargs=fastvideo_args.torch_compile_kwargs)\n\n\n    total_params = sum(p.numel() for p in model.parameters())\n    logger.info(\"Loaded model with %.2fB parameters\", total_params / 1e9)\n\n    assert next(model.parameters()).dtype == default_dtype, \"Model dtype does not match default dtype\"\n\n    model = model.eval()\n    return model\n</code></pre> fastvideo.models.loader.component_loader.VAELoader \u00b6 <pre><code>VAELoader(device=None)\n</code></pre> <p>               Bases: <code>ComponentLoader</code></p> <p>Loader for VAE.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def __init__(self, device=None) -&gt; None:\n    self.device = device\n</code></pre> Functions\u00b6 fastvideo.models.loader.component_loader.VAELoader.load \u00b6 <pre><code>load(model_path: str, fastvideo_args: FastVideoArgs)\n</code></pre> <p>Load the VAE based on the model path, and inference args.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def load(self, model_path: str, fastvideo_args: FastVideoArgs):\n    \"\"\"Load the VAE based on the model path, and inference args.\"\"\"\n    config = get_diffusers_config(model=model_path)\n    class_name = config.pop(\"_class_name\")\n    assert class_name is not None, \"Model config does not contain a _class_name attribute. Only diffusers format is supported.\"\n    fastvideo_args.model_paths[\"vae\"] = model_path\n\n    vae_config = fastvideo_args.pipeline_config.vae_config\n    vae_config.update_model_arch(config)\n\n    from fastvideo.platforms import current_platform\n\n    if fastvideo_args.vae_cpu_offload:\n        target_device = torch.device(\"mps\") if current_platform.is_mps() else torch.device(\"cpu\")\n    else:\n        target_device = get_local_torch_device()\n\n    with set_default_torch_dtype(PRECISION_TO_TYPE[\n            fastvideo_args.pipeline_config.vae_precision]):\n        vae_cls, _ = ModelRegistry.resolve_model_cls(class_name)\n        vae = vae_cls(vae_config).to(target_device)\n\n    # Find all safetensors files\n    safetensors_list = glob.glob(\n        os.path.join(str(model_path), \"*.safetensors\"))\n    # TODO(PY)\n    assert len(\n        safetensors_list\n    ) == 1, f\"Found {len(safetensors_list)} safetensors files in {model_path}\"\n    loaded = safetensors_load_file(safetensors_list[0])\n    vae.load_state_dict(\n        loaded, strict=False)  # We might only load encoder or decoder\n\n    return vae.eval()\n</code></pre> Functions\u00b6 fastvideo.models.loader.fsdp_load \u00b6 Functions\u00b6 fastvideo.models.loader.fsdp_load.load_model_from_full_model_state_dict \u00b6 <pre><code>load_model_from_full_model_state_dict(\n    model: FSDPModule | Module,\n    full_sd_iterator: Generator[\n        tuple[str, Tensor], None, None\n    ],\n    device: device,\n    param_dtype: dtype,\n    strict: bool = False,\n    cpu_offload: bool = False,\n    param_names_mapping: Callable[\n        [str], tuple[str, Any, Any]\n    ]\n    | None = None,\n    training_mode: bool = True,\n) -&gt; _IncompatibleKeys\n</code></pre> <p>Converting full state dict into a sharded state dict and loading it into FSDP model (if training) or normal huggingface model Args:     model (Union[FSDPModule, torch.nn.Module]): Model to generate fully qualified names for cpu_state_dict     full_sd_iterator (Generator): an iterator yielding (param_name, tensor) pairs     device (torch.device): device used to move full state dict tensors     param_dtype (torch.dtype): dtype used to move full state dict tensors     strict (bool): flag to check if to load the model in strict mode     cpu_offload (bool): flag to check if FSDP offload is enabled     param_names_mapping (Optional[Callable[[str], str]]): a function that maps full param name to sharded param name     training_mode (bool): apply FSDP only for training Returns:     <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:         * missing_keys is a list of str containing the missing keys         * unexpected_keys is a list of str containing the unexpected keys</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If got FSDP with more than 1D.</p> Source code in <code>fastvideo/models/loader/fsdp_load.py</code> <pre><code>def load_model_from_full_model_state_dict(\n    model: FSDPModule | torch.nn.Module,\n    full_sd_iterator: Generator[tuple[str, torch.Tensor], None, None],\n    device: torch.device,\n    param_dtype: torch.dtype,\n    strict: bool = False,\n    cpu_offload: bool = False,\n    param_names_mapping: Callable[[str], tuple[str, Any, Any]] | None = None,\n    training_mode: bool = True,\n) -&gt; _IncompatibleKeys:\n    \"\"\"\n    Converting full state dict into a sharded state dict\n    and loading it into FSDP model (if training) or normal huggingface model\n    Args:\n        model (Union[FSDPModule, torch.nn.Module]): Model to generate fully qualified names for cpu_state_dict\n        full_sd_iterator (Generator): an iterator yielding (param_name, tensor) pairs\n        device (torch.device): device used to move full state dict tensors\n        param_dtype (torch.dtype): dtype used to move full state dict tensors\n        strict (bool): flag to check if to load the model in strict mode\n        cpu_offload (bool): flag to check if FSDP offload is enabled\n        param_names_mapping (Optional[Callable[[str], str]]): a function that maps full param name to sharded param name\n        training_mode (bool): apply FSDP only for training\n    Returns:\n        ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n            * **missing_keys** is a list of str containing the missing keys\n            * **unexpected_keys** is a list of str containing the unexpected keys\n\n    Raises:\n        NotImplementedError: If got FSDP with more than 1D.\n    \"\"\"\n    meta_sd = model.state_dict()\n    sharded_sd = {}\n    custom_param_sd, reverse_param_names_mapping = hf_to_custom_state_dict(\n        full_sd_iterator, param_names_mapping)  # type: ignore\n    for target_param_name, full_tensor in custom_param_sd.items():\n        meta_sharded_param = meta_sd.get(target_param_name)\n        if meta_sharded_param is None:\n            raise ValueError(\n                f\"Parameter {target_param_name} not found in custom model state dict. The hf to custom mapping may be incorrect.\"\n            )\n        if not hasattr(meta_sharded_param, \"device_mesh\"):\n            full_tensor = full_tensor.to(device=device, dtype=param_dtype)\n            # In cases where parts of the model aren't sharded, some parameters will be plain tensors\n            sharded_tensor = full_tensor\n        else:\n            full_tensor = full_tensor.to(device=device, dtype=param_dtype)\n            sharded_tensor = distribute_tensor(\n                full_tensor,\n                meta_sharded_param.device_mesh,\n                meta_sharded_param.placements,\n            )\n            if cpu_offload:\n                sharded_tensor = sharded_tensor.cpu()\n        sharded_sd[target_param_name] = nn.Parameter(sharded_tensor)\n\n    model.reverse_param_names_mapping = reverse_param_names_mapping\n    unused_keys = set(meta_sd.keys()) - set(sharded_sd.keys())\n    if unused_keys:\n        logger.warning(\"Found unloaded parameters in meta state dict: %s\",\n                       unused_keys)\n\n    # List of allowed parameter name patterns\n    ALLOWED_NEW_PARAM_PATTERNS = [\"gate_compress\"]  # Can be extended as needed\n    for new_param_name in unused_keys:\n        if not any(pattern in new_param_name\n                   for pattern in ALLOWED_NEW_PARAM_PATTERNS):\n            logger.error(\"Unsupported new parameter: %s. Allowed patterns: %s\",\n                         new_param_name, ALLOWED_NEW_PARAM_PATTERNS)\n            raise ValueError(\n                f\"New parameter '{new_param_name}' is not supported. \"\n                f\"Currently only parameters containing {ALLOWED_NEW_PARAM_PATTERNS} are allowed.\"\n            )\n        meta_sharded_param = meta_sd.get(new_param_name)\n        if not hasattr(meta_sharded_param, \"device_mesh\"):\n            # Initialize with zeros\n            sharded_tensor = torch.zeros_like(meta_sharded_param,\n                                              device=device,\n                                              dtype=param_dtype)\n        else:\n            # Initialize with zeros and distribute\n            full_tensor = torch.zeros_like(meta_sharded_param,\n                                           device=device,\n                                           dtype=param_dtype)\n            sharded_tensor = distribute_tensor(\n                full_tensor,\n                meta_sharded_param.device_mesh,\n                meta_sharded_param.placements,\n            )\n            if cpu_offload:\n                sharded_tensor = sharded_tensor.cpu()\n        sharded_sd[new_param_name] = nn.Parameter(sharded_tensor)\n\n    # choose `assign=True` since we cannot call `copy_` on meta tensor\n    return model.load_state_dict(sharded_sd, strict=strict, assign=True)\n</code></pre> fastvideo.models.loader.fsdp_load.maybe_load_fsdp_model \u00b6 <pre><code>maybe_load_fsdp_model(\n    model_cls: type[Module],\n    init_params: dict[str, Any],\n    weight_dir_list: list[str],\n    device: device,\n    hsdp_replicate_dim: int,\n    hsdp_shard_dim: int,\n    default_dtype: dtype,\n    param_dtype: dtype,\n    reduce_dtype: dtype,\n    cpu_offload: bool = False,\n    fsdp_inference: bool = False,\n    output_dtype: dtype | None = None,\n    training_mode: bool = True,\n    pin_cpu_memory: bool = True,\n    enable_torch_compile: bool = False,\n    torch_compile_kwargs: dict[str, Any] | None = None,\n) -&gt; torch.nn.Module\n</code></pre> <p>Load the model with FSDP if is training, else load the model without FSDP.</p> Source code in <code>fastvideo/models/loader/fsdp_load.py</code> <pre><code>def maybe_load_fsdp_model(\n    model_cls: type[nn.Module],\n    init_params: dict[str, Any],\n    weight_dir_list: list[str],\n    device: torch.device,\n    hsdp_replicate_dim: int,\n    hsdp_shard_dim: int,\n    default_dtype: torch.dtype,\n    param_dtype: torch.dtype,\n    reduce_dtype: torch.dtype,\n    cpu_offload: bool = False,\n    fsdp_inference: bool = False,\n    output_dtype: torch.dtype | None = None,\n    training_mode: bool = True,\n    pin_cpu_memory: bool = True,\n    enable_torch_compile: bool = False,\n    torch_compile_kwargs: dict[str, Any] | None = None,\n) -&gt; torch.nn.Module:\n    \"\"\"\n    Load the model with FSDP if is training, else load the model without FSDP.\n    \"\"\"\n    # NOTE(will): cast_forward_inputs=True shouldn't be needed as we are\n    # manually casting the inputs to the model\n    mp_policy = MixedPrecisionPolicy(param_dtype,\n                                     reduce_dtype,\n                                     output_dtype,\n                                     cast_forward_inputs=False)\n\n    set_mixed_precision_policy(\n        param_dtype=param_dtype,\n        reduce_dtype=reduce_dtype,\n        output_dtype=output_dtype,\n        mp_policy=mp_policy,\n    )\n\n    logger.info(\"Loading model with default_dtype: %s\", default_dtype)\n    with set_default_dtype(default_dtype), torch.device(\"meta\"):\n        model = model_cls(**init_params)\n\n    # Check if we should use FSDP\n    use_fsdp = training_mode or fsdp_inference\n\n    # Disable FSDP for MPS as it's not compatible\n    from fastvideo.platforms import current_platform\n    if current_platform.is_mps():\n        use_fsdp = False\n        logger.info(\"Disabling FSDP for MPS platform as it's not compatible\")\n\n    if use_fsdp:\n        world_size = hsdp_replicate_dim * hsdp_shard_dim\n        if not training_mode and not fsdp_inference:\n            hsdp_replicate_dim = world_size\n            hsdp_shard_dim = 1\n\n        if current_platform.is_npu():\n            with torch.device(\"cpu\"):\n                device_mesh = init_device_mesh(\n                    \"npu\",\n                    # (Replicate(), Shard(dim=0))\n                    mesh_shape=(hsdp_replicate_dim, hsdp_shard_dim),\n                    mesh_dim_names=(\"replicate\", \"shard\"),\n                )\n        else:\n            device_mesh = init_device_mesh(\n            \"cuda\",\n            # (Replicate(), Shard(dim=0))\n            mesh_shape=(hsdp_replicate_dim, hsdp_shard_dim),\n            mesh_dim_names=(\"replicate\", \"shard\"),\n        )\n        shard_model(model,\n                    cpu_offload=cpu_offload,\n                    reshard_after_forward=True,\n                    mp_policy=mp_policy,\n                    mesh=device_mesh,\n                    fsdp_shard_conditions=model._fsdp_shard_conditions,\n                    pin_cpu_memory=pin_cpu_memory)\n\n    weight_iterator = safetensors_weights_iterator(weight_dir_list)\n    param_names_mapping_fn = get_param_names_mapping(model.param_names_mapping)\n    load_model_from_full_model_state_dict(\n        model,\n        weight_iterator,\n        device,\n        default_dtype,\n        strict=True,\n        cpu_offload=cpu_offload,\n        param_names_mapping=param_names_mapping_fn,\n    )\n    for n, p in chain(model.named_parameters(), model.named_buffers()):\n        if p.is_meta:\n            raise RuntimeError(\n                f\"Unexpected param or buffer {n} on meta device.\")\n        # Avoid unintended computation graph accumulation during inference\n        if isinstance(p, torch.nn.Parameter):\n            p.requires_grad = False\n\n    compile_in_loader = enable_torch_compile and training_mode\n    if compile_in_loader:\n        compile_kwargs = torch_compile_kwargs or {}\n        logger.info(\"Enabling torch.compile for FSDP training module with kwargs=%s\",\n                    compile_kwargs)\n        model = torch.compile(model, **compile_kwargs)\n        logger.info(\"torch.compile enabled for %s\", type(model).__name__)\n    return model\n</code></pre> fastvideo.models.loader.fsdp_load.set_default_dtype \u00b6 <pre><code>set_default_dtype(\n    dtype: dtype,\n) -&gt; Generator[None, None, None]\n</code></pre> <p>Context manager to set torch's default dtype.</p> <p>Parameters:</p> Name Type Description Default <code>dtype</code> <code>dtype</code> <p>The desired default dtype inside the context manager.</p> required <p>Returns:</p> Name Type Description <code>ContextManager</code> <code>None</code> <p>context manager for setting default dtype.</p> Example <p>with set_default_dtype(torch.bfloat16):     x = torch.tensor([1, 2, 3])     x.dtype torch.bfloat16</p> Source code in <code>fastvideo/models/loader/fsdp_load.py</code> <pre><code>@contextlib.contextmanager\ndef set_default_dtype(dtype: torch.dtype) -&gt; Generator[None, None, None]:\n    \"\"\"\n    Context manager to set torch's default dtype.\n\n    Args:\n        dtype (torch.dtype): The desired default dtype inside the context manager.\n\n    Returns:\n        ContextManager: context manager for setting default dtype.\n\n    Example:\n        &gt;&gt;&gt; with set_default_dtype(torch.bfloat16):\n        &gt;&gt;&gt;     x = torch.tensor([1, 2, 3])\n        &gt;&gt;&gt;     x.dtype\n        torch.bfloat16\n\n\n    \"\"\"\n    old_dtype = torch.get_default_dtype()\n    torch.set_default_dtype(dtype)\n    try:\n        yield\n    finally:\n        torch.set_default_dtype(old_dtype)\n</code></pre> fastvideo.models.loader.fsdp_load.shard_model \u00b6 <pre><code>shard_model(\n    model,\n    *,\n    cpu_offload: bool,\n    reshard_after_forward: bool = True,\n    mp_policy: MixedPrecisionPolicy\n    | None = MixedPrecisionPolicy(),\n    mesh: DeviceMesh | None = None,\n    fsdp_shard_conditions: list[\n        Callable[[str, Module], bool]\n    ] = [],\n    pin_cpu_memory: bool = True\n) -&gt; None\n</code></pre> <p>Utility to shard a model with FSDP using the PyTorch Distributed fully_shard API.</p> <p>This method will over the model's named modules from the bottom-up and apply shard modules based on whether they meet any of the criteria from shard_conditions.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>TransformerDecoder</code> <p>Model to shard with FSDP.</p> required <code>shard_conditions</code> <code>List[Callable[[str, Module], bool]]</code> <p>A list of functions to determine which modules to shard with FSDP. Each function should take module name (relative to root) and the module itself, returning True if FSDP should shard the module and False otherwise. If any of shard_conditions return True for a given module, it will be sharded by FSDP.</p> required <code>cpu_offload</code> <code>bool</code> <p>If set to True, FSDP will offload parameters, gradients, and optimizer states to CPU.</p> required <code>reshard_after_forward</code> <code>bool</code> <p>Whether to reshard parameters and buffers after the forward pass. Setting this to True corresponds to the FULL_SHARD sharding strategy from FSDP1, while setting it to False corresponds to the SHARD_GRAD_OP sharding strategy.</p> <code>True</code> <code>mesh</code> <code>Optional[DeviceMesh]</code> <p>Device mesh to use for FSDP sharding under multiple parallelism. Default to None.</p> <code>None</code> <code>fsdp_shard_conditions</code> <code>List[Callable[[str, Module], bool]]</code> <p>A list of functions to determine which modules to shard with FSDP.</p> <code>[]</code> <code>pin_cpu_memory</code> <code>bool</code> <p>If set to True, FSDP will pin the CPU memory of the offloaded parameters.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no layer modules were sharded, indicating that no shard_condition was triggered.</p> Source code in <code>fastvideo/models/loader/fsdp_load.py</code> <pre><code>def shard_model(\n    model,\n    *,\n    cpu_offload: bool,\n    reshard_after_forward: bool = True,\n    mp_policy: MixedPrecisionPolicy | None = MixedPrecisionPolicy(),  # noqa\n    mesh: DeviceMesh | None = None,\n    fsdp_shard_conditions: list[Callable[[str, nn.Module], bool]] = [],  # noqa\n    pin_cpu_memory: bool = True,\n) -&gt; None:\n    \"\"\"\n    Utility to shard a model with FSDP using the PyTorch Distributed fully_shard API.\n\n    This method will over the model's named modules from the bottom-up and apply shard modules\n    based on whether they meet any of the criteria from shard_conditions.\n\n    Args:\n        model (TransformerDecoder): Model to shard with FSDP.\n        shard_conditions (List[Callable[[str, nn.Module], bool]]): A list of functions to determine\n            which modules to shard with FSDP. Each function should take module name (relative to root)\n            and the module itself, returning True if FSDP should shard the module and False otherwise.\n            If any of shard_conditions return True for a given module, it will be sharded by FSDP.\n        cpu_offload (bool): If set to True, FSDP will offload parameters, gradients, and optimizer\n            states to CPU.\n        reshard_after_forward (bool): Whether to reshard parameters and buffers after\n            the forward pass. Setting this to True corresponds to the FULL_SHARD sharding strategy\n            from FSDP1, while setting it to False corresponds to the SHARD_GRAD_OP sharding strategy.\n        mesh (Optional[DeviceMesh]): Device mesh to use for FSDP sharding under multiple parallelism.\n            Default to None.\n        fsdp_shard_conditions (List[Callable[[str, nn.Module], bool]]): A list of functions to determine\n            which modules to shard with FSDP.\n        pin_cpu_memory (bool): If set to True, FSDP will pin the CPU memory of the offloaded parameters.\n\n    Raises:\n        ValueError: If no layer modules were sharded, indicating that no shard_condition was triggered.\n    \"\"\"\n    if fsdp_shard_conditions is None or len(fsdp_shard_conditions) == 0:\n        logger.warning(\n            \"The FSDP shard condition list is empty or None. No modules will be sharded in %s\",\n            type(model).__name__)\n        return\n\n    fsdp_kwargs = {\n        \"reshard_after_forward\": reshard_after_forward,\n        \"mesh\": mesh,\n        \"mp_policy\": mp_policy,\n    }\n    if cpu_offload:\n        fsdp_kwargs[\"offload_policy\"] = CPUOffloadPolicy(\n            pin_memory=pin_cpu_memory)\n\n    # iterating in reverse to start with\n    # lowest-level modules first\n    num_layers_sharded = 0\n    # TODO(will): don't reshard after forward for the last layer to save on the\n    # all-gather that will immediately happen Shard the model with FSDP,\n    for n, m in reversed(list(model.named_modules())):\n        if any([\n                shard_condition(n, m)\n                for shard_condition in fsdp_shard_conditions\n        ]):\n            fully_shard(m, **fsdp_kwargs)\n            num_layers_sharded += 1\n\n    if num_layers_sharded == 0:\n        raise ValueError(\n            \"No layer modules were sharded. Please check if shard conditions are working as expected.\"\n        )\n\n    # Finally shard the entire model to account for any stragglers\n    fully_shard(model, **fsdp_kwargs)\n</code></pre> fastvideo.models.loader.utils \u00b6 <p>Utilities for selecting and loading models.</p> Functions\u00b6 fastvideo.models.loader.utils.get_param_names_mapping \u00b6 <pre><code>get_param_names_mapping(\n    mapping_dict: dict[str, str]\n) -&gt; Callable[[str], tuple[str, Any, Any]]\n</code></pre> <p>Creates a mapping function that transforms parameter names using regex patterns.</p> <p>Parameters:</p> Name Type Description Default <code>mapping_dict</code> <code>Dict[str, str]</code> <p>Dictionary mapping regex patterns to replacement patterns</p> required <code>param_name</code> <code>str</code> <p>The parameter name to be transformed</p> required <p>Returns:</p> Type Description <code>Callable[[str], tuple[str, Any, Any]]</code> <p>Callable[[str], str]: A function that maps parameter names from source to target format</p> Source code in <code>fastvideo/models/loader/utils.py</code> <pre><code>def get_param_names_mapping(\n        mapping_dict: dict[str, str]) -&gt; Callable[[str], tuple[str, Any, Any]]:\n    \"\"\"\n    Creates a mapping function that transforms parameter names using regex patterns.\n\n    Args:\n        mapping_dict (Dict[str, str]): Dictionary mapping regex patterns to replacement patterns\n        param_name (str): The parameter name to be transformed\n\n    Returns:\n        Callable[[str], str]: A function that maps parameter names from source to target format\n    \"\"\"\n\n    def mapping_fn(name: str) -&gt; tuple[str, Any, Any]:\n        # Try to match and transform the name using the regex patterns in mapping_dict\n        for pattern, replacement in mapping_dict.items():\n            match = re.match(pattern, name)\n            if match:\n                merge_index = None\n                total_splitted_params = None\n                if isinstance(replacement, tuple):\n                    merge_index = replacement[1]\n                    total_splitted_params = replacement[2]\n                    replacement = replacement[0]\n                name = re.sub(pattern, replacement, name)\n                return name, merge_index, total_splitted_params\n\n        # If no pattern matches, return the original name\n        return name, None, None\n\n    return mapping_fn\n</code></pre> fastvideo.models.loader.utils.hf_to_custom_state_dict \u00b6 <pre><code>hf_to_custom_state_dict(\n    hf_param_sd: dict[str, Tensor]\n    | Iterator[tuple[str, Tensor]],\n    param_names_mapping: Callable[\n        [str], tuple[str, Any, Any]\n    ],\n) -&gt; tuple[\n    dict[str, torch.Tensor], dict[str, tuple[str, Any, Any]]\n]\n</code></pre> <p>Converts a Hugging Face parameter state dictionary to a custom parameter state dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>hf_param_sd</code> <code>Dict[str, Tensor]</code> <p>The Hugging Face parameter state dictionary</p> required <code>param_names_mapping</code> <code>Callable[[str], tuple[str, Any, Any]]</code> <p>A function that maps parameter names from source to target format</p> required <p>Returns:</p> Name Type Description <code>custom_param_sd</code> <code>Dict[str, Tensor]</code> <p>The custom formatted parameter state dict</p> <code>reverse_param_names_mapping</code> <code>Dict[str, Tuple[str, Any, Any]]</code> <p>Maps back from custom to hf</p> Source code in <code>fastvideo/models/loader/utils.py</code> <pre><code>def hf_to_custom_state_dict(\n    hf_param_sd: dict[str, torch.Tensor] | Iterator[tuple[str, torch.Tensor]],\n    param_names_mapping: Callable[[str], tuple[str, Any, Any]]\n) -&gt; tuple[dict[str, torch.Tensor], dict[str, tuple[str, Any, Any]]]:\n    \"\"\"\n    Converts a Hugging Face parameter state dictionary to a custom parameter state dictionary.\n\n    Args:\n        hf_param_sd (Dict[str, torch.Tensor]): The Hugging Face parameter state dictionary\n        param_names_mapping (Callable[[str], tuple[str, Any, Any]]): A function that maps parameter names from source to target format\n\n    Returns:\n        custom_param_sd (Dict[str, torch.Tensor]): The custom formatted parameter state dict\n        reverse_param_names_mapping (Dict[str, Tuple[str, Any, Any]]): Maps back from custom to hf\n    \"\"\"\n    custom_param_sd = {}\n    to_merge_params = defaultdict(dict)  # type: ignore\n    reverse_param_names_mapping = {}\n    if isinstance(hf_param_sd, dict):\n        hf_param_sd = hf_param_sd.items()  # type: ignore\n    for source_param_name, full_tensor in hf_param_sd:  # type: ignore\n        target_param_name, merge_index, num_params_to_merge = param_names_mapping(\n            source_param_name)\n        reverse_param_names_mapping[target_param_name] = (source_param_name,\n                                                          merge_index,\n                                                          num_params_to_merge)\n        if merge_index is not None:\n            to_merge_params[target_param_name][merge_index] = full_tensor\n            if len(to_merge_params[target_param_name]) == num_params_to_merge:\n                # cat at output dim according to the merge_index order\n                sorted_tensors = [\n                    to_merge_params[target_param_name][i]\n                    for i in range(num_params_to_merge)\n                ]\n                full_tensor = torch.cat(sorted_tensors, dim=0)\n                del to_merge_params[target_param_name]\n            else:\n                continue\n        custom_param_sd[target_param_name] = full_tensor\n    return custom_param_sd, reverse_param_names_mapping\n</code></pre> fastvideo.models.loader.utils.set_default_torch_dtype \u00b6 <pre><code>set_default_torch_dtype(dtype: dtype)\n</code></pre> <p>Sets the default torch dtype to the given dtype.</p> Source code in <code>fastvideo/models/loader/utils.py</code> <pre><code>@contextlib.contextmanager\ndef set_default_torch_dtype(dtype: torch.dtype):\n    \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n    old_dtype = torch.get_default_dtype()\n    torch.set_default_dtype(dtype)\n    yield\n    torch.set_default_dtype(old_dtype)\n</code></pre> fastvideo.models.loader.weight_utils \u00b6 <p>Utilities for downloading and initializing model weights.</p> Functions\u00b6 fastvideo.models.loader.weight_utils.default_weight_loader \u00b6 <pre><code>default_weight_loader(\n    param: Tensor, loaded_weight: Tensor\n) -&gt; None\n</code></pre> <p>Default weight loader.</p> Source code in <code>fastvideo/models/loader/weight_utils.py</code> <pre><code>def default_weight_loader(param: torch.Tensor,\n                          loaded_weight: torch.Tensor) -&gt; None:\n    \"\"\"Default weight loader.\"\"\"\n    try:\n        if param.numel() == 1 and loaded_weight.numel() == 1:\n            # Sometimes scalar values aren't considered tensors with shapes\n            # so if both param and loaded_weight are a scalar,\n            # \"broadcast\" instead of copy\n            param.data.fill_(loaded_weight.item())\n        else:\n            assert param.size() == loaded_weight.size(), (\n                f\"Attempted to load weight ({loaded_weight.size()}) \"\n                f\"into parameter ({param.size()})\")\n\n            param.data.copy_(loaded_weight)\n    except Exception:\n        # NOTE: This exception is added for the purpose of setting breakpoint to\n        # debug weight loading issues.\n        raise\n</code></pre> fastvideo.models.loader.weight_utils.enable_hf_transfer \u00b6 <pre><code>enable_hf_transfer() -&gt; None\n</code></pre> <p>automatically activates hf_transfer</p> Source code in <code>fastvideo/models/loader/weight_utils.py</code> <pre><code>def enable_hf_transfer() -&gt; None:\n    \"\"\"automatically activates hf_transfer\n    \"\"\"\n    if \"HF_HUB_ENABLE_HF_TRANSFER\" not in os.environ:\n        try:\n            # enable hf hub transfer if available\n            import hf_transfer  # type: ignore # noqa\n            huggingface_hub.constants.HF_HUB_ENABLE_HF_TRANSFER = True\n        except ImportError:\n            pass\n</code></pre> fastvideo.models.loader.weight_utils.filter_files_not_needed_for_inference \u00b6 <pre><code>filter_files_not_needed_for_inference(\n    hf_weights_files: list[str],\n) -&gt; list[str]\n</code></pre> <p>Exclude files that are not needed for inference.</p> <p>See https://github.com/huggingface/transformers/blob/v4.34.0/src/transformers/trainer.py#L227-L233</p> Source code in <code>fastvideo/models/loader/weight_utils.py</code> <pre><code>def filter_files_not_needed_for_inference(\n        hf_weights_files: list[str]) -&gt; list[str]:\n    \"\"\"\n    Exclude files that are not needed for inference.\n\n    See https://github.com/huggingface/transformers/blob/v4.34.0/src/transformers/trainer.py#L227-L233\n    \"\"\"\n    blacklist = [\n        \"training_args.bin\",\n        \"optimizer.bin\",\n        \"optimizer.pt\",\n        \"scheduler.pt\",\n        \"scaler.pt\",\n    ]\n    hf_weights_files = [\n        f for f in hf_weights_files\n        if not any(f.endswith(x) for x in blacklist)\n    ]\n    return hf_weights_files\n</code></pre> fastvideo.models.loader.weight_utils.maybe_remap_kv_scale_name \u00b6 <pre><code>maybe_remap_kv_scale_name(\n    name: str, params_dict: dict\n) -&gt; str | None\n</code></pre> <p>Remap the name of FP8 k/v_scale parameters.</p> <p>This function handles the remapping of FP8 k/v_scale parameter names. It detects if the given name ends with a suffix and attempts to remap it to the expected name format in the model. If the remapped name is not found in the params_dict, a warning is printed and None is returned.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The original loaded checkpoint parameter name.</p> required <code>params_dict</code> <code>dict</code> <p>Dictionary containing the model's named parameters.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str | None</code> <p>The remapped parameter name if successful, or the original name  if no remapping is needed.</p> <code>None</code> <code>str | None</code> <p>If the remapped name is not found in params_dict.</p> Source code in <code>fastvideo/models/loader/weight_utils.py</code> <pre><code>def maybe_remap_kv_scale_name(name: str, params_dict: dict) -&gt; str | None:\n    \"\"\"Remap the name of FP8 k/v_scale parameters.\n\n    This function handles the remapping of FP8 k/v_scale parameter names.\n    It detects if the given name ends with a suffix and attempts to remap\n    it to the expected name format in the model. If the remapped name is not\n    found in the params_dict, a warning is printed and None is returned.\n\n    Args:\n        name (str): The original loaded checkpoint parameter name.\n        params_dict (dict): Dictionary containing the model's named parameters.\n\n    Returns:\n        str: The remapped parameter name if successful, or the original name\n             if no remapping is needed.\n        None: If the remapped name is not found in params_dict.\n    \"\"\"\n    if name.endswith(\".kv_scale\"):\n        logger.warning_once(\n            \"DEPRECATED. Found kv_scale in the checkpoint. \"\n            \"This format is deprecated in favor of separate k_scale and \"\n            \"v_scale tensors and will be removed in a future release. \"\n            \"Functionally, we will remap kv_scale to k_scale and duplicate \"\n            \"k_scale to v_scale\")\n        # NOTE: we remap the deprecated kv_scale to k_scale\n        remapped_name = name.replace(\".kv_scale\", \".attn.k_scale\")\n        if remapped_name not in params_dict:\n            logger.warning_once(\n                f\"Found kv_scale in the checkpoint (e.g. {name}), \"\n                \"but not found the expected name in the model \"\n                f\"(e.g. {remapped_name}). kv_scale is \"\n                \"not loaded.\")\n            return None\n        return remapped_name\n\n    possible_scale_names = [\".k_scale\", \".v_scale\"]\n    modelopt_scale_names = [\n        \".self_attn.k_proj.k_scale\", \".self_attn.v_proj.v_scale\"\n    ]\n    for scale_name in possible_scale_names:\n        if name.endswith(scale_name):\n            if any(mo_scale_name in name\n                   for mo_scale_name in modelopt_scale_names):\n                remapped_name = name.replace(\n                    f\".self_attn.{scale_name[1]}_proj{scale_name}\",\n                    f\".self_attn.attn{scale_name}\")\n            else:\n                remapped_name = name.replace(scale_name, f\".attn{scale_name}\")\n            if remapped_name not in params_dict:\n                logger.warning_once(\n                    f\"Found {scale_name} in the checkpoint (e.g. {name}), \"\n                    \"but not found the expected name in the model \"\n                    f\"(e.g. {remapped_name}). {scale_name} is \"\n                    \"not loaded.\")\n                return None\n            return remapped_name\n\n    # If there were no matches, return the untouched param name\n    return name\n</code></pre> fastvideo.models.loader.weight_utils.pt_weights_iterator \u00b6 <pre><code>pt_weights_iterator(\n    hf_weights_files: list[str], to_cpu: bool = True\n) -&gt; Generator[tuple[str, torch.Tensor], None, None]\n</code></pre> <p>Iterate over the weights in the model bin/pt files.</p> Source code in <code>fastvideo/models/loader/weight_utils.py</code> <pre><code>def pt_weights_iterator(\n    hf_weights_files: list[str],\n    to_cpu: bool = True,\n) -&gt; Generator[tuple[str, torch.Tensor], None, None]:\n    \"\"\"Iterate over the weights in the model bin/pt files.\"\"\"\n    device = \"cpu\" if to_cpu else str(get_local_torch_device())\n    enable_tqdm = not torch.distributed.is_initialized(\n    ) or torch.distributed.get_rank() == 0\n    for bin_file in tqdm(\n            hf_weights_files,\n            desc=\"Loading pt checkpoint shards\",\n            disable=not enable_tqdm,\n            bar_format=_BAR_FORMAT,\n    ):\n        state = torch.load(bin_file, map_location=device, weights_only=True)\n        yield from state.items()\n        del state\n</code></pre> fastvideo.models.loader.weight_utils.safetensors_weights_iterator \u00b6 <pre><code>safetensors_weights_iterator(\n    hf_weights_files: list[str], to_cpu: bool = True\n) -&gt; Generator[tuple[str, torch.Tensor], None, None]\n</code></pre> <p>Iterate over the weights in the model safetensor files.</p> Source code in <code>fastvideo/models/loader/weight_utils.py</code> <pre><code>def safetensors_weights_iterator(\n    hf_weights_files: list[str],\n    to_cpu: bool = True,\n) -&gt; Generator[tuple[str, torch.Tensor], None, None]:\n    \"\"\"Iterate over the weights in the model safetensor files.\"\"\"\n    enable_tqdm = not torch.distributed.is_initialized(\n    ) or torch.distributed.get_rank() == 0\n    device = \"cpu\" if to_cpu else str(get_local_torch_device())\n    for st_file in tqdm(\n            hf_weights_files,\n            desc=\"Loading safetensors checkpoint shards\",\n            disable=not enable_tqdm,\n            bar_format=_BAR_FORMAT,\n    ):\n        with safe_open(st_file, framework=\"pt\", device=device) as f:\n            for name in f.keys():  # noqa: SIM118\n                param = f.get_tensor(name)\n                yield name, param\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.models.parameter","title":"fastvideo.models.parameter","text":""},{"location":"api/fastvideo/#fastvideo.models.parameter-classes","title":"Classes","text":"fastvideo.models.parameter.BasevLLMParameter \u00b6 <pre><code>BasevLLMParameter(data: Tensor, weight_loader: Callable)\n</code></pre> <p>               Bases: <code>Parameter</code></p> <p>Base parameter for vLLM linear layers. Extends the torch.nn.parameter by taking in a linear weight loader. Will copy the loaded weight into the parameter when the provided weight loader is called.</p> <p>Initialize the BasevLLMParameter</p> <p>:param data: torch tensor with the parameter data :param weight_loader: weight loader callable</p> <p>:returns: a torch.nn.parameter</p> Source code in <code>fastvideo/models/parameter.py</code> <pre><code>def __init__(self, data: torch.Tensor, weight_loader: Callable):\n    \"\"\"\n    Initialize the BasevLLMParameter\n\n    :param data: torch tensor with the parameter data\n    :param weight_loader: weight loader callable\n\n    :returns: a torch.nn.parameter\n    \"\"\"\n\n    # During weight loading, we often do something like:\n    # narrowed_tensor = param.data.narrow(0, offset, len)\n    # narrowed_tensor.copy_(real_weight)\n    # expecting narrowed_tensor and param.data to share the same storage.\n    # However, on TPUs, narrowed_tensor will lazily propagate to the base\n    # tensor, which is param.data, leading to the redundant memory usage.\n    # This sometimes causes OOM errors during model loading. To avoid this,\n    # we sync the param tensor after its weight loader is called.\n    from fastvideo.platforms import current_platform\n    if current_platform.is_tpu():\n        weight_loader = _make_synced_weight_loader(weight_loader)\n\n    self._weight_loader = weight_loader\n</code></pre> Functions\u00b6 fastvideo.models.parameter.BlockQuantScaleParameter \u00b6 <pre><code>BlockQuantScaleParameter(output_dim: int, **kwargs)\n</code></pre> <p>               Bases: <code>_ColumnvLLMParameter</code>, <code>RowvLLMParameter</code></p> <p>Parameter class for weight scales loaded for weights with block-wise quantization. Uses both column and row parallelism.</p> Source code in <code>fastvideo/models/parameter.py</code> <pre><code>def __init__(self, output_dim: int, **kwargs):\n    self._output_dim = output_dim\n    super().__init__(**kwargs)\n</code></pre> fastvideo.models.parameter.ChannelQuantScaleParameter \u00b6 <pre><code>ChannelQuantScaleParameter(output_dim: int, **kwargs)\n</code></pre> <p>               Bases: <code>_ColumnvLLMParameter</code></p> <p>Parameter class for weight scales loaded for weights with channel-wise quantization. Equivalent to _ColumnvLLMParameter.</p> Source code in <code>fastvideo/models/parameter.py</code> <pre><code>def __init__(self, output_dim: int, **kwargs):\n    self._output_dim = output_dim\n    super().__init__(**kwargs)\n</code></pre> fastvideo.models.parameter.GroupQuantScaleParameter \u00b6 <pre><code>GroupQuantScaleParameter(output_dim: int, **kwargs)\n</code></pre> <p>               Bases: <code>_ColumnvLLMParameter</code>, <code>RowvLLMParameter</code></p> <p>Parameter class for weight scales loaded for weights with grouped quantization. Uses both column and row parallelism.</p> Source code in <code>fastvideo/models/parameter.py</code> <pre><code>def __init__(self, output_dim: int, **kwargs):\n    self._output_dim = output_dim\n    super().__init__(**kwargs)\n</code></pre> fastvideo.models.parameter.ModelWeightParameter \u00b6 <pre><code>ModelWeightParameter(output_dim: int, **kwargs)\n</code></pre> <p>               Bases: <code>_ColumnvLLMParameter</code>, <code>RowvLLMParameter</code></p> <p>Parameter class for linear layer weights. Uses both column and row parallelism.</p> Source code in <code>fastvideo/models/parameter.py</code> <pre><code>def __init__(self, output_dim: int, **kwargs):\n    self._output_dim = output_dim\n    super().__init__(**kwargs)\n</code></pre> fastvideo.models.parameter.PackedColumnParameter \u00b6 <pre><code>PackedColumnParameter(\n    packed_factor: int | Fraction, packed_dim: int, **kwargs\n)\n</code></pre> <p>               Bases: <code>_ColumnvLLMParameter</code></p> <p>Parameter for model parameters which are packed on disk and support column parallelism only. See PackedvLLMParameter for more details on the packed properties.</p> Source code in <code>fastvideo/models/parameter.py</code> <pre><code>def __init__(self, packed_factor: int | Fraction, packed_dim: int,\n             **kwargs):\n    self._packed_factor = packed_factor\n    self._packed_dim = packed_dim\n    super().__init__(**kwargs)\n</code></pre> fastvideo.models.parameter.PackedvLLMParameter \u00b6 <pre><code>PackedvLLMParameter(\n    packed_factor: int | Fraction, packed_dim: int, **kwargs\n)\n</code></pre> <p>               Bases: <code>ModelWeightParameter</code></p> <p>Parameter for model weights which are packed on disk. Example: GPTQ Marlin weights are int4 or int8, packed into int32. Extends the ModelWeightParameter to take in the packed factor, the packed dimension, and optionally, marlin tile size for marlin kernels. Adjusts the shard_size and  shard_offset for fused linear layers model weight loading by accounting for packing and optionally, marlin tile size.</p> Source code in <code>fastvideo/models/parameter.py</code> <pre><code>def __init__(self, packed_factor: int | Fraction, packed_dim: int,\n             **kwargs):\n    self._packed_factor = packed_factor\n    self._packed_dim = packed_dim\n    super().__init__(**kwargs)\n</code></pre> fastvideo.models.parameter.PerTensorScaleParameter \u00b6 <pre><code>PerTensorScaleParameter(**kwargs)\n</code></pre> <p>               Bases: <code>BasevLLMParameter</code></p> <p>Parameter class for scales where the number of scales is equivalent to the number of logical matrices in fused linear layers (e.g. for QKV, there are 3 scales loaded from disk). This is relevant to weights with per-tensor quantization. Adds functionality to map the scalers to a shard during weight loading. </p> <p>Note: additional parameter manipulation may be handled  for each quantization config specifically, within  process_weights_after_loading</p> Source code in <code>fastvideo/models/parameter.py</code> <pre><code>def __init__(self, **kwargs):\n    self.qkv_idxs = {\"q\": 0, \"k\": 1, \"v\": 2}\n    super().__init__(**kwargs)\n</code></pre> fastvideo.models.parameter.RowvLLMParameter \u00b6 <pre><code>RowvLLMParameter(input_dim: int, **kwargs)\n</code></pre> <p>               Bases: <code>BasevLLMParameter</code></p> <p>Parameter class defining weight_loading functionality (load_row_parallel_weight) for parameters being loaded into linear layers with row parallel functionality. Requires an input_dim to be defined.</p> Source code in <code>fastvideo/models/parameter.py</code> <pre><code>def __init__(self, input_dim: int, **kwargs):\n    self._input_dim = input_dim\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.models.parameter-functions","title":"Functions","text":"fastvideo.models.parameter.permute_param_layout_ \u00b6 <pre><code>permute_param_layout_(\n    param: BasevLLMParameter,\n    input_dim: int,\n    output_dim: int,\n    **kwargs\n) -&gt; BasevLLMParameter\n</code></pre> <p>Permute a parameter's layout to the specified input and output dimensions,  useful for forcing the parameter into a known layout, for example, if I need a packed (quantized) weight matrix to be in the layout      {input_dim = 0, output_dim = 1, packed_dim = 0} then I can call:     permute_param_layout_(x, input_dim=0, output_dim=1, packed_dim=0) to ensure x is in the correct layout (permuting it to the correct layout if  required, asserting if it cannot get it to the correct layout)</p> Source code in <code>fastvideo/models/parameter.py</code> <pre><code>def permute_param_layout_(param: BasevLLMParameter, input_dim: int,\n                          output_dim: int, **kwargs) -&gt; BasevLLMParameter:\n    \"\"\"\n    Permute a parameter's layout to the specified input and output dimensions, \n    useful for forcing the parameter into a known layout, for example, if I need\n    a packed (quantized) weight matrix to be in the layout \n        {input_dim = 0, output_dim = 1, packed_dim = 0}\n    then I can call:\n        permute_param_layout_(x, input_dim=0, output_dim=1, packed_dim=0)\n    to ensure x is in the correct layout (permuting it to the correct layout if \n    required, asserting if it cannot get it to the correct layout)\n    \"\"\"\n\n    curr_input_dim = getattr(param, \"input_dim\", None)\n    curr_output_dim = getattr(param, \"output_dim\", None)\n\n    if curr_input_dim is None or curr_output_dim is None:\n        assert param.data.dim() == 2,\\\n            \"permute_param_layout_ only supports 2D parameters when either \"\\\n            \"input_dim or output_dim is not set\"\n\n    # if one of the dimensions is not set, set it to the opposite of the other\n    #  we can only do this since we asserted the parameter is 2D above\n    if curr_input_dim is None:\n        assert curr_output_dim is not None,\\\n            \"either input or output dim must be set\"\n        curr_input_dim = (curr_output_dim + 1) % 2\n    if curr_output_dim is None:\n        assert curr_input_dim is not None,\\\n            \"either input or output dim must be set\"\n        curr_output_dim = (curr_input_dim + 1) % 2\n\n    # create permutation from the current layout to the layout with\n    # self.input_dim at input_dim and self.output_dim at output_dim preserving\n    # other dimensions\n    perm = [\n        i for i in range(param.data.dim())\n        if i not in [curr_input_dim, curr_output_dim]\n    ]\n    perm.insert(input_dim, curr_input_dim)\n    perm.insert(output_dim, curr_output_dim)\n\n    if \"packed_dim\" in kwargs:\n        assert hasattr(param, \"packed_dim\") and\\\n            param.packed_dim == perm[kwargs[\"packed_dim\"]],\\\n            \"permute_param_layout_ currently doesn't support repacking\"\n\n    param.data = param.data.permute(*perm)\n    if hasattr(param, \"_input_dim\"):\n        param._input_dim = input_dim\n    if hasattr(param, \"_output_dim\"):\n        param._output_dim = output_dim\n    if \"packed_dim\" in kwargs and hasattr(param, \"_packed_dim\"):\n        param._packed_dim = kwargs[\"packed_dim\"]\n\n    return param\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.models.registry","title":"fastvideo.models.registry","text":""},{"location":"api/fastvideo/#fastvideo.models.registry-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideo.models.utils","title":"fastvideo.models.utils","text":"<p>Utils for model executor.</p>"},{"location":"api/fastvideo/#fastvideo.models.utils-functions","title":"Functions","text":"fastvideo.models.utils.auto_attributes \u00b6 <pre><code>auto_attributes(init_func)\n</code></pre> <p>Decorator that automatically adds all initialization arguments as object attributes.</p> Example <p>@auto_attributes def init(self, a=1, b=2):     pass</p> Source code in <code>fastvideo/models/utils.py</code> <pre><code>def auto_attributes(init_func):\n    \"\"\"\n    Decorator that automatically adds all initialization arguments as object attributes.\n\n    Example:\n        @auto_attributes\n        def __init__(self, a=1, b=2):\n            pass\n\n        # This will automatically set:\n        # - self.a = 1 and self.b = 2\n        # - self.config.a = 1 and self.config.b = 2\n    \"\"\"\n\n    def wrapper(self, *args, **kwargs):\n        # Get the function signature\n        import inspect\n        signature = inspect.signature(init_func)\n        parameters = signature.parameters\n\n        # Get parameter names (excluding 'self')\n        param_names = list(parameters.keys())[1:]\n\n        # Bind arguments to parameters\n        bound_args = signature.bind(self, *args, **kwargs)\n        bound_args.apply_defaults()\n\n        # Create config object if it doesn't exist\n        if not hasattr(self, 'config'):\n            self.config = type('Config', (), {})()\n\n        # Set attributes on self and self.config\n        for name in param_names:\n            if name in bound_args.arguments:\n                value = bound_args.arguments[name]\n                setattr(self, name, value)\n                setattr(self.config, name, value)\n\n        # Call the original __init__ function\n        return init_func(self, *args, **kwargs)\n\n    return wrapper\n</code></pre> fastvideo.models.utils.extract_layer_index \u00b6 <pre><code>extract_layer_index(layer_name: str) -&gt; int\n</code></pre> <p>Extract the layer index from the module name. Examples: - \"encoder.layers.0\" -&gt; 0 - \"encoder.layers.1.self_attn\" -&gt; 1 - \"2.self_attn\" -&gt; 2 - \"model.encoder.layers.0.sub.1\" -&gt; ValueError</p> Source code in <code>fastvideo/models/utils.py</code> <pre><code>def extract_layer_index(layer_name: str) -&gt; int:\n    \"\"\"\n    Extract the layer index from the module name.\n    Examples:\n    - \"encoder.layers.0\" -&gt; 0\n    - \"encoder.layers.1.self_attn\" -&gt; 1\n    - \"2.self_attn\" -&gt; 2\n    - \"model.encoder.layers.0.sub.1\" -&gt; ValueError\n    \"\"\"\n    subnames = layer_name.split(\".\")\n    int_vals: list[int] = []\n    for subname in subnames:\n        try:\n            int_vals.append(int(subname))\n        except ValueError:\n            continue\n    assert len(int_vals) == 1, (f\"layer name {layer_name} should\"\n                                \" only contain one integer\")\n    return int_vals[0]\n</code></pre> fastvideo.models.utils.modulate \u00b6 <pre><code>modulate(\n    x: Tensor,\n    shift: Tensor | None = None,\n    scale: Tensor | None = None,\n) -&gt; torch.Tensor\n</code></pre> <p>modulate by shift and scale</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor.</p> required <code>shift</code> <code>Tensor</code> <p>shift tensor. Defaults to None.</p> <code>None</code> <code>scale</code> <code>Tensor</code> <p>scale tensor. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: the output tensor after modulate.</p> Source code in <code>fastvideo/models/utils.py</code> <pre><code>def modulate(x: torch.Tensor,\n             shift: torch.Tensor | None = None,\n             scale: torch.Tensor | None = None) -&gt; torch.Tensor:\n    \"\"\"modulate by shift and scale\n\n    Args:\n        x (torch.Tensor): input tensor.\n        shift (torch.Tensor, optional): shift tensor. Defaults to None.\n        scale (torch.Tensor, optional): scale tensor. Defaults to None.\n\n    Returns:\n        torch.Tensor: the output tensor after modulate.\n    \"\"\"\n    if scale is None and shift is None:\n        return x\n    elif shift is None:\n        return x * (1 + scale.unsqueeze(1))  # type: ignore[union-attr]\n    elif scale is None:\n        return x + shift.unsqueeze(1)  # type: ignore[union-attr]\n    else:\n        return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(\n            1)  # type: ignore[union-attr]\n</code></pre> fastvideo.models.utils.pred_noise_to_pred_video \u00b6 <pre><code>pred_noise_to_pred_video(\n    pred_noise: Tensor,\n    noise_input_latent: Tensor,\n    timestep: Tensor,\n    scheduler: Any,\n) -&gt; torch.Tensor\n</code></pre> <p>Convert predicted noise to clean latent.</p> <p>pred_noise: the predicted noise with shape [B, C, H, W]     where B is batch_size or batch_size * num_frames noise_input_latent: the noisy latent with shape [B, C, H, W], timestep: the timestep with shape [1] or [bs * num_frames] or [bs, num_frames] scheduler: the scheduler</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>the predicted video with shape [B, C, H, W]</p> Source code in <code>fastvideo/models/utils.py</code> <pre><code>def pred_noise_to_pred_video(pred_noise: torch.Tensor,\n                             noise_input_latent: torch.Tensor,\n                             timestep: torch.Tensor,\n                             scheduler: Any) -&gt; torch.Tensor:\n    \"\"\"\n    Convert predicted noise to clean latent.\n\n    Args:\n    pred_noise: the predicted noise with shape [B, C, H, W]\n        where B is batch_size or batch_size * num_frames\n    noise_input_latent: the noisy latent with shape [B, C, H, W],\n    timestep: the timestep with shape [1] or [bs * num_frames] or [bs, num_frames]\n    scheduler: the scheduler\n\n    Returns:\n        the predicted video with shape [B, C, H, W]\n    \"\"\"\n    # If timestep is [bs, num_frames]\n    if timestep.ndim == 2:\n        timestep = timestep.flatten(0, 1)\n        assert timestep.numel() == noise_input_latent.shape[0]\n    elif timestep.ndim == 1:\n        # If timestep is [1]\n        if timestep.shape[0] == 1:\n            timestep = timestep.expand(noise_input_latent.shape[0])\n        else:\n            assert timestep.numel() == noise_input_latent.shape[0]\n    else:\n        raise ValueError(f\"[pred_noise_to_pred_video] Invalid timestep shape: {timestep.shape}\")\n    # timestep shape should be [B]\n    dtype = pred_noise.dtype\n    device = pred_noise.device\n    pred_noise = pred_noise.double().to(device)\n    noise_input_latent = noise_input_latent.double().to(device)\n    sigmas = scheduler.sigmas.double().to(device)\n    timesteps = scheduler.timesteps.double().to(device)\n    timestep_id = torch.argmin(\n        (timesteps.unsqueeze(0) - timestep.unsqueeze(1)).abs(), dim=1)\n    sigma_t = sigmas[timestep_id].reshape(-1, 1, 1, 1)\n    pred_video = noise_input_latent - sigma_t * pred_noise\n    return pred_video.to(dtype)\n</code></pre> fastvideo.models.utils.set_weight_attrs \u00b6 <pre><code>set_weight_attrs(\n    weight: Tensor, weight_attrs: dict[str, Any] | None\n)\n</code></pre> <p>Set attributes on a weight tensor.</p> <p>This method is used to set attributes on a weight tensor. This method will not overwrite existing attributes.</p> <p>Parameters:</p> Name Type Description Default <code>weight</code> <code>Tensor</code> <p>The weight tensor.</p> required <code>weight_attrs</code> <code>dict[str, Any] | None</code> <p>A dictionary of attributes to set on the weight tensor.</p> required Source code in <code>fastvideo/models/utils.py</code> <pre><code>def set_weight_attrs(\n    weight: torch.Tensor,\n    weight_attrs: dict[str, Any] | None,\n):\n    \"\"\"Set attributes on a weight tensor.\n\n    This method is used to set attributes on a weight tensor. This method\n    will not overwrite existing attributes.\n\n    Args:\n        weight: The weight tensor.\n        weight_attrs: A dictionary of attributes to set on the weight tensor.\n    \"\"\"\n    if weight_attrs is None:\n        return\n    for key, value in weight_attrs.items():\n        assert not hasattr(\n            weight, key), (f\"Overwriting existing tensor attribute: {key}\")\n\n        # NOTE(woosuk): During weight loading, we often do something like:\n        # narrowed_tensor = param.data.narrow(0, offset, len)\n        # narrowed_tensor.copy_(real_weight)\n        # expecting narrowed_tensor and param.data to share the same storage.\n        # However, on TPUs, narrowed_tensor will lazily propagate to the base\n        # tensor, which is param.data, leading to the redundant memory usage.\n        # This sometimes causes OOM errors during model loading. To avoid this,\n        # we sync the param tensor after its weight loader is called.\n        # TODO(woosuk): Remove this hack once we have a better solution.\n        from fastvideo.platforms import current_platform\n        if current_platform.is_tpu() and key == \"weight_loader\":\n            value = _make_synced_weight_loader(value)\n        setattr(weight, key, value)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.models.utils.auto_attributes--this-will-automatically-set","title":"This will automatically set:","text":""},{"location":"api/fastvideo/#fastvideo.models.utils.auto_attributes---selfa-1-and-selfb-2","title":"- self.a = 1 and self.b = 2","text":""},{"location":"api/fastvideo/#fastvideo.models.utils.auto_attributes---selfconfiga-1-and-selfconfigb-2","title":"- self.config.a = 1 and self.config.b = 2","text":""},{"location":"api/fastvideo/#fastvideo.models.vision_utils","title":"fastvideo.models.vision_utils","text":""},{"location":"api/fastvideo/#fastvideo.models.vision_utils-functions","title":"Functions","text":"fastvideo.models.vision_utils.create_default_image \u00b6 <pre><code>create_default_image(\n    width: int = 512,\n    height: int = 512,\n    color: tuple[int, int, int] = (0, 0, 0),\n) -&gt; PIL.Image.Image\n</code></pre> <p>Create a default black PIL image.</p> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>int</code> <p>Image width in pixels</p> <code>512</code> <code>height</code> <code>int</code> <p>Image height in pixels</p> <code>512</code> <code>color</code> <code>tuple[int, int, int]</code> <p>RGB color tuple</p> <code>(0, 0, 0)</code> <p>Returns:</p> Type Description <code>Image</code> <p>PIL.Image.Image: A new PIL image with specified dimensions and color</p> Source code in <code>fastvideo/models/vision_utils.py</code> <pre><code>def create_default_image(width: int = 512, height: int = 512, color: tuple[int, int, int] = (0, 0, 0)) -&gt; PIL.Image.Image:\n    \"\"\"\n    Create a default black PIL image.\n\n    Args:\n        width: Image width in pixels\n        height: Image height in pixels\n        color: RGB color tuple\n\n    Returns:\n        PIL.Image.Image: A new PIL image with specified dimensions and color\n    \"\"\"\n    return PIL.Image.new(\"RGB\", (width, height), color=color)\n</code></pre> fastvideo.models.vision_utils.get_default_height_width \u00b6 <pre><code>get_default_height_width(\n    image: Image | ndarray | Tensor,\n    vae_scale_factor: int,\n    height: int | None = None,\n    width: int | None = None,\n) -&gt; tuple[int, int]\n</code></pre> <p>Returns the height and width of the image, downscaled to the next integer multiple of <code>vae_scale_factor</code>.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>`Union[PIL.Image.Image, np.ndarray, torch.Tensor]`</code> <p>The image input, which can be a PIL image, NumPy array, or PyTorch tensor. If it is a NumPy array, it should have shape <code>[batch, height, width]</code> or <code>[batch, height, width, channels]</code>. If it is a PyTorch tensor, it should have shape <code>[batch, channels, height, width]</code>.</p> required <code>height</code> <code>`Optional[int]`, *optional*, defaults to `None`</code> <p>The height of the preprocessed image. If <code>None</code>, the height of the <code>image</code> input will be used.</p> <code>None</code> <code>width</code> <code>`Optional[int]`, *optional*, defaults to `None`</code> <p>The width of the preprocessed image. If <code>None</code>, the width of the <code>image</code> input will be used.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p><code>Tuple[int, int]</code>: A tuple containing the height and width, both resized to the nearest integer multiple of <code>vae_scale_factor</code>.</p> Source code in <code>fastvideo/models/vision_utils.py</code> <pre><code>def get_default_height_width(\n    image: PIL.Image.Image | np.ndarray | torch.Tensor,\n    vae_scale_factor: int,\n    height: int | None = None,\n    width: int | None = None,\n) -&gt; tuple[int, int]:\n    r\"\"\"\n    Returns the height and width of the image, downscaled to the next integer multiple of `vae_scale_factor`.\n\n    Args:\n        image (`Union[PIL.Image.Image, np.ndarray, torch.Tensor]`):\n            The image input, which can be a PIL image, NumPy array, or PyTorch tensor. If it is a NumPy array, it\n            should have shape `[batch, height, width]` or `[batch, height, width, channels]`. If it is a PyTorch\n            tensor, it should have shape `[batch, channels, height, width]`.\n        height (`Optional[int]`, *optional*, defaults to `None`):\n            The height of the preprocessed image. If `None`, the height of the `image` input will be used.\n        width (`Optional[int]`, *optional*, defaults to `None`):\n            The width of the preprocessed image. If `None`, the width of the `image` input will be used.\n\n    Returns:\n        `Tuple[int, int]`:\n            A tuple containing the height and width, both resized to the nearest integer multiple of\n            `vae_scale_factor`.\n    \"\"\"\n\n    if height is None:\n        if isinstance(image, PIL.Image.Image):\n            height = image.height\n        elif isinstance(image, torch.Tensor):\n            height = image.shape[2]\n        else:\n            height = image.shape[1]\n\n    if width is None:\n        if isinstance(image, PIL.Image.Image):\n            width = image.width\n        elif isinstance(image, torch.Tensor):\n            width = image.shape[3]\n        else:\n            width = image.shape[2]\n\n    width, height = (x - x % vae_scale_factor for x in (width, height)\n                     )  # resize to integer multiple of vae_scale_factor\n\n    return height, width\n</code></pre> fastvideo.models.vision_utils.load_image \u00b6 <pre><code>load_image(\n    image: str | Image,\n    convert_method: Callable[[Image], Image] | None = None,\n) -&gt; PIL.Image.Image\n</code></pre> <p>Loads <code>image</code> to a PIL Image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>`str` or `PIL.Image.Image`</code> <p>The image to convert to the PIL Image format.</p> required <code>convert_method</code> <code>Callable[[PIL.Image.Image], PIL.Image.Image], *optional*</code> <p>A conversion method to apply to the image after loading it. When set to <code>None</code> the image will be converted \"RGB\".</p> <code>None</code> <p>Returns:</p> Type Description <code>Image</code> <p><code>PIL.Image.Image</code>: A PIL Image.</p> Source code in <code>fastvideo/models/vision_utils.py</code> <pre><code>def load_image(\n    image: str | PIL.Image.Image,\n    convert_method: Callable[[PIL.Image.Image], PIL.Image.Image] | None = None\n) -&gt; PIL.Image.Image:\n    \"\"\"\n    Loads `image` to a PIL Image.\n\n    Args:\n        image (`str` or `PIL.Image.Image`):\n            The image to convert to the PIL Image format.\n        convert_method (Callable[[PIL.Image.Image], PIL.Image.Image], *optional*):\n            A conversion method to apply to the image after loading it. When set to `None` the image will be converted\n            \"RGB\".\n\n    Returns:\n        `PIL.Image.Image`:\n            A PIL Image.\n    \"\"\"\n    if isinstance(image, str):\n        if image.startswith(\"http://\") or image.startswith(\"https://\"):\n            image = PIL.Image.open(requests.get(image, stream=True).raw)\n        elif os.path.isfile(image):\n            image = PIL.Image.open(image)\n        else:\n            raise ValueError(\n                f\"Incorrect path or URL. URLs must start with `http://` or `https://`, and {image} is not a valid path.\"\n            )\n    elif isinstance(image, PIL.Image.Image):\n        image = image\n    else:\n        raise ValueError(\n            \"Incorrect format used for the image. Should be a URL linking to an image, a local path, or a PIL image.\"\n        )\n\n    image = PIL.ImageOps.exif_transpose(image)\n\n    if convert_method is not None:\n        image = convert_method(image)\n    else:\n        image = image.convert(\"RGB\")\n\n    return image\n</code></pre> fastvideo.models.vision_utils.load_video \u00b6 <pre><code>load_video(\n    video: str,\n    convert_method: Callable[[list[Image]], list[Image]]\n    | None = None,\n    return_fps: bool = False,\n) -&gt; tuple[list[PIL.Image.Image], float | Any] | list[\n    PIL.Image.Image\n]\n</code></pre> <p>Loads <code>video</code> to a list of PIL Image. Args:     video (<code>str</code>):         A URL or Path to a video to convert to a list of PIL Image format.     convert_method (Callable[[List[PIL.Image.Image]], List[PIL.Image.Image]], optional):         A conversion method to apply to the video after loading it. When set to <code>None</code> the images will be converted         to \"RGB\".     return_fps (<code>bool</code>, optional, defaults to <code>False</code>):         Whether to return the FPS of the video. If <code>True</code>, returns a tuple of (images, fps).         If <code>False</code>, returns only the list of images. Returns:     <code>List[PIL.Image.Image]</code> or <code>Tuple[List[PIL.Image.Image], float | None]</code>:         The video as a list of PIL images. If <code>return_fps</code> is True, also returns the original FPS.</p> Source code in <code>fastvideo/models/vision_utils.py</code> <pre><code>def load_video(\n    video: str,\n    convert_method: Callable[[list[PIL.Image.Image]], list[PIL.Image.Image]]\n    | None = None,\n    return_fps: bool = False,\n) -&gt; tuple[list[PIL.Image.Image], float | Any] | list[PIL.Image.Image]:\n    \"\"\"\n    Loads `video` to a list of PIL Image.\n    Args:\n        video (`str`):\n            A URL or Path to a video to convert to a list of PIL Image format.\n        convert_method (Callable[[List[PIL.Image.Image]], List[PIL.Image.Image]], *optional*):\n            A conversion method to apply to the video after loading it. When set to `None` the images will be converted\n            to \"RGB\".\n        return_fps (`bool`, *optional*, defaults to `False`):\n            Whether to return the FPS of the video. If `True`, returns a tuple of (images, fps).\n            If `False`, returns only the list of images.\n    Returns:\n        `List[PIL.Image.Image]` or `Tuple[List[PIL.Image.Image], float | None]`:\n            The video as a list of PIL images. If `return_fps` is True, also returns the original FPS.\n    \"\"\"\n    is_url = video.startswith(\"http://\") or video.startswith(\"https://\")\n    is_file = os.path.isfile(video)\n    was_tempfile_created = False\n\n    if not (is_url or is_file):\n        raise ValueError(\n            f\"Incorrect path or URL. URLs must start with `http://` or `https://`, and {video} is not a valid path.\"\n        )\n\n    if is_url:\n        response = requests.get(video, stream=True)\n        if response.status_code != 200:\n            raise ValueError(\n                f\"Failed to download video. Status code: {response.status_code}\"\n            )\n\n        parsed_url = urlparse(video)\n        file_name = os.path.basename(unquote(parsed_url.path))\n\n        suffix = os.path.splitext(file_name)[1] or \".mp4\"\n        with tempfile.NamedTemporaryFile(suffix=suffix,\n                                         delete=False) as temp_file:\n            video_path = temp_file.name\n            video_data = response.iter_content(chunk_size=8192)\n            for chunk in video_data:\n                temp_file.write(chunk)\n        was_tempfile_created = True\n    else:\n        video_path = video\n\n    pil_images = []\n    original_fps = None\n\n    try:\n        if video_path.endswith(\".gif\"):\n            pil_images, original_fps = _load_gif(video_path)\n        else:\n            pil_images, original_fps = _load_video_with_ffmpeg(video_path)\n    finally:\n        # Clean up temporary file if it was created\n        if was_tempfile_created and os.path.exists(video_path):\n            os.remove(video_path)\n\n    if convert_method is not None:\n        pil_images = convert_method(pil_images)\n\n    return pil_images, original_fps if return_fps else pil_images\n</code></pre> fastvideo.models.vision_utils.normalize \u00b6 <pre><code>normalize(\n    images: ndarray | Tensor,\n) -&gt; np.ndarray | torch.Tensor\n</code></pre> <p>Normalize an image array to [-1,1].</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>`np.ndarray` or `torch.Tensor`</code> <p>The image array to normalize.</p> required <p>Returns:</p> Type Description <code>ndarray | Tensor</code> <p><code>np.ndarray</code> or <code>torch.Tensor</code>: The normalized image array.</p> Source code in <code>fastvideo/models/vision_utils.py</code> <pre><code>def normalize(images: np.ndarray | torch.Tensor) -&gt; np.ndarray | torch.Tensor:\n    r\"\"\"\n    Normalize an image array to [-1,1].\n\n    Args:\n        images (`np.ndarray` or `torch.Tensor`):\n            The image array to normalize.\n\n    Returns:\n        `np.ndarray` or `torch.Tensor`:\n            The normalized image array.\n    \"\"\"\n    return 2.0 * images - 1.0\n</code></pre> fastvideo.models.vision_utils.numpy_to_pt \u00b6 <pre><code>numpy_to_pt(images: ndarray) -&gt; torch.Tensor\n</code></pre> <p>Convert a NumPy image to a PyTorch tensor.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>`np.ndarray`</code> <p>The NumPy image array to convert to PyTorch format.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p><code>torch.Tensor</code>: A PyTorch tensor representation of the images.</p> Source code in <code>fastvideo/models/vision_utils.py</code> <pre><code>def numpy_to_pt(images: np.ndarray) -&gt; torch.Tensor:\n    r\"\"\"\n    Convert a NumPy image to a PyTorch tensor.\n\n    Args:\n        images (`np.ndarray`):\n            The NumPy image array to convert to PyTorch format.\n\n    Returns:\n        `torch.Tensor`:\n            A PyTorch tensor representation of the images.\n    \"\"\"\n    if images.ndim == 3:\n        images = images[..., None]\n\n    images = torch.from_numpy(images.transpose(0, 3, 1, 2))\n    return images\n</code></pre> fastvideo.models.vision_utils.pil_to_numpy \u00b6 <pre><code>pil_to_numpy(images: list[Image] | Image) -&gt; np.ndarray\n</code></pre> <p>Convert a PIL image or a list of PIL images to NumPy arrays.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>`PIL.Image.Image` or `List[PIL.Image.Image]`</code> <p>The PIL image or list of images to convert to NumPy format.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p><code>np.ndarray</code>: A NumPy array representation of the images.</p> Source code in <code>fastvideo/models/vision_utils.py</code> <pre><code>def pil_to_numpy(images: list[PIL.Image.Image] | PIL.Image.Image) -&gt; np.ndarray:\n    r\"\"\"\n    Convert a PIL image or a list of PIL images to NumPy arrays.\n\n    Args:\n        images (`PIL.Image.Image` or `List[PIL.Image.Image]`):\n            The PIL image or list of images to convert to NumPy format.\n\n    Returns:\n        `np.ndarray`:\n            A NumPy array representation of the images.\n    \"\"\"\n    if not isinstance(images, list):\n        images = [images]\n    images = [np.array(image).astype(np.float32) / 255.0 for image in images]\n    images_arr: np.ndarray = np.stack(images, axis=0)\n\n    return images_arr\n</code></pre> fastvideo.models.vision_utils.preprocess_reference_image_for_clip \u00b6 <pre><code>preprocess_reference_image_for_clip(\n    image: Image, device: device\n) -&gt; PIL.Image.Image\n</code></pre> <p>Preprocess reference image to match CLIP encoder requirements.</p> <p>Applies normalization, resizing to 224x224, and denormalization to ensure the image is in the correct format for CLIP processing.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Input PIL image</p> required <code>device</code> <code>device</code> <p>Target device for tensor operations</p> required <p>Returns:</p> Type Description <code>Image</code> <p>Preprocessed PIL image ready for CLIP encoder</p> Source code in <code>fastvideo/models/vision_utils.py</code> <pre><code>def preprocess_reference_image_for_clip(image: PIL.Image.Image, device: torch.device) -&gt; PIL.Image.Image:\n    \"\"\"\n    Preprocess reference image to match CLIP encoder requirements.\n\n    Applies normalization, resizing to 224x224, and denormalization to ensure\n    the image is in the correct format for CLIP processing.\n\n    Args:\n        image: Input PIL image\n        device: Target device for tensor operations\n\n    Returns:\n        Preprocessed PIL image ready for CLIP encoder\n    \"\"\"\n    # Convert PIL to tensor and normalize to [-1, 1] range\n    image_tensor = TF.to_tensor(image).sub_(0.5).div_(0.5).to(device)\n\n    # Resize to CLIP's expected input size (224x224) using bicubic interpolation\n    resized_tensor = F.interpolate(\n        image_tensor.unsqueeze(0),\n        size=(224, 224),\n        mode='bicubic',\n        align_corners=False\n    ).squeeze(0)\n\n    # Denormalize back to [0, 1] range\n    denormalized_tensor = resized_tensor.mul_(0.5).add_(0.5)\n\n    return TF.to_pil_image(denormalized_tensor)\n</code></pre> fastvideo.models.vision_utils.resize \u00b6 <pre><code>resize(\n    image: Image | ndarray | Tensor,\n    height: int,\n    width: int,\n    resize_mode: str = \"default\",\n    resample: str = \"lanczos\",\n) -&gt; PIL.Image.Image | np.ndarray | torch.Tensor\n</code></pre> <p>Resize image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>`PIL.Image.Image`, `np.ndarray` or `torch.Tensor`</code> <p>The image input, can be a PIL image, numpy array or pytorch tensor.</p> required <code>height</code> <code>`int`</code> <p>The height to resize to.</p> required <code>width</code> <code>`int`</code> <p>The width to resize to.</p> required <code>resize_mode</code> <code>`str`, *optional*, defaults to `default`</code> <p>The resize mode to use, can be one of <code>default</code> or <code>fill</code>. If <code>default</code>, will resize the image to fit within the specified width and height, and it may not maintaining the original aspect ratio. If <code>fill</code>, will resize the image to fit within the specified width and height, maintaining the aspect ratio, and then center the image within the dimensions, filling empty with data from image. If <code>crop</code>, will resize the image to fit within the specified width and height, maintaining the aspect ratio, and then center the image within the dimensions, cropping the excess. Note that resize_mode <code>fill</code> and <code>crop</code> are only supported for PIL image input.</p> <code>'default'</code> <p>Returns:</p> Type Description <code>Image | ndarray | Tensor</code> <p><code>PIL.Image.Image</code>, <code>np.ndarray</code> or <code>torch.Tensor</code>: The resized image.</p> Source code in <code>fastvideo/models/vision_utils.py</code> <pre><code>def resize(\n    image: PIL.Image.Image | np.ndarray | torch.Tensor,\n    height: int,\n    width: int,\n    resize_mode: str = \"default\",  # \"default\", \"fill\", \"crop\"\n    resample: str = \"lanczos\",\n) -&gt; PIL.Image.Image | np.ndarray | torch.Tensor:\n    \"\"\"\n    Resize image.\n\n    Args:\n        image (`PIL.Image.Image`, `np.ndarray` or `torch.Tensor`):\n            The image input, can be a PIL image, numpy array or pytorch tensor.\n        height (`int`):\n            The height to resize to.\n        width (`int`):\n            The width to resize to.\n        resize_mode (`str`, *optional*, defaults to `default`):\n            The resize mode to use, can be one of `default` or `fill`. If `default`, will resize the image to fit\n            within the specified width and height, and it may not maintaining the original aspect ratio. If `fill`,\n            will resize the image to fit within the specified width and height, maintaining the aspect ratio, and\n            then center the image within the dimensions, filling empty with data from image. If `crop`, will resize\n            the image to fit within the specified width and height, maintaining the aspect ratio, and then center\n            the image within the dimensions, cropping the excess. Note that resize_mode `fill` and `crop` are only\n            supported for PIL image input.\n\n    Returns:\n        `PIL.Image.Image`, `np.ndarray` or `torch.Tensor`:\n            The resized image.\n    \"\"\"\n    if resize_mode != \"default\" and not isinstance(image, PIL.Image.Image):\n        raise ValueError(\n            f\"Only PIL image input is supported for resize_mode {resize_mode}\")\n    assert isinstance(image, PIL.Image.Image)\n    if resize_mode == \"default\":\n        image = image.resize((width, height),\n                             resample=PIL_INTERPOLATION[resample])\n    else:\n        raise ValueError(f\"resize_mode {resize_mode} is not supported\")\n    return image\n</code></pre>"},{"location":"api/fastvideo/#submodules_1","title":"Submodules","text":""},{"location":"api/fastvideo/#fastvideomodelsregistry","title":"fastvideo.models.registry","text":""},{"location":"api/fastvideo/#fastvideo.models.registry","title":"registry","text":""},{"location":"api/fastvideo/#fastvideo.models.registry-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideomodelsloader","title":"fastvideo.models.loader","text":""},{"location":"api/fastvideo/#fastvideo.models.loader","title":"loader","text":""},{"location":"api/fastvideo/#fastvideo.models.loader-modules","title":"Modules","text":""},{"location":"api/fastvideo/#fastvideo.models.loader.component_loader","title":"fastvideo.models.loader.component_loader","text":"Classes\u00b6 fastvideo.models.loader.component_loader.ComponentLoader \u00b6 <pre><code>ComponentLoader(device=None)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for loading a specific type of model component.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def __init__(self, device=None) -&gt; None:\n    self.device = device\n</code></pre> Functions\u00b6 fastvideo.models.loader.component_loader.ComponentLoader.for_module_type <code>classmethod</code> \u00b6 <pre><code>for_module_type(\n    module_type: str, transformers_or_diffusers: str\n) -&gt; ComponentLoader\n</code></pre> <p>Factory method to create a component loader for a specific module type.</p> <p>Parameters:</p> Name Type Description Default <code>module_type</code> <code>str</code> <p>Type of module (e.g., \"vae\", \"text_encoder\", \"transformer\", \"scheduler\")</p> required <code>transformers_or_diffusers</code> <code>str</code> <p>Whether the module is from transformers or diffusers</p> required <p>Returns:</p> Type Description <code>ComponentLoader</code> <p>A component loader for the specified module type</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>@classmethod\ndef for_module_type(cls, module_type: str,\n                    transformers_or_diffusers: str) -&gt; 'ComponentLoader':\n    \"\"\"\n    Factory method to create a component loader for a specific module type.\n\n    Args:\n        module_type: Type of module (e.g., \"vae\", \"text_encoder\", \"transformer\", \"scheduler\")\n        transformers_or_diffusers: Whether the module is from transformers or diffusers\n\n    Returns:\n        A component loader for the specified module type\n    \"\"\"\n    # Map of module types to their loader classes and expected library\n    module_loaders = {\n        \"scheduler\": (SchedulerLoader, \"diffusers\"),\n        \"transformer\": (TransformerLoader, \"diffusers\"),\n        \"transformer_2\": (TransformerLoader, \"diffusers\"),\n        \"vae\": (VAELoader, \"diffusers\"),\n        \"text_encoder\": (TextEncoderLoader, \"transformers\"),\n        \"text_encoder_2\": (TextEncoderLoader, \"transformers\"),\n        \"tokenizer\": (TokenizerLoader, \"transformers\"),\n        \"tokenizer_2\": (TokenizerLoader, \"transformers\"),\n        \"image_processor\": (ImageProcessorLoader, \"transformers\"),\n        \"image_encoder\": (ImageEncoderLoader, \"transformers\"),\n    }\n\n    if module_type in module_loaders:\n        loader_cls, expected_library = module_loaders[module_type]\n        # Assert that the library matches what's expected for this module type\n        assert transformers_or_diffusers == expected_library, f\"{module_type} must be loaded from {expected_library}, got {transformers_or_diffusers}\"\n        return loader_cls()\n\n    # For unknown module types, use a generic loader\n    logger.warning(\n        \"No specific loader found for module type: %s. Using generic loader.\",\n        module_type)\n    return GenericComponentLoader(transformers_or_diffusers)\n</code></pre> fastvideo.models.loader.component_loader.ComponentLoader.load <code>abstractmethod</code> \u00b6 <pre><code>load(model_path: str, fastvideo_args: FastVideoArgs)\n</code></pre> <p>Load the component based on the model path, architecture, and inference args.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the component model</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>FastVideoArgs</p> required <p>Returns:</p> Type Description <p>The loaded component</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>@abstractmethod\ndef load(self, model_path: str, fastvideo_args: FastVideoArgs):\n    \"\"\"\n    Load the component based on the model path, architecture, and inference args.\n\n    Args:\n        model_path: Path to the component model\n        fastvideo_args: FastVideoArgs\n\n    Returns:\n        The loaded component\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.models.loader.component_loader.GenericComponentLoader \u00b6 <pre><code>GenericComponentLoader(library='transformers')\n</code></pre> <p>               Bases: <code>ComponentLoader</code></p> <p>Generic loader for components that don't have a specific loader.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def __init__(self, library=\"transformers\") -&gt; None:\n    super().__init__()\n    self.library = library\n</code></pre> Functions\u00b6 fastvideo.models.loader.component_loader.GenericComponentLoader.load \u00b6 <pre><code>load(model_path: str, fastvideo_args: FastVideoArgs)\n</code></pre> <p>Load a generic component based on the model path, and inference args.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def load(self, model_path: str, fastvideo_args: FastVideoArgs):\n    \"\"\"Load a generic component based on the model path, and inference args.\"\"\"\n    logger.warning(\"Using generic loader for %s with library %s\",\n                   model_path, self.library)\n\n    if self.library == \"transformers\":\n        from transformers import AutoModel\n\n        model = AutoModel.from_pretrained(\n            model_path,\n            trust_remote_code=fastvideo_args.trust_remote_code,\n            revision=fastvideo_args.revision,\n        )\n        logger.info(\"Loaded generic transformers model: %s\",\n                    model.__class__.__name__)\n        return model\n    elif self.library == \"diffusers\":\n        logger.warning(\n            \"Generic loading for diffusers components is not fully implemented\"\n        )\n\n        model_config = get_diffusers_config(model=model_path)\n        logger.info(\"Diffusers Model config: %s\", model_config)\n        # This is a placeholder - in a real implementation, you'd need to handle this properly\n        return None\n    else:\n        raise ValueError(f\"Unsupported library: {self.library}\")\n</code></pre> fastvideo.models.loader.component_loader.ImageEncoderLoader \u00b6 <pre><code>ImageEncoderLoader(device=None)\n</code></pre> <p>               Bases: <code>TextEncoderLoader</code></p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def __init__(self, device=None) -&gt; None:\n    self.device = device\n</code></pre> Functions\u00b6 fastvideo.models.loader.component_loader.ImageEncoderLoader.load \u00b6 <pre><code>load(model_path: str, fastvideo_args: FastVideoArgs)\n</code></pre> <p>Load the text encoders based on the model path, and inference args.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def load(self, model_path: str, fastvideo_args: FastVideoArgs):\n    \"\"\"Load the text encoders based on the model path, and inference args.\"\"\"\n    # model_config: PretrainedConfig = get_hf_config(\n    #     model=model_path,\n    #     trust_remote_code=fastvideo_args.trust_remote_code,\n    #     revision=fastvideo_args.revision,\n    #     model_override_args=None,\n    # )\n    with open(os.path.join(model_path, \"config.json\")) as f:\n        model_config = json.load(f)\n    model_config.pop(\"_name_or_path\", None)\n    model_config.pop(\"transformers_version\", None)\n    model_config.pop(\"torch_dtype\", None)\n    model_config.pop(\"model_type\", None)\n    logger.info(\"HF Model config: %s\", model_config)\n\n    encoder_config = fastvideo_args.pipeline_config.image_encoder_config\n    encoder_config.update_model_arch(model_config)\n\n    from fastvideo.platforms import current_platform\n\n    if fastvideo_args.image_encoder_cpu_offload:\n        target_device = torch.device(\"mps\") if current_platform.is_mps() else torch.device(\"cpu\")\n    else:\n        target_device = get_local_torch_device()\n    # TODO(will): add support for other dtypes\n    return self.load_model(\n        model_path, encoder_config, target_device, fastvideo_args,\n        fastvideo_args.pipeline_config.image_encoder_precision)\n</code></pre> fastvideo.models.loader.component_loader.ImageProcessorLoader \u00b6 <pre><code>ImageProcessorLoader(device=None)\n</code></pre> <p>               Bases: <code>ComponentLoader</code></p> <p>Loader for image processor.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def __init__(self, device=None) -&gt; None:\n    self.device = device\n</code></pre> Functions\u00b6 fastvideo.models.loader.component_loader.ImageProcessorLoader.load \u00b6 <pre><code>load(model_path: str, fastvideo_args: FastVideoArgs)\n</code></pre> <p>Load the image processor based on the model path, and inference args.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def load(self, model_path: str, fastvideo_args: FastVideoArgs):\n    \"\"\"Load the image processor based on the model path, and inference args.\"\"\"\n    logger.info(\"Loading image processor from %s\", model_path)\n\n    image_processor = AutoImageProcessor.from_pretrained(model_path, )\n    logger.info(\"Loaded image processor: %s\",\n                image_processor.__class__.__name__)\n    return image_processor\n</code></pre> fastvideo.models.loader.component_loader.PipelineComponentLoader \u00b6 <p>Utility class for loading pipeline components. This replaces the chain of if-else statements in load_pipeline_module.</p> Functions\u00b6 fastvideo.models.loader.component_loader.PipelineComponentLoader.load_module <code>staticmethod</code> \u00b6 <pre><code>load_module(\n    module_name: str,\n    component_model_path: str,\n    transformers_or_diffusers: str,\n    fastvideo_args: FastVideoArgs,\n)\n</code></pre> <p>Load a pipeline module.</p> <p>Parameters:</p> Name Type Description Default <code>module_name</code> <code>str</code> <p>Name of the module (e.g., \"vae\", \"text_encoder\", \"transformer\", \"scheduler\")</p> required <code>component_model_path</code> <code>str</code> <p>Path to the component model</p> required <code>transformers_or_diffusers</code> <code>str</code> <p>Whether the module is from transformers or diffusers</p> required <code>pipeline_args</code> <p>Inference arguments</p> required <p>Returns:</p> Type Description <p>The loaded module</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>@staticmethod\ndef load_module(module_name: str, component_model_path: str,\n                transformers_or_diffusers: str,\n                fastvideo_args: FastVideoArgs):\n    \"\"\"\n    Load a pipeline module.\n\n    Args:\n        module_name: Name of the module (e.g., \"vae\", \"text_encoder\", \"transformer\", \"scheduler\")\n        component_model_path: Path to the component model\n        transformers_or_diffusers: Whether the module is from transformers or diffusers\n        pipeline_args: Inference arguments\n\n    Returns:\n        The loaded module\n    \"\"\"\n    logger.info(\n        \"Loading %s using %s from %s\",\n        module_name,\n        transformers_or_diffusers,\n        component_model_path,\n    )\n\n    # Get the appropriate loader for this module type\n    loader = ComponentLoader.for_module_type(module_name,\n                                             transformers_or_diffusers)\n\n    # Load the module\n    return loader.load(component_model_path, fastvideo_args)\n</code></pre> fastvideo.models.loader.component_loader.SchedulerLoader \u00b6 <pre><code>SchedulerLoader(device=None)\n</code></pre> <p>               Bases: <code>ComponentLoader</code></p> <p>Loader for scheduler.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def __init__(self, device=None) -&gt; None:\n    self.device = device\n</code></pre> Functions\u00b6 fastvideo.models.loader.component_loader.SchedulerLoader.load \u00b6 <pre><code>load(model_path: str, fastvideo_args: FastVideoArgs)\n</code></pre> <p>Load the scheduler based on the model path, and inference args.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def load(self, model_path: str, fastvideo_args: FastVideoArgs):\n    \"\"\"Load the scheduler based on the model path, and inference args.\"\"\"\n    config = get_diffusers_config(model=model_path)\n\n    class_name = config.pop(\"_class_name\")\n    assert class_name is not None, \"Model config does not contain a _class_name attribute. Only diffusers format is supported.\"\n\n    scheduler_cls, _ = ModelRegistry.resolve_model_cls(class_name)\n\n    scheduler = scheduler_cls(**config)\n    if fastvideo_args.pipeline_config.flow_shift is not None:\n        scheduler.set_shift(fastvideo_args.pipeline_config.flow_shift)\n    if fastvideo_args.pipeline_config.timesteps_scale is not None:\n        scheduler.set_timesteps_scale(\n            fastvideo_args.pipeline_config.timesteps_scale)\n    return scheduler\n</code></pre> fastvideo.models.loader.component_loader.TextEncoderLoader \u00b6 <pre><code>TextEncoderLoader(device=None)\n</code></pre> <p>               Bases: <code>ComponentLoader</code></p> <p>Loader for text encoders.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def __init__(self, device=None) -&gt; None:\n    self.device = device\n</code></pre> Classes\u00b6 fastvideo.models.loader.component_loader.TextEncoderLoader.Source <code>dataclass</code> \u00b6 <pre><code>Source(\n    model_or_path: str,\n    prefix: str = \"\",\n    fall_back_to_pt: bool = True,\n    allow_patterns_overrides: list[str] | None = None,\n)\n</code></pre> <p>A source for weights.</p> Attributes\u00b6 fastvideo.models.loader.component_loader.TextEncoderLoader.Source.allow_patterns_overrides <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>allow_patterns_overrides: list[str] | None = None\n</code></pre> <p>If defined, weights will load exclusively using these patterns.</p> fastvideo.models.loader.component_loader.TextEncoderLoader.Source.fall_back_to_pt <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>fall_back_to_pt: bool = True\n</code></pre> <p>Whether .pt weights can be used.</p> fastvideo.models.loader.component_loader.TextEncoderLoader.Source.model_or_path <code>instance-attribute</code> \u00b6 <pre><code>model_or_path: str\n</code></pre> <p>The model ID or path.</p> fastvideo.models.loader.component_loader.TextEncoderLoader.Source.prefix <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>prefix: str = ''\n</code></pre> <p>A prefix to prepend to all weights.</p> Functions\u00b6 fastvideo.models.loader.component_loader.TextEncoderLoader.load \u00b6 <pre><code>load(model_path: str, fastvideo_args: FastVideoArgs)\n</code></pre> <p>Load the text encoders based on the model path, and inference args.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def load(self, model_path: str, fastvideo_args: FastVideoArgs):\n    \"\"\"Load the text encoders based on the model path, and inference args.\"\"\"\n    # model_config: PretrainedConfig = get_hf_config(\n    #     model=model_path,\n    #     trust_remote_code=fastvideo_args.trust_remote_code,\n    #     revision=fastvideo_args.revision,\n    #     model_override_args=None,\n    # )\n    model_config = get_diffusers_config(model=model_path)\n    model_config.pop(\"_name_or_path\", None)\n    model_config.pop(\"transformers_version\", None)\n    model_config.pop(\"model_type\", None)\n    model_config.pop(\"tokenizer_class\", None)\n    model_config.pop(\"torch_dtype\", None)\n    logger.info(\"HF Model config: %s\", model_config)\n\n    # @TODO(Wei): Better way to handle this?\n    try:\n        encoder_config = fastvideo_args.pipeline_config.text_encoder_configs[\n            0]\n        encoder_config.update_model_arch(model_config)\n        encoder_precision = fastvideo_args.pipeline_config.text_encoder_precisions[\n            0]\n    except Exception:\n        encoder_config = fastvideo_args.pipeline_config.text_encoder_configs[\n            1]\n        encoder_config.update_model_arch(model_config)\n        encoder_precision = fastvideo_args.pipeline_config.text_encoder_precisions[\n            1]\n\n    target_device = get_local_torch_device()\n    # TODO(will): add support for other dtypes\n    return self.load_model(model_path, encoder_config, target_device,\n                           fastvideo_args, encoder_precision)\n</code></pre> fastvideo.models.loader.component_loader.TokenizerLoader \u00b6 <pre><code>TokenizerLoader(device=None)\n</code></pre> <p>               Bases: <code>ComponentLoader</code></p> <p>Loader for tokenizers.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def __init__(self, device=None) -&gt; None:\n    self.device = device\n</code></pre> Functions\u00b6 fastvideo.models.loader.component_loader.TokenizerLoader.load \u00b6 <pre><code>load(model_path: str, fastvideo_args: FastVideoArgs)\n</code></pre> <p>Load the tokenizer based on the model path, and inference args.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def load(self, model_path: str, fastvideo_args: FastVideoArgs):\n    \"\"\"Load the tokenizer based on the model path, and inference args.\"\"\"\n    logger.info(\"Loading tokenizer from %s\", model_path)\n\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_path,  # \"&lt;path to model&gt;/tokenizer\"\n        # in v0, this was same string as encoder_name \"ClipTextModel\"\n        # TODO(will): pass these tokenizer kwargs from inference args? Maybe\n        # other method of config?\n        padding_size='right',\n    )\n    logger.info(\"Loaded tokenizer: %s\", tokenizer.__class__.__name__)\n    return tokenizer\n</code></pre> fastvideo.models.loader.component_loader.TransformerLoader \u00b6 <pre><code>TransformerLoader(device=None)\n</code></pre> <p>               Bases: <code>ComponentLoader</code></p> <p>Loader for transformer.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def __init__(self, device=None) -&gt; None:\n    self.device = device\n</code></pre> Functions\u00b6 fastvideo.models.loader.component_loader.TransformerLoader.load \u00b6 <pre><code>load(model_path: str, fastvideo_args: FastVideoArgs)\n</code></pre> <p>Load the transformer based on the model path, and inference args.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def load(self, model_path: str, fastvideo_args: FastVideoArgs):\n    \"\"\"Load the transformer based on the model path, and inference args.\"\"\"\n    config = get_diffusers_config(model=model_path)\n    hf_config = deepcopy(config)\n    cls_name = config.pop(\"_class_name\")\n    if cls_name is None:\n        raise ValueError(\n            \"Model config does not contain a _class_name attribute. \"\n            \"Only diffusers format is supported.\")\n\n    logger.info(\"transformer cls_name: %s\", cls_name)\n    if fastvideo_args.override_transformer_cls_name is not None:\n        cls_name = fastvideo_args.override_transformer_cls_name\n        logger.info(\"Overriding transformer cls_name to %s\", cls_name)\n\n    fastvideo_args.model_paths[\"transformer\"] = model_path\n\n    # Config from Diffusers supersedes fastvideo's model config\n    dit_config = fastvideo_args.pipeline_config.dit_config\n    dit_config.update_model_arch(config)\n\n    model_cls, _ = ModelRegistry.resolve_model_cls(cls_name)\n\n    # Find all safetensors files\n    safetensors_list = glob.glob(\n        os.path.join(str(model_path), \"*.safetensors\"))\n    if not safetensors_list:\n        raise ValueError(f\"No safetensors files found in {model_path}\")\n\n    # Check if we should use custom initialization weights\n    custom_weights_path = getattr(fastvideo_args, 'init_weights_from_safetensors', None)\n    use_custom_weights = (custom_weights_path and os.path.exists(custom_weights_path) and \n                        not hasattr(fastvideo_args, '_loading_teacher_critic_model'))\n\n    if use_custom_weights:\n        if 'transformer_2' in model_path:\n            custom_weights_path = getattr(fastvideo_args, 'init_weights_from_safetensors_2', None)\n        assert custom_weights_path is not None, \"Custom initialization weights must be provided\"\n        if os.path.isdir(custom_weights_path):\n            safetensors_list = glob.glob(\n                os.path.join(str(custom_weights_path), \"*.safetensors\"))\n        else:\n            assert custom_weights_path.endswith(\".safetensors\"), \"Custom initialization weights must be a safetensors file\"\n            safetensors_list = [custom_weights_path]\n\n    logger.info(\"Loading model from %s safetensors files: %s\",\n                len(safetensors_list), safetensors_list)\n\n    default_dtype = PRECISION_TO_TYPE[\n        fastvideo_args.pipeline_config.dit_precision]\n\n    # Load the model using FSDP loader\n    logger.info(\"Loading model from %s, default_dtype: %s\", cls_name,\n                default_dtype)\n    assert fastvideo_args.hsdp_shard_dim is not None\n    model = maybe_load_fsdp_model(\n        model_cls=model_cls,\n        init_params={\n            \"config\": dit_config,\n            \"hf_config\": hf_config\n        },\n        weight_dir_list=safetensors_list,\n        device=get_local_torch_device(),\n        hsdp_replicate_dim=fastvideo_args.hsdp_replicate_dim,\n        hsdp_shard_dim=fastvideo_args.hsdp_shard_dim,\n        cpu_offload=fastvideo_args.dit_cpu_offload,\n        pin_cpu_memory=fastvideo_args.pin_cpu_memory,\n        fsdp_inference=fastvideo_args.use_fsdp_inference,\n        # TODO(will): make these configurable\n        default_dtype=default_dtype,\n        param_dtype=torch.bfloat16,\n        reduce_dtype=torch.float32,\n        output_dtype=None,\n        training_mode=fastvideo_args.training_mode,\n        enable_torch_compile=fastvideo_args.enable_torch_compile,\n        torch_compile_kwargs=fastvideo_args.torch_compile_kwargs)\n\n\n    total_params = sum(p.numel() for p in model.parameters())\n    logger.info(\"Loaded model with %.2fB parameters\", total_params / 1e9)\n\n    assert next(model.parameters()).dtype == default_dtype, \"Model dtype does not match default dtype\"\n\n    model = model.eval()\n    return model\n</code></pre> fastvideo.models.loader.component_loader.VAELoader \u00b6 <pre><code>VAELoader(device=None)\n</code></pre> <p>               Bases: <code>ComponentLoader</code></p> <p>Loader for VAE.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def __init__(self, device=None) -&gt; None:\n    self.device = device\n</code></pre> Functions\u00b6 fastvideo.models.loader.component_loader.VAELoader.load \u00b6 <pre><code>load(model_path: str, fastvideo_args: FastVideoArgs)\n</code></pre> <p>Load the VAE based on the model path, and inference args.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def load(self, model_path: str, fastvideo_args: FastVideoArgs):\n    \"\"\"Load the VAE based on the model path, and inference args.\"\"\"\n    config = get_diffusers_config(model=model_path)\n    class_name = config.pop(\"_class_name\")\n    assert class_name is not None, \"Model config does not contain a _class_name attribute. Only diffusers format is supported.\"\n    fastvideo_args.model_paths[\"vae\"] = model_path\n\n    vae_config = fastvideo_args.pipeline_config.vae_config\n    vae_config.update_model_arch(config)\n\n    from fastvideo.platforms import current_platform\n\n    if fastvideo_args.vae_cpu_offload:\n        target_device = torch.device(\"mps\") if current_platform.is_mps() else torch.device(\"cpu\")\n    else:\n        target_device = get_local_torch_device()\n\n    with set_default_torch_dtype(PRECISION_TO_TYPE[\n            fastvideo_args.pipeline_config.vae_precision]):\n        vae_cls, _ = ModelRegistry.resolve_model_cls(class_name)\n        vae = vae_cls(vae_config).to(target_device)\n\n    # Find all safetensors files\n    safetensors_list = glob.glob(\n        os.path.join(str(model_path), \"*.safetensors\"))\n    # TODO(PY)\n    assert len(\n        safetensors_list\n    ) == 1, f\"Found {len(safetensors_list)} safetensors files in {model_path}\"\n    loaded = safetensors_load_file(safetensors_list[0])\n    vae.load_state_dict(\n        loaded, strict=False)  # We might only load encoder or decoder\n\n    return vae.eval()\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/#fastvideo.models.loader.fsdp_load","title":"fastvideo.models.loader.fsdp_load","text":"Functions\u00b6 fastvideo.models.loader.fsdp_load.load_model_from_full_model_state_dict \u00b6 <pre><code>load_model_from_full_model_state_dict(\n    model: FSDPModule | Module,\n    full_sd_iterator: Generator[\n        tuple[str, Tensor], None, None\n    ],\n    device: device,\n    param_dtype: dtype,\n    strict: bool = False,\n    cpu_offload: bool = False,\n    param_names_mapping: Callable[\n        [str], tuple[str, Any, Any]\n    ]\n    | None = None,\n    training_mode: bool = True,\n) -&gt; _IncompatibleKeys\n</code></pre> <p>Converting full state dict into a sharded state dict and loading it into FSDP model (if training) or normal huggingface model Args:     model (Union[FSDPModule, torch.nn.Module]): Model to generate fully qualified names for cpu_state_dict     full_sd_iterator (Generator): an iterator yielding (param_name, tensor) pairs     device (torch.device): device used to move full state dict tensors     param_dtype (torch.dtype): dtype used to move full state dict tensors     strict (bool): flag to check if to load the model in strict mode     cpu_offload (bool): flag to check if FSDP offload is enabled     param_names_mapping (Optional[Callable[[str], str]]): a function that maps full param name to sharded param name     training_mode (bool): apply FSDP only for training Returns:     <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:         * missing_keys is a list of str containing the missing keys         * unexpected_keys is a list of str containing the unexpected keys</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If got FSDP with more than 1D.</p> Source code in <code>fastvideo/models/loader/fsdp_load.py</code> <pre><code>def load_model_from_full_model_state_dict(\n    model: FSDPModule | torch.nn.Module,\n    full_sd_iterator: Generator[tuple[str, torch.Tensor], None, None],\n    device: torch.device,\n    param_dtype: torch.dtype,\n    strict: bool = False,\n    cpu_offload: bool = False,\n    param_names_mapping: Callable[[str], tuple[str, Any, Any]] | None = None,\n    training_mode: bool = True,\n) -&gt; _IncompatibleKeys:\n    \"\"\"\n    Converting full state dict into a sharded state dict\n    and loading it into FSDP model (if training) or normal huggingface model\n    Args:\n        model (Union[FSDPModule, torch.nn.Module]): Model to generate fully qualified names for cpu_state_dict\n        full_sd_iterator (Generator): an iterator yielding (param_name, tensor) pairs\n        device (torch.device): device used to move full state dict tensors\n        param_dtype (torch.dtype): dtype used to move full state dict tensors\n        strict (bool): flag to check if to load the model in strict mode\n        cpu_offload (bool): flag to check if FSDP offload is enabled\n        param_names_mapping (Optional[Callable[[str], str]]): a function that maps full param name to sharded param name\n        training_mode (bool): apply FSDP only for training\n    Returns:\n        ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n            * **missing_keys** is a list of str containing the missing keys\n            * **unexpected_keys** is a list of str containing the unexpected keys\n\n    Raises:\n        NotImplementedError: If got FSDP with more than 1D.\n    \"\"\"\n    meta_sd = model.state_dict()\n    sharded_sd = {}\n    custom_param_sd, reverse_param_names_mapping = hf_to_custom_state_dict(\n        full_sd_iterator, param_names_mapping)  # type: ignore\n    for target_param_name, full_tensor in custom_param_sd.items():\n        meta_sharded_param = meta_sd.get(target_param_name)\n        if meta_sharded_param is None:\n            raise ValueError(\n                f\"Parameter {target_param_name} not found in custom model state dict. The hf to custom mapping may be incorrect.\"\n            )\n        if not hasattr(meta_sharded_param, \"device_mesh\"):\n            full_tensor = full_tensor.to(device=device, dtype=param_dtype)\n            # In cases where parts of the model aren't sharded, some parameters will be plain tensors\n            sharded_tensor = full_tensor\n        else:\n            full_tensor = full_tensor.to(device=device, dtype=param_dtype)\n            sharded_tensor = distribute_tensor(\n                full_tensor,\n                meta_sharded_param.device_mesh,\n                meta_sharded_param.placements,\n            )\n            if cpu_offload:\n                sharded_tensor = sharded_tensor.cpu()\n        sharded_sd[target_param_name] = nn.Parameter(sharded_tensor)\n\n    model.reverse_param_names_mapping = reverse_param_names_mapping\n    unused_keys = set(meta_sd.keys()) - set(sharded_sd.keys())\n    if unused_keys:\n        logger.warning(\"Found unloaded parameters in meta state dict: %s\",\n                       unused_keys)\n\n    # List of allowed parameter name patterns\n    ALLOWED_NEW_PARAM_PATTERNS = [\"gate_compress\"]  # Can be extended as needed\n    for new_param_name in unused_keys:\n        if not any(pattern in new_param_name\n                   for pattern in ALLOWED_NEW_PARAM_PATTERNS):\n            logger.error(\"Unsupported new parameter: %s. Allowed patterns: %s\",\n                         new_param_name, ALLOWED_NEW_PARAM_PATTERNS)\n            raise ValueError(\n                f\"New parameter '{new_param_name}' is not supported. \"\n                f\"Currently only parameters containing {ALLOWED_NEW_PARAM_PATTERNS} are allowed.\"\n            )\n        meta_sharded_param = meta_sd.get(new_param_name)\n        if not hasattr(meta_sharded_param, \"device_mesh\"):\n            # Initialize with zeros\n            sharded_tensor = torch.zeros_like(meta_sharded_param,\n                                              device=device,\n                                              dtype=param_dtype)\n        else:\n            # Initialize with zeros and distribute\n            full_tensor = torch.zeros_like(meta_sharded_param,\n                                           device=device,\n                                           dtype=param_dtype)\n            sharded_tensor = distribute_tensor(\n                full_tensor,\n                meta_sharded_param.device_mesh,\n                meta_sharded_param.placements,\n            )\n            if cpu_offload:\n                sharded_tensor = sharded_tensor.cpu()\n        sharded_sd[new_param_name] = nn.Parameter(sharded_tensor)\n\n    # choose `assign=True` since we cannot call `copy_` on meta tensor\n    return model.load_state_dict(sharded_sd, strict=strict, assign=True)\n</code></pre> fastvideo.models.loader.fsdp_load.maybe_load_fsdp_model \u00b6 <pre><code>maybe_load_fsdp_model(\n    model_cls: type[Module],\n    init_params: dict[str, Any],\n    weight_dir_list: list[str],\n    device: device,\n    hsdp_replicate_dim: int,\n    hsdp_shard_dim: int,\n    default_dtype: dtype,\n    param_dtype: dtype,\n    reduce_dtype: dtype,\n    cpu_offload: bool = False,\n    fsdp_inference: bool = False,\n    output_dtype: dtype | None = None,\n    training_mode: bool = True,\n    pin_cpu_memory: bool = True,\n    enable_torch_compile: bool = False,\n    torch_compile_kwargs: dict[str, Any] | None = None,\n) -&gt; torch.nn.Module\n</code></pre> <p>Load the model with FSDP if is training, else load the model without FSDP.</p> Source code in <code>fastvideo/models/loader/fsdp_load.py</code> <pre><code>def maybe_load_fsdp_model(\n    model_cls: type[nn.Module],\n    init_params: dict[str, Any],\n    weight_dir_list: list[str],\n    device: torch.device,\n    hsdp_replicate_dim: int,\n    hsdp_shard_dim: int,\n    default_dtype: torch.dtype,\n    param_dtype: torch.dtype,\n    reduce_dtype: torch.dtype,\n    cpu_offload: bool = False,\n    fsdp_inference: bool = False,\n    output_dtype: torch.dtype | None = None,\n    training_mode: bool = True,\n    pin_cpu_memory: bool = True,\n    enable_torch_compile: bool = False,\n    torch_compile_kwargs: dict[str, Any] | None = None,\n) -&gt; torch.nn.Module:\n    \"\"\"\n    Load the model with FSDP if is training, else load the model without FSDP.\n    \"\"\"\n    # NOTE(will): cast_forward_inputs=True shouldn't be needed as we are\n    # manually casting the inputs to the model\n    mp_policy = MixedPrecisionPolicy(param_dtype,\n                                     reduce_dtype,\n                                     output_dtype,\n                                     cast_forward_inputs=False)\n\n    set_mixed_precision_policy(\n        param_dtype=param_dtype,\n        reduce_dtype=reduce_dtype,\n        output_dtype=output_dtype,\n        mp_policy=mp_policy,\n    )\n\n    logger.info(\"Loading model with default_dtype: %s\", default_dtype)\n    with set_default_dtype(default_dtype), torch.device(\"meta\"):\n        model = model_cls(**init_params)\n\n    # Check if we should use FSDP\n    use_fsdp = training_mode or fsdp_inference\n\n    # Disable FSDP for MPS as it's not compatible\n    from fastvideo.platforms import current_platform\n    if current_platform.is_mps():\n        use_fsdp = False\n        logger.info(\"Disabling FSDP for MPS platform as it's not compatible\")\n\n    if use_fsdp:\n        world_size = hsdp_replicate_dim * hsdp_shard_dim\n        if not training_mode and not fsdp_inference:\n            hsdp_replicate_dim = world_size\n            hsdp_shard_dim = 1\n\n        if current_platform.is_npu():\n            with torch.device(\"cpu\"):\n                device_mesh = init_device_mesh(\n                    \"npu\",\n                    # (Replicate(), Shard(dim=0))\n                    mesh_shape=(hsdp_replicate_dim, hsdp_shard_dim),\n                    mesh_dim_names=(\"replicate\", \"shard\"),\n                )\n        else:\n            device_mesh = init_device_mesh(\n            \"cuda\",\n            # (Replicate(), Shard(dim=0))\n            mesh_shape=(hsdp_replicate_dim, hsdp_shard_dim),\n            mesh_dim_names=(\"replicate\", \"shard\"),\n        )\n        shard_model(model,\n                    cpu_offload=cpu_offload,\n                    reshard_after_forward=True,\n                    mp_policy=mp_policy,\n                    mesh=device_mesh,\n                    fsdp_shard_conditions=model._fsdp_shard_conditions,\n                    pin_cpu_memory=pin_cpu_memory)\n\n    weight_iterator = safetensors_weights_iterator(weight_dir_list)\n    param_names_mapping_fn = get_param_names_mapping(model.param_names_mapping)\n    load_model_from_full_model_state_dict(\n        model,\n        weight_iterator,\n        device,\n        default_dtype,\n        strict=True,\n        cpu_offload=cpu_offload,\n        param_names_mapping=param_names_mapping_fn,\n    )\n    for n, p in chain(model.named_parameters(), model.named_buffers()):\n        if p.is_meta:\n            raise RuntimeError(\n                f\"Unexpected param or buffer {n} on meta device.\")\n        # Avoid unintended computation graph accumulation during inference\n        if isinstance(p, torch.nn.Parameter):\n            p.requires_grad = False\n\n    compile_in_loader = enable_torch_compile and training_mode\n    if compile_in_loader:\n        compile_kwargs = torch_compile_kwargs or {}\n        logger.info(\"Enabling torch.compile for FSDP training module with kwargs=%s\",\n                    compile_kwargs)\n        model = torch.compile(model, **compile_kwargs)\n        logger.info(\"torch.compile enabled for %s\", type(model).__name__)\n    return model\n</code></pre> fastvideo.models.loader.fsdp_load.set_default_dtype \u00b6 <pre><code>set_default_dtype(\n    dtype: dtype,\n) -&gt; Generator[None, None, None]\n</code></pre> <p>Context manager to set torch's default dtype.</p> <p>Parameters:</p> Name Type Description Default <code>dtype</code> <code>dtype</code> <p>The desired default dtype inside the context manager.</p> required <p>Returns:</p> Name Type Description <code>ContextManager</code> <code>None</code> <p>context manager for setting default dtype.</p> Example <p>with set_default_dtype(torch.bfloat16):     x = torch.tensor([1, 2, 3])     x.dtype torch.bfloat16</p> Source code in <code>fastvideo/models/loader/fsdp_load.py</code> <pre><code>@contextlib.contextmanager\ndef set_default_dtype(dtype: torch.dtype) -&gt; Generator[None, None, None]:\n    \"\"\"\n    Context manager to set torch's default dtype.\n\n    Args:\n        dtype (torch.dtype): The desired default dtype inside the context manager.\n\n    Returns:\n        ContextManager: context manager for setting default dtype.\n\n    Example:\n        &gt;&gt;&gt; with set_default_dtype(torch.bfloat16):\n        &gt;&gt;&gt;     x = torch.tensor([1, 2, 3])\n        &gt;&gt;&gt;     x.dtype\n        torch.bfloat16\n\n\n    \"\"\"\n    old_dtype = torch.get_default_dtype()\n    torch.set_default_dtype(dtype)\n    try:\n        yield\n    finally:\n        torch.set_default_dtype(old_dtype)\n</code></pre> fastvideo.models.loader.fsdp_load.shard_model \u00b6 <pre><code>shard_model(\n    model,\n    *,\n    cpu_offload: bool,\n    reshard_after_forward: bool = True,\n    mp_policy: MixedPrecisionPolicy\n    | None = MixedPrecisionPolicy(),\n    mesh: DeviceMesh | None = None,\n    fsdp_shard_conditions: list[\n        Callable[[str, Module], bool]\n    ] = [],\n    pin_cpu_memory: bool = True\n) -&gt; None\n</code></pre> <p>Utility to shard a model with FSDP using the PyTorch Distributed fully_shard API.</p> <p>This method will over the model's named modules from the bottom-up and apply shard modules based on whether they meet any of the criteria from shard_conditions.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>TransformerDecoder</code> <p>Model to shard with FSDP.</p> required <code>shard_conditions</code> <code>List[Callable[[str, Module], bool]]</code> <p>A list of functions to determine which modules to shard with FSDP. Each function should take module name (relative to root) and the module itself, returning True if FSDP should shard the module and False otherwise. If any of shard_conditions return True for a given module, it will be sharded by FSDP.</p> required <code>cpu_offload</code> <code>bool</code> <p>If set to True, FSDP will offload parameters, gradients, and optimizer states to CPU.</p> required <code>reshard_after_forward</code> <code>bool</code> <p>Whether to reshard parameters and buffers after the forward pass. Setting this to True corresponds to the FULL_SHARD sharding strategy from FSDP1, while setting it to False corresponds to the SHARD_GRAD_OP sharding strategy.</p> <code>True</code> <code>mesh</code> <code>Optional[DeviceMesh]</code> <p>Device mesh to use for FSDP sharding under multiple parallelism. Default to None.</p> <code>None</code> <code>fsdp_shard_conditions</code> <code>List[Callable[[str, Module], bool]]</code> <p>A list of functions to determine which modules to shard with FSDP.</p> <code>[]</code> <code>pin_cpu_memory</code> <code>bool</code> <p>If set to True, FSDP will pin the CPU memory of the offloaded parameters.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no layer modules were sharded, indicating that no shard_condition was triggered.</p> Source code in <code>fastvideo/models/loader/fsdp_load.py</code> <pre><code>def shard_model(\n    model,\n    *,\n    cpu_offload: bool,\n    reshard_after_forward: bool = True,\n    mp_policy: MixedPrecisionPolicy | None = MixedPrecisionPolicy(),  # noqa\n    mesh: DeviceMesh | None = None,\n    fsdp_shard_conditions: list[Callable[[str, nn.Module], bool]] = [],  # noqa\n    pin_cpu_memory: bool = True,\n) -&gt; None:\n    \"\"\"\n    Utility to shard a model with FSDP using the PyTorch Distributed fully_shard API.\n\n    This method will over the model's named modules from the bottom-up and apply shard modules\n    based on whether they meet any of the criteria from shard_conditions.\n\n    Args:\n        model (TransformerDecoder): Model to shard with FSDP.\n        shard_conditions (List[Callable[[str, nn.Module], bool]]): A list of functions to determine\n            which modules to shard with FSDP. Each function should take module name (relative to root)\n            and the module itself, returning True if FSDP should shard the module and False otherwise.\n            If any of shard_conditions return True for a given module, it will be sharded by FSDP.\n        cpu_offload (bool): If set to True, FSDP will offload parameters, gradients, and optimizer\n            states to CPU.\n        reshard_after_forward (bool): Whether to reshard parameters and buffers after\n            the forward pass. Setting this to True corresponds to the FULL_SHARD sharding strategy\n            from FSDP1, while setting it to False corresponds to the SHARD_GRAD_OP sharding strategy.\n        mesh (Optional[DeviceMesh]): Device mesh to use for FSDP sharding under multiple parallelism.\n            Default to None.\n        fsdp_shard_conditions (List[Callable[[str, nn.Module], bool]]): A list of functions to determine\n            which modules to shard with FSDP.\n        pin_cpu_memory (bool): If set to True, FSDP will pin the CPU memory of the offloaded parameters.\n\n    Raises:\n        ValueError: If no layer modules were sharded, indicating that no shard_condition was triggered.\n    \"\"\"\n    if fsdp_shard_conditions is None or len(fsdp_shard_conditions) == 0:\n        logger.warning(\n            \"The FSDP shard condition list is empty or None. No modules will be sharded in %s\",\n            type(model).__name__)\n        return\n\n    fsdp_kwargs = {\n        \"reshard_after_forward\": reshard_after_forward,\n        \"mesh\": mesh,\n        \"mp_policy\": mp_policy,\n    }\n    if cpu_offload:\n        fsdp_kwargs[\"offload_policy\"] = CPUOffloadPolicy(\n            pin_memory=pin_cpu_memory)\n\n    # iterating in reverse to start with\n    # lowest-level modules first\n    num_layers_sharded = 0\n    # TODO(will): don't reshard after forward for the last layer to save on the\n    # all-gather that will immediately happen Shard the model with FSDP,\n    for n, m in reversed(list(model.named_modules())):\n        if any([\n                shard_condition(n, m)\n                for shard_condition in fsdp_shard_conditions\n        ]):\n            fully_shard(m, **fsdp_kwargs)\n            num_layers_sharded += 1\n\n    if num_layers_sharded == 0:\n        raise ValueError(\n            \"No layer modules were sharded. Please check if shard conditions are working as expected.\"\n        )\n\n    # Finally shard the entire model to account for any stragglers\n    fully_shard(model, **fsdp_kwargs)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.models.loader.utils","title":"fastvideo.models.loader.utils","text":"<p>Utilities for selecting and loading models.</p> Functions\u00b6 fastvideo.models.loader.utils.get_param_names_mapping \u00b6 <pre><code>get_param_names_mapping(\n    mapping_dict: dict[str, str]\n) -&gt; Callable[[str], tuple[str, Any, Any]]\n</code></pre> <p>Creates a mapping function that transforms parameter names using regex patterns.</p> <p>Parameters:</p> Name Type Description Default <code>mapping_dict</code> <code>Dict[str, str]</code> <p>Dictionary mapping regex patterns to replacement patterns</p> required <code>param_name</code> <code>str</code> <p>The parameter name to be transformed</p> required <p>Returns:</p> Type Description <code>Callable[[str], tuple[str, Any, Any]]</code> <p>Callable[[str], str]: A function that maps parameter names from source to target format</p> Source code in <code>fastvideo/models/loader/utils.py</code> <pre><code>def get_param_names_mapping(\n        mapping_dict: dict[str, str]) -&gt; Callable[[str], tuple[str, Any, Any]]:\n    \"\"\"\n    Creates a mapping function that transforms parameter names using regex patterns.\n\n    Args:\n        mapping_dict (Dict[str, str]): Dictionary mapping regex patterns to replacement patterns\n        param_name (str): The parameter name to be transformed\n\n    Returns:\n        Callable[[str], str]: A function that maps parameter names from source to target format\n    \"\"\"\n\n    def mapping_fn(name: str) -&gt; tuple[str, Any, Any]:\n        # Try to match and transform the name using the regex patterns in mapping_dict\n        for pattern, replacement in mapping_dict.items():\n            match = re.match(pattern, name)\n            if match:\n                merge_index = None\n                total_splitted_params = None\n                if isinstance(replacement, tuple):\n                    merge_index = replacement[1]\n                    total_splitted_params = replacement[2]\n                    replacement = replacement[0]\n                name = re.sub(pattern, replacement, name)\n                return name, merge_index, total_splitted_params\n\n        # If no pattern matches, return the original name\n        return name, None, None\n\n    return mapping_fn\n</code></pre> fastvideo.models.loader.utils.hf_to_custom_state_dict \u00b6 <pre><code>hf_to_custom_state_dict(\n    hf_param_sd: dict[str, Tensor]\n    | Iterator[tuple[str, Tensor]],\n    param_names_mapping: Callable[\n        [str], tuple[str, Any, Any]\n    ],\n) -&gt; tuple[\n    dict[str, torch.Tensor], dict[str, tuple[str, Any, Any]]\n]\n</code></pre> <p>Converts a Hugging Face parameter state dictionary to a custom parameter state dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>hf_param_sd</code> <code>Dict[str, Tensor]</code> <p>The Hugging Face parameter state dictionary</p> required <code>param_names_mapping</code> <code>Callable[[str], tuple[str, Any, Any]]</code> <p>A function that maps parameter names from source to target format</p> required <p>Returns:</p> Name Type Description <code>custom_param_sd</code> <code>Dict[str, Tensor]</code> <p>The custom formatted parameter state dict</p> <code>reverse_param_names_mapping</code> <code>Dict[str, Tuple[str, Any, Any]]</code> <p>Maps back from custom to hf</p> Source code in <code>fastvideo/models/loader/utils.py</code> <pre><code>def hf_to_custom_state_dict(\n    hf_param_sd: dict[str, torch.Tensor] | Iterator[tuple[str, torch.Tensor]],\n    param_names_mapping: Callable[[str], tuple[str, Any, Any]]\n) -&gt; tuple[dict[str, torch.Tensor], dict[str, tuple[str, Any, Any]]]:\n    \"\"\"\n    Converts a Hugging Face parameter state dictionary to a custom parameter state dictionary.\n\n    Args:\n        hf_param_sd (Dict[str, torch.Tensor]): The Hugging Face parameter state dictionary\n        param_names_mapping (Callable[[str], tuple[str, Any, Any]]): A function that maps parameter names from source to target format\n\n    Returns:\n        custom_param_sd (Dict[str, torch.Tensor]): The custom formatted parameter state dict\n        reverse_param_names_mapping (Dict[str, Tuple[str, Any, Any]]): Maps back from custom to hf\n    \"\"\"\n    custom_param_sd = {}\n    to_merge_params = defaultdict(dict)  # type: ignore\n    reverse_param_names_mapping = {}\n    if isinstance(hf_param_sd, dict):\n        hf_param_sd = hf_param_sd.items()  # type: ignore\n    for source_param_name, full_tensor in hf_param_sd:  # type: ignore\n        target_param_name, merge_index, num_params_to_merge = param_names_mapping(\n            source_param_name)\n        reverse_param_names_mapping[target_param_name] = (source_param_name,\n                                                          merge_index,\n                                                          num_params_to_merge)\n        if merge_index is not None:\n            to_merge_params[target_param_name][merge_index] = full_tensor\n            if len(to_merge_params[target_param_name]) == num_params_to_merge:\n                # cat at output dim according to the merge_index order\n                sorted_tensors = [\n                    to_merge_params[target_param_name][i]\n                    for i in range(num_params_to_merge)\n                ]\n                full_tensor = torch.cat(sorted_tensors, dim=0)\n                del to_merge_params[target_param_name]\n            else:\n                continue\n        custom_param_sd[target_param_name] = full_tensor\n    return custom_param_sd, reverse_param_names_mapping\n</code></pre> fastvideo.models.loader.utils.set_default_torch_dtype \u00b6 <pre><code>set_default_torch_dtype(dtype: dtype)\n</code></pre> <p>Sets the default torch dtype to the given dtype.</p> Source code in <code>fastvideo/models/loader/utils.py</code> <pre><code>@contextlib.contextmanager\ndef set_default_torch_dtype(dtype: torch.dtype):\n    \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n    old_dtype = torch.get_default_dtype()\n    torch.set_default_dtype(dtype)\n    yield\n    torch.set_default_dtype(old_dtype)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.models.loader.weight_utils","title":"fastvideo.models.loader.weight_utils","text":"<p>Utilities for downloading and initializing model weights.</p> Functions\u00b6 fastvideo.models.loader.weight_utils.default_weight_loader \u00b6 <pre><code>default_weight_loader(\n    param: Tensor, loaded_weight: Tensor\n) -&gt; None\n</code></pre> <p>Default weight loader.</p> Source code in <code>fastvideo/models/loader/weight_utils.py</code> <pre><code>def default_weight_loader(param: torch.Tensor,\n                          loaded_weight: torch.Tensor) -&gt; None:\n    \"\"\"Default weight loader.\"\"\"\n    try:\n        if param.numel() == 1 and loaded_weight.numel() == 1:\n            # Sometimes scalar values aren't considered tensors with shapes\n            # so if both param and loaded_weight are a scalar,\n            # \"broadcast\" instead of copy\n            param.data.fill_(loaded_weight.item())\n        else:\n            assert param.size() == loaded_weight.size(), (\n                f\"Attempted to load weight ({loaded_weight.size()}) \"\n                f\"into parameter ({param.size()})\")\n\n            param.data.copy_(loaded_weight)\n    except Exception:\n        # NOTE: This exception is added for the purpose of setting breakpoint to\n        # debug weight loading issues.\n        raise\n</code></pre> fastvideo.models.loader.weight_utils.enable_hf_transfer \u00b6 <pre><code>enable_hf_transfer() -&gt; None\n</code></pre> <p>automatically activates hf_transfer</p> Source code in <code>fastvideo/models/loader/weight_utils.py</code> <pre><code>def enable_hf_transfer() -&gt; None:\n    \"\"\"automatically activates hf_transfer\n    \"\"\"\n    if \"HF_HUB_ENABLE_HF_TRANSFER\" not in os.environ:\n        try:\n            # enable hf hub transfer if available\n            import hf_transfer  # type: ignore # noqa\n            huggingface_hub.constants.HF_HUB_ENABLE_HF_TRANSFER = True\n        except ImportError:\n            pass\n</code></pre> fastvideo.models.loader.weight_utils.filter_files_not_needed_for_inference \u00b6 <pre><code>filter_files_not_needed_for_inference(\n    hf_weights_files: list[str],\n) -&gt; list[str]\n</code></pre> <p>Exclude files that are not needed for inference.</p> <p>See https://github.com/huggingface/transformers/blob/v4.34.0/src/transformers/trainer.py#L227-L233</p> Source code in <code>fastvideo/models/loader/weight_utils.py</code> <pre><code>def filter_files_not_needed_for_inference(\n        hf_weights_files: list[str]) -&gt; list[str]:\n    \"\"\"\n    Exclude files that are not needed for inference.\n\n    See https://github.com/huggingface/transformers/blob/v4.34.0/src/transformers/trainer.py#L227-L233\n    \"\"\"\n    blacklist = [\n        \"training_args.bin\",\n        \"optimizer.bin\",\n        \"optimizer.pt\",\n        \"scheduler.pt\",\n        \"scaler.pt\",\n    ]\n    hf_weights_files = [\n        f for f in hf_weights_files\n        if not any(f.endswith(x) for x in blacklist)\n    ]\n    return hf_weights_files\n</code></pre> fastvideo.models.loader.weight_utils.maybe_remap_kv_scale_name \u00b6 <pre><code>maybe_remap_kv_scale_name(\n    name: str, params_dict: dict\n) -&gt; str | None\n</code></pre> <p>Remap the name of FP8 k/v_scale parameters.</p> <p>This function handles the remapping of FP8 k/v_scale parameter names. It detects if the given name ends with a suffix and attempts to remap it to the expected name format in the model. If the remapped name is not found in the params_dict, a warning is printed and None is returned.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The original loaded checkpoint parameter name.</p> required <code>params_dict</code> <code>dict</code> <p>Dictionary containing the model's named parameters.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str | None</code> <p>The remapped parameter name if successful, or the original name  if no remapping is needed.</p> <code>None</code> <code>str | None</code> <p>If the remapped name is not found in params_dict.</p> Source code in <code>fastvideo/models/loader/weight_utils.py</code> <pre><code>def maybe_remap_kv_scale_name(name: str, params_dict: dict) -&gt; str | None:\n    \"\"\"Remap the name of FP8 k/v_scale parameters.\n\n    This function handles the remapping of FP8 k/v_scale parameter names.\n    It detects if the given name ends with a suffix and attempts to remap\n    it to the expected name format in the model. If the remapped name is not\n    found in the params_dict, a warning is printed and None is returned.\n\n    Args:\n        name (str): The original loaded checkpoint parameter name.\n        params_dict (dict): Dictionary containing the model's named parameters.\n\n    Returns:\n        str: The remapped parameter name if successful, or the original name\n             if no remapping is needed.\n        None: If the remapped name is not found in params_dict.\n    \"\"\"\n    if name.endswith(\".kv_scale\"):\n        logger.warning_once(\n            \"DEPRECATED. Found kv_scale in the checkpoint. \"\n            \"This format is deprecated in favor of separate k_scale and \"\n            \"v_scale tensors and will be removed in a future release. \"\n            \"Functionally, we will remap kv_scale to k_scale and duplicate \"\n            \"k_scale to v_scale\")\n        # NOTE: we remap the deprecated kv_scale to k_scale\n        remapped_name = name.replace(\".kv_scale\", \".attn.k_scale\")\n        if remapped_name not in params_dict:\n            logger.warning_once(\n                f\"Found kv_scale in the checkpoint (e.g. {name}), \"\n                \"but not found the expected name in the model \"\n                f\"(e.g. {remapped_name}). kv_scale is \"\n                \"not loaded.\")\n            return None\n        return remapped_name\n\n    possible_scale_names = [\".k_scale\", \".v_scale\"]\n    modelopt_scale_names = [\n        \".self_attn.k_proj.k_scale\", \".self_attn.v_proj.v_scale\"\n    ]\n    for scale_name in possible_scale_names:\n        if name.endswith(scale_name):\n            if any(mo_scale_name in name\n                   for mo_scale_name in modelopt_scale_names):\n                remapped_name = name.replace(\n                    f\".self_attn.{scale_name[1]}_proj{scale_name}\",\n                    f\".self_attn.attn{scale_name}\")\n            else:\n                remapped_name = name.replace(scale_name, f\".attn{scale_name}\")\n            if remapped_name not in params_dict:\n                logger.warning_once(\n                    f\"Found {scale_name} in the checkpoint (e.g. {name}), \"\n                    \"but not found the expected name in the model \"\n                    f\"(e.g. {remapped_name}). {scale_name} is \"\n                    \"not loaded.\")\n                return None\n            return remapped_name\n\n    # If there were no matches, return the untouched param name\n    return name\n</code></pre> fastvideo.models.loader.weight_utils.pt_weights_iterator \u00b6 <pre><code>pt_weights_iterator(\n    hf_weights_files: list[str], to_cpu: bool = True\n) -&gt; Generator[tuple[str, torch.Tensor], None, None]\n</code></pre> <p>Iterate over the weights in the model bin/pt files.</p> Source code in <code>fastvideo/models/loader/weight_utils.py</code> <pre><code>def pt_weights_iterator(\n    hf_weights_files: list[str],\n    to_cpu: bool = True,\n) -&gt; Generator[tuple[str, torch.Tensor], None, None]:\n    \"\"\"Iterate over the weights in the model bin/pt files.\"\"\"\n    device = \"cpu\" if to_cpu else str(get_local_torch_device())\n    enable_tqdm = not torch.distributed.is_initialized(\n    ) or torch.distributed.get_rank() == 0\n    for bin_file in tqdm(\n            hf_weights_files,\n            desc=\"Loading pt checkpoint shards\",\n            disable=not enable_tqdm,\n            bar_format=_BAR_FORMAT,\n    ):\n        state = torch.load(bin_file, map_location=device, weights_only=True)\n        yield from state.items()\n        del state\n</code></pre> fastvideo.models.loader.weight_utils.safetensors_weights_iterator \u00b6 <pre><code>safetensors_weights_iterator(\n    hf_weights_files: list[str], to_cpu: bool = True\n) -&gt; Generator[tuple[str, torch.Tensor], None, None]\n</code></pre> <p>Iterate over the weights in the model safetensor files.</p> Source code in <code>fastvideo/models/loader/weight_utils.py</code> <pre><code>def safetensors_weights_iterator(\n    hf_weights_files: list[str],\n    to_cpu: bool = True,\n) -&gt; Generator[tuple[str, torch.Tensor], None, None]:\n    \"\"\"Iterate over the weights in the model safetensor files.\"\"\"\n    enable_tqdm = not torch.distributed.is_initialized(\n    ) or torch.distributed.get_rank() == 0\n    device = \"cpu\" if to_cpu else str(get_local_torch_device())\n    for st_file in tqdm(\n            hf_weights_files,\n            desc=\"Loading safetensors checkpoint shards\",\n            disable=not enable_tqdm,\n            bar_format=_BAR_FORMAT,\n    ):\n        with safe_open(st_file, framework=\"pt\", device=device) as f:\n            for name in f.keys():  # noqa: SIM118\n                param = f.get_tensor(name)\n                yield name, param\n</code></pre>"},{"location":"api/fastvideo/#fastvideopipelines","title":"fastvideo.pipelines","text":""},{"location":"api/fastvideo/#fastvideo.pipelines","title":"pipelines","text":"<p>Diffusion pipelines for fastvideo.</p> <p>This package contains diffusion pipelines for generating videos and images.</p>"},{"location":"api/fastvideo/#fastvideo.pipelines-classes","title":"Classes","text":""},{"location":"api/fastvideo/#fastvideo.pipelines.ComposedPipelineBase","title":"fastvideo.pipelines.ComposedPipelineBase","text":"<pre><code>ComposedPipelineBase(\n    model_path: str,\n    fastvideo_args: FastVideoArgs | TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for pipelines composed of multiple stages.</p> <p>This class provides the framework for creating pipelines by composing multiple stages together. Each stage is responsible for a specific part of the diffusion process, and the pipeline orchestrates the execution of these stages.</p> <p>Initialize the pipeline. After init, the pipeline should be ready to use. The pipeline should be stateless and not hold any batch state.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def __init__(self,\n             model_path: str,\n             fastvideo_args: FastVideoArgs | TrainingArgs,\n             required_config_modules: list[str] | None = None,\n             loaded_modules: dict[str, torch.nn.Module] | None = None):\n    \"\"\"\n    Initialize the pipeline. After __init__, the pipeline should be ready to\n    use. The pipeline should be stateless and not hold any batch state.\n    \"\"\"\n    self.fastvideo_args = fastvideo_args\n\n    self.model_path: str = model_path\n    self._stages: list[PipelineStage] = []\n    self._stage_name_mapping: dict[str, PipelineStage] = {}\n\n    if required_config_modules is not None:\n        self._required_config_modules = required_config_modules\n\n    if self._required_config_modules is None:\n        raise NotImplementedError(\n            \"Subclass must set _required_config_modules\")\n\n    maybe_init_distributed_environment_and_model_parallel(\n        fastvideo_args.tp_size, fastvideo_args.sp_size)\n\n    # Torch profiler. Enabled and configured through env vars:\n    # FASTVIDEO_TORCH_PROFILER_DIR=/path/to/save/trace\n    trace_dir = envs.FASTVIDEO_TORCH_PROFILER_DIR\n    self.profiler_controller = get_or_create_profiler(trace_dir)\n    self.profiler = self.profiler_controller.profiler\n\n    self.local_rank = get_world_group().local_rank\n\n    # Load modules directly in initialization\n    logger.info(\"Loading pipeline modules...\")\n    with self.profiler_controller.region(\"profiler_region_model_loading\"):\n        self.modules = self.load_modules(fastvideo_args, loaded_modules)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.ComposedPipelineBase-attributes","title":"Attributes","text":"fastvideo.pipelines.ComposedPipelineBase.required_config_modules <code>property</code> \u00b6 <pre><code>required_config_modules: list[str]\n</code></pre> <p>List of modules that are required by the pipeline. The names should match the diffusers directory and model_index.json file. These modules will be loaded using the PipelineComponentLoader and made available in the modules dictionary. Access these modules using the get_module method.</p> <p>class ConcretePipeline(ComposedPipelineBase):     _required_config_modules = [\"vae\", \"text_encoder\", \"transformer\", \"scheduler\", \"tokenizer\"]</p> <pre><code>@property\ndef required_config_modules(self):\n    return self._required_config_modules\n</code></pre> fastvideo.pipelines.ComposedPipelineBase.stages <code>property</code> \u00b6 <pre><code>stages: list[PipelineStage]\n</code></pre> <p>List of stages in the pipeline.</p>"},{"location":"api/fastvideo/#fastvideo.pipelines.ComposedPipelineBase-functions","title":"Functions","text":"fastvideo.pipelines.ComposedPipelineBase.create_pipeline_stages <code>abstractmethod</code> \u00b6 <pre><code>create_pipeline_stages(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Create the inference pipeline stages.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>@abstractmethod\ndef create_pipeline_stages(self, fastvideo_args: FastVideoArgs):\n    \"\"\"\n    Create the inference pipeline stages.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.pipelines.ComposedPipelineBase.create_training_stages \u00b6 <pre><code>create_training_stages(training_args: TrainingArgs)\n</code></pre> <p>Create the training pipeline stages.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def create_training_stages(self, training_args: TrainingArgs):\n    \"\"\"\n    Create the training pipeline stages.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.pipelines.ComposedPipelineBase.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Generate a video or image using the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The batch to generate from.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:     ForwardBatch: The batch with the generated video or image.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Generate a video or image using the pipeline.\n\n    Args:\n        batch: The batch to generate from.\n        fastvideo_args: The inference arguments.\n    Returns:\n        ForwardBatch: The batch with the generated video or image.\n    \"\"\"\n    if not self.post_init_called:\n        self.post_init()\n\n    # Execute each stage\n    logger.info(\"Running pipeline stages: %s\",\n                self._stage_name_mapping.keys())\n    # logger.info(\"Batch: %s\", batch)\n    for stage in self.stages:\n        batch = stage(batch, fastvideo_args)\n\n    # Return the output\n    return batch\n</code></pre> fastvideo.pipelines.ComposedPipelineBase.from_pretrained <code>classmethod</code> \u00b6 <pre><code>from_pretrained(\n    model_path: str,\n    device: str | None = None,\n    torch_dtype: dtype | None = None,\n    pipeline_config: str | PipelineConfig | None = None,\n    args: Namespace | None = None,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n    **kwargs\n) -&gt; ComposedPipelineBase\n</code></pre> <p>Load a pipeline from a pretrained model. loaded_modules: Optional[Dict[str, torch.nn.Module]] = None, If provided, loaded_modules will be used instead of loading from config/pretrained weights.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>@classmethod\ndef from_pretrained(cls,\n                    model_path: str,\n                    device: str | None = None,\n                    torch_dtype: torch.dtype | None = None,\n                    pipeline_config: str | PipelineConfig | None = None,\n                    args: argparse.Namespace | None = None,\n                    required_config_modules: list[str] | None = None,\n                    loaded_modules: dict[str, torch.nn.Module]\n                    | None = None,\n                    **kwargs) -&gt; \"ComposedPipelineBase\":\n    \"\"\"\n    Load a pipeline from a pretrained model.\n    loaded_modules: Optional[Dict[str, torch.nn.Module]] = None,\n    If provided, loaded_modules will be used instead of loading from config/pretrained weights.\n    \"\"\"\n    if args is None or args.inference_mode:\n\n        kwargs['model_path'] = model_path\n        fastvideo_args = FastVideoArgs.from_kwargs(**kwargs)\n    else:\n        assert args is not None, \"args must be provided for training mode\"\n        fastvideo_args = TrainingArgs.from_cli_args(args)\n        # TODO(will): fix this so that its not so ugly\n        fastvideo_args.model_path = model_path\n        for key, value in kwargs.items():\n            setattr(fastvideo_args, key, value)\n\n        fastvideo_args.dit_cpu_offload = False\n        # we hijack the precision to be the master weight type so that the\n        # model is loaded with the correct precision. Subsequently we will\n        # use FSDP2's MixedPrecisionPolicy to set the precision for the\n        # fwd, bwd, and other operations' precision.\n        assert fastvideo_args.pipeline_config.dit_precision == 'fp32', 'only fp32 is supported for training'\n\n    logger.info(\"fastvideo_args in from_pretrained: %s\", fastvideo_args)\n\n    pipe = cls(model_path,\n               fastvideo_args,\n               required_config_modules=required_config_modules,\n               loaded_modules=loaded_modules)\n    pipe.post_init()\n    return pipe\n</code></pre> fastvideo.pipelines.ComposedPipelineBase.initialize_pipeline \u00b6 <pre><code>initialize_pipeline(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Initialize the pipeline.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def initialize_pipeline(self, fastvideo_args: FastVideoArgs):\n    \"\"\"\n    Initialize the pipeline.\n    \"\"\"\n    return\n</code></pre> fastvideo.pipelines.ComposedPipelineBase.load_modules \u00b6 <pre><code>load_modules(\n    fastvideo_args: FastVideoArgs,\n    loaded_modules: dict[str, Module] | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Load the modules from the config. loaded_modules: Optional[Dict[str, torch.nn.Module]] = None,  If provided, loaded_modules will be used instead of loading from config/pretrained weights.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def load_modules(\n    self,\n    fastvideo_args: FastVideoArgs,\n    loaded_modules: dict[str, torch.nn.Module] | None = None\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Load the modules from the config.\n    loaded_modules: Optional[Dict[str, torch.nn.Module]] = None, \n    If provided, loaded_modules will be used instead of loading from config/pretrained weights.\n    \"\"\"\n\n    model_index = self._load_config(self.model_path)\n    logger.info(\"Loading pipeline modules from config: %s\", model_index)\n\n    # remove keys that are not pipeline modules\n    model_index.pop(\"_class_name\")\n    model_index.pop(\"_diffusers_version\")\n    if \"boundary_ratio\" in model_index and model_index[\n            \"boundary_ratio\"] is not None:\n        logger.info(\n            \"MoE pipeline detected. Adding transformer_2 to self.required_config_modules...\"\n        )\n        self.required_config_modules.append(\"transformer_2\")\n        logger.info(\"MoE pipeline detected. Setting boundary ratio to %s\",\n                    model_index[\"boundary_ratio\"])\n        fastvideo_args.pipeline_config.dit_config.boundary_ratio = model_index[\n            \"boundary_ratio\"]\n\n    model_index.pop(\"boundary_ratio\", None)\n    # used by Wan2.2 ti2v\n    model_index.pop(\"expand_timesteps\", None)\n\n    # some sanity checks\n    assert len(\n        model_index\n    ) &gt; 1, \"model_index.json must contain at least one pipeline module\"\n\n    for module_name in self.required_config_modules:\n        if module_name not in model_index and module_name in self._extra_config_module_map:\n            extra_module_value = self._extra_config_module_map[module_name]\n            logger.warning(\n                \"model_index.json does not contain a %s module, but found {%s: %s} in _extra_config_module_map, adding to model_index.\",\n                module_name, module_name, extra_module_value)\n            if extra_module_value in model_index:\n                logger.info(\"Using module %s for %s\", extra_module_value,\n                            module_name)\n                model_index[module_name] = model_index[extra_module_value]\n                continue\n            else:\n                raise ValueError(\n                    f\"Required module key: {module_name} value: {model_index.get(module_name)} was not found in loaded modules {model_index.keys()}\"\n                )\n\n    # all the component models used by the pipeline\n    required_modules = self.required_config_modules\n    logger.info(\"Loading required modules: %s\", required_modules)\n\n    modules = {}\n    for module_name, (transformers_or_diffusers,\n                      architecture) in model_index.items():\n        if transformers_or_diffusers is None:\n            logger.warning(\n                \"Module %s in model_index.json has null value, removing from required_config_modules\",\n                module_name)\n            if module_name in self.required_config_modules:\n                self.required_config_modules.remove(module_name)\n            continue\n        if module_name not in required_modules:\n            logger.info(\"Skipping module %s\", module_name)\n            continue\n        if loaded_modules is not None and module_name in loaded_modules:\n            logger.info(\"Using module %s already provided\", module_name)\n            modules[module_name] = loaded_modules[module_name]\n            continue\n\n        # we load the module from the extra config module map if it exists\n        if module_name in self._extra_config_module_map:\n            load_module_name = self._extra_config_module_map[module_name]\n        else:\n            load_module_name = module_name\n\n        component_model_path = os.path.join(self.model_path,\n                                            load_module_name)\n        module = PipelineComponentLoader.load_module(\n            module_name=load_module_name,\n            component_model_path=component_model_path,\n            transformers_or_diffusers=transformers_or_diffusers,\n            fastvideo_args=fastvideo_args,\n        )\n        logger.info(\"Loaded module %s from %s\", module_name,\n                    component_model_path)\n\n        if module_name in modules:\n            logger.warning(\"Overwriting module %s\", module_name)\n        modules[module_name] = module\n\n    # Check if all required modules were loaded\n    for module_name in required_modules:\n        if module_name not in modules or modules[module_name] is None:\n            raise ValueError(\n                f\"Required module key: {module_name} value: {modules.get(module_name)} was not found in loaded modules {modules.keys()}\"\n            )\n\n    return modules\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.ForwardBatch","title":"fastvideo.pipelines.ForwardBatch  <code>dataclass</code>","text":"<pre><code>ForwardBatch(\n    data_type: str,\n    generator: Generator | list[Generator] | None = None,\n    image_path: str | None = None,\n    image_embeds: list[Tensor] = list(),\n    pil_image: Tensor | Image | None = None,\n    preprocessed_image: Tensor | None = None,\n    prompt: str | list[str] | None = None,\n    negative_prompt: str | list[str] | None = None,\n    prompt_path: str | None = None,\n    output_path: str = \"outputs/\",\n    output_video_name: str | None = None,\n    video_path: str | None = None,\n    video_latent: Tensor | None = None,\n    prompt_embeds: list[Tensor] = list(),\n    negative_prompt_embeds: list[Tensor] | None = None,\n    prompt_attention_mask: list[Tensor] | None = None,\n    negative_attention_mask: list[Tensor] | None = None,\n    clip_embedding_pos: list[Tensor] | None = None,\n    clip_embedding_neg: list[Tensor] | None = None,\n    max_sequence_length: int | None = None,\n    prompt_template: dict[str, Any] | None = None,\n    do_classifier_free_guidance: bool = False,\n    batch_size: int | None = None,\n    num_videos_per_prompt: int = 1,\n    seed: int | None = None,\n    seeds: list[int] | None = None,\n    is_prompt_processed: bool = False,\n    latents: Tensor | None = None,\n    raw_latent_shape: Tensor | None = None,\n    noise_pred: Tensor | None = None,\n    image_latent: Tensor | None = None,\n    height_latents: list[int] | int | None = None,\n    width_latents: list[int] | int | None = None,\n    num_frames: list[int] | int = 1,\n    num_frames_round_down: bool = False,\n    height: list[int] | int | None = None,\n    width: list[int] | int | None = None,\n    fps: list[int] | int | None = None,\n    timesteps: Tensor | None = None,\n    timestep: Tensor | float | int | None = None,\n    step_index: int | None = None,\n    boundary_ratio: float | None = None,\n    num_inference_steps: int = 50,\n    guidance_scale: float = 1.0,\n    guidance_scale_2: float | None = None,\n    guidance_rescale: float = 0.0,\n    eta: float = 0.0,\n    sigmas: list[float] | None = None,\n    n_tokens: int | None = None,\n    extra_step_kwargs: dict[str, Any] = dict(),\n    modules: dict[str, Any] = dict(),\n    output: Tensor | None = None,\n    return_trajectory_latents: bool = False,\n    return_trajectory_decoded: bool = False,\n    trajectory_timesteps: list[Tensor] | None = None,\n    trajectory_latents: Tensor | None = None,\n    trajectory_decoded: list[Tensor] | None = None,\n    extra: dict[str, Any] = dict(),\n    save_video: bool = True,\n    return_frames: bool = False,\n    enable_teacache: bool = False,\n    teacache_params: TeaCacheParams\n    | WanTeaCacheParams\n    | None = None,\n    STA_param: list | None = None,\n    is_cfg_negative: bool = False,\n    mask_search_final_result_pos: list[list] | None = None,\n    mask_search_final_result_neg: list[list] | None = None,\n    VSA_sparsity: float = 0.0,\n    logging_info: PipelineLoggingInfo = PipelineLoggingInfo(),\n)\n</code></pre> <p>Complete state passed through the pipeline execution.</p> <p>This dataclass contains all information needed during the diffusion pipeline execution, allowing methods to update specific components without needing to manage numerous individual parameters.</p>"},{"location":"api/fastvideo/#fastvideo.pipelines.ForwardBatch-functions","title":"Functions","text":"fastvideo.pipelines.ForwardBatch.__post_init__ \u00b6 <pre><code>__post_init__()\n</code></pre> <p>Initialize dependent fields after dataclass initialization.</p> Source code in <code>fastvideo/pipelines/pipeline_batch_info.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Initialize dependent fields after dataclass initialization.\"\"\"\n\n    # Set do_classifier_free_guidance based on guidance scale and negative prompt\n    if self.guidance_scale &gt; 1.0:\n        self.do_classifier_free_guidance = True\n    if self.negative_prompt_embeds is None:\n        self.negative_prompt_embeds = []\n    if self.guidance_scale_2 is None:\n        self.guidance_scale_2 = self.guidance_scale\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.LoRAPipeline","title":"fastvideo.pipelines.LoRAPipeline","text":"<pre><code>LoRAPipeline(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ComposedPipelineBase</code></p> <p>Pipeline that supports injecting LoRA adapters into the diffusion transformer. TODO: support training.</p> Source code in <code>fastvideo/pipelines/lora_pipeline.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.device = get_local_torch_device()\n    self.exclude_lora_layers = self.modules[\n        \"transformer\"].config.arch_config.exclude_lora_layers\n    self.lora_target_modules = self.fastvideo_args.lora_target_modules\n    self.lora_path = self.fastvideo_args.lora_path\n    self.lora_nickname = self.fastvideo_args.lora_nickname\n    self.training_mode = self.fastvideo_args.training_mode\n    if self.training_mode and getattr(self.fastvideo_args, \"lora_training\",\n                                      False):\n        assert isinstance(self.fastvideo_args, TrainingArgs)\n        if self.fastvideo_args.lora_alpha is None:\n            self.fastvideo_args.lora_alpha = self.fastvideo_args.lora_rank\n        self.lora_rank = self.fastvideo_args.lora_rank  # type: ignore\n        self.lora_alpha = self.fastvideo_args.lora_alpha  # type: ignore\n        logger.info(\"Using LoRA training with rank %d and alpha %d\",\n                    self.lora_rank, self.lora_alpha)\n        if self.lora_target_modules is None:\n            self.lora_target_modules = [\n                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"to_q\", \"to_k\",\n                \"to_v\", \"to_out\", \"to_qkv\"\n            ]\n        self.convert_to_lora_layers()\n    # Inference\n    elif not self.training_mode and self.lora_path is not None:\n        self.convert_to_lora_layers()\n        self.set_lora_adapter(\n            self.lora_nickname,  # type: ignore\n            self.lora_path)  # type: ignore\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.LoRAPipeline-functions","title":"Functions","text":"fastvideo.pipelines.LoRAPipeline.convert_to_lora_layers \u00b6 <pre><code>convert_to_lora_layers() -&gt; None\n</code></pre> <p>Unified method to convert the transformer to a LoRA transformer.</p> Source code in <code>fastvideo/pipelines/lora_pipeline.py</code> <pre><code>def convert_to_lora_layers(self) -&gt; None:\n    \"\"\"\n    Unified method to convert the transformer to a LoRA transformer.\n    \"\"\"\n    if self.lora_initialized:\n        return\n    self.lora_initialized = True\n    converted_count = 0\n    for name, layer in self.modules[\"transformer\"].named_modules():\n        if not self.is_target_layer(name):\n            continue\n\n        excluded = False\n        for exclude_layer in self.exclude_lora_layers:\n            if exclude_layer in name:\n                excluded = True\n                break\n        if excluded:\n            continue\n\n        layer = get_lora_layer(layer,\n                               lora_rank=self.lora_rank,\n                               lora_alpha=self.lora_alpha,\n                               training_mode=self.training_mode)\n        if layer is not None:\n            self.lora_layers[name] = layer\n            replace_submodule(self.modules[\"transformer\"], name, layer)\n            converted_count += 1\n    logger.info(\"Converted %d layers to LoRA layers\", converted_count)\n\n    if \"fake_score_transformer\" in self.modules:\n        for name, layer in self.modules[\n                \"fake_score_transformer\"].named_modules():\n            if not self.is_target_layer(name):\n                continue\n            layer = get_lora_layer(layer,\n                                   lora_rank=self.lora_rank,\n                                   lora_alpha=self.lora_alpha,\n                                   training_mode=self.training_mode)\n            if layer is not None:\n                self.lora_layers_critic[name] = layer\n                replace_submodule(self.modules[\"fake_score_transformer\"],\n                                  name, layer)\n                converted_count += 1\n        logger.info(\n            \"Converted %d layers to LoRA layers in the critic model\",\n            converted_count)\n</code></pre> fastvideo.pipelines.LoRAPipeline.set_lora_adapter \u00b6 <pre><code>set_lora_adapter(\n    lora_nickname: str, lora_path: str | None = None\n)\n</code></pre> <p>Load a LoRA adapter into the pipeline and merge it into the transformer. Args:     lora_nickname: The \"nick name\" of the adapter when referenced in the pipeline.     lora_path: The path to the adapter, either a local path or a Hugging Face repo id.</p> Source code in <code>fastvideo/pipelines/lora_pipeline.py</code> <pre><code>def set_lora_adapter(self,\n                     lora_nickname: str,\n                     lora_path: str | None = None):  # type: ignore\n    \"\"\"\n    Load a LoRA adapter into the pipeline and merge it into the transformer.\n    Args:\n        lora_nickname: The \"nick name\" of the adapter when referenced in the pipeline.\n        lora_path: The path to the adapter, either a local path or a Hugging Face repo id.\n    \"\"\"\n\n    if lora_nickname not in self.lora_adapters and lora_path is None:\n        raise ValueError(\n            f\"Adapter {lora_nickname} not found in the pipeline. Please provide lora_path to load it.\"\n        )\n    if not self.lora_initialized:\n        self.convert_to_lora_layers()\n    adapter_updated = False\n    rank = dist.get_rank()\n    if lora_path is not None and lora_path != self.cur_adapter_path:\n        lora_local_path = maybe_download_lora(lora_path)\n        lora_state_dict = load_file(lora_local_path)\n\n        # Map the hf layer names to our custom layer names\n        param_names_mapping_fn = get_param_names_mapping(\n            self.modules[\"transformer\"].param_names_mapping)\n        lora_param_names_mapping_fn = get_param_names_mapping(\n            self.modules[\"transformer\"].lora_param_names_mapping)\n\n        to_merge_params: defaultdict[Hashable,\n                                     dict[Any, Any]] = defaultdict(dict)\n        for name, weight in lora_state_dict.items():\n            name = name.replace(\"diffusion_model.\", \"\")\n            name = name.replace(\".weight\", \"\")\n            name, _, _ = lora_param_names_mapping_fn(name)\n            target_name, merge_index, num_params_to_merge = param_names_mapping_fn(\n                name)\n            # for (in_dim, r) @ (r, out_dim), we only merge (r, out_dim * n) where n is the number of linear layers to fuse\n            # see param mapping in HunyuanVideoArchConfig\n            if merge_index is not None and \"lora_B\" in name:\n                to_merge_params[target_name][merge_index] = weight\n                if len(to_merge_params[target_name]) == num_params_to_merge:\n                    # cat at output dim according to the merge_index order\n                    sorted_tensors = [\n                        to_merge_params[target_name][i]\n                        for i in range(num_params_to_merge)\n                    ]\n                    weight = torch.cat(sorted_tensors, dim=1)\n                    del to_merge_params[target_name]\n                else:\n                    continue\n\n            if target_name in self.lora_adapters[lora_nickname]:\n                raise ValueError(\n                    f\"Target name {target_name} already exists in lora_adapters[{lora_nickname}]\"\n                )\n            self.lora_adapters[lora_nickname][target_name] = weight.to(\n                self.device)\n        adapter_updated = True\n        self.cur_adapter_path = lora_path\n        logger.info(\"Rank %d: loaded LoRA adapter %s\", rank, lora_path)\n\n    if not adapter_updated and self.cur_adapter_name == lora_nickname:\n        return\n    self.cur_adapter_name = lora_nickname\n\n    # Merge the new adapter\n    adapted_count = 0\n    for name, layer in self.lora_layers.items():\n        lora_A_name = name + \".lora_A\"\n        lora_B_name = name + \".lora_B\"\n        if lora_A_name in self.lora_adapters[lora_nickname]\\\n            and lora_B_name in self.lora_adapters[lora_nickname]:\n            layer.set_lora_weights(\n                self.lora_adapters[lora_nickname][lora_A_name],\n                self.lora_adapters[lora_nickname][lora_B_name],\n                training_mode=self.fastvideo_args.training_mode,\n                lora_path=lora_path)\n            adapted_count += 1\n        else:\n            if rank == 0:\n                logger.warning(\n                    \"LoRA adapter %s does not contain the weights for layer %s. LoRA will not be applied to it.\",\n                    lora_path, name)\n            layer.disable_lora = True\n    logger.info(\"Rank %d: LoRA adapter %s applied to %d layers\", rank,\n                lora_path, adapted_count)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.PipelineWithLoRA","title":"fastvideo.pipelines.PipelineWithLoRA","text":"<pre><code>PipelineWithLoRA(*args, **kwargs)\n</code></pre> <p>               Bases: <code>LoRAPipeline</code>, <code>ComposedPipelineBase</code></p> <p>Type for a pipeline that has both ComposedPipelineBase and LoRAPipeline functionality.</p> Source code in <code>fastvideo/pipelines/lora_pipeline.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.device = get_local_torch_device()\n    self.exclude_lora_layers = self.modules[\n        \"transformer\"].config.arch_config.exclude_lora_layers\n    self.lora_target_modules = self.fastvideo_args.lora_target_modules\n    self.lora_path = self.fastvideo_args.lora_path\n    self.lora_nickname = self.fastvideo_args.lora_nickname\n    self.training_mode = self.fastvideo_args.training_mode\n    if self.training_mode and getattr(self.fastvideo_args, \"lora_training\",\n                                      False):\n        assert isinstance(self.fastvideo_args, TrainingArgs)\n        if self.fastvideo_args.lora_alpha is None:\n            self.fastvideo_args.lora_alpha = self.fastvideo_args.lora_rank\n        self.lora_rank = self.fastvideo_args.lora_rank  # type: ignore\n        self.lora_alpha = self.fastvideo_args.lora_alpha  # type: ignore\n        logger.info(\"Using LoRA training with rank %d and alpha %d\",\n                    self.lora_rank, self.lora_alpha)\n        if self.lora_target_modules is None:\n            self.lora_target_modules = [\n                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"to_q\", \"to_k\",\n                \"to_v\", \"to_out\", \"to_qkv\"\n            ]\n        self.convert_to_lora_layers()\n    # Inference\n    elif not self.training_mode and self.lora_path is not None:\n        self.convert_to_lora_layers()\n        self.set_lora_adapter(\n            self.lora_nickname,  # type: ignore\n            self.lora_path)  # type: ignore\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideo.pipelines.build_pipeline","title":"fastvideo.pipelines.build_pipeline","text":"<pre><code>build_pipeline(\n    fastvideo_args: FastVideoArgs,\n    pipeline_type: PipelineType | str = PipelineType.BASIC,\n) -&gt; PipelineWithLoRA\n</code></pre> <p>Only works with valid hf diffusers configs. (model_index.json) We want to build a pipeline based on the inference args mode_path: 1. download the model from the hub if it's not already downloaded 2. verify the model config and directory 3. based on the config, determine the pipeline class</p> Source code in <code>fastvideo/pipelines/__init__.py</code> <pre><code>def build_pipeline(\n        fastvideo_args: FastVideoArgs,\n        pipeline_type: PipelineType | str = PipelineType.BASIC\n) -&gt; PipelineWithLoRA:\n    \"\"\"\n    Only works with valid hf diffusers configs. (model_index.json)\n    We want to build a pipeline based on the inference args mode_path:\n    1. download the model from the hub if it's not already downloaded\n    2. verify the model config and directory\n    3. based on the config, determine the pipeline class \n    \"\"\"\n    # Get pipeline type\n    model_path = fastvideo_args.model_path\n    model_path = maybe_download_model(model_path)\n    # fastvideo_args.downloaded_model_path = model_path\n    logger.info(\"Model path: %s\", model_path)\n\n    config = verify_model_config_and_directory(model_path)\n    pipeline_name = config.get(\"_class_name\")\n    if pipeline_name is None:\n        raise ValueError(\n            \"Model config does not contain a _class_name attribute. \"\n            \"Only diffusers format is supported.\")\n\n    # Get the appropriate pipeline registry based on pipeline_type\n    logger.info(\n        \"Building pipeline of type: %s\", pipeline_type.value if isinstance(\n            pipeline_type, PipelineType) else pipeline_type)\n    pipeline_registry = get_pipeline_registry(pipeline_type)\n\n    if isinstance(pipeline_type, str):\n        pipeline_type = PipelineType.from_string(pipeline_type)\n\n    pipeline_cls = pipeline_registry.resolve_pipeline_cls(\n        pipeline_name, pipeline_type, fastvideo_args.workload_type)\n\n    # instantiate the pipelines\n    pipeline = pipeline_cls(model_path, fastvideo_args)\n\n    logger.info(\"Pipelines instantiated\")\n\n    return cast(PipelineWithLoRA, pipeline)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines-modules","title":"Modules","text":""},{"location":"api/fastvideo/#fastvideo.pipelines.basic","title":"fastvideo.pipelines.basic","text":"<p>Basic inference pipelines for fastvideo.</p> <p>This package contains basic pipelines for video and image generation.</p>"},{"location":"api/fastvideo/#fastvideo.pipelines.basic-modules","title":"Modules","text":"fastvideo.pipelines.basic.hunyuan \u00b6 Modules\u00b6 fastvideo.pipelines.basic.hunyuan.hunyuan_pipeline \u00b6 <p>Hunyuan video diffusion pipeline implementation.</p> <p>This module contains an implementation of the Hunyuan video diffusion pipeline using the modular pipeline architecture.</p> Classes\u00b6 fastvideo.pipelines.basic.hunyuan.hunyuan_pipeline.HunyuanVideoPipeline \u00b6 <pre><code>HunyuanVideoPipeline(\n    model_path: str,\n    fastvideo_args: FastVideoArgs | TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>ComposedPipelineBase</code></p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def __init__(self,\n             model_path: str,\n             fastvideo_args: FastVideoArgs | TrainingArgs,\n             required_config_modules: list[str] | None = None,\n             loaded_modules: dict[str, torch.nn.Module] | None = None):\n    \"\"\"\n    Initialize the pipeline. After __init__, the pipeline should be ready to\n    use. The pipeline should be stateless and not hold any batch state.\n    \"\"\"\n    self.fastvideo_args = fastvideo_args\n\n    self.model_path: str = model_path\n    self._stages: list[PipelineStage] = []\n    self._stage_name_mapping: dict[str, PipelineStage] = {}\n\n    if required_config_modules is not None:\n        self._required_config_modules = required_config_modules\n\n    if self._required_config_modules is None:\n        raise NotImplementedError(\n            \"Subclass must set _required_config_modules\")\n\n    maybe_init_distributed_environment_and_model_parallel(\n        fastvideo_args.tp_size, fastvideo_args.sp_size)\n\n    # Torch profiler. Enabled and configured through env vars:\n    # FASTVIDEO_TORCH_PROFILER_DIR=/path/to/save/trace\n    trace_dir = envs.FASTVIDEO_TORCH_PROFILER_DIR\n    self.profiler_controller = get_or_create_profiler(trace_dir)\n    self.profiler = self.profiler_controller.profiler\n\n    self.local_rank = get_world_group().local_rank\n\n    # Load modules directly in initialization\n    logger.info(\"Loading pipeline modules...\")\n    with self.profiler_controller.region(\"profiler_region_model_loading\"):\n        self.modules = self.load_modules(fastvideo_args, loaded_modules)\n</code></pre> Functions\u00b6 fastvideo.pipelines.basic.hunyuan.hunyuan_pipeline.HunyuanVideoPipeline.create_pipeline_stages \u00b6 <pre><code>create_pipeline_stages(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Set up pipeline stages with proper dependency injection.</p> Source code in <code>fastvideo/pipelines/basic/hunyuan/hunyuan_pipeline.py</code> <pre><code>def create_pipeline_stages(self, fastvideo_args: FastVideoArgs):\n    \"\"\"Set up pipeline stages with proper dependency injection.\"\"\"\n\n    self.add_stage(stage_name=\"input_validation_stage\",\n                   stage=InputValidationStage())\n\n    self.add_stage(stage_name=\"prompt_encoding_stage_primary\",\n                   stage=TextEncodingStage(\n                       text_encoders=[\n                           self.get_module(\"text_encoder\"),\n                           self.get_module(\"text_encoder_2\")\n                       ],\n                       tokenizers=[\n                           self.get_module(\"tokenizer\"),\n                           self.get_module(\"tokenizer_2\")\n                       ],\n                   ))\n\n    self.add_stage(stage_name=\"conditioning_stage\",\n                   stage=ConditioningStage())\n\n    self.add_stage(stage_name=\"timestep_preparation_stage\",\n                   stage=TimestepPreparationStage(\n                       scheduler=self.get_module(\"scheduler\")))\n\n    self.add_stage(stage_name=\"latent_preparation_stage\",\n                   stage=LatentPreparationStage(\n                       scheduler=self.get_module(\"scheduler\"),\n                       transformer=self.get_module(\"transformer\")))\n\n    self.add_stage(stage_name=\"denoising_stage\",\n                   stage=DenoisingStage(\n                       transformer=self.get_module(\"transformer\"),\n                       scheduler=self.get_module(\"scheduler\")))\n\n    self.add_stage(stage_name=\"decoding_stage\",\n                   stage=DecodingStage(vae=self.get_module(\"vae\")))\n</code></pre> Functions\u00b6 fastvideo.pipelines.basic.stepvideo \u00b6 Modules\u00b6 fastvideo.pipelines.basic.stepvideo.stepvideo_pipeline \u00b6 <p>Hunyuan video diffusion pipeline implementation.</p> <p>This module contains an implementation of the Hunyuan video diffusion pipeline using the modular pipeline architecture.</p> Classes\u00b6 fastvideo.pipelines.basic.stepvideo.stepvideo_pipeline.StepVideoPipeline \u00b6 <pre><code>StepVideoPipeline(*args, **kwargs)\n</code></pre> <p>               Bases: <code>LoRAPipeline</code>, <code>ComposedPipelineBase</code></p> Source code in <code>fastvideo/pipelines/lora_pipeline.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.device = get_local_torch_device()\n    self.exclude_lora_layers = self.modules[\n        \"transformer\"].config.arch_config.exclude_lora_layers\n    self.lora_target_modules = self.fastvideo_args.lora_target_modules\n    self.lora_path = self.fastvideo_args.lora_path\n    self.lora_nickname = self.fastvideo_args.lora_nickname\n    self.training_mode = self.fastvideo_args.training_mode\n    if self.training_mode and getattr(self.fastvideo_args, \"lora_training\",\n                                      False):\n        assert isinstance(self.fastvideo_args, TrainingArgs)\n        if self.fastvideo_args.lora_alpha is None:\n            self.fastvideo_args.lora_alpha = self.fastvideo_args.lora_rank\n        self.lora_rank = self.fastvideo_args.lora_rank  # type: ignore\n        self.lora_alpha = self.fastvideo_args.lora_alpha  # type: ignore\n        logger.info(\"Using LoRA training with rank %d and alpha %d\",\n                    self.lora_rank, self.lora_alpha)\n        if self.lora_target_modules is None:\n            self.lora_target_modules = [\n                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"to_q\", \"to_k\",\n                \"to_v\", \"to_out\", \"to_qkv\"\n            ]\n        self.convert_to_lora_layers()\n    # Inference\n    elif not self.training_mode and self.lora_path is not None:\n        self.convert_to_lora_layers()\n        self.set_lora_adapter(\n            self.lora_nickname,  # type: ignore\n            self.lora_path)  # type: ignore\n</code></pre> Functions\u00b6 fastvideo.pipelines.basic.stepvideo.stepvideo_pipeline.StepVideoPipeline.create_pipeline_stages \u00b6 <pre><code>create_pipeline_stages(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Set up pipeline stages with proper dependency injection.</p> Source code in <code>fastvideo/pipelines/basic/stepvideo/stepvideo_pipeline.py</code> <pre><code>def create_pipeline_stages(self, fastvideo_args: FastVideoArgs):\n    \"\"\"Set up pipeline stages with proper dependency injection.\"\"\"\n\n    self.add_stage(stage_name=\"input_validation_stage\",\n                   stage=InputValidationStage())\n\n    self.add_stage(stage_name=\"prompt_encoding_stage\",\n                   stage=StepvideoPromptEncodingStage(\n                       stepllm=self.get_module(\"text_encoder\"),\n                       clip=self.get_module(\"text_encoder_2\"),\n                   ))\n\n    self.add_stage(stage_name=\"timestep_preparation_stage\",\n                   stage=TimestepPreparationStage(\n                       scheduler=self.get_module(\"scheduler\")))\n\n    self.add_stage(stage_name=\"latent_preparation_stage\",\n                   stage=LatentPreparationStage(\n                       scheduler=self.get_module(\"scheduler\"),\n                       transformer=self.get_module(\"transformer\"),\n                   ))\n\n    self.add_stage(stage_name=\"denoising_stage\",\n                   stage=DenoisingStage(\n                       transformer=self.get_module(\"transformer\"),\n                       scheduler=self.get_module(\"scheduler\")))\n\n    self.add_stage(stage_name=\"decoding_stage\",\n                   stage=DecodingStage(vae=self.get_module(\"vae\")))\n</code></pre> fastvideo.pipelines.basic.stepvideo.stepvideo_pipeline.StepVideoPipeline.initialize_pipeline \u00b6 <pre><code>initialize_pipeline(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Initialize the pipeline.</p> Source code in <code>fastvideo/pipelines/basic/stepvideo/stepvideo_pipeline.py</code> <pre><code>def initialize_pipeline(self, fastvideo_args: FastVideoArgs):\n    \"\"\"\n    Initialize the pipeline.\n    \"\"\"\n    target_device = get_local_torch_device()\n    llm_dir = os.path.join(self.model_path, \"step_llm\")\n    clip_dir = os.path.join(self.model_path, \"hunyuan_clip\")\n    text_enc = self.build_llm(llm_dir, target_device)\n    clip_enc = self.build_clip(clip_dir, target_device)\n    self.add_module(\"text_encoder\", text_enc)\n    self.add_module(\"text_encoder_2\", clip_enc)\n    lib_path = (\n        os.path.join(\n            fastvideo_args.model_path,\n            'lib/liboptimus_ths-torch2.5-cu124.cpython-310-x86_64-linux-gnu.so'\n        ) if os.path.isdir(fastvideo_args.model_path)  # local checkout\n        else hf_hub_download(\n            repo_id=fastvideo_args.model_path,\n            filename=\n            'lib/liboptimus_ths-torch2.5-cu124.cpython-310-x86_64-linux-gnu.so'\n        ))\n    torch.ops.load_library(lib_path)\n</code></pre> fastvideo.pipelines.basic.stepvideo.stepvideo_pipeline.StepVideoPipeline.load_modules \u00b6 <pre><code>load_modules(\n    fastvideo_args: FastVideoArgs,\n) -&gt; dict[str, Any]\n</code></pre> <p>Load the modules from the config.</p> Source code in <code>fastvideo/pipelines/basic/stepvideo/stepvideo_pipeline.py</code> <pre><code>def load_modules(self, fastvideo_args: FastVideoArgs) -&gt; dict[str, Any]:\n    \"\"\"\n    Load the modules from the config.\n    \"\"\"\n    model_index = self._load_config(self.model_path)\n    logger.info(\"Loading pipeline modules from config: %s\", model_index)\n\n    # remove keys that are not pipeline modules\n    model_index.pop(\"_class_name\")\n    model_index.pop(\"_diffusers_version\")\n\n    # some sanity checks\n    assert len(\n        model_index\n    ) &gt; 1, \"model_index.json must contain at least one pipeline module\"\n\n    required_modules = [\"transformer\", \"scheduler\", \"vae\"]\n    for module_name in required_modules:\n        if module_name not in model_index:\n            raise ValueError(\n                f\"model_index.json must contain a {module_name} module\")\n    logger.info(\"Diffusers config passed sanity checks\")\n\n    # all the component models used by the pipeline\n    modules = {}\n    for module_name, (transformers_or_diffusers,\n                      architecture) in model_index.items():\n        component_model_path = os.path.join(self.model_path, module_name)\n        module = PipelineComponentLoader.load_module(\n            module_name=module_name,\n            component_model_path=component_model_path,\n            transformers_or_diffusers=transformers_or_diffusers,\n            fastvideo_args=fastvideo_args,\n        )\n        logger.info(\"Loaded module %s from %s\", module_name,\n                    component_model_path)\n\n        if module_name in modules:\n            logger.warning(\"Overwriting module %s\", module_name)\n        modules[module_name] = module\n\n    required_modules = self.required_config_modules\n    # Check if all required modules were loaded\n    for module_name in required_modules:\n        if module_name not in modules or modules[module_name] is None:\n            raise ValueError(\n                f\"Required module {module_name} was not loaded properly\")\n\n    return modules\n</code></pre> Functions\u00b6 fastvideo.pipelines.basic.wan \u00b6 Modules\u00b6 fastvideo.pipelines.basic.wan.wan_causal_dmd_pipeline \u00b6 <p>Wan causal DMD pipeline implementation.</p> <p>This module wires the causal DMD denoising stage into the modular pipeline.</p> Classes\u00b6 fastvideo.pipelines.basic.wan.wan_causal_dmd_pipeline.WanCausalDMDPipeline \u00b6 <pre><code>WanCausalDMDPipeline(*args, **kwargs)\n</code></pre> <p>               Bases: <code>LoRAPipeline</code>, <code>ComposedPipelineBase</code></p> Source code in <code>fastvideo/pipelines/lora_pipeline.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.device = get_local_torch_device()\n    self.exclude_lora_layers = self.modules[\n        \"transformer\"].config.arch_config.exclude_lora_layers\n    self.lora_target_modules = self.fastvideo_args.lora_target_modules\n    self.lora_path = self.fastvideo_args.lora_path\n    self.lora_nickname = self.fastvideo_args.lora_nickname\n    self.training_mode = self.fastvideo_args.training_mode\n    if self.training_mode and getattr(self.fastvideo_args, \"lora_training\",\n                                      False):\n        assert isinstance(self.fastvideo_args, TrainingArgs)\n        if self.fastvideo_args.lora_alpha is None:\n            self.fastvideo_args.lora_alpha = self.fastvideo_args.lora_rank\n        self.lora_rank = self.fastvideo_args.lora_rank  # type: ignore\n        self.lora_alpha = self.fastvideo_args.lora_alpha  # type: ignore\n        logger.info(\"Using LoRA training with rank %d and alpha %d\",\n                    self.lora_rank, self.lora_alpha)\n        if self.lora_target_modules is None:\n            self.lora_target_modules = [\n                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"to_q\", \"to_k\",\n                \"to_v\", \"to_out\", \"to_qkv\"\n            ]\n        self.convert_to_lora_layers()\n    # Inference\n    elif not self.training_mode and self.lora_path is not None:\n        self.convert_to_lora_layers()\n        self.set_lora_adapter(\n            self.lora_nickname,  # type: ignore\n            self.lora_path)  # type: ignore\n</code></pre> Functions\u00b6 fastvideo.pipelines.basic.wan.wan_causal_dmd_pipeline.WanCausalDMDPipeline.create_pipeline_stages \u00b6 <pre><code>create_pipeline_stages(\n    fastvideo_args: FastVideoArgs,\n) -&gt; None\n</code></pre> <p>Set up pipeline stages with proper dependency injection.</p> Source code in <code>fastvideo/pipelines/basic/wan/wan_causal_dmd_pipeline.py</code> <pre><code>def create_pipeline_stages(self, fastvideo_args: FastVideoArgs) -&gt; None:\n    \"\"\"Set up pipeline stages with proper dependency injection.\"\"\"\n\n    self.add_stage(stage_name=\"input_validation_stage\",\n                   stage=InputValidationStage())\n\n    self.add_stage(stage_name=\"prompt_encoding_stage\",\n                   stage=TextEncodingStage(\n                       text_encoders=[self.get_module(\"text_encoder\")],\n                       tokenizers=[self.get_module(\"tokenizer\")],\n                   ))\n\n    self.add_stage(stage_name=\"conditioning_stage\",\n                   stage=ConditioningStage())\n\n    self.add_stage(stage_name=\"latent_preparation_stage\",\n                   stage=LatentPreparationStage(\n                       scheduler=self.get_module(\"scheduler\"),\n                       transformer=self.get_module(\"transformer\", None)))\n\n    self.add_stage(stage_name=\"denoising_stage\",\n                   stage=CausalDMDDenosingStage(\n                       transformer=self.get_module(\"transformer\"),\n                       transformer_2=self.get_module(\"transformer_2\", None),\n                       scheduler=self.get_module(\"scheduler\")))\n\n    self.add_stage(stage_name=\"decoding_stage\",\n                   stage=DecodingStage(vae=self.get_module(\"vae\")))\n</code></pre> Functions\u00b6 fastvideo.pipelines.basic.wan.wan_dmd_pipeline \u00b6 <p>Wan video diffusion pipeline implementation.</p> <p>This module contains an implementation of the Wan video diffusion pipeline using the modular pipeline architecture.</p> Classes\u00b6 fastvideo.pipelines.basic.wan.wan_dmd_pipeline.WanDMDPipeline \u00b6 <pre><code>WanDMDPipeline(*args, **kwargs)\n</code></pre> <p>               Bases: <code>LoRAPipeline</code>, <code>ComposedPipelineBase</code></p> <p>Wan video diffusion pipeline with LoRA support.</p> Source code in <code>fastvideo/pipelines/lora_pipeline.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.device = get_local_torch_device()\n    self.exclude_lora_layers = self.modules[\n        \"transformer\"].config.arch_config.exclude_lora_layers\n    self.lora_target_modules = self.fastvideo_args.lora_target_modules\n    self.lora_path = self.fastvideo_args.lora_path\n    self.lora_nickname = self.fastvideo_args.lora_nickname\n    self.training_mode = self.fastvideo_args.training_mode\n    if self.training_mode and getattr(self.fastvideo_args, \"lora_training\",\n                                      False):\n        assert isinstance(self.fastvideo_args, TrainingArgs)\n        if self.fastvideo_args.lora_alpha is None:\n            self.fastvideo_args.lora_alpha = self.fastvideo_args.lora_rank\n        self.lora_rank = self.fastvideo_args.lora_rank  # type: ignore\n        self.lora_alpha = self.fastvideo_args.lora_alpha  # type: ignore\n        logger.info(\"Using LoRA training with rank %d and alpha %d\",\n                    self.lora_rank, self.lora_alpha)\n        if self.lora_target_modules is None:\n            self.lora_target_modules = [\n                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"to_q\", \"to_k\",\n                \"to_v\", \"to_out\", \"to_qkv\"\n            ]\n        self.convert_to_lora_layers()\n    # Inference\n    elif not self.training_mode and self.lora_path is not None:\n        self.convert_to_lora_layers()\n        self.set_lora_adapter(\n            self.lora_nickname,  # type: ignore\n            self.lora_path)  # type: ignore\n</code></pre> Functions\u00b6 fastvideo.pipelines.basic.wan.wan_dmd_pipeline.WanDMDPipeline.create_pipeline_stages \u00b6 <pre><code>create_pipeline_stages(\n    fastvideo_args: FastVideoArgs,\n) -&gt; None\n</code></pre> <p>Set up pipeline stages with proper dependency injection.</p> Source code in <code>fastvideo/pipelines/basic/wan/wan_dmd_pipeline.py</code> <pre><code>def create_pipeline_stages(self, fastvideo_args: FastVideoArgs) -&gt; None:\n    \"\"\"Set up pipeline stages with proper dependency injection.\"\"\"\n\n    self.add_stage(stage_name=\"input_validation_stage\",\n                   stage=InputValidationStage())\n\n    self.add_stage(stage_name=\"prompt_encoding_stage\",\n                   stage=TextEncodingStage(\n                       text_encoders=[self.get_module(\"text_encoder\")],\n                       tokenizers=[self.get_module(\"tokenizer\")],\n                   ))\n\n    self.add_stage(stage_name=\"conditioning_stage\",\n                   stage=ConditioningStage())\n\n    self.add_stage(stage_name=\"timestep_preparation_stage\",\n                   stage=TimestepPreparationStage(\n                       scheduler=self.get_module(\"scheduler\")))\n\n    self.add_stage(stage_name=\"latent_preparation_stage\",\n                   stage=LatentPreparationStage(\n                       scheduler=self.get_module(\"scheduler\"),\n                       transformer=self.get_module(\"transformer\", None)))\n\n    self.add_stage(stage_name=\"denoising_stage\",\n                   stage=DmdDenoisingStage(\n                       transformer=self.get_module(\"transformer\"),\n                       scheduler=self.get_module(\"scheduler\")))\n\n    self.add_stage(stage_name=\"decoding_stage\",\n                   stage=DecodingStage(vae=self.get_module(\"vae\")))\n</code></pre> Functions\u00b6 fastvideo.pipelines.basic.wan.wan_i2v_dmd_pipeline \u00b6 <p>Wan video diffusion pipeline implementation.</p> <p>This module contains an implementation of the Wan video diffusion pipeline using the modular pipeline architecture.</p> Classes\u00b6 fastvideo.pipelines.basic.wan.wan_i2v_dmd_pipeline.WanImageToVideoDmdPipeline \u00b6 <pre><code>WanImageToVideoDmdPipeline(*args, **kwargs)\n</code></pre> <p>               Bases: <code>LoRAPipeline</code>, <code>ComposedPipelineBase</code></p> Source code in <code>fastvideo/pipelines/lora_pipeline.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.device = get_local_torch_device()\n    self.exclude_lora_layers = self.modules[\n        \"transformer\"].config.arch_config.exclude_lora_layers\n    self.lora_target_modules = self.fastvideo_args.lora_target_modules\n    self.lora_path = self.fastvideo_args.lora_path\n    self.lora_nickname = self.fastvideo_args.lora_nickname\n    self.training_mode = self.fastvideo_args.training_mode\n    if self.training_mode and getattr(self.fastvideo_args, \"lora_training\",\n                                      False):\n        assert isinstance(self.fastvideo_args, TrainingArgs)\n        if self.fastvideo_args.lora_alpha is None:\n            self.fastvideo_args.lora_alpha = self.fastvideo_args.lora_rank\n        self.lora_rank = self.fastvideo_args.lora_rank  # type: ignore\n        self.lora_alpha = self.fastvideo_args.lora_alpha  # type: ignore\n        logger.info(\"Using LoRA training with rank %d and alpha %d\",\n                    self.lora_rank, self.lora_alpha)\n        if self.lora_target_modules is None:\n            self.lora_target_modules = [\n                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"to_q\", \"to_k\",\n                \"to_v\", \"to_out\", \"to_qkv\"\n            ]\n        self.convert_to_lora_layers()\n    # Inference\n    elif not self.training_mode and self.lora_path is not None:\n        self.convert_to_lora_layers()\n        self.set_lora_adapter(\n            self.lora_nickname,  # type: ignore\n            self.lora_path)  # type: ignore\n</code></pre> Functions\u00b6 fastvideo.pipelines.basic.wan.wan_i2v_dmd_pipeline.WanImageToVideoDmdPipeline.create_pipeline_stages \u00b6 <pre><code>create_pipeline_stages(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Set up pipeline stages with proper dependency injection.</p> Source code in <code>fastvideo/pipelines/basic/wan/wan_i2v_dmd_pipeline.py</code> <pre><code>def create_pipeline_stages(self, fastvideo_args: FastVideoArgs):\n    \"\"\"Set up pipeline stages with proper dependency injection.\"\"\"\n\n    self.add_stage(stage_name=\"input_validation_stage\",\n                   stage=InputValidationStage())\n\n    self.add_stage(stage_name=\"prompt_encoding_stage\",\n                   stage=TextEncodingStage(\n                       text_encoders=[self.get_module(\"text_encoder\")],\n                       tokenizers=[self.get_module(\"tokenizer\")],\n                   ))\n\n    self.add_stage(stage_name=\"image_encoding_stage\",\n                   stage=ImageEncodingStage(\n                       image_encoder=self.get_module(\"image_encoder\"),\n                       image_processor=self.get_module(\"image_processor\"),\n                   ))\n\n    self.add_stage(stage_name=\"conditioning_stage\",\n                   stage=ConditioningStage())\n\n    self.add_stage(stage_name=\"timestep_preparation_stage\",\n                   stage=TimestepPreparationStage(\n                       scheduler=self.get_module(\"scheduler\")))\n\n    self.add_stage(stage_name=\"latent_preparation_stage\",\n                   stage=LatentPreparationStage(\n                       scheduler=self.get_module(\"scheduler\"),\n                       transformer=self.get_module(\"transformer\")))\n\n    self.add_stage(stage_name=\"image_latent_preparation_stage\",\n                   stage=ImageVAEEncodingStage(vae=self.get_module(\"vae\")))\n\n    self.add_stage(stage_name=\"denoising_stage\",\n                   stage=DmdDenoisingStage(\n                       transformer=self.get_module(\"transformer\"),\n                       scheduler=self.get_module(\"scheduler\")))\n\n    self.add_stage(stage_name=\"decoding_stage\",\n                   stage=DecodingStage(vae=self.get_module(\"vae\")))\n</code></pre> Functions\u00b6 fastvideo.pipelines.basic.wan.wan_i2v_pipeline \u00b6 <p>Wan video diffusion pipeline implementation.</p> <p>This module contains an implementation of the Wan video diffusion pipeline using the modular pipeline architecture.</p> Classes\u00b6 fastvideo.pipelines.basic.wan.wan_i2v_pipeline.WanImageToVideoPipeline \u00b6 <pre><code>WanImageToVideoPipeline(*args, **kwargs)\n</code></pre> <p>               Bases: <code>LoRAPipeline</code>, <code>ComposedPipelineBase</code></p> Source code in <code>fastvideo/pipelines/lora_pipeline.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.device = get_local_torch_device()\n    self.exclude_lora_layers = self.modules[\n        \"transformer\"].config.arch_config.exclude_lora_layers\n    self.lora_target_modules = self.fastvideo_args.lora_target_modules\n    self.lora_path = self.fastvideo_args.lora_path\n    self.lora_nickname = self.fastvideo_args.lora_nickname\n    self.training_mode = self.fastvideo_args.training_mode\n    if self.training_mode and getattr(self.fastvideo_args, \"lora_training\",\n                                      False):\n        assert isinstance(self.fastvideo_args, TrainingArgs)\n        if self.fastvideo_args.lora_alpha is None:\n            self.fastvideo_args.lora_alpha = self.fastvideo_args.lora_rank\n        self.lora_rank = self.fastvideo_args.lora_rank  # type: ignore\n        self.lora_alpha = self.fastvideo_args.lora_alpha  # type: ignore\n        logger.info(\"Using LoRA training with rank %d and alpha %d\",\n                    self.lora_rank, self.lora_alpha)\n        if self.lora_target_modules is None:\n            self.lora_target_modules = [\n                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"to_q\", \"to_k\",\n                \"to_v\", \"to_out\", \"to_qkv\"\n            ]\n        self.convert_to_lora_layers()\n    # Inference\n    elif not self.training_mode and self.lora_path is not None:\n        self.convert_to_lora_layers()\n        self.set_lora_adapter(\n            self.lora_nickname,  # type: ignore\n            self.lora_path)  # type: ignore\n</code></pre> Functions\u00b6 fastvideo.pipelines.basic.wan.wan_i2v_pipeline.WanImageToVideoPipeline.create_pipeline_stages \u00b6 <pre><code>create_pipeline_stages(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Set up pipeline stages with proper dependency injection.</p> Source code in <code>fastvideo/pipelines/basic/wan/wan_i2v_pipeline.py</code> <pre><code>def create_pipeline_stages(self, fastvideo_args: FastVideoArgs):\n    \"\"\"Set up pipeline stages with proper dependency injection.\"\"\"\n\n    self.add_stage(stage_name=\"input_validation_stage\",\n                   stage=InputValidationStage())\n\n    self.add_stage(stage_name=\"prompt_encoding_stage\",\n                   stage=TextEncodingStage(\n                       text_encoders=[self.get_module(\"text_encoder\")],\n                       tokenizers=[self.get_module(\"tokenizer\")],\n                   ))\n\n    if (self.get_module(\"image_encoder\") is not None\n            and self.get_module(\"image_processor\") is not None):\n        self.add_stage(\n            stage_name=\"image_encoding_stage\",\n            stage=ImageEncodingStage(\n                image_encoder=self.get_module(\"image_encoder\"),\n                image_processor=self.get_module(\"image_processor\"),\n            ))\n\n    self.add_stage(stage_name=\"conditioning_stage\",\n                   stage=ConditioningStage())\n\n    self.add_stage(stage_name=\"timestep_preparation_stage\",\n                   stage=TimestepPreparationStage(\n                       scheduler=self.get_module(\"scheduler\")))\n\n    self.add_stage(stage_name=\"latent_preparation_stage\",\n                   stage=LatentPreparationStage(\n                       scheduler=self.get_module(\"scheduler\"),\n                       transformer=self.get_module(\"transformer\")))\n\n    self.add_stage(stage_name=\"image_latent_preparation_stage\",\n                   stage=ImageVAEEncodingStage(vae=self.get_module(\"vae\")))\n\n    self.add_stage(stage_name=\"denoising_stage\",\n                   stage=DenoisingStage(\n                       transformer=self.get_module(\"transformer\"),\n                       transformer_2=self.get_module(\"transformer_2\"),\n                       scheduler=self.get_module(\"scheduler\")))\n\n    self.add_stage(stage_name=\"decoding_stage\",\n                   stage=DecodingStage(vae=self.get_module(\"vae\")))\n</code></pre> Functions\u00b6 fastvideo.pipelines.basic.wan.wan_pipeline \u00b6 <p>Wan video diffusion pipeline implementation.</p> <p>This module contains an implementation of the Wan video diffusion pipeline using the modular pipeline architecture.</p> Classes\u00b6 fastvideo.pipelines.basic.wan.wan_pipeline.WanPipeline \u00b6 <pre><code>WanPipeline(*args, **kwargs)\n</code></pre> <p>               Bases: <code>LoRAPipeline</code>, <code>ComposedPipelineBase</code></p> <p>Wan video diffusion pipeline with LoRA support.</p> Source code in <code>fastvideo/pipelines/lora_pipeline.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.device = get_local_torch_device()\n    self.exclude_lora_layers = self.modules[\n        \"transformer\"].config.arch_config.exclude_lora_layers\n    self.lora_target_modules = self.fastvideo_args.lora_target_modules\n    self.lora_path = self.fastvideo_args.lora_path\n    self.lora_nickname = self.fastvideo_args.lora_nickname\n    self.training_mode = self.fastvideo_args.training_mode\n    if self.training_mode and getattr(self.fastvideo_args, \"lora_training\",\n                                      False):\n        assert isinstance(self.fastvideo_args, TrainingArgs)\n        if self.fastvideo_args.lora_alpha is None:\n            self.fastvideo_args.lora_alpha = self.fastvideo_args.lora_rank\n        self.lora_rank = self.fastvideo_args.lora_rank  # type: ignore\n        self.lora_alpha = self.fastvideo_args.lora_alpha  # type: ignore\n        logger.info(\"Using LoRA training with rank %d and alpha %d\",\n                    self.lora_rank, self.lora_alpha)\n        if self.lora_target_modules is None:\n            self.lora_target_modules = [\n                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"to_q\", \"to_k\",\n                \"to_v\", \"to_out\", \"to_qkv\"\n            ]\n        self.convert_to_lora_layers()\n    # Inference\n    elif not self.training_mode and self.lora_path is not None:\n        self.convert_to_lora_layers()\n        self.set_lora_adapter(\n            self.lora_nickname,  # type: ignore\n            self.lora_path)  # type: ignore\n</code></pre> Functions\u00b6 fastvideo.pipelines.basic.wan.wan_pipeline.WanPipeline.create_pipeline_stages \u00b6 <pre><code>create_pipeline_stages(\n    fastvideo_args: FastVideoArgs,\n) -&gt; None\n</code></pre> <p>Set up pipeline stages with proper dependency injection.</p> Source code in <code>fastvideo/pipelines/basic/wan/wan_pipeline.py</code> <pre><code>def create_pipeline_stages(self, fastvideo_args: FastVideoArgs) -&gt; None:\n    \"\"\"Set up pipeline stages with proper dependency injection.\"\"\"\n\n    self.add_stage(stage_name=\"input_validation_stage\",\n                   stage=InputValidationStage())\n\n    self.add_stage(stage_name=\"prompt_encoding_stage\",\n                   stage=TextEncodingStage(\n                       text_encoders=[self.get_module(\"text_encoder\")],\n                       tokenizers=[self.get_module(\"tokenizer\")],\n                   ))\n\n    self.add_stage(stage_name=\"conditioning_stage\",\n                   stage=ConditioningStage())\n\n    self.add_stage(stage_name=\"timestep_preparation_stage\",\n                   stage=TimestepPreparationStage(\n                       scheduler=self.get_module(\"scheduler\")))\n\n    self.add_stage(stage_name=\"latent_preparation_stage\",\n                   stage=LatentPreparationStage(\n                       scheduler=self.get_module(\"scheduler\"),\n                       transformer=self.get_module(\"transformer\", None)))\n\n    self.add_stage(stage_name=\"denoising_stage\",\n                   stage=DenoisingStage(\n                       transformer=self.get_module(\"transformer\"),\n                       transformer_2=self.get_module(\"transformer_2\", None),\n                       scheduler=self.get_module(\"scheduler\"),\n                       vae=self.get_module(\"vae\"),\n                       pipeline=self))\n\n    self.add_stage(stage_name=\"decoding_stage\",\n                   stage=DecodingStage(vae=self.get_module(\"vae\"),\n                                       pipeline=self))\n</code></pre> Functions\u00b6 fastvideo.pipelines.basic.wan.wan_v2v_pipeline \u00b6 <p>Wan video-to-video diffusion pipeline implementation.</p> <p>This module contains an implementation of the Wan video-to-video diffusion pipeline using the modular pipeline architecture.</p> Classes\u00b6 fastvideo.pipelines.basic.wan.wan_v2v_pipeline.WanVideoToVideoPipeline \u00b6 <pre><code>WanVideoToVideoPipeline(*args, **kwargs)\n</code></pre> <p>               Bases: <code>LoRAPipeline</code>, <code>ComposedPipelineBase</code></p> Source code in <code>fastvideo/pipelines/lora_pipeline.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.device = get_local_torch_device()\n    self.exclude_lora_layers = self.modules[\n        \"transformer\"].config.arch_config.exclude_lora_layers\n    self.lora_target_modules = self.fastvideo_args.lora_target_modules\n    self.lora_path = self.fastvideo_args.lora_path\n    self.lora_nickname = self.fastvideo_args.lora_nickname\n    self.training_mode = self.fastvideo_args.training_mode\n    if self.training_mode and getattr(self.fastvideo_args, \"lora_training\",\n                                      False):\n        assert isinstance(self.fastvideo_args, TrainingArgs)\n        if self.fastvideo_args.lora_alpha is None:\n            self.fastvideo_args.lora_alpha = self.fastvideo_args.lora_rank\n        self.lora_rank = self.fastvideo_args.lora_rank  # type: ignore\n        self.lora_alpha = self.fastvideo_args.lora_alpha  # type: ignore\n        logger.info(\"Using LoRA training with rank %d and alpha %d\",\n                    self.lora_rank, self.lora_alpha)\n        if self.lora_target_modules is None:\n            self.lora_target_modules = [\n                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"to_q\", \"to_k\",\n                \"to_v\", \"to_out\", \"to_qkv\"\n            ]\n        self.convert_to_lora_layers()\n    # Inference\n    elif not self.training_mode and self.lora_path is not None:\n        self.convert_to_lora_layers()\n        self.set_lora_adapter(\n            self.lora_nickname,  # type: ignore\n            self.lora_path)  # type: ignore\n</code></pre> Functions\u00b6 fastvideo.pipelines.basic.wan.wan_v2v_pipeline.WanVideoToVideoPipeline.create_pipeline_stages \u00b6 <pre><code>create_pipeline_stages(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Set up pipeline stages with proper dependency injection.</p> Source code in <code>fastvideo/pipelines/basic/wan/wan_v2v_pipeline.py</code> <pre><code>def create_pipeline_stages(self, fastvideo_args: FastVideoArgs):\n    \"\"\"Set up pipeline stages with proper dependency injection.\"\"\"\n\n    self.add_stage(stage_name=\"input_validation_stage\",\n                   stage=InputValidationStage())\n\n    self.add_stage(stage_name=\"prompt_encoding_stage\",\n                   stage=TextEncodingStage(\n                       text_encoders=[self.get_module(\"text_encoder\")],\n                       tokenizers=[self.get_module(\"tokenizer\")],\n                   ))\n\n    if (self.get_module(\"image_encoder\") is not None\n            and self.get_module(\"image_processor\") is not None):\n        self.add_stage(\n            stage_name=\"ref_image_encoding_stage\",\n            stage=RefImageEncodingStage(\n                image_encoder=self.get_module(\"image_encoder\"),\n                image_processor=self.get_module(\"image_processor\"),\n            ))\n\n    self.add_stage(stage_name=\"conditioning_stage\",\n                   stage=ConditioningStage())\n\n    self.add_stage(stage_name=\"timestep_preparation_stage\",\n                   stage=TimestepPreparationStage(\n                       scheduler=self.get_module(\"scheduler\")))\n\n    self.add_stage(stage_name=\"latent_preparation_stage\",\n                   stage=LatentPreparationStage(\n                       scheduler=self.get_module(\"scheduler\"),\n                       transformer=self.get_module(\"transformer\")))\n\n    self.add_stage(stage_name=\"video_latent_preparation_stage\",\n                   stage=VideoVAEEncodingStage(vae=self.get_module(\"vae\")))\n\n    self.add_stage(stage_name=\"denoising_stage\",\n                   stage=DenoisingStage(\n                       transformer=self.get_module(\"transformer\"),\n                       transformer_2=self.get_module(\"transformer_2\"),\n                       scheduler=self.get_module(\"scheduler\")))\n\n    self.add_stage(stage_name=\"decoding_stage\",\n                   stage=DecodingStage(vae=self.get_module(\"vae\")))\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/#fastvideo.pipelines.composed_pipeline_base","title":"fastvideo.pipelines.composed_pipeline_base","text":"<p>Base class for composed pipelines.</p> <p>This module defines the base class for pipelines that are composed of multiple stages.</p>"},{"location":"api/fastvideo/#fastvideo.pipelines.composed_pipeline_base-classes","title":"Classes","text":"fastvideo.pipelines.composed_pipeline_base.ComposedPipelineBase \u00b6 <pre><code>ComposedPipelineBase(\n    model_path: str,\n    fastvideo_args: FastVideoArgs | TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for pipelines composed of multiple stages.</p> <p>This class provides the framework for creating pipelines by composing multiple stages together. Each stage is responsible for a specific part of the diffusion process, and the pipeline orchestrates the execution of these stages.</p> <p>Initialize the pipeline. After init, the pipeline should be ready to use. The pipeline should be stateless and not hold any batch state.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def __init__(self,\n             model_path: str,\n             fastvideo_args: FastVideoArgs | TrainingArgs,\n             required_config_modules: list[str] | None = None,\n             loaded_modules: dict[str, torch.nn.Module] | None = None):\n    \"\"\"\n    Initialize the pipeline. After __init__, the pipeline should be ready to\n    use. The pipeline should be stateless and not hold any batch state.\n    \"\"\"\n    self.fastvideo_args = fastvideo_args\n\n    self.model_path: str = model_path\n    self._stages: list[PipelineStage] = []\n    self._stage_name_mapping: dict[str, PipelineStage] = {}\n\n    if required_config_modules is not None:\n        self._required_config_modules = required_config_modules\n\n    if self._required_config_modules is None:\n        raise NotImplementedError(\n            \"Subclass must set _required_config_modules\")\n\n    maybe_init_distributed_environment_and_model_parallel(\n        fastvideo_args.tp_size, fastvideo_args.sp_size)\n\n    # Torch profiler. Enabled and configured through env vars:\n    # FASTVIDEO_TORCH_PROFILER_DIR=/path/to/save/trace\n    trace_dir = envs.FASTVIDEO_TORCH_PROFILER_DIR\n    self.profiler_controller = get_or_create_profiler(trace_dir)\n    self.profiler = self.profiler_controller.profiler\n\n    self.local_rank = get_world_group().local_rank\n\n    # Load modules directly in initialization\n    logger.info(\"Loading pipeline modules...\")\n    with self.profiler_controller.region(\"profiler_region_model_loading\"):\n        self.modules = self.load_modules(fastvideo_args, loaded_modules)\n</code></pre> Attributes\u00b6 fastvideo.pipelines.composed_pipeline_base.ComposedPipelineBase.required_config_modules <code>property</code> \u00b6 <pre><code>required_config_modules: list[str]\n</code></pre> <p>List of modules that are required by the pipeline. The names should match the diffusers directory and model_index.json file. These modules will be loaded using the PipelineComponentLoader and made available in the modules dictionary. Access these modules using the get_module method.</p> <p>class ConcretePipeline(ComposedPipelineBase):     _required_config_modules = [\"vae\", \"text_encoder\", \"transformer\", \"scheduler\", \"tokenizer\"]</p> <pre><code>@property\ndef required_config_modules(self):\n    return self._required_config_modules\n</code></pre> fastvideo.pipelines.composed_pipeline_base.ComposedPipelineBase.stages <code>property</code> \u00b6 <pre><code>stages: list[PipelineStage]\n</code></pre> <p>List of stages in the pipeline.</p> Functions\u00b6 fastvideo.pipelines.composed_pipeline_base.ComposedPipelineBase.create_pipeline_stages <code>abstractmethod</code> \u00b6 <pre><code>create_pipeline_stages(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Create the inference pipeline stages.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>@abstractmethod\ndef create_pipeline_stages(self, fastvideo_args: FastVideoArgs):\n    \"\"\"\n    Create the inference pipeline stages.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.pipelines.composed_pipeline_base.ComposedPipelineBase.create_training_stages \u00b6 <pre><code>create_training_stages(training_args: TrainingArgs)\n</code></pre> <p>Create the training pipeline stages.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def create_training_stages(self, training_args: TrainingArgs):\n    \"\"\"\n    Create the training pipeline stages.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.pipelines.composed_pipeline_base.ComposedPipelineBase.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Generate a video or image using the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The batch to generate from.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:     ForwardBatch: The batch with the generated video or image.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Generate a video or image using the pipeline.\n\n    Args:\n        batch: The batch to generate from.\n        fastvideo_args: The inference arguments.\n    Returns:\n        ForwardBatch: The batch with the generated video or image.\n    \"\"\"\n    if not self.post_init_called:\n        self.post_init()\n\n    # Execute each stage\n    logger.info(\"Running pipeline stages: %s\",\n                self._stage_name_mapping.keys())\n    # logger.info(\"Batch: %s\", batch)\n    for stage in self.stages:\n        batch = stage(batch, fastvideo_args)\n\n    # Return the output\n    return batch\n</code></pre> fastvideo.pipelines.composed_pipeline_base.ComposedPipelineBase.from_pretrained <code>classmethod</code> \u00b6 <pre><code>from_pretrained(\n    model_path: str,\n    device: str | None = None,\n    torch_dtype: dtype | None = None,\n    pipeline_config: str | PipelineConfig | None = None,\n    args: Namespace | None = None,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n    **kwargs\n) -&gt; ComposedPipelineBase\n</code></pre> <p>Load a pipeline from a pretrained model. loaded_modules: Optional[Dict[str, torch.nn.Module]] = None, If provided, loaded_modules will be used instead of loading from config/pretrained weights.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>@classmethod\ndef from_pretrained(cls,\n                    model_path: str,\n                    device: str | None = None,\n                    torch_dtype: torch.dtype | None = None,\n                    pipeline_config: str | PipelineConfig | None = None,\n                    args: argparse.Namespace | None = None,\n                    required_config_modules: list[str] | None = None,\n                    loaded_modules: dict[str, torch.nn.Module]\n                    | None = None,\n                    **kwargs) -&gt; \"ComposedPipelineBase\":\n    \"\"\"\n    Load a pipeline from a pretrained model.\n    loaded_modules: Optional[Dict[str, torch.nn.Module]] = None,\n    If provided, loaded_modules will be used instead of loading from config/pretrained weights.\n    \"\"\"\n    if args is None or args.inference_mode:\n\n        kwargs['model_path'] = model_path\n        fastvideo_args = FastVideoArgs.from_kwargs(**kwargs)\n    else:\n        assert args is not None, \"args must be provided for training mode\"\n        fastvideo_args = TrainingArgs.from_cli_args(args)\n        # TODO(will): fix this so that its not so ugly\n        fastvideo_args.model_path = model_path\n        for key, value in kwargs.items():\n            setattr(fastvideo_args, key, value)\n\n        fastvideo_args.dit_cpu_offload = False\n        # we hijack the precision to be the master weight type so that the\n        # model is loaded with the correct precision. Subsequently we will\n        # use FSDP2's MixedPrecisionPolicy to set the precision for the\n        # fwd, bwd, and other operations' precision.\n        assert fastvideo_args.pipeline_config.dit_precision == 'fp32', 'only fp32 is supported for training'\n\n    logger.info(\"fastvideo_args in from_pretrained: %s\", fastvideo_args)\n\n    pipe = cls(model_path,\n               fastvideo_args,\n               required_config_modules=required_config_modules,\n               loaded_modules=loaded_modules)\n    pipe.post_init()\n    return pipe\n</code></pre> fastvideo.pipelines.composed_pipeline_base.ComposedPipelineBase.initialize_pipeline \u00b6 <pre><code>initialize_pipeline(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Initialize the pipeline.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def initialize_pipeline(self, fastvideo_args: FastVideoArgs):\n    \"\"\"\n    Initialize the pipeline.\n    \"\"\"\n    return\n</code></pre> fastvideo.pipelines.composed_pipeline_base.ComposedPipelineBase.load_modules \u00b6 <pre><code>load_modules(\n    fastvideo_args: FastVideoArgs,\n    loaded_modules: dict[str, Module] | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Load the modules from the config. loaded_modules: Optional[Dict[str, torch.nn.Module]] = None,  If provided, loaded_modules will be used instead of loading from config/pretrained weights.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def load_modules(\n    self,\n    fastvideo_args: FastVideoArgs,\n    loaded_modules: dict[str, torch.nn.Module] | None = None\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Load the modules from the config.\n    loaded_modules: Optional[Dict[str, torch.nn.Module]] = None, \n    If provided, loaded_modules will be used instead of loading from config/pretrained weights.\n    \"\"\"\n\n    model_index = self._load_config(self.model_path)\n    logger.info(\"Loading pipeline modules from config: %s\", model_index)\n\n    # remove keys that are not pipeline modules\n    model_index.pop(\"_class_name\")\n    model_index.pop(\"_diffusers_version\")\n    if \"boundary_ratio\" in model_index and model_index[\n            \"boundary_ratio\"] is not None:\n        logger.info(\n            \"MoE pipeline detected. Adding transformer_2 to self.required_config_modules...\"\n        )\n        self.required_config_modules.append(\"transformer_2\")\n        logger.info(\"MoE pipeline detected. Setting boundary ratio to %s\",\n                    model_index[\"boundary_ratio\"])\n        fastvideo_args.pipeline_config.dit_config.boundary_ratio = model_index[\n            \"boundary_ratio\"]\n\n    model_index.pop(\"boundary_ratio\", None)\n    # used by Wan2.2 ti2v\n    model_index.pop(\"expand_timesteps\", None)\n\n    # some sanity checks\n    assert len(\n        model_index\n    ) &gt; 1, \"model_index.json must contain at least one pipeline module\"\n\n    for module_name in self.required_config_modules:\n        if module_name not in model_index and module_name in self._extra_config_module_map:\n            extra_module_value = self._extra_config_module_map[module_name]\n            logger.warning(\n                \"model_index.json does not contain a %s module, but found {%s: %s} in _extra_config_module_map, adding to model_index.\",\n                module_name, module_name, extra_module_value)\n            if extra_module_value in model_index:\n                logger.info(\"Using module %s for %s\", extra_module_value,\n                            module_name)\n                model_index[module_name] = model_index[extra_module_value]\n                continue\n            else:\n                raise ValueError(\n                    f\"Required module key: {module_name} value: {model_index.get(module_name)} was not found in loaded modules {model_index.keys()}\"\n                )\n\n    # all the component models used by the pipeline\n    required_modules = self.required_config_modules\n    logger.info(\"Loading required modules: %s\", required_modules)\n\n    modules = {}\n    for module_name, (transformers_or_diffusers,\n                      architecture) in model_index.items():\n        if transformers_or_diffusers is None:\n            logger.warning(\n                \"Module %s in model_index.json has null value, removing from required_config_modules\",\n                module_name)\n            if module_name in self.required_config_modules:\n                self.required_config_modules.remove(module_name)\n            continue\n        if module_name not in required_modules:\n            logger.info(\"Skipping module %s\", module_name)\n            continue\n        if loaded_modules is not None and module_name in loaded_modules:\n            logger.info(\"Using module %s already provided\", module_name)\n            modules[module_name] = loaded_modules[module_name]\n            continue\n\n        # we load the module from the extra config module map if it exists\n        if module_name in self._extra_config_module_map:\n            load_module_name = self._extra_config_module_map[module_name]\n        else:\n            load_module_name = module_name\n\n        component_model_path = os.path.join(self.model_path,\n                                            load_module_name)\n        module = PipelineComponentLoader.load_module(\n            module_name=load_module_name,\n            component_model_path=component_model_path,\n            transformers_or_diffusers=transformers_or_diffusers,\n            fastvideo_args=fastvideo_args,\n        )\n        logger.info(\"Loaded module %s from %s\", module_name,\n                    component_model_path)\n\n        if module_name in modules:\n            logger.warning(\"Overwriting module %s\", module_name)\n        modules[module_name] = module\n\n    # Check if all required modules were loaded\n    for module_name in required_modules:\n        if module_name not in modules or modules[module_name] is None:\n            raise ValueError(\n                f\"Required module key: {module_name} value: {modules.get(module_name)} was not found in loaded modules {modules.keys()}\"\n            )\n\n    return modules\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.composed_pipeline_base-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideo.pipelines.lora_pipeline","title":"fastvideo.pipelines.lora_pipeline","text":""},{"location":"api/fastvideo/#fastvideo.pipelines.lora_pipeline-classes","title":"Classes","text":"fastvideo.pipelines.lora_pipeline.LoRAPipeline \u00b6 <pre><code>LoRAPipeline(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ComposedPipelineBase</code></p> <p>Pipeline that supports injecting LoRA adapters into the diffusion transformer. TODO: support training.</p> Source code in <code>fastvideo/pipelines/lora_pipeline.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.device = get_local_torch_device()\n    self.exclude_lora_layers = self.modules[\n        \"transformer\"].config.arch_config.exclude_lora_layers\n    self.lora_target_modules = self.fastvideo_args.lora_target_modules\n    self.lora_path = self.fastvideo_args.lora_path\n    self.lora_nickname = self.fastvideo_args.lora_nickname\n    self.training_mode = self.fastvideo_args.training_mode\n    if self.training_mode and getattr(self.fastvideo_args, \"lora_training\",\n                                      False):\n        assert isinstance(self.fastvideo_args, TrainingArgs)\n        if self.fastvideo_args.lora_alpha is None:\n            self.fastvideo_args.lora_alpha = self.fastvideo_args.lora_rank\n        self.lora_rank = self.fastvideo_args.lora_rank  # type: ignore\n        self.lora_alpha = self.fastvideo_args.lora_alpha  # type: ignore\n        logger.info(\"Using LoRA training with rank %d and alpha %d\",\n                    self.lora_rank, self.lora_alpha)\n        if self.lora_target_modules is None:\n            self.lora_target_modules = [\n                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"to_q\", \"to_k\",\n                \"to_v\", \"to_out\", \"to_qkv\"\n            ]\n        self.convert_to_lora_layers()\n    # Inference\n    elif not self.training_mode and self.lora_path is not None:\n        self.convert_to_lora_layers()\n        self.set_lora_adapter(\n            self.lora_nickname,  # type: ignore\n            self.lora_path)  # type: ignore\n</code></pre> Functions\u00b6 fastvideo.pipelines.lora_pipeline.LoRAPipeline.convert_to_lora_layers \u00b6 <pre><code>convert_to_lora_layers() -&gt; None\n</code></pre> <p>Unified method to convert the transformer to a LoRA transformer.</p> Source code in <code>fastvideo/pipelines/lora_pipeline.py</code> <pre><code>def convert_to_lora_layers(self) -&gt; None:\n    \"\"\"\n    Unified method to convert the transformer to a LoRA transformer.\n    \"\"\"\n    if self.lora_initialized:\n        return\n    self.lora_initialized = True\n    converted_count = 0\n    for name, layer in self.modules[\"transformer\"].named_modules():\n        if not self.is_target_layer(name):\n            continue\n\n        excluded = False\n        for exclude_layer in self.exclude_lora_layers:\n            if exclude_layer in name:\n                excluded = True\n                break\n        if excluded:\n            continue\n\n        layer = get_lora_layer(layer,\n                               lora_rank=self.lora_rank,\n                               lora_alpha=self.lora_alpha,\n                               training_mode=self.training_mode)\n        if layer is not None:\n            self.lora_layers[name] = layer\n            replace_submodule(self.modules[\"transformer\"], name, layer)\n            converted_count += 1\n    logger.info(\"Converted %d layers to LoRA layers\", converted_count)\n\n    if \"fake_score_transformer\" in self.modules:\n        for name, layer in self.modules[\n                \"fake_score_transformer\"].named_modules():\n            if not self.is_target_layer(name):\n                continue\n            layer = get_lora_layer(layer,\n                                   lora_rank=self.lora_rank,\n                                   lora_alpha=self.lora_alpha,\n                                   training_mode=self.training_mode)\n            if layer is not None:\n                self.lora_layers_critic[name] = layer\n                replace_submodule(self.modules[\"fake_score_transformer\"],\n                                  name, layer)\n                converted_count += 1\n        logger.info(\n            \"Converted %d layers to LoRA layers in the critic model\",\n            converted_count)\n</code></pre> fastvideo.pipelines.lora_pipeline.LoRAPipeline.set_lora_adapter \u00b6 <pre><code>set_lora_adapter(\n    lora_nickname: str, lora_path: str | None = None\n)\n</code></pre> <p>Load a LoRA adapter into the pipeline and merge it into the transformer. Args:     lora_nickname: The \"nick name\" of the adapter when referenced in the pipeline.     lora_path: The path to the adapter, either a local path or a Hugging Face repo id.</p> Source code in <code>fastvideo/pipelines/lora_pipeline.py</code> <pre><code>def set_lora_adapter(self,\n                     lora_nickname: str,\n                     lora_path: str | None = None):  # type: ignore\n    \"\"\"\n    Load a LoRA adapter into the pipeline and merge it into the transformer.\n    Args:\n        lora_nickname: The \"nick name\" of the adapter when referenced in the pipeline.\n        lora_path: The path to the adapter, either a local path or a Hugging Face repo id.\n    \"\"\"\n\n    if lora_nickname not in self.lora_adapters and lora_path is None:\n        raise ValueError(\n            f\"Adapter {lora_nickname} not found in the pipeline. Please provide lora_path to load it.\"\n        )\n    if not self.lora_initialized:\n        self.convert_to_lora_layers()\n    adapter_updated = False\n    rank = dist.get_rank()\n    if lora_path is not None and lora_path != self.cur_adapter_path:\n        lora_local_path = maybe_download_lora(lora_path)\n        lora_state_dict = load_file(lora_local_path)\n\n        # Map the hf layer names to our custom layer names\n        param_names_mapping_fn = get_param_names_mapping(\n            self.modules[\"transformer\"].param_names_mapping)\n        lora_param_names_mapping_fn = get_param_names_mapping(\n            self.modules[\"transformer\"].lora_param_names_mapping)\n\n        to_merge_params: defaultdict[Hashable,\n                                     dict[Any, Any]] = defaultdict(dict)\n        for name, weight in lora_state_dict.items():\n            name = name.replace(\"diffusion_model.\", \"\")\n            name = name.replace(\".weight\", \"\")\n            name, _, _ = lora_param_names_mapping_fn(name)\n            target_name, merge_index, num_params_to_merge = param_names_mapping_fn(\n                name)\n            # for (in_dim, r) @ (r, out_dim), we only merge (r, out_dim * n) where n is the number of linear layers to fuse\n            # see param mapping in HunyuanVideoArchConfig\n            if merge_index is not None and \"lora_B\" in name:\n                to_merge_params[target_name][merge_index] = weight\n                if len(to_merge_params[target_name]) == num_params_to_merge:\n                    # cat at output dim according to the merge_index order\n                    sorted_tensors = [\n                        to_merge_params[target_name][i]\n                        for i in range(num_params_to_merge)\n                    ]\n                    weight = torch.cat(sorted_tensors, dim=1)\n                    del to_merge_params[target_name]\n                else:\n                    continue\n\n            if target_name in self.lora_adapters[lora_nickname]:\n                raise ValueError(\n                    f\"Target name {target_name} already exists in lora_adapters[{lora_nickname}]\"\n                )\n            self.lora_adapters[lora_nickname][target_name] = weight.to(\n                self.device)\n        adapter_updated = True\n        self.cur_adapter_path = lora_path\n        logger.info(\"Rank %d: loaded LoRA adapter %s\", rank, lora_path)\n\n    if not adapter_updated and self.cur_adapter_name == lora_nickname:\n        return\n    self.cur_adapter_name = lora_nickname\n\n    # Merge the new adapter\n    adapted_count = 0\n    for name, layer in self.lora_layers.items():\n        lora_A_name = name + \".lora_A\"\n        lora_B_name = name + \".lora_B\"\n        if lora_A_name in self.lora_adapters[lora_nickname]\\\n            and lora_B_name in self.lora_adapters[lora_nickname]:\n            layer.set_lora_weights(\n                self.lora_adapters[lora_nickname][lora_A_name],\n                self.lora_adapters[lora_nickname][lora_B_name],\n                training_mode=self.fastvideo_args.training_mode,\n                lora_path=lora_path)\n            adapted_count += 1\n        else:\n            if rank == 0:\n                logger.warning(\n                    \"LoRA adapter %s does not contain the weights for layer %s. LoRA will not be applied to it.\",\n                    lora_path, name)\n            layer.disable_lora = True\n    logger.info(\"Rank %d: LoRA adapter %s applied to %d layers\", rank,\n                lora_path, adapted_count)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.lora_pipeline-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideo.pipelines.pipeline_batch_info","title":"fastvideo.pipelines.pipeline_batch_info","text":"<p>Data structures for functional pipeline processing.</p> <p>This module defines the dataclasses used to pass state between pipeline components in a functional manner, reducing the need for explicit parameter passing.</p>"},{"location":"api/fastvideo/#fastvideo.pipelines.pipeline_batch_info-classes","title":"Classes","text":"fastvideo.pipelines.pipeline_batch_info.ForwardBatch <code>dataclass</code> \u00b6 <pre><code>ForwardBatch(\n    data_type: str,\n    generator: Generator | list[Generator] | None = None,\n    image_path: str | None = None,\n    image_embeds: list[Tensor] = list(),\n    pil_image: Tensor | Image | None = None,\n    preprocessed_image: Tensor | None = None,\n    prompt: str | list[str] | None = None,\n    negative_prompt: str | list[str] | None = None,\n    prompt_path: str | None = None,\n    output_path: str = \"outputs/\",\n    output_video_name: str | None = None,\n    video_path: str | None = None,\n    video_latent: Tensor | None = None,\n    prompt_embeds: list[Tensor] = list(),\n    negative_prompt_embeds: list[Tensor] | None = None,\n    prompt_attention_mask: list[Tensor] | None = None,\n    negative_attention_mask: list[Tensor] | None = None,\n    clip_embedding_pos: list[Tensor] | None = None,\n    clip_embedding_neg: list[Tensor] | None = None,\n    max_sequence_length: int | None = None,\n    prompt_template: dict[str, Any] | None = None,\n    do_classifier_free_guidance: bool = False,\n    batch_size: int | None = None,\n    num_videos_per_prompt: int = 1,\n    seed: int | None = None,\n    seeds: list[int] | None = None,\n    is_prompt_processed: bool = False,\n    latents: Tensor | None = None,\n    raw_latent_shape: Tensor | None = None,\n    noise_pred: Tensor | None = None,\n    image_latent: Tensor | None = None,\n    height_latents: list[int] | int | None = None,\n    width_latents: list[int] | int | None = None,\n    num_frames: list[int] | int = 1,\n    num_frames_round_down: bool = False,\n    height: list[int] | int | None = None,\n    width: list[int] | int | None = None,\n    fps: list[int] | int | None = None,\n    timesteps: Tensor | None = None,\n    timestep: Tensor | float | int | None = None,\n    step_index: int | None = None,\n    boundary_ratio: float | None = None,\n    num_inference_steps: int = 50,\n    guidance_scale: float = 1.0,\n    guidance_scale_2: float | None = None,\n    guidance_rescale: float = 0.0,\n    eta: float = 0.0,\n    sigmas: list[float] | None = None,\n    n_tokens: int | None = None,\n    extra_step_kwargs: dict[str, Any] = dict(),\n    modules: dict[str, Any] = dict(),\n    output: Tensor | None = None,\n    return_trajectory_latents: bool = False,\n    return_trajectory_decoded: bool = False,\n    trajectory_timesteps: list[Tensor] | None = None,\n    trajectory_latents: Tensor | None = None,\n    trajectory_decoded: list[Tensor] | None = None,\n    extra: dict[str, Any] = dict(),\n    save_video: bool = True,\n    return_frames: bool = False,\n    enable_teacache: bool = False,\n    teacache_params: TeaCacheParams\n    | WanTeaCacheParams\n    | None = None,\n    STA_param: list | None = None,\n    is_cfg_negative: bool = False,\n    mask_search_final_result_pos: list[list] | None = None,\n    mask_search_final_result_neg: list[list] | None = None,\n    VSA_sparsity: float = 0.0,\n    logging_info: PipelineLoggingInfo = PipelineLoggingInfo(),\n)\n</code></pre> <p>Complete state passed through the pipeline execution.</p> <p>This dataclass contains all information needed during the diffusion pipeline execution, allowing methods to update specific components without needing to manage numerous individual parameters.</p> Functions\u00b6 fastvideo.pipelines.pipeline_batch_info.ForwardBatch.__post_init__ \u00b6 <pre><code>__post_init__()\n</code></pre> <p>Initialize dependent fields after dataclass initialization.</p> Source code in <code>fastvideo/pipelines/pipeline_batch_info.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Initialize dependent fields after dataclass initialization.\"\"\"\n\n    # Set do_classifier_free_guidance based on guidance scale and negative prompt\n    if self.guidance_scale &gt; 1.0:\n        self.do_classifier_free_guidance = True\n    if self.negative_prompt_embeds is None:\n        self.negative_prompt_embeds = []\n    if self.guidance_scale_2 is None:\n        self.guidance_scale_2 = self.guidance_scale\n</code></pre> fastvideo.pipelines.pipeline_batch_info.PipelineLoggingInfo \u00b6 <pre><code>PipelineLoggingInfo()\n</code></pre> <p>Simple approach using OrderedDict to track stage metrics.</p> Source code in <code>fastvideo/pipelines/pipeline_batch_info.py</code> <pre><code>def __init__(self):\n    # OrderedDict preserves insertion order and allows easy access\n    self.stages: OrderedDict[str, dict[str, Any]] = OrderedDict()\n</code></pre> Functions\u00b6 fastvideo.pipelines.pipeline_batch_info.PipelineLoggingInfo.add_stage_execution_time \u00b6 <pre><code>add_stage_execution_time(\n    stage_name: str, execution_time: float\n)\n</code></pre> <p>Add execution time for a stage.</p> Source code in <code>fastvideo/pipelines/pipeline_batch_info.py</code> <pre><code>def add_stage_execution_time(self, stage_name: str, execution_time: float):\n    \"\"\"Add execution time for a stage.\"\"\"\n    if stage_name not in self.stages:\n        self.stages[stage_name] = {}\n    self.stages[stage_name]['execution_time'] = execution_time\n    self.stages[stage_name]['timestamp'] = time.time()\n</code></pre> fastvideo.pipelines.pipeline_batch_info.PipelineLoggingInfo.add_stage_metric \u00b6 <pre><code>add_stage_metric(\n    stage_name: str, metric_name: str, value: Any\n)\n</code></pre> <p>Add any metric for a stage.</p> Source code in <code>fastvideo/pipelines/pipeline_batch_info.py</code> <pre><code>def add_stage_metric(self, stage_name: str, metric_name: str, value: Any):\n    \"\"\"Add any metric for a stage.\"\"\"\n    if stage_name not in self.stages:\n        self.stages[stage_name] = {}\n    self.stages[stage_name][metric_name] = value\n</code></pre> fastvideo.pipelines.pipeline_batch_info.PipelineLoggingInfo.get_execution_order \u00b6 <pre><code>get_execution_order() -&gt; list[str]\n</code></pre> <p>Get stages in execution order.</p> Source code in <code>fastvideo/pipelines/pipeline_batch_info.py</code> <pre><code>def get_execution_order(self) -&gt; list[str]:\n    \"\"\"Get stages in execution order.\"\"\"\n    return list(self.stages.keys())\n</code></pre> fastvideo.pipelines.pipeline_batch_info.PipelineLoggingInfo.get_stage_info \u00b6 <pre><code>get_stage_info(stage_name: str) -&gt; dict[str, Any]\n</code></pre> <p>Get all info for a specific stage.</p> Source code in <code>fastvideo/pipelines/pipeline_batch_info.py</code> <pre><code>def get_stage_info(self, stage_name: str) -&gt; dict[str, Any]:\n    \"\"\"Get all info for a specific stage.\"\"\"\n    return self.stages.get(stage_name, {})\n</code></pre> fastvideo.pipelines.pipeline_batch_info.PipelineLoggingInfo.get_total_execution_time \u00b6 <pre><code>get_total_execution_time() -&gt; float\n</code></pre> <p>Get total pipeline execution time.</p> Source code in <code>fastvideo/pipelines/pipeline_batch_info.py</code> <pre><code>def get_total_execution_time(self) -&gt; float:\n    \"\"\"Get total pipeline execution time.\"\"\"\n    return sum(\n        stage.get('execution_time', 0) for stage in self.stages.values())\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.pipeline_registry","title":"fastvideo.pipelines.pipeline_registry","text":""},{"location":"api/fastvideo/#fastvideo.pipelines.pipeline_registry-classes","title":"Classes","text":"fastvideo.pipelines.pipeline_registry.PipelineType \u00b6 <p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration for different pipeline types.</p> <p>Inherits from str to allow string comparison for backward compatibility.</p> Functions\u00b6 fastvideo.pipelines.pipeline_registry.PipelineType.choices <code>classmethod</code> \u00b6 <pre><code>choices() -&gt; list[str]\n</code></pre> <p>Get all available choices as strings.</p> Source code in <code>fastvideo/pipelines/pipeline_registry.py</code> <pre><code>@classmethod\ndef choices(cls) -&gt; list[str]:\n    \"\"\"Get all available choices as strings.\"\"\"\n    return [pipeline_type.value for pipeline_type in cls]\n</code></pre> fastvideo.pipelines.pipeline_registry.PipelineType.from_string <code>classmethod</code> \u00b6 <pre><code>from_string(value: str) -&gt; PipelineType\n</code></pre> <p>Convert string to PipelineType enum.</p> Source code in <code>fastvideo/pipelines/pipeline_registry.py</code> <pre><code>@classmethod\ndef from_string(cls, value: str) -&gt; \"PipelineType\":\n    \"\"\"Convert string to PipelineType enum.\"\"\"\n    try:\n        return cls(value.lower())\n    except ValueError:\n        raise ValueError(\n            f\"Invalid pipeline type: {value}. Must be one of: {', '.join([t.value for t in cls])}\"\n        ) from None\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.pipeline_registry-functions","title":"Functions","text":"fastvideo.pipelines.pipeline_registry.get_pipeline_registry \u00b6 <pre><code>get_pipeline_registry(\n    pipeline_type: PipelineType | str | None = None,\n) -&gt; _PipelineRegistry\n</code></pre> <p>Get a pipeline registry for the specified mode, pipeline type, and workload type.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_type</code> <code>PipelineType | str | None</code> <p>Pipeline type to load. If None and mode is provided, will be derived from mode.</p> <code>None</code> <p>Returns:</p> Type Description <code>_PipelineRegistry</code> <p>A pipeline registry instance.</p> Source code in <code>fastvideo/pipelines/pipeline_registry.py</code> <pre><code>def get_pipeline_registry(\n        pipeline_type: PipelineType | str | None = None) -&gt; _PipelineRegistry:\n    \"\"\"\n    Get a pipeline registry for the specified mode, pipeline type, and workload type.\n\n    Args:\n        pipeline_type: Pipeline type to load. If None and mode is provided, will be derived from mode.\n\n    Returns:\n        A pipeline registry instance.\n    \"\"\"\n    if isinstance(pipeline_type, str):\n        pipeline_type = PipelineType.from_string(pipeline_type)\n\n    pipeline_classes = import_pipeline_classes(pipeline_type)\n    return _PipelineRegistry(pipeline_classes)\n</code></pre> fastvideo.pipelines.pipeline_registry.import_pipeline_classes <code>cached</code> \u00b6 <pre><code>import_pipeline_classes(\n    pipeline_types: list[PipelineType]\n    | PipelineType\n    | None = None,\n) -&gt; dict[\n    str,\n    dict[str, dict[str, type[ComposedPipelineBase] | None]],\n]\n</code></pre> <p>Import pipeline classes based on the pipeline type and workload type.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_types</code> <code>list[PipelineType] | PipelineType | None</code> <p>The pipeline types to load (basic, preprocess, training).            If None, loads all types.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, dict[str, dict[str, type[ComposedPipelineBase] | None]]]</code> <p>A three-level nested dictionary:</p> <code>dict[str, dict[str, dict[str, type[ComposedPipelineBase] | None]]]</code> <p>{pipeline_type: {architecture_name: {pipeline_name: pipeline_cls}}}</p> <code>dict[str, dict[str, dict[str, type[ComposedPipelineBase] | None]]]</code> <p>e.g., {\"basic\": {\"wan\": {\"WanPipeline\": WanPipeline}}}</p> Source code in <code>fastvideo/pipelines/pipeline_registry.py</code> <pre><code>@lru_cache\ndef import_pipeline_classes(\n    pipeline_types: list[PipelineType] | PipelineType | None = None\n) -&gt; dict[str, dict[str, dict[str, type[ComposedPipelineBase] | None]]]:\n    \"\"\"\n    Import pipeline classes based on the pipeline type and workload type.\n\n    Args:\n        pipeline_types: The pipeline types to load (basic, preprocess, training). \n                      If None, loads all types.\n\n    Returns:\n        A three-level nested dictionary:\n        {pipeline_type: {architecture_name: {pipeline_name: pipeline_cls}}}\n        e.g., {\"basic\": {\"wan\": {\"WanPipeline\": WanPipeline}}}\n    \"\"\"\n    type_to_arch_to_pipeline_dict: dict[str,\n                                        dict[str,\n                                             dict[str,\n                                                  type[ComposedPipelineBase]\n                                                  | None]]] = {}\n    package_name: str = \"fastvideo.pipelines\"\n\n    # Determine which pipeline types to scan\n    if isinstance(pipeline_types, list):\n        pipeline_types_to_scan = [\n            pipeline_type.value for pipeline_type in pipeline_types\n        ]\n    elif isinstance(pipeline_types, PipelineType):\n        pipeline_types_to_scan = [pipeline_types.value]\n    else:\n        pipeline_types_to_scan = [pt.value for pt in PipelineType]\n\n    logger.info(\"Loading pipelines for types: %s\", pipeline_types_to_scan)\n\n    for pipeline_type_str in pipeline_types_to_scan:\n        arch_to_pipeline_dict: dict[str, dict[str, type[ComposedPipelineBase]\n                                              | None]] = {}\n\n        # Try to load from pipeline-type-specific directory first\n        pipeline_type_package_name = f\"{package_name}.{pipeline_type_str}\"\n\n        try:\n            pipeline_type_package = importlib.import_module(\n                pipeline_type_package_name)\n            logger.debug(\"Successfully imported %s\", pipeline_type_package_name)\n\n            for _, arch, ispkg in pkgutil.iter_modules(\n                    pipeline_type_package.__path__):\n                pipeline_dict: dict[str, type[ComposedPipelineBase] | None] = {}\n\n                arch_package_name = f\"{pipeline_type_package_name}.{arch}\"\n                if ispkg:\n                    arch_package = importlib.import_module(arch_package_name)\n                    for _, module_name, ispkg in pkgutil.walk_packages(\n                            arch_package.__path__, arch_package_name + \".\"):\n                        if not ispkg:\n                            pipeline_module = importlib.import_module(\n                                module_name)\n                            if hasattr(pipeline_module, \"EntryClass\"):\n                                if isinstance(pipeline_module.EntryClass, list):\n                                    for pipeline in pipeline_module.EntryClass:\n                                        pipeline_name = pipeline.__name__\n                                        assert (\n                                            pipeline_name not in pipeline_dict\n                                        ), f\"Duplicated pipeline implementation for {pipeline_name} in {pipeline_type_str}.{arch_package_name}\"\n                                        pipeline_dict[pipeline_name] = pipeline\n                                else:\n                                    pipeline_name = pipeline_module.EntryClass.__name__\n                                    assert (\n                                        pipeline_name not in pipeline_dict\n                                    ), f\"Duplicated pipeline implementation for {pipeline_name} in {pipeline_type_str}.{arch_package_name}\"\n                                    pipeline_dict[\n                                        pipeline_name] = pipeline_module.EntryClass\n\n                arch_to_pipeline_dict[arch] = pipeline_dict\n\n        except ImportError as e:\n            raise ImportError(\n                f\"Could not import {pipeline_type_package_name} when importing pipeline classes: {e}\"\n            ) from None\n\n        type_to_arch_to_pipeline_dict[pipeline_type_str] = arch_to_pipeline_dict\n\n    # Log summary\n    total_pipelines = sum(\n        len(pipeline_dict)\n        for arch_to_pipeline_dict in type_to_arch_to_pipeline_dict.values()\n        for pipeline_dict in arch_to_pipeline_dict.values())\n    logger.info(\"Loaded %d pipeline classes across %d types\", total_pipelines,\n                len(pipeline_types_to_scan))\n\n    return type_to_arch_to_pipeline_dict\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.preprocess","title":"fastvideo.pipelines.preprocess","text":""},{"location":"api/fastvideo/#fastvideo.pipelines.preprocess-modules","title":"Modules","text":"fastvideo.pipelines.preprocess.preprocess_pipeline_base \u00b6 Classes\u00b6 fastvideo.pipelines.preprocess.preprocess_pipeline_base.BasePreprocessPipeline \u00b6 <pre><code>BasePreprocessPipeline(\n    model_path: str,\n    fastvideo_args: FastVideoArgs | TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>ComposedPipelineBase</code></p> <p>Base class for preprocessing pipelines that handles common functionality.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def __init__(self,\n             model_path: str,\n             fastvideo_args: FastVideoArgs | TrainingArgs,\n             required_config_modules: list[str] | None = None,\n             loaded_modules: dict[str, torch.nn.Module] | None = None):\n    \"\"\"\n    Initialize the pipeline. After __init__, the pipeline should be ready to\n    use. The pipeline should be stateless and not hold any batch state.\n    \"\"\"\n    self.fastvideo_args = fastvideo_args\n\n    self.model_path: str = model_path\n    self._stages: list[PipelineStage] = []\n    self._stage_name_mapping: dict[str, PipelineStage] = {}\n\n    if required_config_modules is not None:\n        self._required_config_modules = required_config_modules\n\n    if self._required_config_modules is None:\n        raise NotImplementedError(\n            \"Subclass must set _required_config_modules\")\n\n    maybe_init_distributed_environment_and_model_parallel(\n        fastvideo_args.tp_size, fastvideo_args.sp_size)\n\n    # Torch profiler. Enabled and configured through env vars:\n    # FASTVIDEO_TORCH_PROFILER_DIR=/path/to/save/trace\n    trace_dir = envs.FASTVIDEO_TORCH_PROFILER_DIR\n    self.profiler_controller = get_or_create_profiler(trace_dir)\n    self.profiler = self.profiler_controller.profiler\n\n    self.local_rank = get_world_group().local_rank\n\n    # Load modules directly in initialization\n    logger.info(\"Loading pipeline modules...\")\n    with self.profiler_controller.region(\"profiler_region_model_loading\"):\n        self.modules = self.load_modules(fastvideo_args, loaded_modules)\n</code></pre> Functions\u00b6 fastvideo.pipelines.preprocess.preprocess_pipeline_base.BasePreprocessPipeline.create_pipeline_stages \u00b6 <pre><code>create_pipeline_stages(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Set up pipeline stages with proper dependency injection.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_pipeline_base.py</code> <pre><code>def create_pipeline_stages(self, fastvideo_args: FastVideoArgs):\n    \"\"\"Set up pipeline stages with proper dependency injection.\"\"\"\n    self.add_stage(stage_name=\"prompt_encoding_stage\",\n                   stage=TextEncodingStage(\n                       text_encoders=[self.get_module(\"text_encoder\")],\n                       tokenizers=[self.get_module(\"tokenizer\")],\n                   ))\n</code></pre> fastvideo.pipelines.preprocess.preprocess_pipeline_base.BasePreprocessPipeline.create_record \u00b6 <pre><code>create_record(\n    video_name: str,\n    vae_latent: ndarray,\n    text_embedding: ndarray,\n    valid_data: dict[str, Any],\n    idx: int,\n    extra_features: dict[str, Any] | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Create a record for the Parquet dataset.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_pipeline_base.py</code> <pre><code>def create_record(\n        self,\n        video_name: str,\n        vae_latent: np.ndarray,\n        text_embedding: np.ndarray,\n        valid_data: dict[str, Any],\n        idx: int,\n        extra_features: dict[str, Any] | None = None) -&gt; dict[str, Any]:\n    \"\"\"Create a record for the Parquet dataset.\"\"\"\n    record = {\n        \"id\":\n        video_name,\n        \"vae_latent_bytes\":\n        vae_latent.tobytes(),\n        \"vae_latent_shape\":\n        list(vae_latent.shape),\n        \"vae_latent_dtype\":\n        str(vae_latent.dtype),\n        \"text_embedding_bytes\":\n        text_embedding.tobytes(),\n        \"text_embedding_shape\":\n        list(text_embedding.shape),\n        \"text_embedding_dtype\":\n        str(text_embedding.dtype),\n        \"file_name\":\n        video_name,\n        \"caption\":\n        valid_data[\"text\"][idx] if len(valid_data[\"text\"]) &gt; 0 else \"\",\n        \"media_type\":\n        \"video\",\n        \"width\":\n        valid_data[\"pixel_values\"][idx].shape[-2]\n        if len(valid_data[\"pixel_values\"]) &gt; 0 else 0,\n        \"height\":\n        valid_data[\"pixel_values\"][idx].shape[-1]\n        if len(valid_data[\"pixel_values\"]) &gt; 0 else 0,\n        \"num_frames\":\n        vae_latent.shape[1] if len(vae_latent.shape) &gt; 1 else 0,\n        \"duration_sec\":\n        float(valid_data[\"duration\"][idx])\n        if len(valid_data[\"duration\"]) &gt; 0 else 0.0,\n        \"fps\":\n        float(valid_data[\"fps\"][idx])\n        if len(valid_data[\"fps\"]) &gt; 0 else 0.0,\n    }\n    if extra_features:\n        record.update(extra_features)\n    return record\n</code></pre> fastvideo.pipelines.preprocess.preprocess_pipeline_base.BasePreprocessPipeline.create_record_for_schema \u00b6 <pre><code>create_record_for_schema(\n    preprocess_batch: PreprocessBatch,\n    schema: Schema,\n    strict: bool = False,\n) -&gt; dict[str, Any]\n</code></pre> <p>Create a record for the Parquet dataset using a generic schema-based approach.</p> <p>Parameters:</p> Name Type Description Default <code>preprocess_batch</code> <code>PreprocessBatch</code> <p>The batch containing the data to extract</p> required <code>schema</code> <code>Schema</code> <p>PyArrow schema defining the expected fields</p> required <code>strict</code> <code>bool</code> <p>If True, raises an exception when required fields are missing or unfilled</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary record matching the schema</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If strict=True and required fields are missing or unfilled</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_pipeline_base.py</code> <pre><code>def create_record_for_schema(self,\n                             preprocess_batch: PreprocessBatch,\n                             schema: pa.Schema,\n                             strict: bool = False) -&gt; dict[str, Any]:\n    \"\"\"Create a record for the Parquet dataset using a generic schema-based approach.\n\n    Args:\n        preprocess_batch: The batch containing the data to extract\n        schema: PyArrow schema defining the expected fields\n        strict: If True, raises an exception when required fields are missing or unfilled\n\n    Returns:\n        Dictionary record matching the schema\n\n    Raises:\n        ValueError: If strict=True and required fields are missing or unfilled\n    \"\"\"\n    record = {}\n    unfilled_fields = []\n\n    for field in schema.names:\n        field_filled = False\n\n        if field.endswith('_bytes'):\n            # Handle binary tensor data - convert numpy array or tensor to bytes\n            tensor_name = field.replace('_bytes', '')\n            tensor_data = getattr(preprocess_batch, tensor_name, None)\n            if tensor_data is not None:\n                try:\n                    if hasattr(tensor_data, 'numpy'):  # torch tensor\n                        record[field] = tensor_data.cpu().numpy().tobytes()\n                        field_filled = True\n                    elif hasattr(tensor_data, 'tobytes'):  # numpy array\n                        record[field] = tensor_data.tobytes()\n                        field_filled = True\n                    else:\n                        raise ValueError(\n                            f\"Unsupported tensor type for field {field}: {type(tensor_data)}\"\n                        )\n                except Exception as e:\n                    if strict:\n                        raise ValueError(\n                            f\"Failed to convert tensor {tensor_name} to bytes: {e}\"\n                        ) from e\n                    record[field] = b''  # Empty bytes for missing data\n            else:\n                record[field] = b''  # Empty bytes for missing data\n\n        elif field.endswith('_shape'):\n            # Handle tensor shape info\n            tensor_name = field.replace('_shape', '')\n            tensor_data = getattr(preprocess_batch, tensor_name, None)\n            if tensor_data is not None and hasattr(tensor_data, 'shape'):\n                record[field] = list(tensor_data.shape)\n                field_filled = True\n            else:\n                record[field] = []\n\n        elif field.endswith('_dtype'):\n            # Handle tensor dtype info\n            tensor_name = field.replace('_dtype', '')\n            tensor_data = getattr(preprocess_batch, tensor_name, None)\n            if tensor_data is not None and hasattr(tensor_data, 'dtype'):\n                record[field] = str(tensor_data.dtype)\n                field_filled = True\n            else:\n                record[field] = 'unknown'\n\n        elif field in ['width', 'height', 'num_frames']:\n            # Handle integer metadata fields\n            value = getattr(preprocess_batch, field, None)\n            if value is not None:\n                try:\n                    record[field] = int(value)\n                    field_filled = True\n                except (ValueError, TypeError) as e:\n                    if strict:\n                        raise ValueError(\n                            f\"Failed to convert field {field} to int: {e}\"\n                        ) from e\n                    record[field] = 0\n            else:\n                record[field] = 0\n\n        elif field in ['duration_sec', 'fps']:\n            # Handle float metadata fields\n            # Map schema field names to batch attribute names\n            attr_name = 'duration' if field == 'duration_sec' else field\n            value = getattr(preprocess_batch, attr_name, None)\n            if value is not None:\n                try:\n                    record[field] = float(value)\n                    field_filled = True\n                except (ValueError, TypeError) as e:\n                    if strict:\n                        raise ValueError(\n                            f\"Failed to convert field {field} to float: {e}\"\n                        ) from e\n                    record[field] = 0.0\n            else:\n                record[field] = 0.0\n\n        else:\n            # Handle string fields (id, file_name, caption, media_type, etc.)\n            # Map common schema field names to batch attribute names\n            attr_name = field\n            if field == 'caption':\n                attr_name = 'text'\n            elif field == 'file_name':\n                attr_name = 'path'\n            elif field == 'id':\n                # Generate ID from path if available\n                path_value = getattr(preprocess_batch, 'path', None)\n                if path_value:\n                    import os\n                    record[field] = os.path.basename(path_value).split(\n                        '.')[0]\n                    field_filled = True\n                else:\n                    record[field] = \"\"\n                continue\n            elif field == 'media_type':\n                # Determine media type from path\n                path_value = getattr(preprocess_batch, 'path', None)\n                if path_value:\n                    record[field] = 'video' if path_value.endswith(\n                        '.mp4') else 'image'\n                    field_filled = True\n                else:\n                    record[field] = \"\"\n                continue\n\n            value = getattr(preprocess_batch, attr_name, None)\n            if value is not None:\n                record[field] = str(value)\n                field_filled = True\n            else:\n                record[field] = \"\"\n\n        # Track unfilled fields\n        if not field_filled:\n            unfilled_fields.append(field)\n\n    # Handle strict mode\n    if strict and unfilled_fields:\n        raise ValueError(\n            f\"Required fields were not filled: {unfilled_fields}\")\n\n    # Log unfilled fields as warning if not in strict mode\n    if unfilled_fields:\n        logger.warning(\n            \"Some fields were not filled and got default values: %s\",\n            unfilled_fields)\n\n    return record\n</code></pre> fastvideo.pipelines.preprocess.preprocess_pipeline_base.BasePreprocessPipeline.get_extra_features \u00b6 <pre><code>get_extra_features(\n    valid_data: dict[str, Any],\n    fastvideo_args: FastVideoArgs,\n) -&gt; dict[str, Any]\n</code></pre> <p>Get additional features specific to the pipeline type. Override in subclasses.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_pipeline_base.py</code> <pre><code>def get_extra_features(self, valid_data: dict[str, Any],\n                       fastvideo_args: FastVideoArgs) -&gt; dict[str, Any]:\n    \"\"\"Get additional features specific to the pipeline type. Override in subclasses.\"\"\"\n    return {}\n</code></pre> fastvideo.pipelines.preprocess.preprocess_pipeline_base.BasePreprocessPipeline.get_pyarrow_schema \u00b6 <pre><code>get_pyarrow_schema() -&gt; pa.Schema\n</code></pre> <p>Return the PyArrow schema for this pipeline. Must be overridden.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_pipeline_base.py</code> <pre><code>def get_pyarrow_schema(self) -&gt; pa.Schema:\n    \"\"\"Return the PyArrow schema for this pipeline. Must be overridden.\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.pipelines.preprocess.preprocess_pipeline_base.BasePreprocessPipeline.get_schema_fields \u00b6 <pre><code>get_schema_fields() -&gt; list[str]\n</code></pre> <p>Get the schema fields for the pipeline type.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_pipeline_base.py</code> <pre><code>def get_schema_fields(self) -&gt; list[str]:\n    \"\"\"Get the schema fields for the pipeline type.\"\"\"\n    return [f.name for f in self.get_pyarrow_schema()]\n</code></pre> Functions\u00b6 fastvideo.pipelines.preprocess.preprocess_pipeline_i2v \u00b6 <p>I2V Data Preprocessing pipeline implementation.</p> <p>This module contains an implementation of the I2V Data Preprocessing pipeline using the modular pipeline architecture.</p> Classes\u00b6 fastvideo.pipelines.preprocess.preprocess_pipeline_i2v.PreprocessPipeline_I2V \u00b6 <pre><code>PreprocessPipeline_I2V(\n    model_path: str,\n    fastvideo_args: FastVideoArgs | TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>BasePreprocessPipeline</code></p> <p>I2V preprocessing pipeline implementation.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def __init__(self,\n             model_path: str,\n             fastvideo_args: FastVideoArgs | TrainingArgs,\n             required_config_modules: list[str] | None = None,\n             loaded_modules: dict[str, torch.nn.Module] | None = None):\n    \"\"\"\n    Initialize the pipeline. After __init__, the pipeline should be ready to\n    use. The pipeline should be stateless and not hold any batch state.\n    \"\"\"\n    self.fastvideo_args = fastvideo_args\n\n    self.model_path: str = model_path\n    self._stages: list[PipelineStage] = []\n    self._stage_name_mapping: dict[str, PipelineStage] = {}\n\n    if required_config_modules is not None:\n        self._required_config_modules = required_config_modules\n\n    if self._required_config_modules is None:\n        raise NotImplementedError(\n            \"Subclass must set _required_config_modules\")\n\n    maybe_init_distributed_environment_and_model_parallel(\n        fastvideo_args.tp_size, fastvideo_args.sp_size)\n\n    # Torch profiler. Enabled and configured through env vars:\n    # FASTVIDEO_TORCH_PROFILER_DIR=/path/to/save/trace\n    trace_dir = envs.FASTVIDEO_TORCH_PROFILER_DIR\n    self.profiler_controller = get_or_create_profiler(trace_dir)\n    self.profiler = self.profiler_controller.profiler\n\n    self.local_rank = get_world_group().local_rank\n\n    # Load modules directly in initialization\n    logger.info(\"Loading pipeline modules...\")\n    with self.profiler_controller.region(\"profiler_region_model_loading\"):\n        self.modules = self.load_modules(fastvideo_args, loaded_modules)\n</code></pre> Functions\u00b6 fastvideo.pipelines.preprocess.preprocess_pipeline_i2v.PreprocessPipeline_I2V.create_record \u00b6 <pre><code>create_record(\n    video_name: str,\n    vae_latent: ndarray,\n    text_embedding: ndarray,\n    valid_data: dict[str, Any],\n    idx: int,\n    extra_features: dict[str, Any] | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Create a record for the Parquet dataset with CLIP features.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_pipeline_i2v.py</code> <pre><code>def create_record(\n        self,\n        video_name: str,\n        vae_latent: np.ndarray,\n        text_embedding: np.ndarray,\n        valid_data: dict[str, Any],\n        idx: int,\n        extra_features: dict[str, Any] | None = None) -&gt; dict[str, Any]:\n    \"\"\"Create a record for the Parquet dataset with CLIP features.\"\"\"\n    record = super().create_record(video_name=video_name,\n                                   vae_latent=vae_latent,\n                                   text_embedding=text_embedding,\n                                   valid_data=valid_data,\n                                   idx=idx,\n                                   extra_features=extra_features)\n\n    if extra_features and \"clip_feature\" in extra_features:\n        clip_feature = extra_features[\"clip_feature\"]\n        record.update({\n            \"clip_feature_bytes\": clip_feature.tobytes(),\n            \"clip_feature_shape\": list(clip_feature.shape),\n            \"clip_feature_dtype\": str(clip_feature.dtype),\n        })\n    else:\n        record.update({\n            \"clip_feature_bytes\": b\"\",\n            \"clip_feature_shape\": [],\n            \"clip_feature_dtype\": \"\",\n        })\n\n    if extra_features and \"first_frame_latent\" in extra_features:\n        first_frame_latent = extra_features[\"first_frame_latent\"]\n        record.update({\n            \"first_frame_latent_bytes\":\n            first_frame_latent.tobytes(),\n            \"first_frame_latent_shape\":\n            list(first_frame_latent.shape),\n            \"first_frame_latent_dtype\":\n            str(first_frame_latent.dtype),\n        })\n    else:\n        record.update({\n            \"first_frame_latent_bytes\": b\"\",\n            \"first_frame_latent_shape\": [],\n            \"first_frame_latent_dtype\": \"\",\n        })\n\n    if extra_features and \"pil_image\" in extra_features:\n        pil_image = extra_features[\"pil_image\"]\n        record.update({\n            \"pil_image_bytes\": pil_image.tobytes(),\n            \"pil_image_shape\": list(pil_image.shape),\n            \"pil_image_dtype\": str(pil_image.dtype),\n        })\n    else:\n        record.update({\n            \"pil_image_bytes\": b\"\",\n            \"pil_image_shape\": [],\n            \"pil_image_dtype\": \"\",\n        })\n\n    return record\n</code></pre> fastvideo.pipelines.preprocess.preprocess_pipeline_i2v.PreprocessPipeline_I2V.get_pyarrow_schema \u00b6 <pre><code>get_pyarrow_schema()\n</code></pre> <p>Return the PyArrow schema for I2V pipeline.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_pipeline_i2v.py</code> <pre><code>def get_pyarrow_schema(self):\n    \"\"\"Return the PyArrow schema for I2V pipeline.\"\"\"\n    return pyarrow_schema_i2v\n</code></pre> Functions\u00b6 fastvideo.pipelines.preprocess.preprocess_pipeline_ode_trajectory \u00b6 <p>ODE Trajectory Data Preprocessing pipeline implementation.</p> <p>This module contains an implementation of the ODE Trajectory Data Preprocessing pipeline using the modular pipeline architecture.</p> <p>Sec 4.3 of CausVid paper: https://arxiv.org/pdf/2412.07772</p> Classes\u00b6 fastvideo.pipelines.preprocess.preprocess_pipeline_ode_trajectory.PreprocessPipeline_ODE_Trajectory \u00b6 <pre><code>PreprocessPipeline_ODE_Trajectory(\n    model_path: str,\n    fastvideo_args: FastVideoArgs | TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>BasePreprocessPipeline</code></p> <p>ODE Trajectory preprocessing pipeline implementation.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def __init__(self,\n             model_path: str,\n             fastvideo_args: FastVideoArgs | TrainingArgs,\n             required_config_modules: list[str] | None = None,\n             loaded_modules: dict[str, torch.nn.Module] | None = None):\n    \"\"\"\n    Initialize the pipeline. After __init__, the pipeline should be ready to\n    use. The pipeline should be stateless and not hold any batch state.\n    \"\"\"\n    self.fastvideo_args = fastvideo_args\n\n    self.model_path: str = model_path\n    self._stages: list[PipelineStage] = []\n    self._stage_name_mapping: dict[str, PipelineStage] = {}\n\n    if required_config_modules is not None:\n        self._required_config_modules = required_config_modules\n\n    if self._required_config_modules is None:\n        raise NotImplementedError(\n            \"Subclass must set _required_config_modules\")\n\n    maybe_init_distributed_environment_and_model_parallel(\n        fastvideo_args.tp_size, fastvideo_args.sp_size)\n\n    # Torch profiler. Enabled and configured through env vars:\n    # FASTVIDEO_TORCH_PROFILER_DIR=/path/to/save/trace\n    trace_dir = envs.FASTVIDEO_TORCH_PROFILER_DIR\n    self.profiler_controller = get_or_create_profiler(trace_dir)\n    self.profiler = self.profiler_controller.profiler\n\n    self.local_rank = get_world_group().local_rank\n\n    # Load modules directly in initialization\n    logger.info(\"Loading pipeline modules...\")\n    with self.profiler_controller.region(\"profiler_region_model_loading\"):\n        self.modules = self.load_modules(fastvideo_args, loaded_modules)\n</code></pre> Functions\u00b6 fastvideo.pipelines.preprocess.preprocess_pipeline_ode_trajectory.PreprocessPipeline_ODE_Trajectory.create_pipeline_stages \u00b6 <pre><code>create_pipeline_stages(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Set up pipeline stages with proper dependency injection.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_pipeline_ode_trajectory.py</code> <pre><code>def create_pipeline_stages(self, fastvideo_args: FastVideoArgs):\n    \"\"\"Set up pipeline stages with proper dependency injection.\"\"\"\n    assert fastvideo_args.pipeline_config.flow_shift == 5\n    self.modules[\"scheduler\"] = SelfForcingFlowMatchScheduler(\n        shift=fastvideo_args.pipeline_config.flow_shift,\n        sigma_min=0.0,\n        extra_one_step=True)\n    self.modules[\"scheduler\"].set_timesteps(num_inference_steps=48,\n                                            denoising_strength=1.0)\n\n    self.add_stage(stage_name=\"input_validation_stage\",\n                   stage=InputValidationStage())\n    self.add_stage(stage_name=\"prompt_encoding_stage\",\n                   stage=TextEncodingStage(\n                       text_encoders=[self.get_module(\"text_encoder\")],\n                       tokenizers=[self.get_module(\"tokenizer\")],\n                   ))\n    self.add_stage(stage_name=\"timestep_preparation_stage\",\n                   stage=TimestepPreparationStage(\n                       scheduler=self.get_module(\"scheduler\")))\n    self.add_stage(stage_name=\"latent_preparation_stage\",\n                   stage=LatentPreparationStage(\n                       scheduler=self.get_module(\"scheduler\"),\n                       transformer=self.get_module(\"transformer\", None)))\n    self.add_stage(stage_name=\"denoising_stage\",\n                   stage=DenoisingStage(\n                       transformer=self.get_module(\"transformer\"),\n                       scheduler=self.get_module(\"scheduler\"),\n                       pipeline=self,\n                   ))\n    self.add_stage(stage_name=\"decoding_stage\",\n                   stage=DecodingStage(vae=self.get_module(\"vae\")))\n</code></pre> fastvideo.pipelines.preprocess.preprocess_pipeline_ode_trajectory.PreprocessPipeline_ODE_Trajectory.get_pyarrow_schema \u00b6 <pre><code>get_pyarrow_schema() -&gt; pa.Schema\n</code></pre> <p>Return the PyArrow schema for ODE Trajectory pipeline.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_pipeline_ode_trajectory.py</code> <pre><code>def get_pyarrow_schema(self) -&gt; pa.Schema:\n    \"\"\"Return the PyArrow schema for ODE Trajectory pipeline.\"\"\"\n    return pyarrow_schema_ode_trajectory_text_only\n</code></pre> fastvideo.pipelines.preprocess.preprocess_pipeline_ode_trajectory.PreprocessPipeline_ODE_Trajectory.preprocess_text_and_trajectory \u00b6 <pre><code>preprocess_text_and_trajectory(\n    fastvideo_args: FastVideoArgs, args\n)\n</code></pre> <p>Preprocess text-only data and generate trajectory information.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_pipeline_ode_trajectory.py</code> <pre><code>def preprocess_text_and_trajectory(self, fastvideo_args: FastVideoArgs,\n                                   args):\n    \"\"\"Preprocess text-only data and generate trajectory information.\"\"\"\n\n    for batch_idx, data in enumerate(self.pbar):\n        if data is None:\n            continue\n\n        with torch.inference_mode():\n            # For text-only processing, we only need text data\n            # Filter out samples without text\n            valid_indices = []\n            for i, text in enumerate(data[\"text\"]):\n                if text and text.strip():  # Check if text is not empty\n                    valid_indices.append(i)\n            self.num_processed_samples += len(valid_indices)\n\n            if not valid_indices:\n                continue\n\n            # Create new batch with only valid samples (text-only)\n            valid_data = {\n                \"text\": [data[\"text\"][i] for i in valid_indices],\n                \"path\": [data[\"path\"][i] for i in valid_indices],\n            }\n\n            # Add fps and duration if available in data\n            if \"fps\" in data:\n                valid_data[\"fps\"] = [data[\"fps\"][i] for i in valid_indices]\n            if \"duration\" in data:\n                valid_data[\"duration\"] = [\n                    data[\"duration\"][i] for i in valid_indices\n                ]\n\n            batch_captions = valid_data[\"text\"]\n            # Encode text using the standalone TextEncodingStage API\n            prompt_embeds_list, prompt_masks_list = self.prompt_encoding_stage.encode_text(\n                batch_captions,\n                fastvideo_args,\n                encoder_index=[0],\n                return_attention_mask=True,\n            )\n            prompt_embeds = prompt_embeds_list[0]\n            prompt_attention_masks = prompt_masks_list[0]\n            assert prompt_embeds.shape[0] == prompt_attention_masks.shape[0]\n\n            sampling_params = SamplingParam.from_pretrained(args.model_path)\n\n            # encode negative prompt for trajectory collection\n            if sampling_params.guidance_scale &gt; 1 and sampling_params.negative_prompt is not None:\n                negative_prompt_embeds_list, negative_prompt_masks_list = self.prompt_encoding_stage.encode_text(\n                    sampling_params.negative_prompt,\n                    fastvideo_args,\n                    encoder_index=[0],\n                    return_attention_mask=True,\n                )\n                negative_prompt_embed = negative_prompt_embeds_list[0][0]\n                negative_prompt_attention_mask = negative_prompt_masks_list[\n                    0][0]\n            else:\n                negative_prompt_embed = None\n                negative_prompt_attention_mask = None\n\n            trajectory_latents = []\n            trajectory_timesteps = []\n            trajectory_decoded = []\n\n            for i, (prompt_embed, prompt_attention_mask) in enumerate(\n                    zip(prompt_embeds, prompt_attention_masks,\n                        strict=False)):\n                prompt_embed = prompt_embed.unsqueeze(0)\n                prompt_attention_mask = prompt_attention_mask.unsqueeze(0)\n\n                # Collect the trajectory data (text-to-video generation)\n                batch = ForwardBatch(**shallow_asdict(sampling_params), )\n                batch.prompt_embeds = [prompt_embed]\n                batch.prompt_attention_mask = [prompt_attention_mask]\n                batch.negative_prompt_embeds = [negative_prompt_embed]\n                batch.negative_attention_mask = [\n                    negative_prompt_attention_mask\n                ]\n                batch.num_inference_steps = 48\n                batch.return_trajectory_latents = True\n                # Enabling this will save the decoded trajectory videos.\n                # Used for debugging.\n                batch.return_trajectory_decoded = False\n                batch.height = args.max_height\n                batch.width = args.max_width\n                batch.fps = args.train_fps\n                batch.guidance_scale = 6.0\n                batch.do_classifier_free_guidance = True\n\n                result_batch = self.input_validation_stage(\n                    batch, fastvideo_args)\n                result_batch = self.timestep_preparation_stage(\n                    batch, fastvideo_args)\n                result_batch = self.latent_preparation_stage(\n                    result_batch, fastvideo_args)\n                result_batch = self.denoising_stage(result_batch,\n                                                    fastvideo_args)\n                result_batch = self.decoding_stage(result_batch,\n                                                   fastvideo_args)\n\n                trajectory_latents.append(\n                    result_batch.trajectory_latents.cpu())\n                trajectory_timesteps.append(\n                    result_batch.trajectory_timesteps.cpu())\n                trajectory_decoded.append(result_batch.trajectory_decoded)\n\n            # Prepare extra features for text-only processing\n            extra_features = {\n                \"trajectory_latents\": trajectory_latents,\n                \"trajectory_timesteps\": trajectory_timesteps\n            }\n\n            if batch.return_trajectory_decoded:\n                for i, decoded_frames in enumerate(trajectory_decoded):\n                    for j, decoded_frame in enumerate(decoded_frames):\n                        save_decoded_latents_as_video(\n                            decoded_frame,\n                            f\"decoded_videos/trajectory_decoded_{i}_{j}.mp4\",\n                            args.train_fps)\n\n            # Prepare batch data for Parquet dataset\n            batch_data: list[dict[str, Any]] = []\n\n            # Add progress bar for saving outputs\n            save_pbar = tqdm(enumerate(valid_data[\"path\"]),\n                             desc=\"Saving outputs\",\n                             unit=\"item\",\n                             leave=False)\n\n            for idx, video_path in save_pbar:\n                video_name = os.path.basename(video_path).split(\".\")[0]\n\n                # Convert tensors to numpy arrays\n                text_embedding = prompt_embeds[idx].cpu().numpy()\n\n                # Get extra features for this sample\n                sample_extra_features = {}\n                if extra_features:\n                    for key, value in extra_features.items():\n                        if isinstance(value, torch.Tensor):\n                            sample_extra_features[key] = value[idx].cpu(\n                            ).numpy()\n                        else:\n                            assert isinstance(value, list)\n                            if isinstance(value[idx], torch.Tensor):\n                                sample_extra_features[key] = value[idx].cpu(\n                                ).float().numpy()\n                            else:\n                                sample_extra_features[key] = value[idx]\n\n                # Create record for Parquet dataset (text-only ODE schema)\n                record: dict[str, Any] = ode_text_only_record_creator(\n                    video_name=video_name,\n                    text_embedding=text_embedding,\n                    caption=valid_data[\"text\"][idx],\n                    trajectory_latents=sample_extra_features[\n                        \"trajectory_latents\"],\n                    trajectory_timesteps=sample_extra_features[\n                        \"trajectory_timesteps\"],\n                )\n                batch_data.append(record)\n\n            if batch_data:\n                write_pbar = tqdm(total=1,\n                                  desc=\"Writing to Parquet dataset\",\n                                  unit=\"batch\")\n                table = records_to_table(batch_data,\n                                         self.get_pyarrow_schema())\n                write_pbar.update(1)\n                write_pbar.close()\n\n                if not hasattr(self, 'dataset_writer'):\n                    self.dataset_writer = ParquetDatasetWriter(\n                        out_dir=self.combined_parquet_dir,\n                        samples_per_file=args.samples_per_file,\n                    )\n                self.dataset_writer.append_table(table)\n\n                logger.info(\"Collected batch with %s samples\", len(table))\n\n            if self.num_processed_samples &gt;= args.flush_frequency:\n                written = self.dataset_writer.flush()\n                logger.info(\"Flushed %s samples to parquet\", written)\n                self.num_processed_samples = 0\n\n    # Final flush for any remaining samples\n    if hasattr(self, 'dataset_writer'):\n        written = self.dataset_writer.flush(write_remainder=True)\n        if written:\n            logger.info(\"Final flush wrote %s samples\", written)\n</code></pre> Functions\u00b6 fastvideo.pipelines.preprocess.preprocess_pipeline_t2v \u00b6 <p>T2V Data Preprocessing pipeline implementation.</p> <p>This module contains an implementation of the T2V Data Preprocessing pipeline using the modular pipeline architecture.</p> Classes\u00b6 fastvideo.pipelines.preprocess.preprocess_pipeline_t2v.PreprocessPipeline_T2V \u00b6 <pre><code>PreprocessPipeline_T2V(\n    model_path: str,\n    fastvideo_args: FastVideoArgs | TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>BasePreprocessPipeline</code></p> <p>T2V preprocessing pipeline implementation.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def __init__(self,\n             model_path: str,\n             fastvideo_args: FastVideoArgs | TrainingArgs,\n             required_config_modules: list[str] | None = None,\n             loaded_modules: dict[str, torch.nn.Module] | None = None):\n    \"\"\"\n    Initialize the pipeline. After __init__, the pipeline should be ready to\n    use. The pipeline should be stateless and not hold any batch state.\n    \"\"\"\n    self.fastvideo_args = fastvideo_args\n\n    self.model_path: str = model_path\n    self._stages: list[PipelineStage] = []\n    self._stage_name_mapping: dict[str, PipelineStage] = {}\n\n    if required_config_modules is not None:\n        self._required_config_modules = required_config_modules\n\n    if self._required_config_modules is None:\n        raise NotImplementedError(\n            \"Subclass must set _required_config_modules\")\n\n    maybe_init_distributed_environment_and_model_parallel(\n        fastvideo_args.tp_size, fastvideo_args.sp_size)\n\n    # Torch profiler. Enabled and configured through env vars:\n    # FASTVIDEO_TORCH_PROFILER_DIR=/path/to/save/trace\n    trace_dir = envs.FASTVIDEO_TORCH_PROFILER_DIR\n    self.profiler_controller = get_or_create_profiler(trace_dir)\n    self.profiler = self.profiler_controller.profiler\n\n    self.local_rank = get_world_group().local_rank\n\n    # Load modules directly in initialization\n    logger.info(\"Loading pipeline modules...\")\n    with self.profiler_controller.region(\"profiler_region_model_loading\"):\n        self.modules = self.load_modules(fastvideo_args, loaded_modules)\n</code></pre> Functions\u00b6 fastvideo.pipelines.preprocess.preprocess_pipeline_t2v.PreprocessPipeline_T2V.get_pyarrow_schema \u00b6 <pre><code>get_pyarrow_schema()\n</code></pre> <p>Return the PyArrow schema for T2V pipeline.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_pipeline_t2v.py</code> <pre><code>def get_pyarrow_schema(self):\n    \"\"\"Return the PyArrow schema for T2V pipeline.\"\"\"\n    return pyarrow_schema_t2v\n</code></pre> fastvideo.pipelines.preprocess.preprocess_pipeline_text \u00b6 <p>Text-only Data Preprocessing pipeline implementation.</p> <p>This module contains an implementation of the Text-only Data Preprocessing pipeline using the modular pipeline architecture, based on the ODE Trajectory preprocessing.</p> Classes\u00b6 fastvideo.pipelines.preprocess.preprocess_pipeline_text.PreprocessPipeline_Text \u00b6 <pre><code>PreprocessPipeline_Text(\n    model_path: str,\n    fastvideo_args: FastVideoArgs | TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>BasePreprocessPipeline</code></p> <p>Text-only preprocessing pipeline implementation.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def __init__(self,\n             model_path: str,\n             fastvideo_args: FastVideoArgs | TrainingArgs,\n             required_config_modules: list[str] | None = None,\n             loaded_modules: dict[str, torch.nn.Module] | None = None):\n    \"\"\"\n    Initialize the pipeline. After __init__, the pipeline should be ready to\n    use. The pipeline should be stateless and not hold any batch state.\n    \"\"\"\n    self.fastvideo_args = fastvideo_args\n\n    self.model_path: str = model_path\n    self._stages: list[PipelineStage] = []\n    self._stage_name_mapping: dict[str, PipelineStage] = {}\n\n    if required_config_modules is not None:\n        self._required_config_modules = required_config_modules\n\n    if self._required_config_modules is None:\n        raise NotImplementedError(\n            \"Subclass must set _required_config_modules\")\n\n    maybe_init_distributed_environment_and_model_parallel(\n        fastvideo_args.tp_size, fastvideo_args.sp_size)\n\n    # Torch profiler. Enabled and configured through env vars:\n    # FASTVIDEO_TORCH_PROFILER_DIR=/path/to/save/trace\n    trace_dir = envs.FASTVIDEO_TORCH_PROFILER_DIR\n    self.profiler_controller = get_or_create_profiler(trace_dir)\n    self.profiler = self.profiler_controller.profiler\n\n    self.local_rank = get_world_group().local_rank\n\n    # Load modules directly in initialization\n    logger.info(\"Loading pipeline modules...\")\n    with self.profiler_controller.region(\"profiler_region_model_loading\"):\n        self.modules = self.load_modules(fastvideo_args, loaded_modules)\n</code></pre> Functions\u00b6 fastvideo.pipelines.preprocess.preprocess_pipeline_text.PreprocessPipeline_Text.create_pipeline_stages \u00b6 <pre><code>create_pipeline_stages(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Set up pipeline stages with proper dependency injection.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_pipeline_text.py</code> <pre><code>def create_pipeline_stages(self, fastvideo_args: FastVideoArgs):\n    \"\"\"Set up pipeline stages with proper dependency injection.\"\"\"\n    self.add_stage(stage_name=\"prompt_encoding_stage\",\n                   stage=TextEncodingStage(\n                       text_encoders=[self.get_module(\"text_encoder\")],\n                       tokenizers=[self.get_module(\"tokenizer\")],\n                   ))\n</code></pre> fastvideo.pipelines.preprocess.preprocess_pipeline_text.PreprocessPipeline_Text.get_pyarrow_schema \u00b6 <pre><code>get_pyarrow_schema()\n</code></pre> <p>Return the PyArrow schema for text-only pipeline.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_pipeline_text.py</code> <pre><code>def get_pyarrow_schema(self):\n    \"\"\"Return the PyArrow schema for text-only pipeline.\"\"\"\n    return pyarrow_schema_text_only\n</code></pre> fastvideo.pipelines.preprocess.preprocess_pipeline_text.PreprocessPipeline_Text.preprocess_text_only \u00b6 <pre><code>preprocess_text_only(fastvideo_args: FastVideoArgs, args)\n</code></pre> <p>Preprocess text-only data.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_pipeline_text.py</code> <pre><code>def preprocess_text_only(self, fastvideo_args: FastVideoArgs, args):\n    \"\"\"Preprocess text-only data.\"\"\"\n\n    for batch_idx, data in enumerate(self.pbar):\n        if data is None:\n            continue\n\n        with torch.inference_mode():\n            # For text-only processing, we only need text data\n            # Filter out samples without text\n            valid_indices = []\n            for i, text in enumerate(data[\"text\"]):\n                if text and text.strip():  # Check if text is not empty\n                    valid_indices.append(i)\n            self.num_processed_samples += len(valid_indices)\n\n            if not valid_indices:\n                continue\n\n            # Create new batch with only valid samples (text-only)\n            valid_data = {\n                \"text\": [data[\"text\"][i] for i in valid_indices],\n                \"path\": [data[\"path\"][i] for i in valid_indices],\n            }\n\n            batch_captions = valid_data[\"text\"]\n            # Encode text using the standalone TextEncodingStage API\n            prompt_embeds_list, prompt_masks_list = self.prompt_encoding_stage.encode_text(\n                batch_captions,\n                fastvideo_args,\n                encoder_index=[0],\n                return_attention_mask=True,\n            )\n            prompt_embeds = prompt_embeds_list[0]\n            prompt_attention_masks = prompt_masks_list[0]\n            assert prompt_embeds.shape[0] == prompt_attention_masks.shape[0]\n\n            logger.info(\"===== prompt_embeds: %s\", prompt_embeds.shape)\n            logger.info(\"===== prompt_attention_masks: %s\",\n                        prompt_attention_masks.shape)\n\n            # Prepare batch data for Parquet dataset\n            batch_data = []\n\n            # Add progress bar for saving outputs\n            save_pbar = tqdm(enumerate(valid_data[\"path\"]),\n                             desc=\"Saving outputs\",\n                             unit=\"item\",\n                             leave=False)\n\n            for idx, text_path in save_pbar:\n                text_name = os.path.basename(text_path).split(\".\")[0]\n\n                # Convert tensors to numpy arrays\n                text_embedding = prompt_embeds[idx].cpu().numpy()\n\n                # Create record for Parquet dataset (text-only schema)\n                record = text_only_record_creator(\n                    text_name=text_name,\n                    text_embedding=text_embedding,\n                    caption=valid_data[\"text\"][idx],\n                )\n                batch_data.append(record)\n\n            if batch_data:\n                write_pbar = tqdm(total=1,\n                                  desc=\"Writing to Parquet dataset\",\n                                  unit=\"batch\")\n                table = records_to_table(batch_data,\n                                         pyarrow_schema_text_only)\n                write_pbar.update(1)\n                write_pbar.close()\n\n                if not hasattr(self, 'dataset_writer'):\n                    self.dataset_writer = ParquetDatasetWriter(\n                        out_dir=self.combined_parquet_dir,\n                        samples_per_file=args.samples_per_file,\n                    )\n                self.dataset_writer.append_table(table)\n\n                logger.info(\"Collected batch with %s samples\", len(table))\n\n            if self.num_processed_samples &gt;= args.flush_frequency:\n                written = self.dataset_writer.flush()\n                logger.info(\"Flushed %s samples to parquet\", written)\n                self.num_processed_samples = 0\n\n    # Final flush for any remaining samples\n    if hasattr(self, 'dataset_writer'):\n        written = self.dataset_writer.flush(write_remainder=True)\n        if written:\n            logger.info(\"Final flush wrote %s samples\", written)\n</code></pre> Functions\u00b6 fastvideo.pipelines.preprocess.preprocess_stages \u00b6 Classes\u00b6 fastvideo.pipelines.preprocess.preprocess_stages.TextTransformStage \u00b6 <pre><code>TextTransformStage(\n    cfg_uncondition_drop_rate: float, seed: int\n)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Process text data according to the cfg rate.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_stages.py</code> <pre><code>def __init__(self, cfg_uncondition_drop_rate: float, seed: int) -&gt; None:\n    self.cfg_rate = cfg_uncondition_drop_rate\n    self.rng = random.Random(seed)\n</code></pre> fastvideo.pipelines.preprocess.preprocess_stages.VideoTransformStage \u00b6 <pre><code>VideoTransformStage(\n    train_fps: int,\n    num_frames: int,\n    max_height: int,\n    max_width: int,\n    do_temporal_sample: bool,\n)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Crop a video in temporal dimension.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_stages.py</code> <pre><code>def __init__(self, train_fps: int, num_frames: int, max_height: int,\n             max_width: int, do_temporal_sample: bool) -&gt; None:\n    self.train_fps = train_fps\n    self.num_frames = num_frames\n    if do_temporal_sample:\n        self.temporal_sample_fn: Callable | None = TemporalRandomCrop(\n            num_frames)\n    else:\n        self.temporal_sample_fn = None\n\n    self.video_transform = transforms.Compose([\n        CenterCropResizeVideo((max_height, max_width)),\n    ])\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.stages","title":"fastvideo.pipelines.stages","text":"<p>Pipeline stages for diffusion models.</p> <p>This package contains the various stages that can be composed to create complete diffusion pipelines.</p>"},{"location":"api/fastvideo/#fastvideo.pipelines.stages-classes","title":"Classes","text":"fastvideo.pipelines.stages.CausalDMDDenosingStage \u00b6 <pre><code>CausalDMDDenosingStage(\n    transformer, scheduler, transformer_2=None\n)\n</code></pre> <p>               Bases: <code>DenoisingStage</code></p> <p>Denoising stage for causal diffusion.</p> Source code in <code>fastvideo/pipelines/stages/causal_denoising.py</code> <pre><code>def __init__(self, transformer, scheduler, transformer_2=None) -&gt; None:\n    super().__init__(transformer, scheduler, transformer_2)\n    # KV and cross-attention cache state (initialized on first forward)\n    self.transformer = transformer\n    self.transformer_2 = transformer_2\n    self.kv_cache1: list | None = None\n    self.crossattn_cache: list | None = None\n    # Model-dependent constants (aligned with causal_inference.py assumptions)\n    self.num_transformer_blocks = len(self.transformer.blocks)\n    self.num_frames_per_block = self.transformer.config.arch_config.num_frames_per_block\n    self.sliding_window_num_frames = self.transformer.config.arch_config.sliding_window_num_frames\n\n    try:\n        self.local_attn_size = getattr(self.transformer.model,\n                                       \"local_attn_size\",\n                                       -1)  # type: ignore\n    except Exception:\n        self.local_attn_size = -1\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.CausalDMDDenosingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify denoising stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/causal_denoising.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify denoising stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds, V.list_not_empty)\n    result.add_check(\"image_embeds\", batch.image_embeds, V.is_list)\n    result.add_check(\"image_latent\", batch.image_latent,\n                     V.none_or_tensor_with_dims(5))\n    result.add_check(\"num_inference_steps\", batch.num_inference_steps,\n                     V.positive_int)\n    result.add_check(\"guidance_scale\", batch.guidance_scale,\n                     V.positive_float)\n    result.add_check(\"eta\", batch.eta, V.non_negative_float)\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    result.add_check(\"do_classifier_free_guidance\",\n                     batch.do_classifier_free_guidance, V.bool_value)\n    result.add_check(\n        \"negative_prompt_embeds\", batch.negative_prompt_embeds, lambda x:\n        not batch.do_classifier_free_guidance or V.list_not_empty(x))\n    return result\n</code></pre> fastvideo.pipelines.stages.ConditioningStage \u00b6 <p>               Bases: <code>PipelineStage</code></p> <p>Stage for applying conditioning to the diffusion process.</p> <p>This stage handles the application of conditioning, such as classifier-free guidance, to the diffusion process.</p> Functions\u00b6 fastvideo.pipelines.stages.ConditioningStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Apply conditioning to the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with applied conditioning.</p> Source code in <code>fastvideo/pipelines/stages/conditioning.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Apply conditioning to the diffusion process.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with applied conditioning.\n    \"\"\"\n    # TODO!!\n    if not batch.do_classifier_free_guidance:\n        return batch\n    else:\n        return batch\n\n    logger.info(\"batch.negative_prompt_embeds: %s\",\n                batch.negative_prompt_embeds)\n    logger.info(\"do_classifier_free_guidance: %s\",\n                batch.do_classifier_free_guidance)\n    logger.info(\"cfg_scale: %s\", batch.guidance_scale)\n\n    # Ensure negative prompt embeddings are available\n    assert batch.negative_prompt_embeds is not None, (\n        \"Negative prompt embeddings are required for classifier-free guidance\"\n    )\n\n    # Concatenate primary embeddings and masks\n    batch.prompt_embeds = torch.cat(\n        [batch.negative_prompt_embeds, batch.prompt_embeds])\n    if batch.attention_mask is not None:\n        batch.attention_mask = torch.cat(\n            [batch.negative_attention_mask, batch.attention_mask])\n\n    # Concatenate secondary embeddings and masks if present\n    if batch.prompt_embeds_2 is not None:\n        batch.prompt_embeds_2 = torch.cat(\n            [batch.negative_prompt_embeds_2, batch.prompt_embeds_2])\n    if batch.attention_mask_2 is not None:\n        batch.attention_mask_2 = torch.cat(\n            [batch.negative_attention_mask_2, batch.attention_mask_2])\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.ConditioningStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify conditioning stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/conditioning.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify conditioning stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"do_classifier_free_guidance\",\n                     batch.do_classifier_free_guidance, V.bool_value)\n    result.add_check(\"guidance_scale\", batch.guidance_scale,\n                     V.positive_float)\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds, V.list_not_empty)\n    result.add_check(\n        \"negative_prompt_embeds\", batch.negative_prompt_embeds, lambda x:\n        not batch.do_classifier_free_guidance or V.list_not_empty(x))\n    return result\n</code></pre> fastvideo.pipelines.stages.ConditioningStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify conditioning stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/conditioning.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify conditioning stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds, V.list_not_empty)\n    return result\n</code></pre> fastvideo.pipelines.stages.DecodingStage \u00b6 <pre><code>DecodingStage(vae, pipeline=None)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for decoding latent representations into pixel space.</p> <p>This stage handles the decoding of latent representations into the final output format (e.g., pixel values).</p> Source code in <code>fastvideo/pipelines/stages/decoding.py</code> <pre><code>def __init__(self, vae, pipeline=None) -&gt; None:\n    self.vae: ParallelTiledVAE = vae\n    self.pipeline = weakref.ref(pipeline) if pipeline else None\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.DecodingStage.decode \u00b6 <pre><code>decode(\n    latents: Tensor, fastvideo_args: FastVideoArgs\n) -&gt; torch.Tensor\n</code></pre> <p>Decode latent representations into pixel space using VAE.</p> <p>Parameters:</p> Name Type Description Default <code>latents</code> <code>Tensor</code> <p>Input latent tensor with shape (batch, channels, frames, height_latents, width_latents)</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>Configuration containing: - disable_autocast: Whether to disable automatic mixed precision (default: False) - pipeline_config.vae_precision: VAE computation precision (\"fp32\", \"fp16\", \"bf16\") - pipeline_config.vae_tiling: Whether to enable VAE tiling for memory efficiency</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Decoded video tensor with shape (batch, channels, frames, height, width), </p> <code>Tensor</code> <p>normalized to [0, 1] range and moved to CPU as float32</p> Source code in <code>fastvideo/pipelines/stages/decoding.py</code> <pre><code>@torch.no_grad()\ndef decode(self, latents: torch.Tensor,\n           fastvideo_args: FastVideoArgs) -&gt; torch.Tensor:\n    \"\"\"\n    Decode latent representations into pixel space using VAE.\n\n    Args:\n        latents: Input latent tensor with shape (batch, channels, frames, height_latents, width_latents)\n        fastvideo_args: Configuration containing:\n            - disable_autocast: Whether to disable automatic mixed precision (default: False)\n            - pipeline_config.vae_precision: VAE computation precision (\"fp32\", \"fp16\", \"bf16\")\n            - pipeline_config.vae_tiling: Whether to enable VAE tiling for memory efficiency\n\n    Returns:\n        Decoded video tensor with shape (batch, channels, frames, height, width), \n        normalized to [0, 1] range and moved to CPU as float32\n    \"\"\"\n    self.vae = self.vae.to(get_local_torch_device())\n    latents = latents.to(get_local_torch_device())\n\n    # Setup VAE precision\n    vae_dtype = PRECISION_TO_TYPE[\n        fastvideo_args.pipeline_config.vae_precision]\n    vae_autocast_enabled = (\n        vae_dtype != torch.float32) and not fastvideo_args.disable_autocast\n\n    if isinstance(self.vae.scaling_factor, torch.Tensor):\n        latents = latents / self.vae.scaling_factor.to(\n            latents.device, latents.dtype)\n    else:\n        latents = latents / self.vae.scaling_factor\n\n    # Apply shifting if needed\n    if (hasattr(self.vae, \"shift_factor\")\n            and self.vae.shift_factor is not None):\n        if isinstance(self.vae.shift_factor, torch.Tensor):\n            latents += self.vae.shift_factor.to(latents.device,\n                                                latents.dtype)\n        else:\n            latents += self.vae.shift_factor\n\n    # Decode latents\n    with torch.autocast(device_type=\"cuda\",\n                        dtype=vae_dtype,\n                        enabled=vae_autocast_enabled):\n        if fastvideo_args.pipeline_config.vae_tiling:\n            self.vae.enable_tiling()\n        # if fastvideo_args.vae_sp:\n        #     self.vae.enable_parallel()\n        if not vae_autocast_enabled:\n            latents = latents.to(vae_dtype)\n        image = self.vae.decode(latents)\n\n    # Normalize image to [0, 1] range\n    image = (image / 2 + 0.5).clamp(0, 1)\n    return image\n</code></pre> fastvideo.pipelines.stages.DecodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Decode latent representations into pixel space.</p> <p>This method processes the batch through the VAE decoder, converting latent representations to pixel-space video/images. It also optionally decodes trajectory latents for visualization purposes.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch containing: - latents: Tensor to decode (batch, channels, frames, height_latents, width_latents) - return_trajectory_decoded (optional): Flag to decode trajectory latents - trajectory_latents (optional): Latents at different timesteps - trajectory_timesteps (optional): Corresponding timesteps</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>Configuration containing: - output_type: \"latent\" to skip decoding, otherwise decode to pixels - vae_cpu_offload: Whether to offload VAE to CPU after decoding - model_loaded: Track VAE loading state - model_paths: Path to VAE model if loading needed</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>Modified batch with: - output: Decoded frames (batch, channels, frames, height, width) as CPU float32 - trajectory_decoded (if requested): List of decoded frames per timestep</p> Source code in <code>fastvideo/pipelines/stages/decoding.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Decode latent representations into pixel space.\n\n    This method processes the batch through the VAE decoder, converting latent\n    representations to pixel-space video/images. It also optionally decodes\n    trajectory latents for visualization purposes.\n\n    Args:\n        batch: The current batch containing:\n            - latents: Tensor to decode (batch, channels, frames, height_latents, width_latents)\n            - return_trajectory_decoded (optional): Flag to decode trajectory latents\n            - trajectory_latents (optional): Latents at different timesteps\n            - trajectory_timesteps (optional): Corresponding timesteps\n        fastvideo_args: Configuration containing:\n            - output_type: \"latent\" to skip decoding, otherwise decode to pixels\n            - vae_cpu_offload: Whether to offload VAE to CPU after decoding\n            - model_loaded: Track VAE loading state\n            - model_paths: Path to VAE model if loading needed\n\n    Returns:\n        Modified batch with:\n            - output: Decoded frames (batch, channels, frames, height, width) as CPU float32\n            - trajectory_decoded (if requested): List of decoded frames per timestep\n    \"\"\"\n    # load vae if not already loaded (used for memory constrained devices)\n    pipeline = self.pipeline() if self.pipeline else None\n    if not fastvideo_args.model_loaded[\"vae\"]:\n        loader = VAELoader()\n        self.vae = loader.load(fastvideo_args.model_paths[\"vae\"],\n                               fastvideo_args)\n        if pipeline:\n            pipeline.add_module(\"vae\", self.vae)\n        fastvideo_args.model_loaded[\"vae\"] = True\n\n    if fastvideo_args.output_type == \"latent\":\n        frames = batch.latents\n    else:\n        frames = self.decode(batch.latents, fastvideo_args)\n\n    # decode trajectory latents if needed\n    if batch.return_trajectory_decoded:\n        batch.trajectory_decoded = []\n        assert batch.trajectory_latents is not None, \"batch should have trajectory latents\"\n        for idx in range(batch.trajectory_latents.shape[1]):\n            # batch.trajectory_latents is [batch_size, timesteps, channels, frames, height, width]\n            cur_latent = batch.trajectory_latents[:, idx, :, :, :, :]\n            cur_timestep = batch.trajectory_timesteps[idx]\n            logger.info(\"decoding trajectory latent for timestep: %s\",\n                        cur_timestep)\n            decoded_frames = self.decode(cur_latent, fastvideo_args)\n            batch.trajectory_decoded.append(decoded_frames.cpu().float())\n\n    # Convert to CPU float32 for compatibility\n    frames = frames.cpu().float()\n\n    # Update batch with decoded image\n    batch.output = frames\n\n    # Offload models if needed\n    if hasattr(self, 'maybe_free_model_hooks'):\n        self.maybe_free_model_hooks()\n\n    if fastvideo_args.vae_cpu_offload:\n        self.vae.to(\"cpu\")\n\n    if torch.backends.mps.is_available():\n        del self.vae\n        if pipeline is not None and \"vae\" in pipeline.modules:\n            del pipeline.modules[\"vae\"]\n        fastvideo_args.model_loaded[\"vae\"] = False\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.DecodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify decoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/decoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify decoding stage inputs.\"\"\"\n    result = VerificationResult()\n    # Denoised latents for VAE decoding: [batch_size, channels, frames, height_latents, width_latents]\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre> fastvideo.pipelines.stages.DecodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify decoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/decoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify decoding stage outputs.\"\"\"\n    result = VerificationResult()\n    # Decoded video/images: [batch_size, channels, frames, height, width]\n    result.add_check(\"output\", batch.output, [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre> fastvideo.pipelines.stages.DenoisingStage \u00b6 <pre><code>DenoisingStage(\n    transformer,\n    scheduler,\n    pipeline=None,\n    transformer_2=None,\n    vae=None,\n)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for running the denoising loop in diffusion pipelines.</p> <p>This stage handles the iterative denoising process that transforms the initial noise into the final output.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def __init__(self,\n             transformer,\n             scheduler,\n             pipeline=None,\n             transformer_2=None,\n             vae=None) -&gt; None:\n    super().__init__()\n    self.transformer = transformer\n    self.transformer_2 = transformer_2\n    self.scheduler = scheduler\n    self.vae = vae\n    self.pipeline = weakref.ref(pipeline) if pipeline else None\n    attn_head_size = self.transformer.hidden_size // self.transformer.num_attention_heads\n    self.attn_backend = get_attn_backend(\n        head_size=attn_head_size,\n        dtype=torch.float16,  # TODO(will): hack\n        supported_attention_backends=(\n            AttentionBackendEnum.SLIDING_TILE_ATTN,\n            AttentionBackendEnum.VIDEO_SPARSE_ATTN,\n            AttentionBackendEnum.VMOBA_ATTN,\n            AttentionBackendEnum.FLASH_ATTN,\n            AttentionBackendEnum.TORCH_SDPA,\n            AttentionBackendEnum.SAGE_ATTN_THREE)  # hack\n    )\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.DenoisingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Run the denoising loop.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with denoised latents.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Run the denoising loop.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with denoised latents.\n    \"\"\"\n    pipeline = self.pipeline() if self.pipeline else None\n    if not fastvideo_args.model_loaded[\"transformer\"]:\n        loader = TransformerLoader()\n        self.transformer = loader.load(\n            fastvideo_args.model_paths[\"transformer\"], fastvideo_args)\n        if pipeline:\n            pipeline.add_module(\"transformer\", self.transformer)\n        fastvideo_args.model_loaded[\"transformer\"] = True\n\n    # Prepare extra step kwargs for scheduler\n    extra_step_kwargs = self.prepare_extra_func_kwargs(\n        self.scheduler.step,\n        {\n            \"generator\": batch.generator,\n            \"eta\": batch.eta\n        },\n    )\n\n    # Setup precision and autocast settings\n    # TODO(will): make the precision configurable for inference\n    # target_dtype = PRECISION_TO_TYPE[fastvideo_args.precision]\n    target_dtype = torch.bfloat16\n    autocast_enabled = (target_dtype != torch.float32\n                        ) and not fastvideo_args.disable_autocast\n\n    # Handle sequence parallelism if enabled\n    sp_world_size, rank_in_sp_group = get_sp_world_size(\n    ), get_sp_parallel_rank()\n    sp_group = sp_world_size &gt; 1\n    if sp_group:\n        latents = rearrange(batch.latents,\n                            \"b c (n t) h w -&gt; b c n t h w\",\n                            n=sp_world_size).contiguous()\n        latents = latents[:, :, rank_in_sp_group, :, :, :]\n        batch.latents = latents\n        if batch.image_latent is not None:\n            image_latent = rearrange(batch.image_latent,\n                                     \"b c (n t) h w -&gt; b c n t h w\",\n                                     n=sp_world_size).contiguous()\n            image_latent = image_latent[:, :, rank_in_sp_group, :, :, :]\n            batch.image_latent = image_latent\n    # Get timesteps and calculate warmup steps\n    timesteps = batch.timesteps\n    # TODO(will): remove this once we add input/output validation for stages\n    if timesteps is None:\n        raise ValueError(\"Timesteps must be provided\")\n    num_inference_steps = batch.num_inference_steps\n    num_warmup_steps = len(\n        timesteps) - num_inference_steps * self.scheduler.order\n\n    # Prepare image latents and embeddings for I2V generation\n    image_embeds = batch.image_embeds\n    if len(image_embeds) &gt; 0:\n        assert not torch.isnan(\n            image_embeds[0]).any(), \"image_embeds contains nan\"\n        image_embeds = [\n            image_embed.to(target_dtype) for image_embed in image_embeds\n        ]\n\n    image_kwargs = self.prepare_extra_func_kwargs(\n        self.transformer.forward,\n        {\n            \"encoder_hidden_states_image\": image_embeds,\n            \"mask_strategy\": dict_to_3d_list(\n                None, t_max=50, l_max=60, h_max=24)\n        },\n    )\n\n    pos_cond_kwargs = self.prepare_extra_func_kwargs(\n        self.transformer.forward,\n        {\n            \"encoder_hidden_states_2\": batch.clip_embedding_pos,\n            \"encoder_attention_mask\": batch.prompt_attention_mask,\n        },\n    )\n\n    neg_cond_kwargs = self.prepare_extra_func_kwargs(\n        self.transformer.forward,\n        {\n            \"encoder_hidden_states_2\": batch.clip_embedding_neg,\n            \"encoder_attention_mask\": batch.negative_attention_mask,\n        },\n    )\n\n    # Prepare STA parameters\n    if st_attn_available and self.attn_backend == SlidingTileAttentionBackend:\n        self.prepare_sta_param(batch, fastvideo_args)\n\n    # Get latents and embeddings\n    latents = batch.latents\n    prompt_embeds = batch.prompt_embeds\n    assert not torch.isnan(\n        prompt_embeds[0]).any(), \"prompt_embeds contains nan\"\n    if batch.do_classifier_free_guidance:\n        neg_prompt_embeds = batch.negative_prompt_embeds\n        assert neg_prompt_embeds is not None\n        assert not torch.isnan(\n            neg_prompt_embeds[0]).any(), \"neg_prompt_embeds contains nan\"\n\n    # (Wan2.2) Calculate timestep to switch from high noise expert to low noise expert\n    boundary_ratio = fastvideo_args.pipeline_config.dit_config.boundary_ratio\n    if batch.boundary_ratio is not None:\n        logger.info(\"Overriding boundary ratio from %s to %s\",\n                    boundary_ratio, batch.boundary_ratio)\n        boundary_ratio = batch.boundary_ratio\n\n    if boundary_ratio is not None:\n        boundary_timestep = boundary_ratio * self.scheduler.num_train_timesteps\n    else:\n        boundary_timestep = None\n    latent_model_input = latents.to(target_dtype)\n    assert latent_model_input.shape[0] == 1, \"only support batch size 1\"\n\n    if fastvideo_args.pipeline_config.ti2v_task and batch.pil_image is not None:\n        # TI2V directly replaces the first frame of the latent with\n        # the image latent instead of appending along the channel dim\n        assert batch.image_latent is None, \"TI2V task should not have image latents\"\n        assert self.vae is not None, \"VAE is not provided for TI2V task\"\n        z = self.vae.encode(batch.pil_image).mean.float()\n        if (hasattr(self.vae, \"shift_factor\")\n                and self.vae.shift_factor is not None):\n            if isinstance(self.vae.shift_factor, torch.Tensor):\n                z -= self.vae.shift_factor.to(z.device, z.dtype)\n            else:\n                z -= self.vae.shift_factor\n\n        if isinstance(self.vae.scaling_factor, torch.Tensor):\n            z = z * self.vae.scaling_factor.to(z.device, z.dtype)\n        else:\n            z = z * self.vae.scaling_factor\n\n        latent_model_input = latent_model_input.squeeze(0)\n        _, mask2 = masks_like([latent_model_input], zero=True)\n\n        latent_model_input = (1. -\n                              mask2[0]) * z + mask2[0] * latent_model_input\n        # latent_model_input = latent_model_input.unsqueeze(0)\n        latent_model_input = latent_model_input.to(get_local_torch_device())\n        latents = latent_model_input\n        F = batch.num_frames\n        temporal_scale = fastvideo_args.pipeline_config.vae_config.arch_config.scale_factor_temporal\n        spatial_scale = fastvideo_args.pipeline_config.vae_config.arch_config.scale_factor_spatial\n        patch_size = fastvideo_args.pipeline_config.dit_config.arch_config.patch_size\n        seq_len = ((F - 1) // temporal_scale +\n                   1) * (batch.height // spatial_scale) * (\n                       batch.width // spatial_scale) // (patch_size[1] *\n                                                         patch_size[2])\n        seq_len = int(math.ceil(seq_len / sp_world_size)) * sp_world_size\n\n    # Initialize lists for ODE trajectory\n    trajectory_timesteps: list[torch.Tensor] = []\n    trajectory_latents: list[torch.Tensor] = []\n\n    # Run denoising loop\n    with self.progress_bar(total=num_inference_steps) as progress_bar:\n        for i, t in enumerate(timesteps):\n            # Skip if interrupted\n            if hasattr(self, 'interrupt') and self.interrupt:\n                continue\n\n            if boundary_timestep is None or t &gt;= boundary_timestep:\n                if (fastvideo_args.dit_cpu_offload\n                        and self.transformer_2 is not None and next(\n                            self.transformer_2.parameters()).device.type\n                        == 'cuda'):\n                    self.transformer_2.to('cpu')\n                current_model = self.transformer\n                current_guidance_scale = batch.guidance_scale\n            else:\n                # low-noise stage in wan2.2\n                if fastvideo_args.dit_cpu_offload and next(\n                        self.transformer.parameters(\n                        )).device.type == 'cuda':\n                    self.transformer.to('cpu')\n                current_model = self.transformer_2\n                current_guidance_scale = batch.guidance_scale_2\n            assert current_model is not None, \"current_model is None\"\n\n            # Expand latents for V2V/I2V\n            latent_model_input = latents.to(target_dtype)\n            if batch.video_latent is not None:\n                latent_model_input = torch.cat([\n                    latent_model_input, batch.video_latent,\n                    torch.zeros_like(latents)\n                ],\n                                               dim=1).to(target_dtype)\n            elif batch.image_latent is not None:\n                assert not fastvideo_args.pipeline_config.ti2v_task, \"image latents should not be provided for TI2V task\"\n                latent_model_input = torch.cat(\n                    [latent_model_input, batch.image_latent],\n                    dim=1).to(target_dtype)\n\n            assert not torch.isnan(\n                latent_model_input).any(), \"latent_model_input contains nan\"\n            if fastvideo_args.pipeline_config.ti2v_task and batch.pil_image is not None:\n                timestep = torch.stack([t]).to(get_local_torch_device())\n                temp_ts = (mask2[0][0][:, ::2, ::2] * timestep).flatten()\n                temp_ts = torch.cat([\n                    temp_ts,\n                    temp_ts.new_ones(seq_len - temp_ts.size(0)) * timestep\n                ])\n                timestep = temp_ts.unsqueeze(0)\n                t_expand = timestep.repeat(latent_model_input.shape[0], 1)\n            else:\n                t_expand = t.repeat(latent_model_input.shape[0])\n\n            latent_model_input = self.scheduler.scale_model_input(\n                latent_model_input, t)\n\n            # Prepare inputs for transformer\n            guidance_expand = (\n                torch.tensor(\n                    [fastvideo_args.pipeline_config.embedded_cfg_scale] *\n                    latent_model_input.shape[0],\n                    dtype=torch.float32,\n                    device=get_local_torch_device(),\n                ).to(target_dtype) *\n                1000.0 if fastvideo_args.pipeline_config.embedded_cfg_scale\n                is not None else None)\n\n            # Predict noise residual\n            with torch.autocast(device_type=\"cuda\",\n                                dtype=target_dtype,\n                                enabled=autocast_enabled):\n                if (st_attn_available\n                        and self.attn_backend == SlidingTileAttentionBackend\n                    ) or (vsa_available and self.attn_backend\n                          == VideoSparseAttentionBackend):\n                    self.attn_metadata_builder_cls = self.attn_backend.get_builder_cls(\n                    )\n\n                    if self.attn_metadata_builder_cls is not None:\n                        self.attn_metadata_builder = self.attn_metadata_builder_cls(\n                        )\n                        # TODO(will): clean this up\n                        attn_metadata = self.attn_metadata_builder.build(  # type: ignore\n                            current_timestep=i,  # type: ignore\n                            raw_latent_shape=batch.\n                            raw_latent_shape[2:5],  # type: ignore\n                            patch_size=fastvideo_args.\n                            pipeline_config.  # type: ignore\n                            dit_config.patch_size,  # type: ignore\n                            STA_param=batch.STA_param,  # type: ignore\n                            VSA_sparsity=fastvideo_args.\n                            VSA_sparsity,  # type: ignore\n                            device=get_local_torch_device(),\n                        )\n                        assert attn_metadata is not None, \"attn_metadata cannot be None\"\n                    else:\n                        attn_metadata = None\n                elif (vmoba_attn_available\n                      and self.attn_backend == VMOBAAttentionBackend):\n                    self.attn_metadata_builder_cls = self.attn_backend.get_builder_cls(\n                    )\n                    if self.attn_metadata_builder_cls is not None:\n                        self.attn_metadata_builder = self.attn_metadata_builder_cls(\n                        )\n                        # Prepare V-MoBA parameters from config\n                        moba_params = fastvideo_args.moba_config.copy()\n                        moba_params.update({\n                            \"current_timestep\":\n                            i,\n                            \"raw_latent_shape\":\n                            batch.raw_latent_shape[2:5],\n                            \"patch_size\":\n                            fastvideo_args.pipeline_config.dit_config.\n                            patch_size,\n                            \"device\":\n                            get_local_torch_device(),\n                        })\n                        attn_metadata = self.attn_metadata_builder.build(\n                            **moba_params)\n                        assert attn_metadata is not None, \"attn_metadata cannot be None\"\n                    else:\n                        attn_metadata = None\n                else:\n                    attn_metadata = None\n                # TODO(will): finalize the interface. vLLM uses this to\n                # support torch dynamo compilation. They pass in\n                # attn_metadata, vllm_config, and num_tokens. We can pass in\n                # fastvideo_args or training_args, and attn_metadata.\n                batch.is_cfg_negative = False\n                with set_forward_context(\n                        current_timestep=i,\n                        attn_metadata=attn_metadata,\n                        forward_batch=batch,\n                        # fastvideo_args=fastvideo_args\n                ):\n                    # Run transformer\n                    noise_pred = current_model(\n                        latent_model_input,\n                        prompt_embeds,\n                        t_expand,\n                        guidance=guidance_expand,\n                        **image_kwargs,\n                        **pos_cond_kwargs,\n                    )\n\n                # Apply guidance\n                if batch.do_classifier_free_guidance:\n                    batch.is_cfg_negative = True\n                    with set_forward_context(\n                            current_timestep=i,\n                            attn_metadata=attn_metadata,\n                            forward_batch=batch,\n                            # fastvideo_args=fastvideo_args\n                    ):\n                        # Run transformer\n                        noise_pred_uncond = current_model(\n                            latent_model_input,\n                            neg_prompt_embeds,\n                            t_expand,\n                            guidance=guidance_expand,\n                            **image_kwargs,\n                            **neg_cond_kwargs,\n                        )\n                    noise_pred_text = noise_pred\n                    noise_pred = noise_pred_uncond + current_guidance_scale * (\n                        noise_pred_text - noise_pred_uncond)\n\n                    # Apply guidance rescale if needed\n                    if batch.guidance_rescale &gt; 0.0:\n                        # Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf\n                        noise_pred = self.rescale_noise_cfg(\n                            noise_pred,\n                            noise_pred_text,\n                            guidance_rescale=batch.guidance_rescale,\n                        )\n                # Compute the previous noisy sample\n                latents = self.scheduler.step(noise_pred,\n                                              t,\n                                              latents,\n                                              **extra_step_kwargs,\n                                              return_dict=False)[0]\n                if fastvideo_args.pipeline_config.ti2v_task and batch.pil_image is not None:\n                    latents = latents.squeeze(0)\n                    latents = (1. - mask2[0]) * z + mask2[0] * latents\n                    # latents = latents.unsqueeze(0)\n\n            # save trajectory latents if needed\n            if batch.return_trajectory_latents:\n                trajectory_timesteps.append(t)\n                trajectory_latents.append(latents)\n\n            # Update progress bar\n            if i == len(timesteps) - 1 or (\n                (i + 1) &gt; num_warmup_steps and\n                (i + 1) % self.scheduler.order == 0\n                    and progress_bar is not None):\n                progress_bar.update()\n\n    # Gather results if using sequence parallelism\n    trajectory_tensor: torch.Tensor | None = None\n    if trajectory_latents:\n        trajectory_tensor = torch.stack(trajectory_latents, dim=1)\n        trajectory_timesteps_tensor = torch.stack(trajectory_timesteps,\n                                                  dim=0)\n    else:\n        trajectory_tensor = None\n        trajectory_timesteps_tensor = None\n\n    # Gather results if using sequence parallelism\n    if sp_group:\n        latents = sequence_model_parallel_all_gather(latents, dim=2)\n        if batch.return_trajectory_latents:\n            trajectory_tensor = trajectory_tensor.to(\n                get_local_torch_device())\n            trajectory_tensor = sequence_model_parallel_all_gather(\n                trajectory_tensor, dim=3)\n\n    if trajectory_tensor is not None and trajectory_timesteps_tensor is not None:\n        batch.trajectory_timesteps = trajectory_timesteps_tensor.cpu()\n        batch.trajectory_latents = trajectory_tensor.cpu()\n\n    # Update batch with final latents\n    batch.latents = latents\n\n    # Save STA mask search results if needed\n    if st_attn_available and self.attn_backend == SlidingTileAttentionBackend and fastvideo_args.STA_mode == STA_Mode.STA_SEARCHING:\n        self.save_sta_search_results(batch)\n\n    # deallocate transformer if on mps\n    if torch.backends.mps.is_available():\n        logger.info(\"Memory before deallocating transformer: %s\",\n                    torch.mps.current_allocated_memory())\n        del self.transformer\n        if pipeline is not None and \"transformer\" in pipeline.modules:\n            del pipeline.modules[\"transformer\"]\n        fastvideo_args.model_loaded[\"transformer\"] = False\n        logger.info(\"Memory after deallocating transformer: %s\",\n                    torch.mps.current_allocated_memory())\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.DenoisingStage.prepare_extra_func_kwargs \u00b6 <pre><code>prepare_extra_func_kwargs(func, kwargs) -&gt; dict[str, Any]\n</code></pre> <p>Prepare extra kwargs for the scheduler step / denoise step.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <p>The function to prepare kwargs for.</p> required <code>kwargs</code> <p>The kwargs to prepare.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The prepared kwargs.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def prepare_extra_func_kwargs(self, func, kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Prepare extra kwargs for the scheduler step / denoise step.\n\n    Args:\n        func: The function to prepare kwargs for.\n        kwargs: The kwargs to prepare.\n\n    Returns:\n        The prepared kwargs.\n    \"\"\"\n    extra_step_kwargs = {}\n    for k, v in kwargs.items():\n        accepts = k in set(inspect.signature(func).parameters.keys())\n        if accepts:\n            extra_step_kwargs[k] = v\n    return extra_step_kwargs\n</code></pre> fastvideo.pipelines.stages.DenoisingStage.prepare_sta_param \u00b6 <pre><code>prepare_sta_param(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n)\n</code></pre> <p>Prepare Sliding Tile Attention (STA) parameters and settings.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def prepare_sta_param(self, batch: ForwardBatch,\n                      fastvideo_args: FastVideoArgs):\n    \"\"\"\n    Prepare Sliding Tile Attention (STA) parameters and settings.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n    \"\"\"\n    # TODO(kevin): STA mask search, currently only support Wan2.1 with 69x768x1280\n    from fastvideo.STA_configuration import configure_sta\n    STA_mode = fastvideo_args.STA_mode\n    skip_time_steps = fastvideo_args.skip_time_steps\n    if batch.timesteps is None:\n        raise ValueError(\"Timesteps must be provided\")\n    timesteps_num = batch.timesteps.shape[0]\n\n    logger.info(\"STA_mode: %s\", STA_mode)\n    if (batch.num_frames, batch.height,\n            batch.width) != (69, 768, 1280) and STA_mode != \"STA_inference\":\n        raise NotImplementedError(\n            \"STA mask search/tuning is not supported for this resolution\")\n\n    if STA_mode == STA_Mode.STA_SEARCHING or STA_mode == STA_Mode.STA_TUNING or STA_mode == STA_Mode.STA_TUNING_CFG:\n        size = (batch.width, batch.height)\n        if size == (1280, 768):\n            # TODO: make it configurable\n            sparse_mask_candidates_searching = [\n                \"3, 1, 10\", \"1, 5, 7\", \"3, 3, 3\", \"1, 6, 5\", \"1, 3, 10\",\n                \"3, 6, 1\"\n            ]\n            sparse_mask_candidates_tuning = [\n                \"3, 1, 10\", \"1, 5, 7\", \"3, 3, 3\", \"1, 6, 5\", \"1, 3, 10\",\n                \"3, 6, 1\"\n            ]\n            full_mask = [\"3,6,10\"]\n        else:\n            raise NotImplementedError(\n                \"STA mask search is not supported for this resolution\")\n    layer_num = self.transformer.config.num_layers\n    # specific for HunyuanVideo\n    if hasattr(self.transformer.config, \"num_single_layers\"):\n        layer_num += self.transformer.config.num_single_layers\n    head_num = self.transformer.config.num_attention_heads\n\n    if STA_mode == STA_Mode.STA_SEARCHING:\n        STA_param = configure_sta(\n            mode=STA_Mode.STA_SEARCHING,\n            layer_num=layer_num,\n            head_num=head_num,\n            time_step_num=timesteps_num,\n            mask_candidates=sparse_mask_candidates_searching +\n            full_mask,  # last is full mask; Can add more sparse masks while keep last one as full mask\n        )\n    elif STA_mode == STA_Mode.STA_TUNING:\n        STA_param = configure_sta(\n            mode=STA_Mode.STA_TUNING,\n            layer_num=layer_num,\n            head_num=head_num,\n            time_step_num=timesteps_num,\n            mask_search_files_path=\n            f'output/mask_search_result_pos_{size[0]}x{size[1]}/',\n            mask_candidates=sparse_mask_candidates_tuning,\n            full_attention_mask=[int(x) for x in full_mask[0].split(',')],\n            skip_time_steps=\n            skip_time_steps,  # Use full attention for first 12 steps\n            save_dir=\n            f'output/mask_search_strategy_{size[0]}x{size[1]}/',  # Custom save directory\n            timesteps=timesteps_num)\n    elif STA_mode == STA_Mode.STA_TUNING_CFG:\n        STA_param = configure_sta(\n            mode=STA_Mode.STA_TUNING_CFG,\n            layer_num=layer_num,\n            head_num=head_num,\n            time_step_num=timesteps_num,\n            mask_search_files_path_pos=\n            f'output/mask_search_result_pos_{size[0]}x{size[1]}/',\n            mask_search_files_path_neg=\n            f'output/mask_search_result_neg_{size[0]}x{size[1]}/',\n            mask_candidates=sparse_mask_candidates_tuning,\n            full_attention_mask=[int(x) for x in full_mask[0].split(',')],\n            skip_time_steps=skip_time_steps,\n            save_dir=f'output/mask_search_strategy_{size[0]}x{size[1]}/',\n            timesteps=timesteps_num)\n    elif STA_mode == STA_Mode.STA_INFERENCE:\n        import fastvideo.envs as envs\n        config_file = envs.FASTVIDEO_ATTENTION_CONFIG\n        if config_file is None:\n            raise ValueError(\"FASTVIDEO_ATTENTION_CONFIG is not set\")\n        STA_param = configure_sta(mode=STA_Mode.STA_INFERENCE,\n                                  layer_num=layer_num,\n                                  head_num=head_num,\n                                  time_step_num=timesteps_num,\n                                  load_path=config_file)\n\n    batch.STA_param = STA_param\n    batch.mask_search_final_result_pos = [[] for _ in range(timesteps_num)]\n    batch.mask_search_final_result_neg = [[] for _ in range(timesteps_num)]\n</code></pre> fastvideo.pipelines.stages.DenoisingStage.progress_bar \u00b6 <pre><code>progress_bar(\n    iterable: Iterable | None = None,\n    total: int | None = None,\n) -&gt; tqdm\n</code></pre> <p>Create a progress bar for the denoising process.</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>Iterable | None</code> <p>The iterable to iterate over.</p> <code>None</code> <code>total</code> <code>int | None</code> <p>The total number of items.</p> <code>None</code> <p>Returns:</p> Type Description <code>tqdm</code> <p>A tqdm progress bar.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def progress_bar(self,\n                 iterable: Iterable | None = None,\n                 total: int | None = None) -&gt; tqdm:\n    \"\"\"\n    Create a progress bar for the denoising process.\n\n    Args:\n        iterable: The iterable to iterate over.\n        total: The total number of items.\n\n    Returns:\n        A tqdm progress bar.\n    \"\"\"\n    local_rank = get_world_group().local_rank\n    if local_rank == 0:\n        return tqdm(iterable=iterable, total=total)\n    else:\n        return tqdm(iterable=iterable, total=total, disable=True)\n</code></pre> fastvideo.pipelines.stages.DenoisingStage.rescale_noise_cfg \u00b6 <pre><code>rescale_noise_cfg(\n    noise_cfg, noise_pred_text, guidance_rescale=0.0\n) -&gt; torch.Tensor\n</code></pre> <p>Rescale noise prediction according to guidance_rescale.</p> <p>Based on findings of \"Common Diffusion Noise Schedules and Sample Steps are Flawed\" (https://arxiv.org/pdf/2305.08891.pdf), Section 3.4.</p> <p>Parameters:</p> Name Type Description Default <code>noise_cfg</code> <p>The noise prediction with guidance.</p> required <code>noise_pred_text</code> <p>The text-conditioned noise prediction.</p> required <code>guidance_rescale</code> <p>The guidance rescale factor.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The rescaled noise prediction.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def rescale_noise_cfg(self,\n                      noise_cfg,\n                      noise_pred_text,\n                      guidance_rescale=0.0) -&gt; torch.Tensor:\n    \"\"\"\n    Rescale noise prediction according to guidance_rescale.\n\n    Based on findings of \"Common Diffusion Noise Schedules and Sample Steps are Flawed\"\n    (https://arxiv.org/pdf/2305.08891.pdf), Section 3.4.\n\n    Args:\n        noise_cfg: The noise prediction with guidance.\n        noise_pred_text: The text-conditioned noise prediction.\n        guidance_rescale: The guidance rescale factor.\n\n    Returns:\n        The rescaled noise prediction.\n    \"\"\"\n    std_text = noise_pred_text.std(dim=list(range(1, noise_pred_text.ndim)),\n                                   keepdim=True)\n    std_cfg = noise_cfg.std(dim=list(range(1, noise_cfg.ndim)),\n                            keepdim=True)\n    # Rescale the results from guidance (fixes overexposure)\n    noise_pred_rescaled = noise_cfg * (std_text / std_cfg)\n    # Mix with the original results from guidance by factor guidance_rescale\n    noise_cfg = (guidance_rescale * noise_pred_rescaled +\n                 (1 - guidance_rescale) * noise_cfg)\n    return noise_cfg\n</code></pre> fastvideo.pipelines.stages.DenoisingStage.save_sta_search_results \u00b6 <pre><code>save_sta_search_results(batch: ForwardBatch)\n</code></pre> <p>Save the STA mask search results.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def save_sta_search_results(self, batch: ForwardBatch):\n    \"\"\"\n    Save the STA mask search results.\n\n    Args:\n        batch: The current batch information.\n    \"\"\"\n    size = (batch.width, batch.height)\n    if size == (1280, 768):\n        # TODO: make it configurable\n        sparse_mask_candidates_searching = [\n            \"3, 1, 10\", \"1, 5, 7\", \"3, 3, 3\", \"1, 6, 5\", \"1, 3, 10\",\n            \"3, 6, 1\"\n        ]\n    else:\n        raise NotImplementedError(\n            \"STA mask search is not supported for this resolution\")\n\n    from fastvideo.STA_configuration import save_mask_search_results\n    if batch.mask_search_final_result_pos is not None and batch.prompt is not None:\n        save_mask_search_results(\n            [\n                dict(layer_data)\n                for layer_data in batch.mask_search_final_result_pos\n            ],\n            prompt=str(batch.prompt),\n            mask_strategies=sparse_mask_candidates_searching,\n            output_dir=f'output/mask_search_result_pos_{size[0]}x{size[1]}/'\n        )\n    if batch.mask_search_final_result_neg is not None and batch.prompt is not None:\n        save_mask_search_results(\n            [\n                dict(layer_data)\n                for layer_data in batch.mask_search_final_result_neg\n            ],\n            prompt=str(batch.prompt),\n            mask_strategies=sparse_mask_candidates_searching,\n            output_dir=f'output/mask_search_result_neg_{size[0]}x{size[1]}/'\n        )\n</code></pre> fastvideo.pipelines.stages.DenoisingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify denoising stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify denoising stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"timesteps\", batch.timesteps,\n                     [V.is_tensor, V.min_dims(1)])\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds, V.list_not_empty)\n    result.add_check(\"image_embeds\", batch.image_embeds, V.is_list)\n    result.add_check(\"image_latent\", batch.image_latent,\n                     V.none_or_tensor_with_dims(5))\n    result.add_check(\"num_inference_steps\", batch.num_inference_steps,\n                     V.positive_int)\n    result.add_check(\"guidance_scale\", batch.guidance_scale,\n                     V.positive_float)\n    result.add_check(\"eta\", batch.eta, V.non_negative_float)\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    result.add_check(\"do_classifier_free_guidance\",\n                     batch.do_classifier_free_guidance, V.bool_value)\n    result.add_check(\n        \"negative_prompt_embeds\", batch.negative_prompt_embeds, lambda x:\n        not batch.do_classifier_free_guidance or V.list_not_empty(x))\n    return result\n</code></pre> fastvideo.pipelines.stages.DenoisingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify denoising stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify denoising stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre> fastvideo.pipelines.stages.DmdDenoisingStage \u00b6 <pre><code>DmdDenoisingStage(transformer, scheduler)\n</code></pre> <p>               Bases: <code>DenoisingStage</code></p> <p>Denoising stage for DMD.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def __init__(self, transformer, scheduler) -&gt; None:\n    super().__init__(transformer, scheduler)\n    self.scheduler = FlowMatchEulerDiscreteScheduler(shift=8.0)\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.DmdDenoisingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Run the denoising loop.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with denoised latents.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Run the denoising loop.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with denoised latents.\n    \"\"\"\n    # Setup precision and autocast settings\n    # TODO(will): make the precision configurable for inference\n    # target_dtype = PRECISION_TO_TYPE[fastvideo_args.precision]\n    target_dtype = torch.bfloat16\n    autocast_enabled = (target_dtype != torch.float32\n                        ) and not fastvideo_args.disable_autocast\n\n    # Get timesteps and calculate warmup steps\n    timesteps = batch.timesteps\n\n    # TODO(will): remove this once we add input/output validation for stages\n    if timesteps is None:\n        raise ValueError(\"Timesteps must be provided\")\n    num_inference_steps = batch.num_inference_steps\n    num_warmup_steps = len(\n        timesteps) - num_inference_steps * self.scheduler.order\n\n    # Prepare image latents and embeddings for I2V generation\n    image_embeds = batch.image_embeds\n    if len(image_embeds) &gt; 0:\n        assert torch.isnan(image_embeds[0]).sum() == 0\n        image_embeds = [\n            image_embed.to(target_dtype) for image_embed in image_embeds\n        ]\n\n    image_kwargs = self.prepare_extra_func_kwargs(\n        self.transformer.forward,\n        {\n            \"encoder_hidden_states_image\": image_embeds,\n            \"mask_strategy\": dict_to_3d_list(\n                None, t_max=50, l_max=60, h_max=24)\n        },\n    )\n\n    pos_cond_kwargs = self.prepare_extra_func_kwargs(\n        self.transformer.forward,\n        {\n            \"encoder_hidden_states_2\": batch.clip_embedding_pos,\n            \"encoder_attention_mask\": batch.prompt_attention_mask,\n        },\n    )\n\n    # Prepare STA parameters\n    if st_attn_available and self.attn_backend == SlidingTileAttentionBackend:\n        self.prepare_sta_param(batch, fastvideo_args)\n\n    # Get latents and embeddings\n    assert batch.latents is not None, \"latents must be provided\"\n    latents = batch.latents\n    latents = latents.permute(0, 2, 1, 3, 4)\n\n    video_raw_latent_shape = latents.shape\n    prompt_embeds = batch.prompt_embeds\n    assert not torch.isnan(\n        prompt_embeds[0]).any(), \"prompt_embeds contains nan\"\n    timesteps = torch.tensor(\n        fastvideo_args.pipeline_config.dmd_denoising_steps,\n        dtype=torch.long,\n        device=get_local_torch_device())\n\n    # Handle sequence parallelism if enabled\n    sp_world_size, rank_in_sp_group = get_sp_world_size(\n    ), get_sp_parallel_rank()\n    sp_group = sp_world_size &gt; 1\n    if sp_group:\n        latents = rearrange(latents,\n                            \"b (n t) c h w -&gt; b n t c h w\",\n                            n=sp_world_size).contiguous()\n        latents = latents[:, rank_in_sp_group, :, :, :, :]\n        if batch.image_latent is not None:\n            image_latent = rearrange(batch.image_latent,\n                                     \"b c (n t) h w -&gt; b c n t h w\",\n                                     n=sp_world_size).contiguous()\n\n            image_latent = image_latent[:, :, rank_in_sp_group, :, :, :]\n            batch.image_latent = image_latent\n\n    # Run denoising loop\n    with self.progress_bar(total=len(timesteps)) as progress_bar:\n        for i, t in enumerate(timesteps):\n            # Skip if interrupted\n            if hasattr(self, 'interrupt') and self.interrupt:\n                continue\n            # Expand latents for I2V\n            noise_latents = latents.clone()\n            latent_model_input = latents.to(target_dtype)\n\n            if batch.image_latent is not None:\n                latent_model_input = torch.cat([\n                    latent_model_input,\n                    batch.image_latent.permute(0, 2, 1, 3, 4)\n                ],\n                                               dim=2).to(target_dtype)\n            assert not torch.isnan(\n                latent_model_input).any(), \"latent_model_input contains nan\"\n\n            # Prepare inputs for transformer\n            t_expand = t.repeat(latent_model_input.shape[0])\n            guidance_expand = (\n                torch.tensor(\n                    [fastvideo_args.pipeline_config.embedded_cfg_scale] *\n                    latent_model_input.shape[0],\n                    dtype=torch.float32,\n                    device=get_local_torch_device(),\n                ).to(target_dtype) *\n                1000.0 if fastvideo_args.pipeline_config.embedded_cfg_scale\n                is not None else None)\n\n            # Predict noise residual\n            with torch.autocast(device_type=\"cuda\",\n                                dtype=target_dtype,\n                                enabled=autocast_enabled):\n                if (vsa_available and self.attn_backend\n                        == VideoSparseAttentionBackend):\n                    self.attn_metadata_builder_cls = self.attn_backend.get_builder_cls(\n                    )\n\n                    if self.attn_metadata_builder_cls is not None:\n                        self.attn_metadata_builder = self.attn_metadata_builder_cls(\n                        )\n                        # TODO(will): clean this up\n                        attn_metadata = self.attn_metadata_builder.build(  # type: ignore\n                            current_timestep=i,  # type: ignore\n                            raw_latent_shape=batch.\n                            raw_latent_shape[2:5],  # type: ignore\n                            patch_size=fastvideo_args.\n                            pipeline_config.  # type: ignore\n                            dit_config.patch_size,  # type: ignore\n                            STA_param=batch.STA_param,  # type: ignore\n                            VSA_sparsity=fastvideo_args.\n                            VSA_sparsity,  # type: ignore\n                            device=get_local_torch_device(),  # type: ignore\n                        )  # type: ignore\n                        assert attn_metadata is not None, \"attn_metadata cannot be None\"\n                    else:\n                        attn_metadata = None\n                else:\n                    attn_metadata = None\n\n                batch.is_cfg_negative = False\n                with set_forward_context(\n                        current_timestep=i,\n                        attn_metadata=attn_metadata,\n                        forward_batch=batch,\n                        # fastvideo_args=fastvideo_args\n                ):\n                    # Run transformer\n                    pred_noise = self.transformer(\n                        latent_model_input.permute(0, 2, 1, 3, 4),\n                        prompt_embeds,\n                        t_expand,\n                        guidance=guidance_expand,\n                        **image_kwargs,\n                        **pos_cond_kwargs,\n                    ).permute(0, 2, 1, 3, 4)\n\n                pred_video = pred_noise_to_pred_video(\n                    pred_noise=pred_noise.flatten(0, 1),\n                    noise_input_latent=noise_latents.flatten(0, 1),\n                    timestep=t_expand,\n                    scheduler=self.scheduler).unflatten(\n                        0, pred_noise.shape[:2])\n\n                if i &lt; len(timesteps) - 1:\n                    next_timestep = timesteps[i + 1] * torch.ones(\n                        [1], dtype=torch.long, device=pred_video.device)\n                    noise = torch.randn(video_raw_latent_shape,\n                                        dtype=pred_video.dtype,\n                                        generator=batch.generator[0]).to(\n                                            self.device)\n                    if sp_group:\n                        noise = rearrange(noise,\n                                          \"b (n t) c h w -&gt; b n t c h w\",\n                                          n=sp_world_size).contiguous()\n                        noise = noise[:, rank_in_sp_group, :, :, :, :]\n                    latents = self.scheduler.add_noise(\n                        pred_video.flatten(0, 1), noise.flatten(0, 1),\n                        next_timestep).unflatten(0, pred_video.shape[:2])\n                else:\n                    latents = pred_video\n\n                # Update progress bar\n                if i == len(timesteps) - 1 or (\n                    (i + 1) &gt; num_warmup_steps and\n                    (i + 1) % self.scheduler.order == 0\n                        and progress_bar is not None):\n                    progress_bar.update()\n\n    # Gather results if using sequence parallelism\n    if sp_group:\n        latents = sequence_model_parallel_all_gather(latents, dim=1)\n    latents = latents.permute(0, 2, 1, 3, 4)\n    # Update batch with final latents\n    batch.latents = latents\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.EncodingStage \u00b6 <pre><code>EncodingStage(vae: ParallelTiledVAE)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for encoding pixel space representations into latent space.</p> <p>This stage handles the encoding of pixel-space video/images into latent representations for further processing in the diffusion pipeline.</p> Source code in <code>fastvideo/pipelines/stages/encoding.py</code> <pre><code>def __init__(self, vae: ParallelTiledVAE) -&gt; None:\n    self.vae: ParallelTiledVAE = vae\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.EncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode pixel space representations into latent space.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded latents.</p> Source code in <code>fastvideo/pipelines/stages/encoding.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode pixel space representations into latent space.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded latents.\n    \"\"\"\n    assert batch.latents is not None and isinstance(batch.latents,\n                                                    torch.Tensor)\n\n    self.vae = self.vae.to(get_local_torch_device())\n\n    # Setup VAE precision\n    vae_dtype = PRECISION_TO_TYPE[\n        fastvideo_args.pipeline_config.vae_precision]\n    vae_autocast_enabled = (\n        vae_dtype != torch.float32) and not fastvideo_args.disable_autocast\n\n    # Normalize input to [-1, 1] range (reverse of decoding normalization)\n    latents = (batch.latents * 2.0 - 1.0).clamp(-1, 1)\n\n    # Move to appropriate device and dtype\n    latents = latents.to(get_local_torch_device())\n\n    # Encode image to latents\n    with torch.autocast(device_type=\"cuda\",\n                        dtype=vae_dtype,\n                        enabled=vae_autocast_enabled):\n        if fastvideo_args.pipeline_config.vae_tiling:\n            self.vae.enable_tiling()\n        # if fastvideo_args.vae_sp:\n        #     self.vae.enable_parallel()\n        if not vae_autocast_enabled:\n            latents = latents.to(vae_dtype)\n        latents = self.vae.encode(latents).mean\n\n    # Update batch with encoded latents\n    batch.latents = latents\n\n    # Offload models if needed\n    if hasattr(self, 'maybe_free_model_hooks'):\n        self.maybe_free_model_hooks()\n\n    if fastvideo_args.vae_cpu_offload:\n        self.vae.to(\"cpu\")\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.EncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/encoding.py</code> <pre><code>@torch.no_grad()\ndef verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    # Input video/images for VAE encoding: [batch_size, channels, frames, height, width]\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre> fastvideo.pipelines.stages.EncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    # Encoded latents: [batch_size, channels, frames, height_latents, width_latents]\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre> fastvideo.pipelines.stages.ImageEncodingStage \u00b6 <pre><code>ImageEncodingStage(image_encoder, image_processor)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for encoding image prompts into embeddings for diffusion models.</p> <p>This stage handles the encoding of image prompts into the embedding space expected by the diffusion model.</p> <p>Initialize the prompt encoding stage.</p> <p>Parameters:</p> Name Type Description Default <code>enable_logging</code> <p>Whether to enable logging for this stage.</p> required <code>is_secondary</code> <p>Whether this is a secondary image encoder.</p> required Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def __init__(self, image_encoder, image_processor) -&gt; None:\n    \"\"\"\n    Initialize the prompt encoding stage.\n\n    Args:\n        enable_logging: Whether to enable logging for this stage.\n        is_secondary: Whether this is a secondary image encoder.\n    \"\"\"\n    super().__init__()\n    self.image_processor = image_processor\n    self.image_encoder = image_encoder\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.ImageEncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode the prompt into image encoder hidden states.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded prompt embeddings.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode the prompt into image encoder hidden states.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded prompt embeddings.\n    \"\"\"\n    self.image_encoder = self.image_encoder.to(get_local_torch_device())\n\n    image = batch.pil_image\n\n    image_inputs = self.image_processor(\n        images=image, return_tensors=\"pt\").to(get_local_torch_device())\n    with set_forward_context(current_timestep=0, attn_metadata=None):\n        outputs = self.image_encoder(**image_inputs)\n        image_embeds = outputs.last_hidden_state\n\n    batch.image_embeds.append(image_embeds)\n\n    if fastvideo_args.image_encoder_cpu_offload:\n        self.image_encoder.to('cpu')\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.ImageEncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify image encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify image encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"pil_image\", batch.pil_image, V.not_none)\n    result.add_check(\"image_embeds\", batch.image_embeds, V.is_list)\n    return result\n</code></pre> fastvideo.pipelines.stages.ImageEncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify image encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify image encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"image_embeds\", batch.image_embeds,\n                     V.list_of_tensors_dims(3))\n    return result\n</code></pre> fastvideo.pipelines.stages.ImageVAEEncodingStage \u00b6 <pre><code>ImageVAEEncodingStage(vae: ParallelTiledVAE)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for encoding image pixel representations into latent space.</p> <p>This stage handles the encoding of image pixel representations into the final input format (e.g., latents) for image-to-video generation.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def __init__(self, vae: ParallelTiledVAE) -&gt; None:\n    self.vae: ParallelTiledVAE = vae\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.ImageVAEEncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode pixel representations into latent space.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded outputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode pixel representations into latent space.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded outputs.\n    \"\"\"\n    assert batch.pil_image is not None\n    if fastvideo_args.mode == ExecutionMode.INFERENCE:\n        assert batch.pil_image is not None and isinstance(\n            batch.pil_image, PIL.Image.Image)\n        assert batch.height is not None and isinstance(batch.height, int)\n        assert batch.width is not None and isinstance(batch.width, int)\n        assert batch.num_frames is not None and isinstance(\n            batch.num_frames, int)\n        height = batch.height\n        width = batch.width\n        num_frames = batch.num_frames\n    elif fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        assert batch.pil_image is not None and isinstance(\n            batch.pil_image, torch.Tensor)\n        assert batch.height is not None and isinstance(batch.height, list)\n        assert batch.width is not None and isinstance(batch.width, list)\n        assert batch.num_frames is not None and isinstance(\n            batch.num_frames, list)\n        num_frames = batch.num_frames[0]\n        height = batch.height[0]\n        width = batch.width[0]\n\n    self.vae = self.vae.to(get_local_torch_device())\n\n    # Process single image for I2V\n    latent_height = height // self.vae.spatial_compression_ratio\n    latent_width = width // self.vae.spatial_compression_ratio\n    image = batch.pil_image\n    image = self.preprocess(\n        image,\n        vae_scale_factor=self.vae.spatial_compression_ratio,\n        height=height,\n        width=width).to(get_local_torch_device(), dtype=torch.float32)\n\n    # (B, C, H, W) -&gt; (B, C, 1, H, W)\n    image = image.unsqueeze(2)\n\n    video_condition = torch.cat([\n        image,\n        image.new_zeros(image.shape[0], image.shape[1], num_frames - 1,\n                        image.shape[3], image.shape[4])\n    ],\n                                dim=2)\n    video_condition = video_condition.to(device=get_local_torch_device(),\n                                         dtype=torch.float32)\n\n    # Setup VAE precision\n    vae_dtype = PRECISION_TO_TYPE[\n        fastvideo_args.pipeline_config.vae_precision]\n    vae_autocast_enabled = (\n        vae_dtype != torch.float32) and not fastvideo_args.disable_autocast\n\n    # Encode Image\n    with torch.autocast(device_type=\"cuda\",\n                        dtype=vae_dtype,\n                        enabled=vae_autocast_enabled):\n        if fastvideo_args.pipeline_config.vae_tiling:\n            self.vae.enable_tiling()\n        # if fastvideo_args.vae_sp:\n        #     self.vae.enable_parallel()\n        if not vae_autocast_enabled:\n            video_condition = video_condition.to(vae_dtype)\n        encoder_output = self.vae.encode(video_condition)\n\n    if fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        latent_condition = encoder_output.mean\n    else:\n        generator = batch.generator\n        if generator is None:\n            raise ValueError(\"Generator must be provided\")\n        latent_condition = self.retrieve_latents(encoder_output, generator)\n\n    # Apply shifting if needed\n    if (hasattr(self.vae, \"shift_factor\")\n            and self.vae.shift_factor is not None):\n        if isinstance(self.vae.shift_factor, torch.Tensor):\n            latent_condition -= self.vae.shift_factor.to(\n                latent_condition.device, latent_condition.dtype)\n        else:\n            latent_condition -= self.vae.shift_factor\n\n    if isinstance(self.vae.scaling_factor, torch.Tensor):\n        latent_condition = latent_condition * self.vae.scaling_factor.to(\n            latent_condition.device, latent_condition.dtype)\n    else:\n        latent_condition = latent_condition * self.vae.scaling_factor\n\n    if fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        batch.image_latent = latent_condition\n    else:\n        mask_lat_size = torch.ones(1, 1, num_frames, latent_height,\n                                   latent_width)\n        mask_lat_size[:, :, list(range(1, num_frames))] = 0\n        first_frame_mask = mask_lat_size[:, :, 0:1]\n        first_frame_mask = torch.repeat_interleave(\n            first_frame_mask,\n            dim=2,\n            repeats=self.vae.temporal_compression_ratio)\n        mask_lat_size = torch.concat(\n            [first_frame_mask, mask_lat_size[:, :, 1:, :]], dim=2)\n        mask_lat_size = mask_lat_size.view(\n            1, -1, self.vae.temporal_compression_ratio, latent_height,\n            latent_width)\n        mask_lat_size = mask_lat_size.transpose(1, 2)\n        mask_lat_size = mask_lat_size.to(latent_condition.device)\n\n        batch.image_latent = torch.concat([mask_lat_size, latent_condition],\n                                          dim=1)\n\n    # Offload models if needed\n    if hasattr(self, 'maybe_free_model_hooks'):\n        self.maybe_free_model_hooks()\n\n    self.vae.to(\"cpu\")\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.ImageVAEEncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    if fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        result.add_check(\"height\", batch.height, V.list_not_empty)\n        result.add_check(\"width\", batch.width, V.list_not_empty)\n        result.add_check(\"num_frames\", batch.num_frames, V.list_not_empty)\n    else:\n        result.add_check(\"height\", batch.height, V.positive_int)\n        result.add_check(\"width\", batch.width, V.positive_int)\n        result.add_check(\"num_frames\", batch.num_frames, V.positive_int)\n    return result\n</code></pre> fastvideo.pipelines.stages.ImageVAEEncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"image_latent\", batch.image_latent,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre> fastvideo.pipelines.stages.InputValidationStage \u00b6 <p>               Bases: <code>PipelineStage</code></p> <p>Stage for validating and preparing inputs for diffusion pipelines.</p> <p>This stage validates that all required inputs are present and properly formatted before proceeding with the diffusion process.</p> Functions\u00b6 fastvideo.pipelines.stages.InputValidationStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Validate and prepare inputs.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The validated batch information.</p> Source code in <code>fastvideo/pipelines/stages/input_validation.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Validate and prepare inputs.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The validated batch information.\n    \"\"\"\n\n    self._generate_seeds(batch, fastvideo_args)\n\n    # Ensure prompt is properly formatted\n    if batch.prompt is None and batch.prompt_embeds is None:\n        raise ValueError(\n            \"Either `prompt` or `prompt_embeds` must be provided\")\n\n    # Ensure negative prompt is properly formatted if using classifier-free guidance\n    if (batch.do_classifier_free_guidance and batch.negative_prompt is None\n            and batch.negative_prompt_embeds is None):\n        raise ValueError(\n            \"For classifier-free guidance, either `negative_prompt` or \"\n            \"`negative_prompt_embeds` must be provided\")\n\n    # Validate height and width\n    if batch.height is None or batch.width is None:\n        raise ValueError(\n            \"Height and width must be provided. Please set `height` and `width`.\"\n        )\n    if batch.height % 8 != 0 or batch.width % 8 != 0:\n        raise ValueError(\n            f\"Height and width must be divisible by 8 but are {batch.height} and {batch.width}.\"\n        )\n\n    # Validate number of inference steps\n    if batch.num_inference_steps &lt;= 0:\n        raise ValueError(\n            f\"Number of inference steps must be positive, but got {batch.num_inference_steps}\"\n        )\n\n    # Validate guidance scale if using classifier-free guidance\n    if batch.do_classifier_free_guidance and batch.guidance_scale &lt;= 0:\n        raise ValueError(\n            f\"Guidance scale must be positive, but got {batch.guidance_scale}\"\n        )\n\n    # for i2v, get image from image_path\n    # @TODO(Wei) hard-coded for wan2.2 5b ti2v for now. Should put this in image_encoding stage\n    if batch.image_path is not None:\n        if batch.image_path.endswith(\".mp4\"):\n            image = load_video(batch.image_path)[0]\n        else:\n            image = load_image(batch.image_path)\n        batch.pil_image = image\n\n    # further processing for ti2v task\n    if fastvideo_args.pipeline_config.ti2v_task and batch.pil_image is not None:\n        img = batch.pil_image\n        ih, iw = img.height, img.width\n        patch_size = fastvideo_args.pipeline_config.dit_config.arch_config.patch_size\n        vae_stride = fastvideo_args.pipeline_config.vae_config.arch_config.scale_factor_spatial\n        dh, dw = patch_size[1] * vae_stride, patch_size[2] * vae_stride\n        max_area = 704 * 1280\n        ow, oh = best_output_size(iw, ih, dw, dh, max_area)\n\n        scale = max(ow / iw, oh / ih)\n        img = img.resize((round(iw * scale), round(ih * scale)),\n                         Image.LANCZOS)\n        logger.info(\"resized img height: %s, img width: %s\", img.height,\n                    img.width)\n\n        # center-crop\n        x1 = (img.width - ow) // 2\n        y1 = (img.height - oh) // 2\n        img = img.crop((x1, y1, x1 + ow, y1 + oh))\n        assert img.width == ow and img.height == oh\n\n        # to tensor\n        img = TF.to_tensor(img).sub_(0.5).div_(0.5).to(\n            self.device).unsqueeze(1)\n        img = img.unsqueeze(0)\n        batch.height = oh\n        batch.width = ow\n        batch.pil_image = img\n\n    # for v2v, get control video from video path\n    if batch.video_path is not None:\n        pil_images, original_fps = load_video(batch.video_path,\n                                              return_fps=True)\n        logger.info(\"Loaded video with %s frames, original FPS: %s\",\n                    len(pil_images), original_fps)\n\n        # Get target parameters from batch\n        target_fps = batch.fps\n        target_num_frames = batch.num_frames\n        target_height = batch.height\n        target_width = batch.width\n\n        if target_fps is not None and original_fps is not None:\n            frame_skip = max(1, int(original_fps // target_fps))\n            if frame_skip &gt; 1:\n                pil_images = pil_images[::frame_skip]\n                effective_fps = original_fps / frame_skip\n                logger.info(\n                    \"Resampled video from %.1f fps to %.1f fps (skip=%s)\",\n                    original_fps, effective_fps, frame_skip)\n\n        # Limit to target number of frames\n        if target_num_frames is not None and len(\n                pil_images) &gt; target_num_frames:\n            pil_images = pil_images[:target_num_frames]\n            logger.info(\"Limited video to %s frames (from %s total)\",\n                        target_num_frames, len(pil_images))\n\n        # Resize each PIL image to target dimensions\n        resized_images = []\n        for pil_img in pil_images:\n            resized_img = resize(pil_img,\n                                 target_height,\n                                 target_width,\n                                 resize_mode=\"default\",\n                                 resample=\"lanczos\")\n            resized_images.append(resized_img)\n\n        # Convert PIL images to numpy array\n        video_numpy = pil_to_numpy(resized_images)\n        video_numpy = normalize(video_numpy)\n        video_tensor = numpy_to_pt(video_numpy)\n\n        # Rearrange to [C, T, H, W] and add batch dimension -&gt; [B, C, T, H, W]\n        input_video = video_tensor.permute(1, 0, 2, 3).unsqueeze(0)\n\n        batch.video_latent = input_video\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.InputValidationStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify input validation stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/input_validation.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify input validation stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"seed\", batch.seed, [V.not_none, V.positive_int])\n    result.add_check(\"num_videos_per_prompt\", batch.num_videos_per_prompt,\n                     V.positive_int)\n    result.add_check(\n        \"prompt_or_embeds\", None, lambda _: V.string_or_list_strings(\n            batch.prompt) or V.list_not_empty(batch.prompt_embeds))\n    result.add_check(\"height\", batch.height, V.positive_int)\n    result.add_check(\"width\", batch.width, V.positive_int)\n    result.add_check(\"num_inference_steps\", batch.num_inference_steps,\n                     V.positive_int)\n    result.add_check(\n        \"guidance_scale\", batch.guidance_scale, lambda x: not batch.\n        do_classifier_free_guidance or V.positive_float(x))\n    return result\n</code></pre> fastvideo.pipelines.stages.InputValidationStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify input validation stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/input_validation.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify input validation stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"seeds\", batch.seeds, V.list_not_empty)\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    return result\n</code></pre> fastvideo.pipelines.stages.LatentPreparationStage \u00b6 <pre><code>LatentPreparationStage(scheduler, transformer)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for preparing initial latent variables for the diffusion process.</p> <p>This stage handles the preparation of the initial latent variables that will be denoised during the diffusion process.</p> Source code in <code>fastvideo/pipelines/stages/latent_preparation.py</code> <pre><code>def __init__(self, scheduler, transformer) -&gt; None:\n    super().__init__()\n    self.scheduler = scheduler\n    self.transformer = transformer\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.LatentPreparationStage.adjust_video_length \u00b6 <pre><code>adjust_video_length(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; int\n</code></pre> <p>Adjust video length based on VAE version.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The batch with adjusted video length.</p> Source code in <code>fastvideo/pipelines/stages/latent_preparation.py</code> <pre><code>def adjust_video_length(self, batch: ForwardBatch,\n                        fastvideo_args: FastVideoArgs) -&gt; int:\n    \"\"\"\n    Adjust video length based on VAE version.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with adjusted video length.\n    \"\"\"\n\n    video_length = batch.num_frames\n    use_temporal_scaling_frames = fastvideo_args.pipeline_config.vae_config.use_temporal_scaling_frames\n    if use_temporal_scaling_frames:\n        temporal_scale_factor = fastvideo_args.pipeline_config.vae_config.arch_config.temporal_compression_ratio\n        latent_num_frames = (video_length - 1) // temporal_scale_factor + 1\n    else:  # stepvideo only\n        latent_num_frames = video_length // 17 * 3\n    return int(latent_num_frames)\n</code></pre> fastvideo.pipelines.stages.LatentPreparationStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Prepare initial latent variables for the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with prepared latent variables.</p> Source code in <code>fastvideo/pipelines/stages/latent_preparation.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Prepare initial latent variables for the diffusion process.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with prepared latent variables.\n    \"\"\"\n\n    latent_num_frames = None\n    # Adjust video length based on VAE version if needed\n    if hasattr(self, 'adjust_video_length'):\n        latent_num_frames = self.adjust_video_length(batch, fastvideo_args)\n    # Determine batch size\n    if isinstance(batch.prompt, list):\n        batch_size = len(batch.prompt)\n    elif batch.prompt is not None:\n        batch_size = 1\n    else:\n        batch_size = batch.prompt_embeds[0].shape[0]\n\n    # Adjust batch size for number of videos per prompt\n    batch_size *= batch.num_videos_per_prompt\n\n    # Get required parameters\n    dtype = batch.prompt_embeds[0].dtype\n    device = get_local_torch_device()\n    generator = batch.generator\n    latents = batch.latents\n    num_frames = latent_num_frames if latent_num_frames is not None else batch.num_frames\n    height = batch.height\n    width = batch.width\n\n    # TODO(will): remove this once we add input/output validation for stages\n    if height is None or width is None:\n        raise ValueError(\"Height and width must be provided\")\n\n    # Calculate latent shape\n    shape = (\n        batch_size,\n        self.transformer.num_channels_latents,\n        num_frames,\n        height // fastvideo_args.pipeline_config.vae_config.arch_config.\n        spatial_compression_ratio,\n        width // fastvideo_args.pipeline_config.vae_config.arch_config.\n        spatial_compression_ratio,\n    )\n\n    # Validate generator if it's a list\n    if isinstance(generator, list) and len(generator) != batch_size:\n        raise ValueError(\n            f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n            f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n        )\n    # Generate or use provided latents\n    if latents is None:\n        latents = randn_tensor(shape,\n                               generator=generator,\n                               device=device,\n                               dtype=dtype)\n    else:\n        latents = latents.to(device)\n\n    # Scale the initial noise if needed\n    if hasattr(self.scheduler, \"init_noise_sigma\"):\n        latents = latents * self.scheduler.init_noise_sigma\n    # Update batch with prepared latents\n    batch.latents = latents\n    batch.raw_latent_shape = latents.shape\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.LatentPreparationStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify latent preparation stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/latent_preparation.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify latent preparation stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\n        \"prompt_or_embeds\", None, lambda _: V.string_or_list_strings(\n            batch.prompt) or V.list_not_empty(batch.prompt_embeds))\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds,\n                     V.list_of_tensors)\n    result.add_check(\"num_videos_per_prompt\", batch.num_videos_per_prompt,\n                     V.positive_int)\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    result.add_check(\"num_frames\", batch.num_frames, V.positive_int)\n    result.add_check(\"height\", batch.height, V.positive_int)\n    result.add_check(\"width\", batch.width, V.positive_int)\n    result.add_check(\"latents\", batch.latents, V.none_or_tensor)\n    return result\n</code></pre> fastvideo.pipelines.stages.LatentPreparationStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify latent preparation stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/latent_preparation.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify latent preparation stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    result.add_check(\"raw_latent_shape\", batch.raw_latent_shape, V.is_tuple)\n    return result\n</code></pre> fastvideo.pipelines.stages.PipelineStage \u00b6 <p>               Bases: <code>ABC</code></p> <p>Abstract base class for all pipeline stages.</p> <p>A pipeline stage represents a discrete step in the diffusion process that can be composed with other stages to create a complete pipeline. Each stage is responsible for a specific part of the process, such as prompt encoding, latent preparation, etc.</p> Attributes\u00b6 fastvideo.pipelines.stages.PipelineStage.device <code>property</code> \u00b6 <pre><code>device: device\n</code></pre> <p>Get the device for this stage.</p> Functions\u00b6 fastvideo.pipelines.stages.PipelineStage.__call__ \u00b6 <pre><code>__call__(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Execute the stage's processing on the batch with optional verification and logging. Should not be overridden by subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The updated batch information after this stage's processing.</p> Source code in <code>fastvideo/pipelines/stages/base.py</code> <pre><code>def __call__(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Execute the stage's processing on the batch with optional verification and logging.\n    Should not be overridden by subclasses.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The updated batch information after this stage's processing.\n    \"\"\"\n    stage_name = self.__class__.__name__\n\n    # Check if verification is enabled (simple approach for prototype)\n    enable_verification = getattr(fastvideo_args,\n                                  'enable_stage_verification', False)\n\n    if enable_verification:\n        # Pre-execution input verification\n        try:\n            input_result = self.verify_input(batch, fastvideo_args)\n            self._run_verification(input_result, stage_name, \"input\")\n        except Exception as e:\n            logger.error(\"Input verification failed for %s: %s\", stage_name,\n                         str(e))\n            raise\n\n    # Execute the actual stage logic\n    if envs.FASTVIDEO_STAGE_LOGGING:\n        logger.info(\"[%s] Starting execution\", stage_name)\n        start_time = time.perf_counter()\n\n        try:\n            result = self.forward(batch, fastvideo_args)\n            execution_time = time.perf_counter() - start_time\n            logger.info(\"[%s] Execution completed in %s ms\", stage_name,\n                        execution_time * 1000)\n            batch.logging_info.add_stage_execution_time(\n                stage_name, execution_time)\n        except Exception as e:\n            execution_time = time.perf_counter() - start_time\n            logger.error(\"[%s] Error during execution after %s ms: %s\",\n                         stage_name, execution_time * 1000, e)\n            logger.error(\"[%s] Traceback: %s\", stage_name,\n                         traceback.format_exc())\n            raise\n    else:\n        # Direct execution (current behavior)\n        result = self.forward(batch, fastvideo_args)\n\n    if enable_verification:\n        # Post-execution output verification\n        try:\n            output_result = self.verify_output(result, fastvideo_args)\n            self._run_verification(output_result, stage_name, \"output\")\n        except Exception as e:\n            logger.error(\"Output verification failed for %s: %s\",\n                         stage_name, str(e))\n            raise\n\n    return result\n</code></pre> fastvideo.pipelines.stages.PipelineStage.forward <code>abstractmethod</code> \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Forward pass of the stage's processing.</p> <p>This method should be implemented by subclasses to provide the forward processing logic for the stage.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The updated batch information after this stage's processing.</p> Source code in <code>fastvideo/pipelines/stages/base.py</code> <pre><code>@abstractmethod\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Forward pass of the stage's processing.\n\n    This method should be implemented by subclasses to provide the forward\n    processing logic for the stage.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The updated batch information after this stage's processing.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.pipelines.stages.PipelineStage.set_logging \u00b6 <pre><code>set_logging(enable: bool)\n</code></pre> <p>Enable or disable logging for this stage.</p> <p>Parameters:</p> Name Type Description Default <code>enable</code> <code>bool</code> <p>Whether to enable logging.</p> required Source code in <code>fastvideo/pipelines/stages/base.py</code> <pre><code>def set_logging(self, enable: bool):\n    \"\"\"\n    Enable or disable logging for this stage.\n\n    Args:\n        enable: Whether to enable logging.\n    \"\"\"\n    self._enable_logging = enable\n</code></pre> fastvideo.pipelines.stages.PipelineStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify the input for the stage.</p> Example <p>from fastvideo.pipelines.stages.validators import V, VerificationResult</p> <p>def verify_input(self, batch, fastvideo_args):     result = VerificationResult()     result.add_check(\"height\", batch.height, V.positive_int_divisible(8))     result.add_check(\"width\", batch.width, V.positive_int_divisible(8))     result.add_check(\"image_latent\", batch.image_latent, V.is_tensor)     return result</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>VerificationResult</code> <p>A VerificationResult containing the verification status.</p> Source code in <code>fastvideo/pipelines/stages/base.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"\n    Verify the input for the stage.\n\n    Example:\n        from fastvideo.pipelines.stages.validators import V, VerificationResult\n\n        def verify_input(self, batch, fastvideo_args):\n            result = VerificationResult()\n            result.add_check(\"height\", batch.height, V.positive_int_divisible(8))\n            result.add_check(\"width\", batch.width, V.positive_int_divisible(8))\n            result.add_check(\"image_latent\", batch.image_latent, V.is_tensor)\n            return result\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        A VerificationResult containing the verification status.\n\n    \"\"\"\n    # Default implementation - no verification\n    return VerificationResult()\n</code></pre> fastvideo.pipelines.stages.PipelineStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify the output for the stage.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>VerificationResult</code> <p>A VerificationResult containing the verification status.</p> Source code in <code>fastvideo/pipelines/stages/base.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"\n    Verify the output for the stage.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        A VerificationResult containing the verification status.\n    \"\"\"\n    # Default implementation - no verification\n    return VerificationResult()\n</code></pre> fastvideo.pipelines.stages.RefImageEncodingStage \u00b6 <pre><code>RefImageEncodingStage(image_encoder, image_processor)\n</code></pre> <p>               Bases: <code>ImageEncodingStage</code></p> <p>Stage for encoding reference image prompts into embeddings for Wan2.1 Control models.</p> <p>This stage extends ImageEncodingStage with specialized preprocessing for reference images.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def __init__(self, image_encoder, image_processor) -&gt; None:\n    \"\"\"\n    Initialize the prompt encoding stage.\n\n    Args:\n        enable_logging: Whether to enable logging for this stage.\n        is_secondary: Whether this is a secondary image encoder.\n    \"\"\"\n    super().__init__()\n    self.image_processor = image_processor\n    self.image_encoder = image_encoder\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.RefImageEncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode the prompt into image encoder hidden states.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded prompt embeddings.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode the prompt into image encoder hidden states.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded prompt embeddings.\n    \"\"\"\n    self.image_encoder = self.image_encoder.to(get_local_torch_device())\n\n    image = batch.pil_image\n    if image is None:\n        image = create_default_image()\n    # Preprocess reference image for CLIP encoder\n    image_tensor = preprocess_reference_image_for_clip(\n        image, get_local_torch_device())\n\n    image_inputs = self.image_processor(images=image_tensor,\n                                        return_tensors=\"pt\").to(\n                                            get_local_torch_device())\n    with set_forward_context(current_timestep=0, attn_metadata=None):\n        outputs = self.image_encoder(**image_inputs)\n        image_embeds = outputs.last_hidden_state\n    batch.image_embeds.append(image_embeds)\n\n    if batch.pil_image is None:\n        batch.image_embeds = [\n            torch.zeros_like(x) for x in batch.image_embeds\n        ]\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.StepvideoPromptEncodingStage \u00b6 <pre><code>StepvideoPromptEncodingStage(stepllm, clip)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for encoding prompts using the remote caption API.</p> <p>This stage applies the magic string transformations and calls the remote caption service asynchronously to get:   - primary prompt embeddings,   - an attention mask,   - and a clip embedding.</p> Source code in <code>fastvideo/pipelines/stages/stepvideo_encoding.py</code> <pre><code>def __init__(self, stepllm, clip) -&gt; None:\n    super().__init__()\n    # self.caption_client = caption_client  # This should have a call_caption(prompts: List[str]) method.\n    self.stepllm = stepllm\n    self.clip = clip\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.StepvideoPromptEncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify stepvideo encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/stepvideo_encoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify stepvideo encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"prompt\", batch.prompt, V.string_not_empty)\n    return result\n</code></pre> fastvideo.pipelines.stages.StepvideoPromptEncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify stepvideo encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/stepvideo_encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify stepvideo encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds,\n                     [V.is_tensor, V.with_dims(3)])\n    result.add_check(\"negative_prompt_embeds\", batch.negative_prompt_embeds,\n                     [V.is_tensor, V.with_dims(3)])\n    result.add_check(\"prompt_attention_mask\", batch.prompt_attention_mask,\n                     [V.is_tensor, V.with_dims(2)])\n    result.add_check(\"negative_attention_mask\",\n                     batch.negative_attention_mask,\n                     [V.is_tensor, V.with_dims(2)])\n    result.add_check(\"clip_embedding_pos\", batch.clip_embedding_pos,\n                     [V.is_tensor, V.with_dims(2)])\n    result.add_check(\"clip_embedding_neg\", batch.clip_embedding_neg,\n                     [V.is_tensor, V.with_dims(2)])\n    return result\n</code></pre> fastvideo.pipelines.stages.TextEncodingStage \u00b6 <pre><code>TextEncodingStage(text_encoders, tokenizers)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for encoding text prompts into embeddings for diffusion models.</p> <p>This stage handles the encoding of text prompts into the embedding space expected by the diffusion model.</p> <p>Initialize the prompt encoding stage.</p> <p>Parameters:</p> Name Type Description Default <code>enable_logging</code> <p>Whether to enable logging for this stage.</p> required <code>is_secondary</code> <p>Whether this is a secondary text encoder.</p> required Source code in <code>fastvideo/pipelines/stages/text_encoding.py</code> <pre><code>def __init__(self, text_encoders, tokenizers) -&gt; None:\n    \"\"\"\n    Initialize the prompt encoding stage.\n\n    Args:\n        enable_logging: Whether to enable logging for this stage.\n        is_secondary: Whether this is a secondary text encoder.\n    \"\"\"\n    super().__init__()\n    self.tokenizers = tokenizers\n    self.text_encoders = text_encoders\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.TextEncodingStage.encode_text \u00b6 <pre><code>encode_text(\n    text: str | list[str],\n    fastvideo_args: FastVideoArgs,\n    encoder_index: int | list[int] | None = None,\n    return_attention_mask: bool = False,\n    return_type: str = \"list\",\n    device: device | str | None = None,\n    dtype: dtype | None = None,\n    max_length: int | None = None,\n    truncation: bool | None = None,\n    padding: bool | str | None = None,\n)\n</code></pre> <p>Encode plain text using selected text encoder(s) and return embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str | list[str]</code> <p>A single string or a list of strings to encode.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments providing pipeline config, including tokenizer and encoder settings, preprocess and postprocess functions.</p> required <code>encoder_index</code> <code>int | list[int] | None</code> <p>Encoder selector by index. Accepts an int or list of ints.</p> <code>None</code> <code>return_attention_mask</code> <code>bool</code> <p>If True, also return attention masks for each selected encoder.</p> <code>False</code> <code>return_type</code> <code>str</code> <p>\"list\" (default) returns a list aligned with selection; \"dict\" returns a dict keyed by encoder index as a string; \"stack\" stacks along a new first dimension (requires matching shapes).</p> <code>'list'</code> <code>device</code> <code>device | str | None</code> <p>Optional device override for inputs; defaults to local torch device.</p> <code>None</code> <code>dtype</code> <code>dtype | None</code> <p>Optional dtype to cast returned embeddings to.</p> <code>None</code> <code>max_length</code> <code>int | None</code> <p>Optional per-call tokenizer override.</p> <code>None</code> <code>truncation</code> <code>bool | None</code> <p>Optional per-call tokenizer override.</p> <code>None</code> <code>padding</code> <code>bool | str | None</code> <p>Optional per-call tokenizer override.</p> <code>None</code> <p>Returns:</p> Type Description <p>Depending on return_type and return_attention_mask:</p> <ul> <li>list: List[Tensor] or (List[Tensor], List[Tensor])</li> </ul> <ul> <li>dict: Dict[str, Tensor] or (Dict[str, Tensor], Dict[str, Tensor])</li> </ul> <ul> <li>stack: Tensor of shape [num_encoders, ...] or a tuple with stacked attention masks</li> </ul> Source code in <code>fastvideo/pipelines/stages/text_encoding.py</code> <pre><code>@torch.no_grad()\ndef encode_text(\n    self,\n    text: str | list[str],\n    fastvideo_args: FastVideoArgs,\n    encoder_index: int | list[int] | None = None,\n    return_attention_mask: bool = False,\n    return_type: str = \"list\",  # one of: \"list\", \"dict\", \"stack\"\n    device: torch.device | str | None = None,\n    dtype: torch.dtype | None = None,\n    max_length: int | None = None,\n    truncation: bool | None = None,\n    padding: bool | str | None = None,\n):\n    \"\"\"\n    Encode plain text using selected text encoder(s) and return embeddings.\n\n    Args:\n        text: A single string or a list of strings to encode.\n        fastvideo_args: The inference arguments providing pipeline config,\n            including tokenizer and encoder settings, preprocess and postprocess\n            functions.\n        encoder_index: Encoder selector by index. Accepts an int or list of ints.\n        return_attention_mask: If True, also return attention masks for each\n            selected encoder.\n        return_type: \"list\" (default) returns a list aligned with selection;\n            \"dict\" returns a dict keyed by encoder index as a string; \"stack\" stacks along a\n            new first dimension (requires matching shapes).\n        device: Optional device override for inputs; defaults to local torch device.\n        dtype: Optional dtype to cast returned embeddings to.\n        max_length: Optional per-call tokenizer override.\n        truncation: Optional per-call tokenizer override.\n        padding: Optional per-call tokenizer override.\n\n    Returns:\n        Depending on return_type and return_attention_mask:\n        - list: List[Tensor] or (List[Tensor], List[Tensor])\n        - dict: Dict[str, Tensor] or (Dict[str, Tensor], Dict[str, Tensor])\n        - stack: Tensor of shape [num_encoders, ...] or a tuple with stacked\n          attention masks\n    \"\"\"\n\n    assert len(self.tokenizers) == len(self.text_encoders)\n    assert len(self.text_encoders) == len(\n        fastvideo_args.pipeline_config.text_encoder_configs)\n\n    # Resolve selection into indices\n    encoder_cfgs = fastvideo_args.pipeline_config.text_encoder_configs\n    if encoder_index is None:\n        indices: list[int] = [0]\n    elif isinstance(encoder_index, int):\n        indices = [encoder_index]\n    else:\n        indices = list(encoder_index)\n    # validate range\n    num_encoders = len(self.text_encoders)\n    for idx in indices:\n        if idx &lt; 0 or idx &gt;= num_encoders:\n            raise IndexError(\n                f\"encoder index {idx} out of range [0, {num_encoders-1}]\")\n\n    # Validate indices are within range\n    num_encoders = len(self.text_encoders)\n\n    # Normalize input to list[str]\n    assert isinstance(text, str | list)\n    if isinstance(text, str):\n        texts: list[str] = [text]\n    else:\n        texts = text\n\n    embeds_list: list[torch.Tensor] = []\n    attn_masks_list: list[torch.Tensor] = []\n\n    preprocess_funcs = fastvideo_args.pipeline_config.preprocess_text_funcs\n    postprocess_funcs = fastvideo_args.pipeline_config.postprocess_text_funcs\n    encoder_cfgs = fastvideo_args.pipeline_config.text_encoder_configs\n\n    if return_type not in (\"list\", \"dict\", \"stack\"):\n        raise ValueError(\n            f\"Invalid return_type '{return_type}'. Expected one of: 'list', 'dict', 'stack'\"\n        )\n\n    target_device = device if device is not None else get_local_torch_device(\n    )\n\n    for i in indices:\n        tokenizer = self.tokenizers[i]\n        text_encoder = self.text_encoders[i]\n        encoder_config = encoder_cfgs[i]\n        preprocess_func = preprocess_funcs[i]\n        postprocess_func = postprocess_funcs[i]\n\n        processed_texts: list[str] = []\n        for prompt_str in texts:\n            processed_texts.append(preprocess_func(prompt_str))\n\n        tok_kwargs = dict(encoder_config.tokenizer_kwargs)\n        if max_length is not None:\n            tok_kwargs[\"max_length\"] = max_length\n        if truncation is not None:\n            tok_kwargs[\"truncation\"] = truncation\n        if padding is not None:\n            tok_kwargs[\"padding\"] = padding\n\n        text_inputs = tokenizer(processed_texts,\n                                **tok_kwargs).to(target_device)\n\n        input_ids = text_inputs[\"input_ids\"]\n        attention_mask = text_inputs[\"attention_mask\"]\n\n        with set_forward_context(current_timestep=0, attn_metadata=None):\n            outputs = text_encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                output_hidden_states=True,\n            )\n\n        prompt_embeds = postprocess_func(outputs)\n        if dtype is not None:\n            prompt_embeds = prompt_embeds.to(dtype=dtype)\n        embeds_list.append(prompt_embeds)\n        if return_attention_mask:\n            attn_masks_list.append(attention_mask)\n\n    # Shape results according to return_type\n    if return_type == \"list\":\n        if return_attention_mask:\n            return embeds_list, attn_masks_list\n        return embeds_list\n\n    if return_type == \"dict\":\n        key_strs = [str(i) for i in indices]\n        embeds_dict = {\n            k: v\n            for k, v in zip(key_strs, embeds_list, strict=False)\n        }\n        if return_attention_mask:\n            attn_dict = {\n                k: v\n                for k, v in zip(key_strs, attn_masks_list, strict=False)\n            }\n            return embeds_dict, attn_dict\n        return embeds_dict\n\n    # return_type == \"stack\"\n    # Validate shapes are compatible\n    base_shape = list(embeds_list[0].shape)\n    for t in embeds_list[1:]:\n        if list(t.shape) != base_shape:\n            raise ValueError(\n                f\"Cannot stack embeddings with differing shapes: {[list(t.shape) for t in embeds_list]}\"\n            )\n    stacked_embeds = torch.stack(embeds_list, dim=0)\n    if return_attention_mask:\n        base_mask_shape = list(attn_masks_list[0].shape)\n        for m in attn_masks_list[1:]:\n            if list(m.shape) != base_mask_shape:\n                raise ValueError(\n                    f\"Cannot stack attention masks with differing shapes: {[list(m.shape) for m in attn_masks_list]}\"\n                )\n        stacked_masks = torch.stack(attn_masks_list, dim=0)\n        return stacked_embeds, stacked_masks\n    return stacked_embeds\n</code></pre> fastvideo.pipelines.stages.TextEncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode the prompt into text encoder hidden states.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded prompt embeddings.</p> Source code in <code>fastvideo/pipelines/stages/text_encoding.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode the prompt into text encoder hidden states.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded prompt embeddings.\n    \"\"\"\n    assert len(self.tokenizers) == len(self.text_encoders)\n    assert len(self.text_encoders) == len(\n        fastvideo_args.pipeline_config.text_encoder_configs)\n\n    # Encode positive prompt with all available encoders\n    assert batch.prompt is not None\n    prompt_text: str | list[str] = batch.prompt\n    all_indices: list[int] = list(range(len(self.text_encoders)))\n    prompt_embeds_list, prompt_masks_list = self.encode_text(\n        prompt_text,\n        fastvideo_args,\n        encoder_index=all_indices,\n        return_attention_mask=True,\n    )\n    for pe in prompt_embeds_list:\n        batch.prompt_embeds.append(pe)\n    if batch.prompt_attention_mask is not None:\n        for am in prompt_masks_list:\n            batch.prompt_attention_mask.append(am)\n\n    # Encode negative prompt if CFG is enabled\n    if batch.do_classifier_free_guidance:\n        assert isinstance(batch.negative_prompt, str)\n        neg_embeds_list, neg_masks_list = self.encode_text(\n            batch.negative_prompt,\n            fastvideo_args,\n            encoder_index=all_indices,\n            return_attention_mask=True,\n        )\n        assert batch.negative_prompt_embeds is not None\n        for ne in neg_embeds_list:\n            batch.negative_prompt_embeds.append(ne)\n        if batch.negative_attention_mask is not None:\n            for nm in neg_masks_list:\n                batch.negative_attention_mask.append(nm)\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.TextEncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify text encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/text_encoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify text encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"prompt\", batch.prompt, V.string_or_list_strings)\n    result.add_check(\n        \"negative_prompt\", batch.negative_prompt, lambda x: not batch.\n        do_classifier_free_guidance or V.string_not_empty(x))\n    result.add_check(\"do_classifier_free_guidance\",\n                     batch.do_classifier_free_guidance, V.bool_value)\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds, V.is_list)\n    result.add_check(\"negative_prompt_embeds\", batch.negative_prompt_embeds,\n                     V.none_or_list)\n    return result\n</code></pre> fastvideo.pipelines.stages.TextEncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify text encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/text_encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify text encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds,\n                     V.list_of_tensors_min_dims(2))\n    result.add_check(\n        \"negative_prompt_embeds\", batch.negative_prompt_embeds,\n        lambda x: not batch.do_classifier_free_guidance or V.\n        list_of_tensors_with_min_dims(x, 2))\n    return result\n</code></pre> fastvideo.pipelines.stages.TimestepPreparationStage \u00b6 <pre><code>TimestepPreparationStage(scheduler)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for preparing timesteps for the diffusion process.</p> <p>This stage handles the preparation of the timestep sequence that will be used during the diffusion process.</p> Source code in <code>fastvideo/pipelines/stages/timestep_preparation.py</code> <pre><code>def __init__(self, scheduler) -&gt; None:\n    self.scheduler = scheduler\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.TimestepPreparationStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Prepare timesteps for the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with prepared timesteps.</p> Source code in <code>fastvideo/pipelines/stages/timestep_preparation.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Prepare timesteps for the diffusion process.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with prepared timesteps.\n    \"\"\"\n    scheduler = self.scheduler\n    device = get_local_torch_device()\n    num_inference_steps = batch.num_inference_steps\n    timesteps = batch.timesteps\n    sigmas = batch.sigmas\n    n_tokens = batch.n_tokens\n\n    # Prepare extra kwargs for set_timesteps\n    extra_set_timesteps_kwargs = {}\n    if n_tokens is not None and \"n_tokens\" in inspect.signature(\n            scheduler.set_timesteps).parameters:\n        extra_set_timesteps_kwargs[\"n_tokens\"] = n_tokens\n\n    # Handle custom timesteps or sigmas\n    if timesteps is not None and sigmas is not None:\n        raise ValueError(\n            \"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\"\n        )\n\n    if timesteps is not None:\n        accepts_timesteps = \"timesteps\" in inspect.signature(\n            scheduler.set_timesteps).parameters\n        if not accepts_timesteps:\n            raise ValueError(\n                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n            )\n        scheduler.set_timesteps(timesteps=timesteps,\n                                device=device,\n                                **extra_set_timesteps_kwargs)\n        timesteps = scheduler.timesteps\n    elif sigmas is not None:\n        accept_sigmas = \"sigmas\" in inspect.signature(\n            scheduler.set_timesteps).parameters\n        if not accept_sigmas:\n            raise ValueError(\n                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n                f\" sigmas schedules. Please check whether you are using the correct scheduler.\"\n            )\n        scheduler.set_timesteps(sigmas=sigmas,\n                                device=device,\n                                **extra_set_timesteps_kwargs)\n        timesteps = scheduler.timesteps\n    else:\n        scheduler.set_timesteps(num_inference_steps,\n                                device=device,\n                                **extra_set_timesteps_kwargs)\n        timesteps = scheduler.timesteps\n\n    # Update batch with prepared timesteps\n    batch.timesteps = timesteps\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.TimestepPreparationStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify timestep preparation stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/timestep_preparation.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify timestep preparation stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"num_inference_steps\", batch.num_inference_steps,\n                     V.positive_int)\n    result.add_check(\"timesteps\", batch.timesteps, V.none_or_tensor)\n    result.add_check(\"sigmas\", batch.sigmas, V.none_or_list)\n    result.add_check(\"n_tokens\", batch.n_tokens, V.none_or_positive_int)\n    return result\n</code></pre> fastvideo.pipelines.stages.TimestepPreparationStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify timestep preparation stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/timestep_preparation.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify timestep preparation stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"timesteps\", batch.timesteps,\n                     [V.is_tensor, V.with_dims(1)])\n    return result\n</code></pre> fastvideo.pipelines.stages.VideoVAEEncodingStage \u00b6 <pre><code>VideoVAEEncodingStage(vae: ParallelTiledVAE)\n</code></pre> <p>               Bases: <code>ImageVAEEncodingStage</code></p> <p>Stage for encoding video pixel representations into latent space.</p> <p>This stage handles the encoding of video pixel representations for video-to-video generation and control. Inherits from ImageVAEEncodingStage to reuse common functionality.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def __init__(self, vae: ParallelTiledVAE) -&gt; None:\n    self.vae: ParallelTiledVAE = vae\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.VideoVAEEncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode video pixel representations into latent space.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded outputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode video pixel representations into latent space.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded outputs.\n    \"\"\"\n    assert batch.video_latent is not None, \"Video latent input is required for VideoVAEEncodingStage\"\n\n    if fastvideo_args.mode == ExecutionMode.INFERENCE:\n        assert batch.height is not None and isinstance(batch.height, int)\n        assert batch.width is not None and isinstance(batch.width, int)\n        assert batch.num_frames is not None and isinstance(\n            batch.num_frames, int)\n        height = batch.height\n        width = batch.width\n        num_frames = batch.num_frames\n    elif fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        assert batch.height is not None and isinstance(batch.height, list)\n        assert batch.width is not None and isinstance(batch.width, list)\n        assert batch.num_frames is not None and isinstance(\n            batch.num_frames, list)\n        num_frames = batch.num_frames[0]\n        height = batch.height[0]\n        width = batch.width[0]\n\n    self.vae = self.vae.to(get_local_torch_device())\n\n    # Prepare video tensor from control video\n    video_condition = self._prepare_control_video_tensor(\n        batch.video_latent, num_frames, height,\n        width).to(get_local_torch_device(), dtype=torch.float32)\n\n    # Setup VAE precision\n    vae_dtype = PRECISION_TO_TYPE[\n        fastvideo_args.pipeline_config.vae_precision]\n    vae_autocast_enabled = (\n        vae_dtype != torch.float32) and not fastvideo_args.disable_autocast\n\n    # Encode control video\n    with torch.autocast(device_type=\"cuda\",\n                        dtype=vae_dtype,\n                        enabled=vae_autocast_enabled):\n        if fastvideo_args.pipeline_config.vae_tiling:\n            self.vae.enable_tiling()\n        if not vae_autocast_enabled:\n            video_condition = video_condition.to(vae_dtype)\n        encoder_output = self.vae.encode(video_condition)\n\n    generator = batch.generator\n    if generator is None:\n        raise ValueError(\"Generator must be provided\")\n    latent_condition = self.retrieve_latents(encoder_output, generator)\n\n    if (hasattr(self.vae, \"shift_factor\")\n            and self.vae.shift_factor is not None):\n        if isinstance(self.vae.shift_factor, torch.Tensor):\n            latent_condition -= self.vae.shift_factor.to(\n                latent_condition.device, latent_condition.dtype)\n        else:\n            latent_condition -= self.vae.shift_factor\n\n    if isinstance(self.vae.scaling_factor, torch.Tensor):\n        latent_condition = latent_condition * self.vae.scaling_factor.to(\n            latent_condition.device, latent_condition.dtype)\n    else:\n        latent_condition = latent_condition * self.vae.scaling_factor\n\n    batch.video_latent = latent_condition\n\n    # Offload models if needed\n    if hasattr(self, 'maybe_free_model_hooks'):\n        self.maybe_free_model_hooks()\n\n    self.vae.to(\"cpu\")\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.VideoVAEEncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify video encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify video encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"video_latent\", batch.video_latent, V.not_none)\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    if fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        result.add_check(\"height\", batch.height, V.list_not_empty)\n        result.add_check(\"width\", batch.width, V.list_not_empty)\n        result.add_check(\"num_frames\", batch.num_frames, V.list_not_empty)\n    else:\n        result.add_check(\"height\", batch.height, V.positive_int)\n        result.add_check(\"width\", batch.width, V.positive_int)\n        result.add_check(\"num_frames\", batch.num_frames, V.positive_int)\n    return result\n</code></pre> fastvideo.pipelines.stages.VideoVAEEncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify video encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify video encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"video_latent\", batch.video_latent,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.stages-modules","title":"Modules","text":"fastvideo.pipelines.stages.base \u00b6 <p>Base classes for pipeline stages.</p> <p>This module defines the abstract base classes for pipeline stages that can be composed to create complete diffusion pipelines.</p> Classes\u00b6 fastvideo.pipelines.stages.base.PipelineStage \u00b6 <p>               Bases: <code>ABC</code></p> <p>Abstract base class for all pipeline stages.</p> <p>A pipeline stage represents a discrete step in the diffusion process that can be composed with other stages to create a complete pipeline. Each stage is responsible for a specific part of the process, such as prompt encoding, latent preparation, etc.</p> Attributes\u00b6 fastvideo.pipelines.stages.base.PipelineStage.device <code>property</code> \u00b6 <pre><code>device: device\n</code></pre> <p>Get the device for this stage.</p> Functions\u00b6 fastvideo.pipelines.stages.base.PipelineStage.__call__ \u00b6 <pre><code>__call__(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Execute the stage's processing on the batch with optional verification and logging. Should not be overridden by subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The updated batch information after this stage's processing.</p> Source code in <code>fastvideo/pipelines/stages/base.py</code> <pre><code>def __call__(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Execute the stage's processing on the batch with optional verification and logging.\n    Should not be overridden by subclasses.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The updated batch information after this stage's processing.\n    \"\"\"\n    stage_name = self.__class__.__name__\n\n    # Check if verification is enabled (simple approach for prototype)\n    enable_verification = getattr(fastvideo_args,\n                                  'enable_stage_verification', False)\n\n    if enable_verification:\n        # Pre-execution input verification\n        try:\n            input_result = self.verify_input(batch, fastvideo_args)\n            self._run_verification(input_result, stage_name, \"input\")\n        except Exception as e:\n            logger.error(\"Input verification failed for %s: %s\", stage_name,\n                         str(e))\n            raise\n\n    # Execute the actual stage logic\n    if envs.FASTVIDEO_STAGE_LOGGING:\n        logger.info(\"[%s] Starting execution\", stage_name)\n        start_time = time.perf_counter()\n\n        try:\n            result = self.forward(batch, fastvideo_args)\n            execution_time = time.perf_counter() - start_time\n            logger.info(\"[%s] Execution completed in %s ms\", stage_name,\n                        execution_time * 1000)\n            batch.logging_info.add_stage_execution_time(\n                stage_name, execution_time)\n        except Exception as e:\n            execution_time = time.perf_counter() - start_time\n            logger.error(\"[%s] Error during execution after %s ms: %s\",\n                         stage_name, execution_time * 1000, e)\n            logger.error(\"[%s] Traceback: %s\", stage_name,\n                         traceback.format_exc())\n            raise\n    else:\n        # Direct execution (current behavior)\n        result = self.forward(batch, fastvideo_args)\n\n    if enable_verification:\n        # Post-execution output verification\n        try:\n            output_result = self.verify_output(result, fastvideo_args)\n            self._run_verification(output_result, stage_name, \"output\")\n        except Exception as e:\n            logger.error(\"Output verification failed for %s: %s\",\n                         stage_name, str(e))\n            raise\n\n    return result\n</code></pre> fastvideo.pipelines.stages.base.PipelineStage.forward <code>abstractmethod</code> \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Forward pass of the stage's processing.</p> <p>This method should be implemented by subclasses to provide the forward processing logic for the stage.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The updated batch information after this stage's processing.</p> Source code in <code>fastvideo/pipelines/stages/base.py</code> <pre><code>@abstractmethod\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Forward pass of the stage's processing.\n\n    This method should be implemented by subclasses to provide the forward\n    processing logic for the stage.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The updated batch information after this stage's processing.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.pipelines.stages.base.PipelineStage.set_logging \u00b6 <pre><code>set_logging(enable: bool)\n</code></pre> <p>Enable or disable logging for this stage.</p> <p>Parameters:</p> Name Type Description Default <code>enable</code> <code>bool</code> <p>Whether to enable logging.</p> required Source code in <code>fastvideo/pipelines/stages/base.py</code> <pre><code>def set_logging(self, enable: bool):\n    \"\"\"\n    Enable or disable logging for this stage.\n\n    Args:\n        enable: Whether to enable logging.\n    \"\"\"\n    self._enable_logging = enable\n</code></pre> fastvideo.pipelines.stages.base.PipelineStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify the input for the stage.</p> Example <p>from fastvideo.pipelines.stages.validators import V, VerificationResult</p> <p>def verify_input(self, batch, fastvideo_args):     result = VerificationResult()     result.add_check(\"height\", batch.height, V.positive_int_divisible(8))     result.add_check(\"width\", batch.width, V.positive_int_divisible(8))     result.add_check(\"image_latent\", batch.image_latent, V.is_tensor)     return result</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>VerificationResult</code> <p>A VerificationResult containing the verification status.</p> Source code in <code>fastvideo/pipelines/stages/base.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"\n    Verify the input for the stage.\n\n    Example:\n        from fastvideo.pipelines.stages.validators import V, VerificationResult\n\n        def verify_input(self, batch, fastvideo_args):\n            result = VerificationResult()\n            result.add_check(\"height\", batch.height, V.positive_int_divisible(8))\n            result.add_check(\"width\", batch.width, V.positive_int_divisible(8))\n            result.add_check(\"image_latent\", batch.image_latent, V.is_tensor)\n            return result\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        A VerificationResult containing the verification status.\n\n    \"\"\"\n    # Default implementation - no verification\n    return VerificationResult()\n</code></pre> fastvideo.pipelines.stages.base.PipelineStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify the output for the stage.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>VerificationResult</code> <p>A VerificationResult containing the verification status.</p> Source code in <code>fastvideo/pipelines/stages/base.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"\n    Verify the output for the stage.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        A VerificationResult containing the verification status.\n    \"\"\"\n    # Default implementation - no verification\n    return VerificationResult()\n</code></pre> fastvideo.pipelines.stages.base.StageVerificationError \u00b6 <p>               Bases: <code>Exception</code></p> <p>Exception raised when stage verification fails.</p> Functions\u00b6 fastvideo.pipelines.stages.causal_denoising \u00b6 Classes\u00b6 fastvideo.pipelines.stages.causal_denoising.CausalDMDDenosingStage \u00b6 <pre><code>CausalDMDDenosingStage(\n    transformer, scheduler, transformer_2=None\n)\n</code></pre> <p>               Bases: <code>DenoisingStage</code></p> <p>Denoising stage for causal diffusion.</p> Source code in <code>fastvideo/pipelines/stages/causal_denoising.py</code> <pre><code>def __init__(self, transformer, scheduler, transformer_2=None) -&gt; None:\n    super().__init__(transformer, scheduler, transformer_2)\n    # KV and cross-attention cache state (initialized on first forward)\n    self.transformer = transformer\n    self.transformer_2 = transformer_2\n    self.kv_cache1: list | None = None\n    self.crossattn_cache: list | None = None\n    # Model-dependent constants (aligned with causal_inference.py assumptions)\n    self.num_transformer_blocks = len(self.transformer.blocks)\n    self.num_frames_per_block = self.transformer.config.arch_config.num_frames_per_block\n    self.sliding_window_num_frames = self.transformer.config.arch_config.sliding_window_num_frames\n\n    try:\n        self.local_attn_size = getattr(self.transformer.model,\n                                       \"local_attn_size\",\n                                       -1)  # type: ignore\n    except Exception:\n        self.local_attn_size = -1\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.causal_denoising.CausalDMDDenosingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify denoising stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/causal_denoising.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify denoising stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds, V.list_not_empty)\n    result.add_check(\"image_embeds\", batch.image_embeds, V.is_list)\n    result.add_check(\"image_latent\", batch.image_latent,\n                     V.none_or_tensor_with_dims(5))\n    result.add_check(\"num_inference_steps\", batch.num_inference_steps,\n                     V.positive_int)\n    result.add_check(\"guidance_scale\", batch.guidance_scale,\n                     V.positive_float)\n    result.add_check(\"eta\", batch.eta, V.non_negative_float)\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    result.add_check(\"do_classifier_free_guidance\",\n                     batch.do_classifier_free_guidance, V.bool_value)\n    result.add_check(\n        \"negative_prompt_embeds\", batch.negative_prompt_embeds, lambda x:\n        not batch.do_classifier_free_guidance or V.list_not_empty(x))\n    return result\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.conditioning \u00b6 <p>Conditioning stage for diffusion pipelines.</p> Classes\u00b6 fastvideo.pipelines.stages.conditioning.ConditioningStage \u00b6 <p>               Bases: <code>PipelineStage</code></p> <p>Stage for applying conditioning to the diffusion process.</p> <p>This stage handles the application of conditioning, such as classifier-free guidance, to the diffusion process.</p> Functions\u00b6 fastvideo.pipelines.stages.conditioning.ConditioningStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Apply conditioning to the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with applied conditioning.</p> Source code in <code>fastvideo/pipelines/stages/conditioning.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Apply conditioning to the diffusion process.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with applied conditioning.\n    \"\"\"\n    # TODO!!\n    if not batch.do_classifier_free_guidance:\n        return batch\n    else:\n        return batch\n\n    logger.info(\"batch.negative_prompt_embeds: %s\",\n                batch.negative_prompt_embeds)\n    logger.info(\"do_classifier_free_guidance: %s\",\n                batch.do_classifier_free_guidance)\n    logger.info(\"cfg_scale: %s\", batch.guidance_scale)\n\n    # Ensure negative prompt embeddings are available\n    assert batch.negative_prompt_embeds is not None, (\n        \"Negative prompt embeddings are required for classifier-free guidance\"\n    )\n\n    # Concatenate primary embeddings and masks\n    batch.prompt_embeds = torch.cat(\n        [batch.negative_prompt_embeds, batch.prompt_embeds])\n    if batch.attention_mask is not None:\n        batch.attention_mask = torch.cat(\n            [batch.negative_attention_mask, batch.attention_mask])\n\n    # Concatenate secondary embeddings and masks if present\n    if batch.prompt_embeds_2 is not None:\n        batch.prompt_embeds_2 = torch.cat(\n            [batch.negative_prompt_embeds_2, batch.prompt_embeds_2])\n    if batch.attention_mask_2 is not None:\n        batch.attention_mask_2 = torch.cat(\n            [batch.negative_attention_mask_2, batch.attention_mask_2])\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.conditioning.ConditioningStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify conditioning stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/conditioning.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify conditioning stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"do_classifier_free_guidance\",\n                     batch.do_classifier_free_guidance, V.bool_value)\n    result.add_check(\"guidance_scale\", batch.guidance_scale,\n                     V.positive_float)\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds, V.list_not_empty)\n    result.add_check(\n        \"negative_prompt_embeds\", batch.negative_prompt_embeds, lambda x:\n        not batch.do_classifier_free_guidance or V.list_not_empty(x))\n    return result\n</code></pre> fastvideo.pipelines.stages.conditioning.ConditioningStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify conditioning stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/conditioning.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify conditioning stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds, V.list_not_empty)\n    return result\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.decoding \u00b6 <p>Decoding stage for diffusion pipelines.</p> Classes\u00b6 fastvideo.pipelines.stages.decoding.DecodingStage \u00b6 <pre><code>DecodingStage(vae, pipeline=None)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for decoding latent representations into pixel space.</p> <p>This stage handles the decoding of latent representations into the final output format (e.g., pixel values).</p> Source code in <code>fastvideo/pipelines/stages/decoding.py</code> <pre><code>def __init__(self, vae, pipeline=None) -&gt; None:\n    self.vae: ParallelTiledVAE = vae\n    self.pipeline = weakref.ref(pipeline) if pipeline else None\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.decoding.DecodingStage.decode \u00b6 <pre><code>decode(\n    latents: Tensor, fastvideo_args: FastVideoArgs\n) -&gt; torch.Tensor\n</code></pre> <p>Decode latent representations into pixel space using VAE.</p> <p>Parameters:</p> Name Type Description Default <code>latents</code> <code>Tensor</code> <p>Input latent tensor with shape (batch, channels, frames, height_latents, width_latents)</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>Configuration containing: - disable_autocast: Whether to disable automatic mixed precision (default: False) - pipeline_config.vae_precision: VAE computation precision (\"fp32\", \"fp16\", \"bf16\") - pipeline_config.vae_tiling: Whether to enable VAE tiling for memory efficiency</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Decoded video tensor with shape (batch, channels, frames, height, width), </p> <code>Tensor</code> <p>normalized to [0, 1] range and moved to CPU as float32</p> Source code in <code>fastvideo/pipelines/stages/decoding.py</code> <pre><code>@torch.no_grad()\ndef decode(self, latents: torch.Tensor,\n           fastvideo_args: FastVideoArgs) -&gt; torch.Tensor:\n    \"\"\"\n    Decode latent representations into pixel space using VAE.\n\n    Args:\n        latents: Input latent tensor with shape (batch, channels, frames, height_latents, width_latents)\n        fastvideo_args: Configuration containing:\n            - disable_autocast: Whether to disable automatic mixed precision (default: False)\n            - pipeline_config.vae_precision: VAE computation precision (\"fp32\", \"fp16\", \"bf16\")\n            - pipeline_config.vae_tiling: Whether to enable VAE tiling for memory efficiency\n\n    Returns:\n        Decoded video tensor with shape (batch, channels, frames, height, width), \n        normalized to [0, 1] range and moved to CPU as float32\n    \"\"\"\n    self.vae = self.vae.to(get_local_torch_device())\n    latents = latents.to(get_local_torch_device())\n\n    # Setup VAE precision\n    vae_dtype = PRECISION_TO_TYPE[\n        fastvideo_args.pipeline_config.vae_precision]\n    vae_autocast_enabled = (\n        vae_dtype != torch.float32) and not fastvideo_args.disable_autocast\n\n    if isinstance(self.vae.scaling_factor, torch.Tensor):\n        latents = latents / self.vae.scaling_factor.to(\n            latents.device, latents.dtype)\n    else:\n        latents = latents / self.vae.scaling_factor\n\n    # Apply shifting if needed\n    if (hasattr(self.vae, \"shift_factor\")\n            and self.vae.shift_factor is not None):\n        if isinstance(self.vae.shift_factor, torch.Tensor):\n            latents += self.vae.shift_factor.to(latents.device,\n                                                latents.dtype)\n        else:\n            latents += self.vae.shift_factor\n\n    # Decode latents\n    with torch.autocast(device_type=\"cuda\",\n                        dtype=vae_dtype,\n                        enabled=vae_autocast_enabled):\n        if fastvideo_args.pipeline_config.vae_tiling:\n            self.vae.enable_tiling()\n        # if fastvideo_args.vae_sp:\n        #     self.vae.enable_parallel()\n        if not vae_autocast_enabled:\n            latents = latents.to(vae_dtype)\n        image = self.vae.decode(latents)\n\n    # Normalize image to [0, 1] range\n    image = (image / 2 + 0.5).clamp(0, 1)\n    return image\n</code></pre> fastvideo.pipelines.stages.decoding.DecodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Decode latent representations into pixel space.</p> <p>This method processes the batch through the VAE decoder, converting latent representations to pixel-space video/images. It also optionally decodes trajectory latents for visualization purposes.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch containing: - latents: Tensor to decode (batch, channels, frames, height_latents, width_latents) - return_trajectory_decoded (optional): Flag to decode trajectory latents - trajectory_latents (optional): Latents at different timesteps - trajectory_timesteps (optional): Corresponding timesteps</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>Configuration containing: - output_type: \"latent\" to skip decoding, otherwise decode to pixels - vae_cpu_offload: Whether to offload VAE to CPU after decoding - model_loaded: Track VAE loading state - model_paths: Path to VAE model if loading needed</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>Modified batch with: - output: Decoded frames (batch, channels, frames, height, width) as CPU float32 - trajectory_decoded (if requested): List of decoded frames per timestep</p> Source code in <code>fastvideo/pipelines/stages/decoding.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Decode latent representations into pixel space.\n\n    This method processes the batch through the VAE decoder, converting latent\n    representations to pixel-space video/images. It also optionally decodes\n    trajectory latents for visualization purposes.\n\n    Args:\n        batch: The current batch containing:\n            - latents: Tensor to decode (batch, channels, frames, height_latents, width_latents)\n            - return_trajectory_decoded (optional): Flag to decode trajectory latents\n            - trajectory_latents (optional): Latents at different timesteps\n            - trajectory_timesteps (optional): Corresponding timesteps\n        fastvideo_args: Configuration containing:\n            - output_type: \"latent\" to skip decoding, otherwise decode to pixels\n            - vae_cpu_offload: Whether to offload VAE to CPU after decoding\n            - model_loaded: Track VAE loading state\n            - model_paths: Path to VAE model if loading needed\n\n    Returns:\n        Modified batch with:\n            - output: Decoded frames (batch, channels, frames, height, width) as CPU float32\n            - trajectory_decoded (if requested): List of decoded frames per timestep\n    \"\"\"\n    # load vae if not already loaded (used for memory constrained devices)\n    pipeline = self.pipeline() if self.pipeline else None\n    if not fastvideo_args.model_loaded[\"vae\"]:\n        loader = VAELoader()\n        self.vae = loader.load(fastvideo_args.model_paths[\"vae\"],\n                               fastvideo_args)\n        if pipeline:\n            pipeline.add_module(\"vae\", self.vae)\n        fastvideo_args.model_loaded[\"vae\"] = True\n\n    if fastvideo_args.output_type == \"latent\":\n        frames = batch.latents\n    else:\n        frames = self.decode(batch.latents, fastvideo_args)\n\n    # decode trajectory latents if needed\n    if batch.return_trajectory_decoded:\n        batch.trajectory_decoded = []\n        assert batch.trajectory_latents is not None, \"batch should have trajectory latents\"\n        for idx in range(batch.trajectory_latents.shape[1]):\n            # batch.trajectory_latents is [batch_size, timesteps, channels, frames, height, width]\n            cur_latent = batch.trajectory_latents[:, idx, :, :, :, :]\n            cur_timestep = batch.trajectory_timesteps[idx]\n            logger.info(\"decoding trajectory latent for timestep: %s\",\n                        cur_timestep)\n            decoded_frames = self.decode(cur_latent, fastvideo_args)\n            batch.trajectory_decoded.append(decoded_frames.cpu().float())\n\n    # Convert to CPU float32 for compatibility\n    frames = frames.cpu().float()\n\n    # Update batch with decoded image\n    batch.output = frames\n\n    # Offload models if needed\n    if hasattr(self, 'maybe_free_model_hooks'):\n        self.maybe_free_model_hooks()\n\n    if fastvideo_args.vae_cpu_offload:\n        self.vae.to(\"cpu\")\n\n    if torch.backends.mps.is_available():\n        del self.vae\n        if pipeline is not None and \"vae\" in pipeline.modules:\n            del pipeline.modules[\"vae\"]\n        fastvideo_args.model_loaded[\"vae\"] = False\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.decoding.DecodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify decoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/decoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify decoding stage inputs.\"\"\"\n    result = VerificationResult()\n    # Denoised latents for VAE decoding: [batch_size, channels, frames, height_latents, width_latents]\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre> fastvideo.pipelines.stages.decoding.DecodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify decoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/decoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify decoding stage outputs.\"\"\"\n    result = VerificationResult()\n    # Decoded video/images: [batch_size, channels, frames, height, width]\n    result.add_check(\"output\", batch.output, [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.denoising \u00b6 <p>Denoising stage for diffusion pipelines.</p> Classes\u00b6 fastvideo.pipelines.stages.denoising.DenoisingStage \u00b6 <pre><code>DenoisingStage(\n    transformer,\n    scheduler,\n    pipeline=None,\n    transformer_2=None,\n    vae=None,\n)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for running the denoising loop in diffusion pipelines.</p> <p>This stage handles the iterative denoising process that transforms the initial noise into the final output.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def __init__(self,\n             transformer,\n             scheduler,\n             pipeline=None,\n             transformer_2=None,\n             vae=None) -&gt; None:\n    super().__init__()\n    self.transformer = transformer\n    self.transformer_2 = transformer_2\n    self.scheduler = scheduler\n    self.vae = vae\n    self.pipeline = weakref.ref(pipeline) if pipeline else None\n    attn_head_size = self.transformer.hidden_size // self.transformer.num_attention_heads\n    self.attn_backend = get_attn_backend(\n        head_size=attn_head_size,\n        dtype=torch.float16,  # TODO(will): hack\n        supported_attention_backends=(\n            AttentionBackendEnum.SLIDING_TILE_ATTN,\n            AttentionBackendEnum.VIDEO_SPARSE_ATTN,\n            AttentionBackendEnum.VMOBA_ATTN,\n            AttentionBackendEnum.FLASH_ATTN,\n            AttentionBackendEnum.TORCH_SDPA,\n            AttentionBackendEnum.SAGE_ATTN_THREE)  # hack\n    )\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.denoising.DenoisingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Run the denoising loop.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with denoised latents.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Run the denoising loop.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with denoised latents.\n    \"\"\"\n    pipeline = self.pipeline() if self.pipeline else None\n    if not fastvideo_args.model_loaded[\"transformer\"]:\n        loader = TransformerLoader()\n        self.transformer = loader.load(\n            fastvideo_args.model_paths[\"transformer\"], fastvideo_args)\n        if pipeline:\n            pipeline.add_module(\"transformer\", self.transformer)\n        fastvideo_args.model_loaded[\"transformer\"] = True\n\n    # Prepare extra step kwargs for scheduler\n    extra_step_kwargs = self.prepare_extra_func_kwargs(\n        self.scheduler.step,\n        {\n            \"generator\": batch.generator,\n            \"eta\": batch.eta\n        },\n    )\n\n    # Setup precision and autocast settings\n    # TODO(will): make the precision configurable for inference\n    # target_dtype = PRECISION_TO_TYPE[fastvideo_args.precision]\n    target_dtype = torch.bfloat16\n    autocast_enabled = (target_dtype != torch.float32\n                        ) and not fastvideo_args.disable_autocast\n\n    # Handle sequence parallelism if enabled\n    sp_world_size, rank_in_sp_group = get_sp_world_size(\n    ), get_sp_parallel_rank()\n    sp_group = sp_world_size &gt; 1\n    if sp_group:\n        latents = rearrange(batch.latents,\n                            \"b c (n t) h w -&gt; b c n t h w\",\n                            n=sp_world_size).contiguous()\n        latents = latents[:, :, rank_in_sp_group, :, :, :]\n        batch.latents = latents\n        if batch.image_latent is not None:\n            image_latent = rearrange(batch.image_latent,\n                                     \"b c (n t) h w -&gt; b c n t h w\",\n                                     n=sp_world_size).contiguous()\n            image_latent = image_latent[:, :, rank_in_sp_group, :, :, :]\n            batch.image_latent = image_latent\n    # Get timesteps and calculate warmup steps\n    timesteps = batch.timesteps\n    # TODO(will): remove this once we add input/output validation for stages\n    if timesteps is None:\n        raise ValueError(\"Timesteps must be provided\")\n    num_inference_steps = batch.num_inference_steps\n    num_warmup_steps = len(\n        timesteps) - num_inference_steps * self.scheduler.order\n\n    # Prepare image latents and embeddings for I2V generation\n    image_embeds = batch.image_embeds\n    if len(image_embeds) &gt; 0:\n        assert not torch.isnan(\n            image_embeds[0]).any(), \"image_embeds contains nan\"\n        image_embeds = [\n            image_embed.to(target_dtype) for image_embed in image_embeds\n        ]\n\n    image_kwargs = self.prepare_extra_func_kwargs(\n        self.transformer.forward,\n        {\n            \"encoder_hidden_states_image\": image_embeds,\n            \"mask_strategy\": dict_to_3d_list(\n                None, t_max=50, l_max=60, h_max=24)\n        },\n    )\n\n    pos_cond_kwargs = self.prepare_extra_func_kwargs(\n        self.transformer.forward,\n        {\n            \"encoder_hidden_states_2\": batch.clip_embedding_pos,\n            \"encoder_attention_mask\": batch.prompt_attention_mask,\n        },\n    )\n\n    neg_cond_kwargs = self.prepare_extra_func_kwargs(\n        self.transformer.forward,\n        {\n            \"encoder_hidden_states_2\": batch.clip_embedding_neg,\n            \"encoder_attention_mask\": batch.negative_attention_mask,\n        },\n    )\n\n    # Prepare STA parameters\n    if st_attn_available and self.attn_backend == SlidingTileAttentionBackend:\n        self.prepare_sta_param(batch, fastvideo_args)\n\n    # Get latents and embeddings\n    latents = batch.latents\n    prompt_embeds = batch.prompt_embeds\n    assert not torch.isnan(\n        prompt_embeds[0]).any(), \"prompt_embeds contains nan\"\n    if batch.do_classifier_free_guidance:\n        neg_prompt_embeds = batch.negative_prompt_embeds\n        assert neg_prompt_embeds is not None\n        assert not torch.isnan(\n            neg_prompt_embeds[0]).any(), \"neg_prompt_embeds contains nan\"\n\n    # (Wan2.2) Calculate timestep to switch from high noise expert to low noise expert\n    boundary_ratio = fastvideo_args.pipeline_config.dit_config.boundary_ratio\n    if batch.boundary_ratio is not None:\n        logger.info(\"Overriding boundary ratio from %s to %s\",\n                    boundary_ratio, batch.boundary_ratio)\n        boundary_ratio = batch.boundary_ratio\n\n    if boundary_ratio is not None:\n        boundary_timestep = boundary_ratio * self.scheduler.num_train_timesteps\n    else:\n        boundary_timestep = None\n    latent_model_input = latents.to(target_dtype)\n    assert latent_model_input.shape[0] == 1, \"only support batch size 1\"\n\n    if fastvideo_args.pipeline_config.ti2v_task and batch.pil_image is not None:\n        # TI2V directly replaces the first frame of the latent with\n        # the image latent instead of appending along the channel dim\n        assert batch.image_latent is None, \"TI2V task should not have image latents\"\n        assert self.vae is not None, \"VAE is not provided for TI2V task\"\n        z = self.vae.encode(batch.pil_image).mean.float()\n        if (hasattr(self.vae, \"shift_factor\")\n                and self.vae.shift_factor is not None):\n            if isinstance(self.vae.shift_factor, torch.Tensor):\n                z -= self.vae.shift_factor.to(z.device, z.dtype)\n            else:\n                z -= self.vae.shift_factor\n\n        if isinstance(self.vae.scaling_factor, torch.Tensor):\n            z = z * self.vae.scaling_factor.to(z.device, z.dtype)\n        else:\n            z = z * self.vae.scaling_factor\n\n        latent_model_input = latent_model_input.squeeze(0)\n        _, mask2 = masks_like([latent_model_input], zero=True)\n\n        latent_model_input = (1. -\n                              mask2[0]) * z + mask2[0] * latent_model_input\n        # latent_model_input = latent_model_input.unsqueeze(0)\n        latent_model_input = latent_model_input.to(get_local_torch_device())\n        latents = latent_model_input\n        F = batch.num_frames\n        temporal_scale = fastvideo_args.pipeline_config.vae_config.arch_config.scale_factor_temporal\n        spatial_scale = fastvideo_args.pipeline_config.vae_config.arch_config.scale_factor_spatial\n        patch_size = fastvideo_args.pipeline_config.dit_config.arch_config.patch_size\n        seq_len = ((F - 1) // temporal_scale +\n                   1) * (batch.height // spatial_scale) * (\n                       batch.width // spatial_scale) // (patch_size[1] *\n                                                         patch_size[2])\n        seq_len = int(math.ceil(seq_len / sp_world_size)) * sp_world_size\n\n    # Initialize lists for ODE trajectory\n    trajectory_timesteps: list[torch.Tensor] = []\n    trajectory_latents: list[torch.Tensor] = []\n\n    # Run denoising loop\n    with self.progress_bar(total=num_inference_steps) as progress_bar:\n        for i, t in enumerate(timesteps):\n            # Skip if interrupted\n            if hasattr(self, 'interrupt') and self.interrupt:\n                continue\n\n            if boundary_timestep is None or t &gt;= boundary_timestep:\n                if (fastvideo_args.dit_cpu_offload\n                        and self.transformer_2 is not None and next(\n                            self.transformer_2.parameters()).device.type\n                        == 'cuda'):\n                    self.transformer_2.to('cpu')\n                current_model = self.transformer\n                current_guidance_scale = batch.guidance_scale\n            else:\n                # low-noise stage in wan2.2\n                if fastvideo_args.dit_cpu_offload and next(\n                        self.transformer.parameters(\n                        )).device.type == 'cuda':\n                    self.transformer.to('cpu')\n                current_model = self.transformer_2\n                current_guidance_scale = batch.guidance_scale_2\n            assert current_model is not None, \"current_model is None\"\n\n            # Expand latents for V2V/I2V\n            latent_model_input = latents.to(target_dtype)\n            if batch.video_latent is not None:\n                latent_model_input = torch.cat([\n                    latent_model_input, batch.video_latent,\n                    torch.zeros_like(latents)\n                ],\n                                               dim=1).to(target_dtype)\n            elif batch.image_latent is not None:\n                assert not fastvideo_args.pipeline_config.ti2v_task, \"image latents should not be provided for TI2V task\"\n                latent_model_input = torch.cat(\n                    [latent_model_input, batch.image_latent],\n                    dim=1).to(target_dtype)\n\n            assert not torch.isnan(\n                latent_model_input).any(), \"latent_model_input contains nan\"\n            if fastvideo_args.pipeline_config.ti2v_task and batch.pil_image is not None:\n                timestep = torch.stack([t]).to(get_local_torch_device())\n                temp_ts = (mask2[0][0][:, ::2, ::2] * timestep).flatten()\n                temp_ts = torch.cat([\n                    temp_ts,\n                    temp_ts.new_ones(seq_len - temp_ts.size(0)) * timestep\n                ])\n                timestep = temp_ts.unsqueeze(0)\n                t_expand = timestep.repeat(latent_model_input.shape[0], 1)\n            else:\n                t_expand = t.repeat(latent_model_input.shape[0])\n\n            latent_model_input = self.scheduler.scale_model_input(\n                latent_model_input, t)\n\n            # Prepare inputs for transformer\n            guidance_expand = (\n                torch.tensor(\n                    [fastvideo_args.pipeline_config.embedded_cfg_scale] *\n                    latent_model_input.shape[0],\n                    dtype=torch.float32,\n                    device=get_local_torch_device(),\n                ).to(target_dtype) *\n                1000.0 if fastvideo_args.pipeline_config.embedded_cfg_scale\n                is not None else None)\n\n            # Predict noise residual\n            with torch.autocast(device_type=\"cuda\",\n                                dtype=target_dtype,\n                                enabled=autocast_enabled):\n                if (st_attn_available\n                        and self.attn_backend == SlidingTileAttentionBackend\n                    ) or (vsa_available and self.attn_backend\n                          == VideoSparseAttentionBackend):\n                    self.attn_metadata_builder_cls = self.attn_backend.get_builder_cls(\n                    )\n\n                    if self.attn_metadata_builder_cls is not None:\n                        self.attn_metadata_builder = self.attn_metadata_builder_cls(\n                        )\n                        # TODO(will): clean this up\n                        attn_metadata = self.attn_metadata_builder.build(  # type: ignore\n                            current_timestep=i,  # type: ignore\n                            raw_latent_shape=batch.\n                            raw_latent_shape[2:5],  # type: ignore\n                            patch_size=fastvideo_args.\n                            pipeline_config.  # type: ignore\n                            dit_config.patch_size,  # type: ignore\n                            STA_param=batch.STA_param,  # type: ignore\n                            VSA_sparsity=fastvideo_args.\n                            VSA_sparsity,  # type: ignore\n                            device=get_local_torch_device(),\n                        )\n                        assert attn_metadata is not None, \"attn_metadata cannot be None\"\n                    else:\n                        attn_metadata = None\n                elif (vmoba_attn_available\n                      and self.attn_backend == VMOBAAttentionBackend):\n                    self.attn_metadata_builder_cls = self.attn_backend.get_builder_cls(\n                    )\n                    if self.attn_metadata_builder_cls is not None:\n                        self.attn_metadata_builder = self.attn_metadata_builder_cls(\n                        )\n                        # Prepare V-MoBA parameters from config\n                        moba_params = fastvideo_args.moba_config.copy()\n                        moba_params.update({\n                            \"current_timestep\":\n                            i,\n                            \"raw_latent_shape\":\n                            batch.raw_latent_shape[2:5],\n                            \"patch_size\":\n                            fastvideo_args.pipeline_config.dit_config.\n                            patch_size,\n                            \"device\":\n                            get_local_torch_device(),\n                        })\n                        attn_metadata = self.attn_metadata_builder.build(\n                            **moba_params)\n                        assert attn_metadata is not None, \"attn_metadata cannot be None\"\n                    else:\n                        attn_metadata = None\n                else:\n                    attn_metadata = None\n                # TODO(will): finalize the interface. vLLM uses this to\n                # support torch dynamo compilation. They pass in\n                # attn_metadata, vllm_config, and num_tokens. We can pass in\n                # fastvideo_args or training_args, and attn_metadata.\n                batch.is_cfg_negative = False\n                with set_forward_context(\n                        current_timestep=i,\n                        attn_metadata=attn_metadata,\n                        forward_batch=batch,\n                        # fastvideo_args=fastvideo_args\n                ):\n                    # Run transformer\n                    noise_pred = current_model(\n                        latent_model_input,\n                        prompt_embeds,\n                        t_expand,\n                        guidance=guidance_expand,\n                        **image_kwargs,\n                        **pos_cond_kwargs,\n                    )\n\n                # Apply guidance\n                if batch.do_classifier_free_guidance:\n                    batch.is_cfg_negative = True\n                    with set_forward_context(\n                            current_timestep=i,\n                            attn_metadata=attn_metadata,\n                            forward_batch=batch,\n                            # fastvideo_args=fastvideo_args\n                    ):\n                        # Run transformer\n                        noise_pred_uncond = current_model(\n                            latent_model_input,\n                            neg_prompt_embeds,\n                            t_expand,\n                            guidance=guidance_expand,\n                            **image_kwargs,\n                            **neg_cond_kwargs,\n                        )\n                    noise_pred_text = noise_pred\n                    noise_pred = noise_pred_uncond + current_guidance_scale * (\n                        noise_pred_text - noise_pred_uncond)\n\n                    # Apply guidance rescale if needed\n                    if batch.guidance_rescale &gt; 0.0:\n                        # Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf\n                        noise_pred = self.rescale_noise_cfg(\n                            noise_pred,\n                            noise_pred_text,\n                            guidance_rescale=batch.guidance_rescale,\n                        )\n                # Compute the previous noisy sample\n                latents = self.scheduler.step(noise_pred,\n                                              t,\n                                              latents,\n                                              **extra_step_kwargs,\n                                              return_dict=False)[0]\n                if fastvideo_args.pipeline_config.ti2v_task and batch.pil_image is not None:\n                    latents = latents.squeeze(0)\n                    latents = (1. - mask2[0]) * z + mask2[0] * latents\n                    # latents = latents.unsqueeze(0)\n\n            # save trajectory latents if needed\n            if batch.return_trajectory_latents:\n                trajectory_timesteps.append(t)\n                trajectory_latents.append(latents)\n\n            # Update progress bar\n            if i == len(timesteps) - 1 or (\n                (i + 1) &gt; num_warmup_steps and\n                (i + 1) % self.scheduler.order == 0\n                    and progress_bar is not None):\n                progress_bar.update()\n\n    # Gather results if using sequence parallelism\n    trajectory_tensor: torch.Tensor | None = None\n    if trajectory_latents:\n        trajectory_tensor = torch.stack(trajectory_latents, dim=1)\n        trajectory_timesteps_tensor = torch.stack(trajectory_timesteps,\n                                                  dim=0)\n    else:\n        trajectory_tensor = None\n        trajectory_timesteps_tensor = None\n\n    # Gather results if using sequence parallelism\n    if sp_group:\n        latents = sequence_model_parallel_all_gather(latents, dim=2)\n        if batch.return_trajectory_latents:\n            trajectory_tensor = trajectory_tensor.to(\n                get_local_torch_device())\n            trajectory_tensor = sequence_model_parallel_all_gather(\n                trajectory_tensor, dim=3)\n\n    if trajectory_tensor is not None and trajectory_timesteps_tensor is not None:\n        batch.trajectory_timesteps = trajectory_timesteps_tensor.cpu()\n        batch.trajectory_latents = trajectory_tensor.cpu()\n\n    # Update batch with final latents\n    batch.latents = latents\n\n    # Save STA mask search results if needed\n    if st_attn_available and self.attn_backend == SlidingTileAttentionBackend and fastvideo_args.STA_mode == STA_Mode.STA_SEARCHING:\n        self.save_sta_search_results(batch)\n\n    # deallocate transformer if on mps\n    if torch.backends.mps.is_available():\n        logger.info(\"Memory before deallocating transformer: %s\",\n                    torch.mps.current_allocated_memory())\n        del self.transformer\n        if pipeline is not None and \"transformer\" in pipeline.modules:\n            del pipeline.modules[\"transformer\"]\n        fastvideo_args.model_loaded[\"transformer\"] = False\n        logger.info(\"Memory after deallocating transformer: %s\",\n                    torch.mps.current_allocated_memory())\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.denoising.DenoisingStage.prepare_extra_func_kwargs \u00b6 <pre><code>prepare_extra_func_kwargs(func, kwargs) -&gt; dict[str, Any]\n</code></pre> <p>Prepare extra kwargs for the scheduler step / denoise step.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <p>The function to prepare kwargs for.</p> required <code>kwargs</code> <p>The kwargs to prepare.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The prepared kwargs.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def prepare_extra_func_kwargs(self, func, kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Prepare extra kwargs for the scheduler step / denoise step.\n\n    Args:\n        func: The function to prepare kwargs for.\n        kwargs: The kwargs to prepare.\n\n    Returns:\n        The prepared kwargs.\n    \"\"\"\n    extra_step_kwargs = {}\n    for k, v in kwargs.items():\n        accepts = k in set(inspect.signature(func).parameters.keys())\n        if accepts:\n            extra_step_kwargs[k] = v\n    return extra_step_kwargs\n</code></pre> fastvideo.pipelines.stages.denoising.DenoisingStage.prepare_sta_param \u00b6 <pre><code>prepare_sta_param(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n)\n</code></pre> <p>Prepare Sliding Tile Attention (STA) parameters and settings.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def prepare_sta_param(self, batch: ForwardBatch,\n                      fastvideo_args: FastVideoArgs):\n    \"\"\"\n    Prepare Sliding Tile Attention (STA) parameters and settings.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n    \"\"\"\n    # TODO(kevin): STA mask search, currently only support Wan2.1 with 69x768x1280\n    from fastvideo.STA_configuration import configure_sta\n    STA_mode = fastvideo_args.STA_mode\n    skip_time_steps = fastvideo_args.skip_time_steps\n    if batch.timesteps is None:\n        raise ValueError(\"Timesteps must be provided\")\n    timesteps_num = batch.timesteps.shape[0]\n\n    logger.info(\"STA_mode: %s\", STA_mode)\n    if (batch.num_frames, batch.height,\n            batch.width) != (69, 768, 1280) and STA_mode != \"STA_inference\":\n        raise NotImplementedError(\n            \"STA mask search/tuning is not supported for this resolution\")\n\n    if STA_mode == STA_Mode.STA_SEARCHING or STA_mode == STA_Mode.STA_TUNING or STA_mode == STA_Mode.STA_TUNING_CFG:\n        size = (batch.width, batch.height)\n        if size == (1280, 768):\n            # TODO: make it configurable\n            sparse_mask_candidates_searching = [\n                \"3, 1, 10\", \"1, 5, 7\", \"3, 3, 3\", \"1, 6, 5\", \"1, 3, 10\",\n                \"3, 6, 1\"\n            ]\n            sparse_mask_candidates_tuning = [\n                \"3, 1, 10\", \"1, 5, 7\", \"3, 3, 3\", \"1, 6, 5\", \"1, 3, 10\",\n                \"3, 6, 1\"\n            ]\n            full_mask = [\"3,6,10\"]\n        else:\n            raise NotImplementedError(\n                \"STA mask search is not supported for this resolution\")\n    layer_num = self.transformer.config.num_layers\n    # specific for HunyuanVideo\n    if hasattr(self.transformer.config, \"num_single_layers\"):\n        layer_num += self.transformer.config.num_single_layers\n    head_num = self.transformer.config.num_attention_heads\n\n    if STA_mode == STA_Mode.STA_SEARCHING:\n        STA_param = configure_sta(\n            mode=STA_Mode.STA_SEARCHING,\n            layer_num=layer_num,\n            head_num=head_num,\n            time_step_num=timesteps_num,\n            mask_candidates=sparse_mask_candidates_searching +\n            full_mask,  # last is full mask; Can add more sparse masks while keep last one as full mask\n        )\n    elif STA_mode == STA_Mode.STA_TUNING:\n        STA_param = configure_sta(\n            mode=STA_Mode.STA_TUNING,\n            layer_num=layer_num,\n            head_num=head_num,\n            time_step_num=timesteps_num,\n            mask_search_files_path=\n            f'output/mask_search_result_pos_{size[0]}x{size[1]}/',\n            mask_candidates=sparse_mask_candidates_tuning,\n            full_attention_mask=[int(x) for x in full_mask[0].split(',')],\n            skip_time_steps=\n            skip_time_steps,  # Use full attention for first 12 steps\n            save_dir=\n            f'output/mask_search_strategy_{size[0]}x{size[1]}/',  # Custom save directory\n            timesteps=timesteps_num)\n    elif STA_mode == STA_Mode.STA_TUNING_CFG:\n        STA_param = configure_sta(\n            mode=STA_Mode.STA_TUNING_CFG,\n            layer_num=layer_num,\n            head_num=head_num,\n            time_step_num=timesteps_num,\n            mask_search_files_path_pos=\n            f'output/mask_search_result_pos_{size[0]}x{size[1]}/',\n            mask_search_files_path_neg=\n            f'output/mask_search_result_neg_{size[0]}x{size[1]}/',\n            mask_candidates=sparse_mask_candidates_tuning,\n            full_attention_mask=[int(x) for x in full_mask[0].split(',')],\n            skip_time_steps=skip_time_steps,\n            save_dir=f'output/mask_search_strategy_{size[0]}x{size[1]}/',\n            timesteps=timesteps_num)\n    elif STA_mode == STA_Mode.STA_INFERENCE:\n        import fastvideo.envs as envs\n        config_file = envs.FASTVIDEO_ATTENTION_CONFIG\n        if config_file is None:\n            raise ValueError(\"FASTVIDEO_ATTENTION_CONFIG is not set\")\n        STA_param = configure_sta(mode=STA_Mode.STA_INFERENCE,\n                                  layer_num=layer_num,\n                                  head_num=head_num,\n                                  time_step_num=timesteps_num,\n                                  load_path=config_file)\n\n    batch.STA_param = STA_param\n    batch.mask_search_final_result_pos = [[] for _ in range(timesteps_num)]\n    batch.mask_search_final_result_neg = [[] for _ in range(timesteps_num)]\n</code></pre> fastvideo.pipelines.stages.denoising.DenoisingStage.progress_bar \u00b6 <pre><code>progress_bar(\n    iterable: Iterable | None = None,\n    total: int | None = None,\n) -&gt; tqdm\n</code></pre> <p>Create a progress bar for the denoising process.</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>Iterable | None</code> <p>The iterable to iterate over.</p> <code>None</code> <code>total</code> <code>int | None</code> <p>The total number of items.</p> <code>None</code> <p>Returns:</p> Type Description <code>tqdm</code> <p>A tqdm progress bar.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def progress_bar(self,\n                 iterable: Iterable | None = None,\n                 total: int | None = None) -&gt; tqdm:\n    \"\"\"\n    Create a progress bar for the denoising process.\n\n    Args:\n        iterable: The iterable to iterate over.\n        total: The total number of items.\n\n    Returns:\n        A tqdm progress bar.\n    \"\"\"\n    local_rank = get_world_group().local_rank\n    if local_rank == 0:\n        return tqdm(iterable=iterable, total=total)\n    else:\n        return tqdm(iterable=iterable, total=total, disable=True)\n</code></pre> fastvideo.pipelines.stages.denoising.DenoisingStage.rescale_noise_cfg \u00b6 <pre><code>rescale_noise_cfg(\n    noise_cfg, noise_pred_text, guidance_rescale=0.0\n) -&gt; torch.Tensor\n</code></pre> <p>Rescale noise prediction according to guidance_rescale.</p> <p>Based on findings of \"Common Diffusion Noise Schedules and Sample Steps are Flawed\" (https://arxiv.org/pdf/2305.08891.pdf), Section 3.4.</p> <p>Parameters:</p> Name Type Description Default <code>noise_cfg</code> <p>The noise prediction with guidance.</p> required <code>noise_pred_text</code> <p>The text-conditioned noise prediction.</p> required <code>guidance_rescale</code> <p>The guidance rescale factor.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The rescaled noise prediction.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def rescale_noise_cfg(self,\n                      noise_cfg,\n                      noise_pred_text,\n                      guidance_rescale=0.0) -&gt; torch.Tensor:\n    \"\"\"\n    Rescale noise prediction according to guidance_rescale.\n\n    Based on findings of \"Common Diffusion Noise Schedules and Sample Steps are Flawed\"\n    (https://arxiv.org/pdf/2305.08891.pdf), Section 3.4.\n\n    Args:\n        noise_cfg: The noise prediction with guidance.\n        noise_pred_text: The text-conditioned noise prediction.\n        guidance_rescale: The guidance rescale factor.\n\n    Returns:\n        The rescaled noise prediction.\n    \"\"\"\n    std_text = noise_pred_text.std(dim=list(range(1, noise_pred_text.ndim)),\n                                   keepdim=True)\n    std_cfg = noise_cfg.std(dim=list(range(1, noise_cfg.ndim)),\n                            keepdim=True)\n    # Rescale the results from guidance (fixes overexposure)\n    noise_pred_rescaled = noise_cfg * (std_text / std_cfg)\n    # Mix with the original results from guidance by factor guidance_rescale\n    noise_cfg = (guidance_rescale * noise_pred_rescaled +\n                 (1 - guidance_rescale) * noise_cfg)\n    return noise_cfg\n</code></pre> fastvideo.pipelines.stages.denoising.DenoisingStage.save_sta_search_results \u00b6 <pre><code>save_sta_search_results(batch: ForwardBatch)\n</code></pre> <p>Save the STA mask search results.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def save_sta_search_results(self, batch: ForwardBatch):\n    \"\"\"\n    Save the STA mask search results.\n\n    Args:\n        batch: The current batch information.\n    \"\"\"\n    size = (batch.width, batch.height)\n    if size == (1280, 768):\n        # TODO: make it configurable\n        sparse_mask_candidates_searching = [\n            \"3, 1, 10\", \"1, 5, 7\", \"3, 3, 3\", \"1, 6, 5\", \"1, 3, 10\",\n            \"3, 6, 1\"\n        ]\n    else:\n        raise NotImplementedError(\n            \"STA mask search is not supported for this resolution\")\n\n    from fastvideo.STA_configuration import save_mask_search_results\n    if batch.mask_search_final_result_pos is not None and batch.prompt is not None:\n        save_mask_search_results(\n            [\n                dict(layer_data)\n                for layer_data in batch.mask_search_final_result_pos\n            ],\n            prompt=str(batch.prompt),\n            mask_strategies=sparse_mask_candidates_searching,\n            output_dir=f'output/mask_search_result_pos_{size[0]}x{size[1]}/'\n        )\n    if batch.mask_search_final_result_neg is not None and batch.prompt is not None:\n        save_mask_search_results(\n            [\n                dict(layer_data)\n                for layer_data in batch.mask_search_final_result_neg\n            ],\n            prompt=str(batch.prompt),\n            mask_strategies=sparse_mask_candidates_searching,\n            output_dir=f'output/mask_search_result_neg_{size[0]}x{size[1]}/'\n        )\n</code></pre> fastvideo.pipelines.stages.denoising.DenoisingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify denoising stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify denoising stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"timesteps\", batch.timesteps,\n                     [V.is_tensor, V.min_dims(1)])\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds, V.list_not_empty)\n    result.add_check(\"image_embeds\", batch.image_embeds, V.is_list)\n    result.add_check(\"image_latent\", batch.image_latent,\n                     V.none_or_tensor_with_dims(5))\n    result.add_check(\"num_inference_steps\", batch.num_inference_steps,\n                     V.positive_int)\n    result.add_check(\"guidance_scale\", batch.guidance_scale,\n                     V.positive_float)\n    result.add_check(\"eta\", batch.eta, V.non_negative_float)\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    result.add_check(\"do_classifier_free_guidance\",\n                     batch.do_classifier_free_guidance, V.bool_value)\n    result.add_check(\n        \"negative_prompt_embeds\", batch.negative_prompt_embeds, lambda x:\n        not batch.do_classifier_free_guidance or V.list_not_empty(x))\n    return result\n</code></pre> fastvideo.pipelines.stages.denoising.DenoisingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify denoising stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify denoising stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre> fastvideo.pipelines.stages.denoising.DmdDenoisingStage \u00b6 <pre><code>DmdDenoisingStage(transformer, scheduler)\n</code></pre> <p>               Bases: <code>DenoisingStage</code></p> <p>Denoising stage for DMD.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def __init__(self, transformer, scheduler) -&gt; None:\n    super().__init__(transformer, scheduler)\n    self.scheduler = FlowMatchEulerDiscreteScheduler(shift=8.0)\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.denoising.DmdDenoisingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Run the denoising loop.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with denoised latents.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Run the denoising loop.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with denoised latents.\n    \"\"\"\n    # Setup precision and autocast settings\n    # TODO(will): make the precision configurable for inference\n    # target_dtype = PRECISION_TO_TYPE[fastvideo_args.precision]\n    target_dtype = torch.bfloat16\n    autocast_enabled = (target_dtype != torch.float32\n                        ) and not fastvideo_args.disable_autocast\n\n    # Get timesteps and calculate warmup steps\n    timesteps = batch.timesteps\n\n    # TODO(will): remove this once we add input/output validation for stages\n    if timesteps is None:\n        raise ValueError(\"Timesteps must be provided\")\n    num_inference_steps = batch.num_inference_steps\n    num_warmup_steps = len(\n        timesteps) - num_inference_steps * self.scheduler.order\n\n    # Prepare image latents and embeddings for I2V generation\n    image_embeds = batch.image_embeds\n    if len(image_embeds) &gt; 0:\n        assert torch.isnan(image_embeds[0]).sum() == 0\n        image_embeds = [\n            image_embed.to(target_dtype) for image_embed in image_embeds\n        ]\n\n    image_kwargs = self.prepare_extra_func_kwargs(\n        self.transformer.forward,\n        {\n            \"encoder_hidden_states_image\": image_embeds,\n            \"mask_strategy\": dict_to_3d_list(\n                None, t_max=50, l_max=60, h_max=24)\n        },\n    )\n\n    pos_cond_kwargs = self.prepare_extra_func_kwargs(\n        self.transformer.forward,\n        {\n            \"encoder_hidden_states_2\": batch.clip_embedding_pos,\n            \"encoder_attention_mask\": batch.prompt_attention_mask,\n        },\n    )\n\n    # Prepare STA parameters\n    if st_attn_available and self.attn_backend == SlidingTileAttentionBackend:\n        self.prepare_sta_param(batch, fastvideo_args)\n\n    # Get latents and embeddings\n    assert batch.latents is not None, \"latents must be provided\"\n    latents = batch.latents\n    latents = latents.permute(0, 2, 1, 3, 4)\n\n    video_raw_latent_shape = latents.shape\n    prompt_embeds = batch.prompt_embeds\n    assert not torch.isnan(\n        prompt_embeds[0]).any(), \"prompt_embeds contains nan\"\n    timesteps = torch.tensor(\n        fastvideo_args.pipeline_config.dmd_denoising_steps,\n        dtype=torch.long,\n        device=get_local_torch_device())\n\n    # Handle sequence parallelism if enabled\n    sp_world_size, rank_in_sp_group = get_sp_world_size(\n    ), get_sp_parallel_rank()\n    sp_group = sp_world_size &gt; 1\n    if sp_group:\n        latents = rearrange(latents,\n                            \"b (n t) c h w -&gt; b n t c h w\",\n                            n=sp_world_size).contiguous()\n        latents = latents[:, rank_in_sp_group, :, :, :, :]\n        if batch.image_latent is not None:\n            image_latent = rearrange(batch.image_latent,\n                                     \"b c (n t) h w -&gt; b c n t h w\",\n                                     n=sp_world_size).contiguous()\n\n            image_latent = image_latent[:, :, rank_in_sp_group, :, :, :]\n            batch.image_latent = image_latent\n\n    # Run denoising loop\n    with self.progress_bar(total=len(timesteps)) as progress_bar:\n        for i, t in enumerate(timesteps):\n            # Skip if interrupted\n            if hasattr(self, 'interrupt') and self.interrupt:\n                continue\n            # Expand latents for I2V\n            noise_latents = latents.clone()\n            latent_model_input = latents.to(target_dtype)\n\n            if batch.image_latent is not None:\n                latent_model_input = torch.cat([\n                    latent_model_input,\n                    batch.image_latent.permute(0, 2, 1, 3, 4)\n                ],\n                                               dim=2).to(target_dtype)\n            assert not torch.isnan(\n                latent_model_input).any(), \"latent_model_input contains nan\"\n\n            # Prepare inputs for transformer\n            t_expand = t.repeat(latent_model_input.shape[0])\n            guidance_expand = (\n                torch.tensor(\n                    [fastvideo_args.pipeline_config.embedded_cfg_scale] *\n                    latent_model_input.shape[0],\n                    dtype=torch.float32,\n                    device=get_local_torch_device(),\n                ).to(target_dtype) *\n                1000.0 if fastvideo_args.pipeline_config.embedded_cfg_scale\n                is not None else None)\n\n            # Predict noise residual\n            with torch.autocast(device_type=\"cuda\",\n                                dtype=target_dtype,\n                                enabled=autocast_enabled):\n                if (vsa_available and self.attn_backend\n                        == VideoSparseAttentionBackend):\n                    self.attn_metadata_builder_cls = self.attn_backend.get_builder_cls(\n                    )\n\n                    if self.attn_metadata_builder_cls is not None:\n                        self.attn_metadata_builder = self.attn_metadata_builder_cls(\n                        )\n                        # TODO(will): clean this up\n                        attn_metadata = self.attn_metadata_builder.build(  # type: ignore\n                            current_timestep=i,  # type: ignore\n                            raw_latent_shape=batch.\n                            raw_latent_shape[2:5],  # type: ignore\n                            patch_size=fastvideo_args.\n                            pipeline_config.  # type: ignore\n                            dit_config.patch_size,  # type: ignore\n                            STA_param=batch.STA_param,  # type: ignore\n                            VSA_sparsity=fastvideo_args.\n                            VSA_sparsity,  # type: ignore\n                            device=get_local_torch_device(),  # type: ignore\n                        )  # type: ignore\n                        assert attn_metadata is not None, \"attn_metadata cannot be None\"\n                    else:\n                        attn_metadata = None\n                else:\n                    attn_metadata = None\n\n                batch.is_cfg_negative = False\n                with set_forward_context(\n                        current_timestep=i,\n                        attn_metadata=attn_metadata,\n                        forward_batch=batch,\n                        # fastvideo_args=fastvideo_args\n                ):\n                    # Run transformer\n                    pred_noise = self.transformer(\n                        latent_model_input.permute(0, 2, 1, 3, 4),\n                        prompt_embeds,\n                        t_expand,\n                        guidance=guidance_expand,\n                        **image_kwargs,\n                        **pos_cond_kwargs,\n                    ).permute(0, 2, 1, 3, 4)\n\n                pred_video = pred_noise_to_pred_video(\n                    pred_noise=pred_noise.flatten(0, 1),\n                    noise_input_latent=noise_latents.flatten(0, 1),\n                    timestep=t_expand,\n                    scheduler=self.scheduler).unflatten(\n                        0, pred_noise.shape[:2])\n\n                if i &lt; len(timesteps) - 1:\n                    next_timestep = timesteps[i + 1] * torch.ones(\n                        [1], dtype=torch.long, device=pred_video.device)\n                    noise = torch.randn(video_raw_latent_shape,\n                                        dtype=pred_video.dtype,\n                                        generator=batch.generator[0]).to(\n                                            self.device)\n                    if sp_group:\n                        noise = rearrange(noise,\n                                          \"b (n t) c h w -&gt; b n t c h w\",\n                                          n=sp_world_size).contiguous()\n                        noise = noise[:, rank_in_sp_group, :, :, :, :]\n                    latents = self.scheduler.add_noise(\n                        pred_video.flatten(0, 1), noise.flatten(0, 1),\n                        next_timestep).unflatten(0, pred_video.shape[:2])\n                else:\n                    latents = pred_video\n\n                # Update progress bar\n                if i == len(timesteps) - 1 or (\n                    (i + 1) &gt; num_warmup_steps and\n                    (i + 1) % self.scheduler.order == 0\n                        and progress_bar is not None):\n                    progress_bar.update()\n\n    # Gather results if using sequence parallelism\n    if sp_group:\n        latents = sequence_model_parallel_all_gather(latents, dim=1)\n    latents = latents.permute(0, 2, 1, 3, 4)\n    # Update batch with final latents\n    batch.latents = latents\n\n    return batch\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.encoding \u00b6 <p>Encoding stage for diffusion pipelines.</p> Classes\u00b6 fastvideo.pipelines.stages.encoding.EncodingStage \u00b6 <pre><code>EncodingStage(vae: ParallelTiledVAE)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for encoding pixel space representations into latent space.</p> <p>This stage handles the encoding of pixel-space video/images into latent representations for further processing in the diffusion pipeline.</p> Source code in <code>fastvideo/pipelines/stages/encoding.py</code> <pre><code>def __init__(self, vae: ParallelTiledVAE) -&gt; None:\n    self.vae: ParallelTiledVAE = vae\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.encoding.EncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode pixel space representations into latent space.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded latents.</p> Source code in <code>fastvideo/pipelines/stages/encoding.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode pixel space representations into latent space.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded latents.\n    \"\"\"\n    assert batch.latents is not None and isinstance(batch.latents,\n                                                    torch.Tensor)\n\n    self.vae = self.vae.to(get_local_torch_device())\n\n    # Setup VAE precision\n    vae_dtype = PRECISION_TO_TYPE[\n        fastvideo_args.pipeline_config.vae_precision]\n    vae_autocast_enabled = (\n        vae_dtype != torch.float32) and not fastvideo_args.disable_autocast\n\n    # Normalize input to [-1, 1] range (reverse of decoding normalization)\n    latents = (batch.latents * 2.0 - 1.0).clamp(-1, 1)\n\n    # Move to appropriate device and dtype\n    latents = latents.to(get_local_torch_device())\n\n    # Encode image to latents\n    with torch.autocast(device_type=\"cuda\",\n                        dtype=vae_dtype,\n                        enabled=vae_autocast_enabled):\n        if fastvideo_args.pipeline_config.vae_tiling:\n            self.vae.enable_tiling()\n        # if fastvideo_args.vae_sp:\n        #     self.vae.enable_parallel()\n        if not vae_autocast_enabled:\n            latents = latents.to(vae_dtype)\n        latents = self.vae.encode(latents).mean\n\n    # Update batch with encoded latents\n    batch.latents = latents\n\n    # Offload models if needed\n    if hasattr(self, 'maybe_free_model_hooks'):\n        self.maybe_free_model_hooks()\n\n    if fastvideo_args.vae_cpu_offload:\n        self.vae.to(\"cpu\")\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.encoding.EncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/encoding.py</code> <pre><code>@torch.no_grad()\ndef verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    # Input video/images for VAE encoding: [batch_size, channels, frames, height, width]\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre> fastvideo.pipelines.stages.encoding.EncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    # Encoded latents: [batch_size, channels, frames, height_latents, width_latents]\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.image_encoding \u00b6 <p>Image and video encoding stages for diffusion pipelines.</p> <p>This module contains implementations of encoding stages for diffusion pipelines: - ImageEncodingStage: Encodes images using image encoders (e.g., CLIP) - RefImageEncodingStage: Encodes reference image for Wan2.1 control pipeline - ImageVAEEncodingStage: Encodes images to latent space using VAE for I2V generation - VideoVAEEncodingStage: Encodes videos to latent space using VAE for V2V and control tasks</p> Classes\u00b6 fastvideo.pipelines.stages.image_encoding.ImageEncodingStage \u00b6 <pre><code>ImageEncodingStage(image_encoder, image_processor)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for encoding image prompts into embeddings for diffusion models.</p> <p>This stage handles the encoding of image prompts into the embedding space expected by the diffusion model.</p> <p>Initialize the prompt encoding stage.</p> <p>Parameters:</p> Name Type Description Default <code>enable_logging</code> <p>Whether to enable logging for this stage.</p> required <code>is_secondary</code> <p>Whether this is a secondary image encoder.</p> required Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def __init__(self, image_encoder, image_processor) -&gt; None:\n    \"\"\"\n    Initialize the prompt encoding stage.\n\n    Args:\n        enable_logging: Whether to enable logging for this stage.\n        is_secondary: Whether this is a secondary image encoder.\n    \"\"\"\n    super().__init__()\n    self.image_processor = image_processor\n    self.image_encoder = image_encoder\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.image_encoding.ImageEncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode the prompt into image encoder hidden states.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded prompt embeddings.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode the prompt into image encoder hidden states.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded prompt embeddings.\n    \"\"\"\n    self.image_encoder = self.image_encoder.to(get_local_torch_device())\n\n    image = batch.pil_image\n\n    image_inputs = self.image_processor(\n        images=image, return_tensors=\"pt\").to(get_local_torch_device())\n    with set_forward_context(current_timestep=0, attn_metadata=None):\n        outputs = self.image_encoder(**image_inputs)\n        image_embeds = outputs.last_hidden_state\n\n    batch.image_embeds.append(image_embeds)\n\n    if fastvideo_args.image_encoder_cpu_offload:\n        self.image_encoder.to('cpu')\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.image_encoding.ImageEncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify image encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify image encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"pil_image\", batch.pil_image, V.not_none)\n    result.add_check(\"image_embeds\", batch.image_embeds, V.is_list)\n    return result\n</code></pre> fastvideo.pipelines.stages.image_encoding.ImageEncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify image encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify image encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"image_embeds\", batch.image_embeds,\n                     V.list_of_tensors_dims(3))\n    return result\n</code></pre> fastvideo.pipelines.stages.image_encoding.ImageVAEEncodingStage \u00b6 <pre><code>ImageVAEEncodingStage(vae: ParallelTiledVAE)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for encoding image pixel representations into latent space.</p> <p>This stage handles the encoding of image pixel representations into the final input format (e.g., latents) for image-to-video generation.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def __init__(self, vae: ParallelTiledVAE) -&gt; None:\n    self.vae: ParallelTiledVAE = vae\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.image_encoding.ImageVAEEncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode pixel representations into latent space.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded outputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode pixel representations into latent space.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded outputs.\n    \"\"\"\n    assert batch.pil_image is not None\n    if fastvideo_args.mode == ExecutionMode.INFERENCE:\n        assert batch.pil_image is not None and isinstance(\n            batch.pil_image, PIL.Image.Image)\n        assert batch.height is not None and isinstance(batch.height, int)\n        assert batch.width is not None and isinstance(batch.width, int)\n        assert batch.num_frames is not None and isinstance(\n            batch.num_frames, int)\n        height = batch.height\n        width = batch.width\n        num_frames = batch.num_frames\n    elif fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        assert batch.pil_image is not None and isinstance(\n            batch.pil_image, torch.Tensor)\n        assert batch.height is not None and isinstance(batch.height, list)\n        assert batch.width is not None and isinstance(batch.width, list)\n        assert batch.num_frames is not None and isinstance(\n            batch.num_frames, list)\n        num_frames = batch.num_frames[0]\n        height = batch.height[0]\n        width = batch.width[0]\n\n    self.vae = self.vae.to(get_local_torch_device())\n\n    # Process single image for I2V\n    latent_height = height // self.vae.spatial_compression_ratio\n    latent_width = width // self.vae.spatial_compression_ratio\n    image = batch.pil_image\n    image = self.preprocess(\n        image,\n        vae_scale_factor=self.vae.spatial_compression_ratio,\n        height=height,\n        width=width).to(get_local_torch_device(), dtype=torch.float32)\n\n    # (B, C, H, W) -&gt; (B, C, 1, H, W)\n    image = image.unsqueeze(2)\n\n    video_condition = torch.cat([\n        image,\n        image.new_zeros(image.shape[0], image.shape[1], num_frames - 1,\n                        image.shape[3], image.shape[4])\n    ],\n                                dim=2)\n    video_condition = video_condition.to(device=get_local_torch_device(),\n                                         dtype=torch.float32)\n\n    # Setup VAE precision\n    vae_dtype = PRECISION_TO_TYPE[\n        fastvideo_args.pipeline_config.vae_precision]\n    vae_autocast_enabled = (\n        vae_dtype != torch.float32) and not fastvideo_args.disable_autocast\n\n    # Encode Image\n    with torch.autocast(device_type=\"cuda\",\n                        dtype=vae_dtype,\n                        enabled=vae_autocast_enabled):\n        if fastvideo_args.pipeline_config.vae_tiling:\n            self.vae.enable_tiling()\n        # if fastvideo_args.vae_sp:\n        #     self.vae.enable_parallel()\n        if not vae_autocast_enabled:\n            video_condition = video_condition.to(vae_dtype)\n        encoder_output = self.vae.encode(video_condition)\n\n    if fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        latent_condition = encoder_output.mean\n    else:\n        generator = batch.generator\n        if generator is None:\n            raise ValueError(\"Generator must be provided\")\n        latent_condition = self.retrieve_latents(encoder_output, generator)\n\n    # Apply shifting if needed\n    if (hasattr(self.vae, \"shift_factor\")\n            and self.vae.shift_factor is not None):\n        if isinstance(self.vae.shift_factor, torch.Tensor):\n            latent_condition -= self.vae.shift_factor.to(\n                latent_condition.device, latent_condition.dtype)\n        else:\n            latent_condition -= self.vae.shift_factor\n\n    if isinstance(self.vae.scaling_factor, torch.Tensor):\n        latent_condition = latent_condition * self.vae.scaling_factor.to(\n            latent_condition.device, latent_condition.dtype)\n    else:\n        latent_condition = latent_condition * self.vae.scaling_factor\n\n    if fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        batch.image_latent = latent_condition\n    else:\n        mask_lat_size = torch.ones(1, 1, num_frames, latent_height,\n                                   latent_width)\n        mask_lat_size[:, :, list(range(1, num_frames))] = 0\n        first_frame_mask = mask_lat_size[:, :, 0:1]\n        first_frame_mask = torch.repeat_interleave(\n            first_frame_mask,\n            dim=2,\n            repeats=self.vae.temporal_compression_ratio)\n        mask_lat_size = torch.concat(\n            [first_frame_mask, mask_lat_size[:, :, 1:, :]], dim=2)\n        mask_lat_size = mask_lat_size.view(\n            1, -1, self.vae.temporal_compression_ratio, latent_height,\n            latent_width)\n        mask_lat_size = mask_lat_size.transpose(1, 2)\n        mask_lat_size = mask_lat_size.to(latent_condition.device)\n\n        batch.image_latent = torch.concat([mask_lat_size, latent_condition],\n                                          dim=1)\n\n    # Offload models if needed\n    if hasattr(self, 'maybe_free_model_hooks'):\n        self.maybe_free_model_hooks()\n\n    self.vae.to(\"cpu\")\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.image_encoding.ImageVAEEncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    if fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        result.add_check(\"height\", batch.height, V.list_not_empty)\n        result.add_check(\"width\", batch.width, V.list_not_empty)\n        result.add_check(\"num_frames\", batch.num_frames, V.list_not_empty)\n    else:\n        result.add_check(\"height\", batch.height, V.positive_int)\n        result.add_check(\"width\", batch.width, V.positive_int)\n        result.add_check(\"num_frames\", batch.num_frames, V.positive_int)\n    return result\n</code></pre> fastvideo.pipelines.stages.image_encoding.ImageVAEEncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"image_latent\", batch.image_latent,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre> fastvideo.pipelines.stages.image_encoding.RefImageEncodingStage \u00b6 <pre><code>RefImageEncodingStage(image_encoder, image_processor)\n</code></pre> <p>               Bases: <code>ImageEncodingStage</code></p> <p>Stage for encoding reference image prompts into embeddings for Wan2.1 Control models.</p> <p>This stage extends ImageEncodingStage with specialized preprocessing for reference images.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def __init__(self, image_encoder, image_processor) -&gt; None:\n    \"\"\"\n    Initialize the prompt encoding stage.\n\n    Args:\n        enable_logging: Whether to enable logging for this stage.\n        is_secondary: Whether this is a secondary image encoder.\n    \"\"\"\n    super().__init__()\n    self.image_processor = image_processor\n    self.image_encoder = image_encoder\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.image_encoding.RefImageEncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode the prompt into image encoder hidden states.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded prompt embeddings.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode the prompt into image encoder hidden states.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded prompt embeddings.\n    \"\"\"\n    self.image_encoder = self.image_encoder.to(get_local_torch_device())\n\n    image = batch.pil_image\n    if image is None:\n        image = create_default_image()\n    # Preprocess reference image for CLIP encoder\n    image_tensor = preprocess_reference_image_for_clip(\n        image, get_local_torch_device())\n\n    image_inputs = self.image_processor(images=image_tensor,\n                                        return_tensors=\"pt\").to(\n                                            get_local_torch_device())\n    with set_forward_context(current_timestep=0, attn_metadata=None):\n        outputs = self.image_encoder(**image_inputs)\n        image_embeds = outputs.last_hidden_state\n    batch.image_embeds.append(image_embeds)\n\n    if batch.pil_image is None:\n        batch.image_embeds = [\n            torch.zeros_like(x) for x in batch.image_embeds\n        ]\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.image_encoding.VideoVAEEncodingStage \u00b6 <pre><code>VideoVAEEncodingStage(vae: ParallelTiledVAE)\n</code></pre> <p>               Bases: <code>ImageVAEEncodingStage</code></p> <p>Stage for encoding video pixel representations into latent space.</p> <p>This stage handles the encoding of video pixel representations for video-to-video generation and control. Inherits from ImageVAEEncodingStage to reuse common functionality.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def __init__(self, vae: ParallelTiledVAE) -&gt; None:\n    self.vae: ParallelTiledVAE = vae\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.image_encoding.VideoVAEEncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode video pixel representations into latent space.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded outputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode video pixel representations into latent space.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded outputs.\n    \"\"\"\n    assert batch.video_latent is not None, \"Video latent input is required for VideoVAEEncodingStage\"\n\n    if fastvideo_args.mode == ExecutionMode.INFERENCE:\n        assert batch.height is not None and isinstance(batch.height, int)\n        assert batch.width is not None and isinstance(batch.width, int)\n        assert batch.num_frames is not None and isinstance(\n            batch.num_frames, int)\n        height = batch.height\n        width = batch.width\n        num_frames = batch.num_frames\n    elif fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        assert batch.height is not None and isinstance(batch.height, list)\n        assert batch.width is not None and isinstance(batch.width, list)\n        assert batch.num_frames is not None and isinstance(\n            batch.num_frames, list)\n        num_frames = batch.num_frames[0]\n        height = batch.height[0]\n        width = batch.width[0]\n\n    self.vae = self.vae.to(get_local_torch_device())\n\n    # Prepare video tensor from control video\n    video_condition = self._prepare_control_video_tensor(\n        batch.video_latent, num_frames, height,\n        width).to(get_local_torch_device(), dtype=torch.float32)\n\n    # Setup VAE precision\n    vae_dtype = PRECISION_TO_TYPE[\n        fastvideo_args.pipeline_config.vae_precision]\n    vae_autocast_enabled = (\n        vae_dtype != torch.float32) and not fastvideo_args.disable_autocast\n\n    # Encode control video\n    with torch.autocast(device_type=\"cuda\",\n                        dtype=vae_dtype,\n                        enabled=vae_autocast_enabled):\n        if fastvideo_args.pipeline_config.vae_tiling:\n            self.vae.enable_tiling()\n        if not vae_autocast_enabled:\n            video_condition = video_condition.to(vae_dtype)\n        encoder_output = self.vae.encode(video_condition)\n\n    generator = batch.generator\n    if generator is None:\n        raise ValueError(\"Generator must be provided\")\n    latent_condition = self.retrieve_latents(encoder_output, generator)\n\n    if (hasattr(self.vae, \"shift_factor\")\n            and self.vae.shift_factor is not None):\n        if isinstance(self.vae.shift_factor, torch.Tensor):\n            latent_condition -= self.vae.shift_factor.to(\n                latent_condition.device, latent_condition.dtype)\n        else:\n            latent_condition -= self.vae.shift_factor\n\n    if isinstance(self.vae.scaling_factor, torch.Tensor):\n        latent_condition = latent_condition * self.vae.scaling_factor.to(\n            latent_condition.device, latent_condition.dtype)\n    else:\n        latent_condition = latent_condition * self.vae.scaling_factor\n\n    batch.video_latent = latent_condition\n\n    # Offload models if needed\n    if hasattr(self, 'maybe_free_model_hooks'):\n        self.maybe_free_model_hooks()\n\n    self.vae.to(\"cpu\")\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.image_encoding.VideoVAEEncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify video encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify video encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"video_latent\", batch.video_latent, V.not_none)\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    if fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        result.add_check(\"height\", batch.height, V.list_not_empty)\n        result.add_check(\"width\", batch.width, V.list_not_empty)\n        result.add_check(\"num_frames\", batch.num_frames, V.list_not_empty)\n    else:\n        result.add_check(\"height\", batch.height, V.positive_int)\n        result.add_check(\"width\", batch.width, V.positive_int)\n        result.add_check(\"num_frames\", batch.num_frames, V.positive_int)\n    return result\n</code></pre> fastvideo.pipelines.stages.image_encoding.VideoVAEEncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify video encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify video encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"video_latent\", batch.video_latent,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.input_validation \u00b6 <p>Input validation stage for diffusion pipelines.</p> Classes\u00b6 fastvideo.pipelines.stages.input_validation.InputValidationStage \u00b6 <p>               Bases: <code>PipelineStage</code></p> <p>Stage for validating and preparing inputs for diffusion pipelines.</p> <p>This stage validates that all required inputs are present and properly formatted before proceeding with the diffusion process.</p> Functions\u00b6 fastvideo.pipelines.stages.input_validation.InputValidationStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Validate and prepare inputs.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The validated batch information.</p> Source code in <code>fastvideo/pipelines/stages/input_validation.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Validate and prepare inputs.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The validated batch information.\n    \"\"\"\n\n    self._generate_seeds(batch, fastvideo_args)\n\n    # Ensure prompt is properly formatted\n    if batch.prompt is None and batch.prompt_embeds is None:\n        raise ValueError(\n            \"Either `prompt` or `prompt_embeds` must be provided\")\n\n    # Ensure negative prompt is properly formatted if using classifier-free guidance\n    if (batch.do_classifier_free_guidance and batch.negative_prompt is None\n            and batch.negative_prompt_embeds is None):\n        raise ValueError(\n            \"For classifier-free guidance, either `negative_prompt` or \"\n            \"`negative_prompt_embeds` must be provided\")\n\n    # Validate height and width\n    if batch.height is None or batch.width is None:\n        raise ValueError(\n            \"Height and width must be provided. Please set `height` and `width`.\"\n        )\n    if batch.height % 8 != 0 or batch.width % 8 != 0:\n        raise ValueError(\n            f\"Height and width must be divisible by 8 but are {batch.height} and {batch.width}.\"\n        )\n\n    # Validate number of inference steps\n    if batch.num_inference_steps &lt;= 0:\n        raise ValueError(\n            f\"Number of inference steps must be positive, but got {batch.num_inference_steps}\"\n        )\n\n    # Validate guidance scale if using classifier-free guidance\n    if batch.do_classifier_free_guidance and batch.guidance_scale &lt;= 0:\n        raise ValueError(\n            f\"Guidance scale must be positive, but got {batch.guidance_scale}\"\n        )\n\n    # for i2v, get image from image_path\n    # @TODO(Wei) hard-coded for wan2.2 5b ti2v for now. Should put this in image_encoding stage\n    if batch.image_path is not None:\n        if batch.image_path.endswith(\".mp4\"):\n            image = load_video(batch.image_path)[0]\n        else:\n            image = load_image(batch.image_path)\n        batch.pil_image = image\n\n    # further processing for ti2v task\n    if fastvideo_args.pipeline_config.ti2v_task and batch.pil_image is not None:\n        img = batch.pil_image\n        ih, iw = img.height, img.width\n        patch_size = fastvideo_args.pipeline_config.dit_config.arch_config.patch_size\n        vae_stride = fastvideo_args.pipeline_config.vae_config.arch_config.scale_factor_spatial\n        dh, dw = patch_size[1] * vae_stride, patch_size[2] * vae_stride\n        max_area = 704 * 1280\n        ow, oh = best_output_size(iw, ih, dw, dh, max_area)\n\n        scale = max(ow / iw, oh / ih)\n        img = img.resize((round(iw * scale), round(ih * scale)),\n                         Image.LANCZOS)\n        logger.info(\"resized img height: %s, img width: %s\", img.height,\n                    img.width)\n\n        # center-crop\n        x1 = (img.width - ow) // 2\n        y1 = (img.height - oh) // 2\n        img = img.crop((x1, y1, x1 + ow, y1 + oh))\n        assert img.width == ow and img.height == oh\n\n        # to tensor\n        img = TF.to_tensor(img).sub_(0.5).div_(0.5).to(\n            self.device).unsqueeze(1)\n        img = img.unsqueeze(0)\n        batch.height = oh\n        batch.width = ow\n        batch.pil_image = img\n\n    # for v2v, get control video from video path\n    if batch.video_path is not None:\n        pil_images, original_fps = load_video(batch.video_path,\n                                              return_fps=True)\n        logger.info(\"Loaded video with %s frames, original FPS: %s\",\n                    len(pil_images), original_fps)\n\n        # Get target parameters from batch\n        target_fps = batch.fps\n        target_num_frames = batch.num_frames\n        target_height = batch.height\n        target_width = batch.width\n\n        if target_fps is not None and original_fps is not None:\n            frame_skip = max(1, int(original_fps // target_fps))\n            if frame_skip &gt; 1:\n                pil_images = pil_images[::frame_skip]\n                effective_fps = original_fps / frame_skip\n                logger.info(\n                    \"Resampled video from %.1f fps to %.1f fps (skip=%s)\",\n                    original_fps, effective_fps, frame_skip)\n\n        # Limit to target number of frames\n        if target_num_frames is not None and len(\n                pil_images) &gt; target_num_frames:\n            pil_images = pil_images[:target_num_frames]\n            logger.info(\"Limited video to %s frames (from %s total)\",\n                        target_num_frames, len(pil_images))\n\n        # Resize each PIL image to target dimensions\n        resized_images = []\n        for pil_img in pil_images:\n            resized_img = resize(pil_img,\n                                 target_height,\n                                 target_width,\n                                 resize_mode=\"default\",\n                                 resample=\"lanczos\")\n            resized_images.append(resized_img)\n\n        # Convert PIL images to numpy array\n        video_numpy = pil_to_numpy(resized_images)\n        video_numpy = normalize(video_numpy)\n        video_tensor = numpy_to_pt(video_numpy)\n\n        # Rearrange to [C, T, H, W] and add batch dimension -&gt; [B, C, T, H, W]\n        input_video = video_tensor.permute(1, 0, 2, 3).unsqueeze(0)\n\n        batch.video_latent = input_video\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.input_validation.InputValidationStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify input validation stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/input_validation.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify input validation stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"seed\", batch.seed, [V.not_none, V.positive_int])\n    result.add_check(\"num_videos_per_prompt\", batch.num_videos_per_prompt,\n                     V.positive_int)\n    result.add_check(\n        \"prompt_or_embeds\", None, lambda _: V.string_or_list_strings(\n            batch.prompt) or V.list_not_empty(batch.prompt_embeds))\n    result.add_check(\"height\", batch.height, V.positive_int)\n    result.add_check(\"width\", batch.width, V.positive_int)\n    result.add_check(\"num_inference_steps\", batch.num_inference_steps,\n                     V.positive_int)\n    result.add_check(\n        \"guidance_scale\", batch.guidance_scale, lambda x: not batch.\n        do_classifier_free_guidance or V.positive_float(x))\n    return result\n</code></pre> fastvideo.pipelines.stages.input_validation.InputValidationStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify input validation stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/input_validation.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify input validation stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"seeds\", batch.seeds, V.list_not_empty)\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    return result\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.latent_preparation \u00b6 <p>Latent preparation stage for diffusion pipelines.</p> Classes\u00b6 fastvideo.pipelines.stages.latent_preparation.LatentPreparationStage \u00b6 <pre><code>LatentPreparationStage(scheduler, transformer)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for preparing initial latent variables for the diffusion process.</p> <p>This stage handles the preparation of the initial latent variables that will be denoised during the diffusion process.</p> Source code in <code>fastvideo/pipelines/stages/latent_preparation.py</code> <pre><code>def __init__(self, scheduler, transformer) -&gt; None:\n    super().__init__()\n    self.scheduler = scheduler\n    self.transformer = transformer\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.latent_preparation.LatentPreparationStage.adjust_video_length \u00b6 <pre><code>adjust_video_length(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; int\n</code></pre> <p>Adjust video length based on VAE version.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The batch with adjusted video length.</p> Source code in <code>fastvideo/pipelines/stages/latent_preparation.py</code> <pre><code>def adjust_video_length(self, batch: ForwardBatch,\n                        fastvideo_args: FastVideoArgs) -&gt; int:\n    \"\"\"\n    Adjust video length based on VAE version.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with adjusted video length.\n    \"\"\"\n\n    video_length = batch.num_frames\n    use_temporal_scaling_frames = fastvideo_args.pipeline_config.vae_config.use_temporal_scaling_frames\n    if use_temporal_scaling_frames:\n        temporal_scale_factor = fastvideo_args.pipeline_config.vae_config.arch_config.temporal_compression_ratio\n        latent_num_frames = (video_length - 1) // temporal_scale_factor + 1\n    else:  # stepvideo only\n        latent_num_frames = video_length // 17 * 3\n    return int(latent_num_frames)\n</code></pre> fastvideo.pipelines.stages.latent_preparation.LatentPreparationStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Prepare initial latent variables for the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with prepared latent variables.</p> Source code in <code>fastvideo/pipelines/stages/latent_preparation.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Prepare initial latent variables for the diffusion process.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with prepared latent variables.\n    \"\"\"\n\n    latent_num_frames = None\n    # Adjust video length based on VAE version if needed\n    if hasattr(self, 'adjust_video_length'):\n        latent_num_frames = self.adjust_video_length(batch, fastvideo_args)\n    # Determine batch size\n    if isinstance(batch.prompt, list):\n        batch_size = len(batch.prompt)\n    elif batch.prompt is not None:\n        batch_size = 1\n    else:\n        batch_size = batch.prompt_embeds[0].shape[0]\n\n    # Adjust batch size for number of videos per prompt\n    batch_size *= batch.num_videos_per_prompt\n\n    # Get required parameters\n    dtype = batch.prompt_embeds[0].dtype\n    device = get_local_torch_device()\n    generator = batch.generator\n    latents = batch.latents\n    num_frames = latent_num_frames if latent_num_frames is not None else batch.num_frames\n    height = batch.height\n    width = batch.width\n\n    # TODO(will): remove this once we add input/output validation for stages\n    if height is None or width is None:\n        raise ValueError(\"Height and width must be provided\")\n\n    # Calculate latent shape\n    shape = (\n        batch_size,\n        self.transformer.num_channels_latents,\n        num_frames,\n        height // fastvideo_args.pipeline_config.vae_config.arch_config.\n        spatial_compression_ratio,\n        width // fastvideo_args.pipeline_config.vae_config.arch_config.\n        spatial_compression_ratio,\n    )\n\n    # Validate generator if it's a list\n    if isinstance(generator, list) and len(generator) != batch_size:\n        raise ValueError(\n            f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n            f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n        )\n    # Generate or use provided latents\n    if latents is None:\n        latents = randn_tensor(shape,\n                               generator=generator,\n                               device=device,\n                               dtype=dtype)\n    else:\n        latents = latents.to(device)\n\n    # Scale the initial noise if needed\n    if hasattr(self.scheduler, \"init_noise_sigma\"):\n        latents = latents * self.scheduler.init_noise_sigma\n    # Update batch with prepared latents\n    batch.latents = latents\n    batch.raw_latent_shape = latents.shape\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.latent_preparation.LatentPreparationStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify latent preparation stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/latent_preparation.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify latent preparation stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\n        \"prompt_or_embeds\", None, lambda _: V.string_or_list_strings(\n            batch.prompt) or V.list_not_empty(batch.prompt_embeds))\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds,\n                     V.list_of_tensors)\n    result.add_check(\"num_videos_per_prompt\", batch.num_videos_per_prompt,\n                     V.positive_int)\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    result.add_check(\"num_frames\", batch.num_frames, V.positive_int)\n    result.add_check(\"height\", batch.height, V.positive_int)\n    result.add_check(\"width\", batch.width, V.positive_int)\n    result.add_check(\"latents\", batch.latents, V.none_or_tensor)\n    return result\n</code></pre> fastvideo.pipelines.stages.latent_preparation.LatentPreparationStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify latent preparation stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/latent_preparation.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify latent preparation stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    result.add_check(\"raw_latent_shape\", batch.raw_latent_shape, V.is_tuple)\n    return result\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.stepvideo_encoding \u00b6 Classes\u00b6 fastvideo.pipelines.stages.stepvideo_encoding.StepvideoPromptEncodingStage \u00b6 <pre><code>StepvideoPromptEncodingStage(stepllm, clip)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for encoding prompts using the remote caption API.</p> <p>This stage applies the magic string transformations and calls the remote caption service asynchronously to get:   - primary prompt embeddings,   - an attention mask,   - and a clip embedding.</p> Source code in <code>fastvideo/pipelines/stages/stepvideo_encoding.py</code> <pre><code>def __init__(self, stepllm, clip) -&gt; None:\n    super().__init__()\n    # self.caption_client = caption_client  # This should have a call_caption(prompts: List[str]) method.\n    self.stepllm = stepllm\n    self.clip = clip\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.stepvideo_encoding.StepvideoPromptEncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify stepvideo encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/stepvideo_encoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify stepvideo encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"prompt\", batch.prompt, V.string_not_empty)\n    return result\n</code></pre> fastvideo.pipelines.stages.stepvideo_encoding.StepvideoPromptEncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify stepvideo encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/stepvideo_encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify stepvideo encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds,\n                     [V.is_tensor, V.with_dims(3)])\n    result.add_check(\"negative_prompt_embeds\", batch.negative_prompt_embeds,\n                     [V.is_tensor, V.with_dims(3)])\n    result.add_check(\"prompt_attention_mask\", batch.prompt_attention_mask,\n                     [V.is_tensor, V.with_dims(2)])\n    result.add_check(\"negative_attention_mask\",\n                     batch.negative_attention_mask,\n                     [V.is_tensor, V.with_dims(2)])\n    result.add_check(\"clip_embedding_pos\", batch.clip_embedding_pos,\n                     [V.is_tensor, V.with_dims(2)])\n    result.add_check(\"clip_embedding_neg\", batch.clip_embedding_neg,\n                     [V.is_tensor, V.with_dims(2)])\n    return result\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.text_encoding \u00b6 <p>Prompt encoding stages for diffusion pipelines.</p> <p>This module contains implementations of prompt encoding stages for diffusion pipelines.</p> Classes\u00b6 fastvideo.pipelines.stages.text_encoding.TextEncodingStage \u00b6 <pre><code>TextEncodingStage(text_encoders, tokenizers)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for encoding text prompts into embeddings for diffusion models.</p> <p>This stage handles the encoding of text prompts into the embedding space expected by the diffusion model.</p> <p>Initialize the prompt encoding stage.</p> <p>Parameters:</p> Name Type Description Default <code>enable_logging</code> <p>Whether to enable logging for this stage.</p> required <code>is_secondary</code> <p>Whether this is a secondary text encoder.</p> required Source code in <code>fastvideo/pipelines/stages/text_encoding.py</code> <pre><code>def __init__(self, text_encoders, tokenizers) -&gt; None:\n    \"\"\"\n    Initialize the prompt encoding stage.\n\n    Args:\n        enable_logging: Whether to enable logging for this stage.\n        is_secondary: Whether this is a secondary text encoder.\n    \"\"\"\n    super().__init__()\n    self.tokenizers = tokenizers\n    self.text_encoders = text_encoders\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.text_encoding.TextEncodingStage.encode_text \u00b6 <pre><code>encode_text(\n    text: str | list[str],\n    fastvideo_args: FastVideoArgs,\n    encoder_index: int | list[int] | None = None,\n    return_attention_mask: bool = False,\n    return_type: str = \"list\",\n    device: device | str | None = None,\n    dtype: dtype | None = None,\n    max_length: int | None = None,\n    truncation: bool | None = None,\n    padding: bool | str | None = None,\n)\n</code></pre> <p>Encode plain text using selected text encoder(s) and return embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str | list[str]</code> <p>A single string or a list of strings to encode.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments providing pipeline config, including tokenizer and encoder settings, preprocess and postprocess functions.</p> required <code>encoder_index</code> <code>int | list[int] | None</code> <p>Encoder selector by index. Accepts an int or list of ints.</p> <code>None</code> <code>return_attention_mask</code> <code>bool</code> <p>If True, also return attention masks for each selected encoder.</p> <code>False</code> <code>return_type</code> <code>str</code> <p>\"list\" (default) returns a list aligned with selection; \"dict\" returns a dict keyed by encoder index as a string; \"stack\" stacks along a new first dimension (requires matching shapes).</p> <code>'list'</code> <code>device</code> <code>device | str | None</code> <p>Optional device override for inputs; defaults to local torch device.</p> <code>None</code> <code>dtype</code> <code>dtype | None</code> <p>Optional dtype to cast returned embeddings to.</p> <code>None</code> <code>max_length</code> <code>int | None</code> <p>Optional per-call tokenizer override.</p> <code>None</code> <code>truncation</code> <code>bool | None</code> <p>Optional per-call tokenizer override.</p> <code>None</code> <code>padding</code> <code>bool | str | None</code> <p>Optional per-call tokenizer override.</p> <code>None</code> <p>Returns:</p> Type Description <p>Depending on return_type and return_attention_mask:</p> <ul> <li>list: List[Tensor] or (List[Tensor], List[Tensor])</li> </ul> <ul> <li>dict: Dict[str, Tensor] or (Dict[str, Tensor], Dict[str, Tensor])</li> </ul> <ul> <li>stack: Tensor of shape [num_encoders, ...] or a tuple with stacked attention masks</li> </ul> Source code in <code>fastvideo/pipelines/stages/text_encoding.py</code> <pre><code>@torch.no_grad()\ndef encode_text(\n    self,\n    text: str | list[str],\n    fastvideo_args: FastVideoArgs,\n    encoder_index: int | list[int] | None = None,\n    return_attention_mask: bool = False,\n    return_type: str = \"list\",  # one of: \"list\", \"dict\", \"stack\"\n    device: torch.device | str | None = None,\n    dtype: torch.dtype | None = None,\n    max_length: int | None = None,\n    truncation: bool | None = None,\n    padding: bool | str | None = None,\n):\n    \"\"\"\n    Encode plain text using selected text encoder(s) and return embeddings.\n\n    Args:\n        text: A single string or a list of strings to encode.\n        fastvideo_args: The inference arguments providing pipeline config,\n            including tokenizer and encoder settings, preprocess and postprocess\n            functions.\n        encoder_index: Encoder selector by index. Accepts an int or list of ints.\n        return_attention_mask: If True, also return attention masks for each\n            selected encoder.\n        return_type: \"list\" (default) returns a list aligned with selection;\n            \"dict\" returns a dict keyed by encoder index as a string; \"stack\" stacks along a\n            new first dimension (requires matching shapes).\n        device: Optional device override for inputs; defaults to local torch device.\n        dtype: Optional dtype to cast returned embeddings to.\n        max_length: Optional per-call tokenizer override.\n        truncation: Optional per-call tokenizer override.\n        padding: Optional per-call tokenizer override.\n\n    Returns:\n        Depending on return_type and return_attention_mask:\n        - list: List[Tensor] or (List[Tensor], List[Tensor])\n        - dict: Dict[str, Tensor] or (Dict[str, Tensor], Dict[str, Tensor])\n        - stack: Tensor of shape [num_encoders, ...] or a tuple with stacked\n          attention masks\n    \"\"\"\n\n    assert len(self.tokenizers) == len(self.text_encoders)\n    assert len(self.text_encoders) == len(\n        fastvideo_args.pipeline_config.text_encoder_configs)\n\n    # Resolve selection into indices\n    encoder_cfgs = fastvideo_args.pipeline_config.text_encoder_configs\n    if encoder_index is None:\n        indices: list[int] = [0]\n    elif isinstance(encoder_index, int):\n        indices = [encoder_index]\n    else:\n        indices = list(encoder_index)\n    # validate range\n    num_encoders = len(self.text_encoders)\n    for idx in indices:\n        if idx &lt; 0 or idx &gt;= num_encoders:\n            raise IndexError(\n                f\"encoder index {idx} out of range [0, {num_encoders-1}]\")\n\n    # Validate indices are within range\n    num_encoders = len(self.text_encoders)\n\n    # Normalize input to list[str]\n    assert isinstance(text, str | list)\n    if isinstance(text, str):\n        texts: list[str] = [text]\n    else:\n        texts = text\n\n    embeds_list: list[torch.Tensor] = []\n    attn_masks_list: list[torch.Tensor] = []\n\n    preprocess_funcs = fastvideo_args.pipeline_config.preprocess_text_funcs\n    postprocess_funcs = fastvideo_args.pipeline_config.postprocess_text_funcs\n    encoder_cfgs = fastvideo_args.pipeline_config.text_encoder_configs\n\n    if return_type not in (\"list\", \"dict\", \"stack\"):\n        raise ValueError(\n            f\"Invalid return_type '{return_type}'. Expected one of: 'list', 'dict', 'stack'\"\n        )\n\n    target_device = device if device is not None else get_local_torch_device(\n    )\n\n    for i in indices:\n        tokenizer = self.tokenizers[i]\n        text_encoder = self.text_encoders[i]\n        encoder_config = encoder_cfgs[i]\n        preprocess_func = preprocess_funcs[i]\n        postprocess_func = postprocess_funcs[i]\n\n        processed_texts: list[str] = []\n        for prompt_str in texts:\n            processed_texts.append(preprocess_func(prompt_str))\n\n        tok_kwargs = dict(encoder_config.tokenizer_kwargs)\n        if max_length is not None:\n            tok_kwargs[\"max_length\"] = max_length\n        if truncation is not None:\n            tok_kwargs[\"truncation\"] = truncation\n        if padding is not None:\n            tok_kwargs[\"padding\"] = padding\n\n        text_inputs = tokenizer(processed_texts,\n                                **tok_kwargs).to(target_device)\n\n        input_ids = text_inputs[\"input_ids\"]\n        attention_mask = text_inputs[\"attention_mask\"]\n\n        with set_forward_context(current_timestep=0, attn_metadata=None):\n            outputs = text_encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                output_hidden_states=True,\n            )\n\n        prompt_embeds = postprocess_func(outputs)\n        if dtype is not None:\n            prompt_embeds = prompt_embeds.to(dtype=dtype)\n        embeds_list.append(prompt_embeds)\n        if return_attention_mask:\n            attn_masks_list.append(attention_mask)\n\n    # Shape results according to return_type\n    if return_type == \"list\":\n        if return_attention_mask:\n            return embeds_list, attn_masks_list\n        return embeds_list\n\n    if return_type == \"dict\":\n        key_strs = [str(i) for i in indices]\n        embeds_dict = {\n            k: v\n            for k, v in zip(key_strs, embeds_list, strict=False)\n        }\n        if return_attention_mask:\n            attn_dict = {\n                k: v\n                for k, v in zip(key_strs, attn_masks_list, strict=False)\n            }\n            return embeds_dict, attn_dict\n        return embeds_dict\n\n    # return_type == \"stack\"\n    # Validate shapes are compatible\n    base_shape = list(embeds_list[0].shape)\n    for t in embeds_list[1:]:\n        if list(t.shape) != base_shape:\n            raise ValueError(\n                f\"Cannot stack embeddings with differing shapes: {[list(t.shape) for t in embeds_list]}\"\n            )\n    stacked_embeds = torch.stack(embeds_list, dim=0)\n    if return_attention_mask:\n        base_mask_shape = list(attn_masks_list[0].shape)\n        for m in attn_masks_list[1:]:\n            if list(m.shape) != base_mask_shape:\n                raise ValueError(\n                    f\"Cannot stack attention masks with differing shapes: {[list(m.shape) for m in attn_masks_list]}\"\n                )\n        stacked_masks = torch.stack(attn_masks_list, dim=0)\n        return stacked_embeds, stacked_masks\n    return stacked_embeds\n</code></pre> fastvideo.pipelines.stages.text_encoding.TextEncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode the prompt into text encoder hidden states.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded prompt embeddings.</p> Source code in <code>fastvideo/pipelines/stages/text_encoding.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode the prompt into text encoder hidden states.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded prompt embeddings.\n    \"\"\"\n    assert len(self.tokenizers) == len(self.text_encoders)\n    assert len(self.text_encoders) == len(\n        fastvideo_args.pipeline_config.text_encoder_configs)\n\n    # Encode positive prompt with all available encoders\n    assert batch.prompt is not None\n    prompt_text: str | list[str] = batch.prompt\n    all_indices: list[int] = list(range(len(self.text_encoders)))\n    prompt_embeds_list, prompt_masks_list = self.encode_text(\n        prompt_text,\n        fastvideo_args,\n        encoder_index=all_indices,\n        return_attention_mask=True,\n    )\n    for pe in prompt_embeds_list:\n        batch.prompt_embeds.append(pe)\n    if batch.prompt_attention_mask is not None:\n        for am in prompt_masks_list:\n            batch.prompt_attention_mask.append(am)\n\n    # Encode negative prompt if CFG is enabled\n    if batch.do_classifier_free_guidance:\n        assert isinstance(batch.negative_prompt, str)\n        neg_embeds_list, neg_masks_list = self.encode_text(\n            batch.negative_prompt,\n            fastvideo_args,\n            encoder_index=all_indices,\n            return_attention_mask=True,\n        )\n        assert batch.negative_prompt_embeds is not None\n        for ne in neg_embeds_list:\n            batch.negative_prompt_embeds.append(ne)\n        if batch.negative_attention_mask is not None:\n            for nm in neg_masks_list:\n                batch.negative_attention_mask.append(nm)\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.text_encoding.TextEncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify text encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/text_encoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify text encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"prompt\", batch.prompt, V.string_or_list_strings)\n    result.add_check(\n        \"negative_prompt\", batch.negative_prompt, lambda x: not batch.\n        do_classifier_free_guidance or V.string_not_empty(x))\n    result.add_check(\"do_classifier_free_guidance\",\n                     batch.do_classifier_free_guidance, V.bool_value)\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds, V.is_list)\n    result.add_check(\"negative_prompt_embeds\", batch.negative_prompt_embeds,\n                     V.none_or_list)\n    return result\n</code></pre> fastvideo.pipelines.stages.text_encoding.TextEncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify text encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/text_encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify text encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds,\n                     V.list_of_tensors_min_dims(2))\n    result.add_check(\n        \"negative_prompt_embeds\", batch.negative_prompt_embeds,\n        lambda x: not batch.do_classifier_free_guidance or V.\n        list_of_tensors_with_min_dims(x, 2))\n    return result\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.timestep_preparation \u00b6 <p>Timestep preparation stages for diffusion pipelines.</p> <p>This module contains implementations of timestep preparation stages for diffusion pipelines.</p> Classes\u00b6 fastvideo.pipelines.stages.timestep_preparation.TimestepPreparationStage \u00b6 <pre><code>TimestepPreparationStage(scheduler)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for preparing timesteps for the diffusion process.</p> <p>This stage handles the preparation of the timestep sequence that will be used during the diffusion process.</p> Source code in <code>fastvideo/pipelines/stages/timestep_preparation.py</code> <pre><code>def __init__(self, scheduler) -&gt; None:\n    self.scheduler = scheduler\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.timestep_preparation.TimestepPreparationStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Prepare timesteps for the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with prepared timesteps.</p> Source code in <code>fastvideo/pipelines/stages/timestep_preparation.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Prepare timesteps for the diffusion process.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with prepared timesteps.\n    \"\"\"\n    scheduler = self.scheduler\n    device = get_local_torch_device()\n    num_inference_steps = batch.num_inference_steps\n    timesteps = batch.timesteps\n    sigmas = batch.sigmas\n    n_tokens = batch.n_tokens\n\n    # Prepare extra kwargs for set_timesteps\n    extra_set_timesteps_kwargs = {}\n    if n_tokens is not None and \"n_tokens\" in inspect.signature(\n            scheduler.set_timesteps).parameters:\n        extra_set_timesteps_kwargs[\"n_tokens\"] = n_tokens\n\n    # Handle custom timesteps or sigmas\n    if timesteps is not None and sigmas is not None:\n        raise ValueError(\n            \"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\"\n        )\n\n    if timesteps is not None:\n        accepts_timesteps = \"timesteps\" in inspect.signature(\n            scheduler.set_timesteps).parameters\n        if not accepts_timesteps:\n            raise ValueError(\n                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n            )\n        scheduler.set_timesteps(timesteps=timesteps,\n                                device=device,\n                                **extra_set_timesteps_kwargs)\n        timesteps = scheduler.timesteps\n    elif sigmas is not None:\n        accept_sigmas = \"sigmas\" in inspect.signature(\n            scheduler.set_timesteps).parameters\n        if not accept_sigmas:\n            raise ValueError(\n                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n                f\" sigmas schedules. Please check whether you are using the correct scheduler.\"\n            )\n        scheduler.set_timesteps(sigmas=sigmas,\n                                device=device,\n                                **extra_set_timesteps_kwargs)\n        timesteps = scheduler.timesteps\n    else:\n        scheduler.set_timesteps(num_inference_steps,\n                                device=device,\n                                **extra_set_timesteps_kwargs)\n        timesteps = scheduler.timesteps\n\n    # Update batch with prepared timesteps\n    batch.timesteps = timesteps\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.timestep_preparation.TimestepPreparationStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify timestep preparation stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/timestep_preparation.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify timestep preparation stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"num_inference_steps\", batch.num_inference_steps,\n                     V.positive_int)\n    result.add_check(\"timesteps\", batch.timesteps, V.none_or_tensor)\n    result.add_check(\"sigmas\", batch.sigmas, V.none_or_list)\n    result.add_check(\"n_tokens\", batch.n_tokens, V.none_or_positive_int)\n    return result\n</code></pre> fastvideo.pipelines.stages.timestep_preparation.TimestepPreparationStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify timestep preparation stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/timestep_preparation.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify timestep preparation stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"timesteps\", batch.timesteps,\n                     [V.is_tensor, V.with_dims(1)])\n    return result\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.validators \u00b6 <p>Common validators for pipeline stage verification.</p> <p>This module provides reusable validation functions that can be used across all pipeline stages for input/output verification.</p> Classes\u00b6 fastvideo.pipelines.stages.validators.StageValidators \u00b6 <p>Common validators for pipeline stages.</p> Functions\u00b6 fastvideo.pipelines.stages.validators.StageValidators.bool_value <code>staticmethod</code> \u00b6 <pre><code>bool_value(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a boolean.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef bool_value(value: Any) -&gt; bool:\n    \"\"\"Check if value is a boolean.\"\"\"\n    return isinstance(value, bool)\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.divisible <code>staticmethod</code> \u00b6 <pre><code>divisible(divisor: int) -&gt; Callable[[Any], bool]\n</code></pre> <p>Return a validator that checks if value is divisible by divisor.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef divisible(divisor: int) -&gt; Callable[[Any], bool]:\n    \"\"\"Return a validator that checks if value is divisible by divisor.\"\"\"\n\n    def validator(value: Any) -&gt; bool:\n        return StageValidators.divisible_by(value, divisor)\n\n    return validator\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.divisible_by <code>staticmethod</code> \u00b6 <pre><code>divisible_by(value: Any, divisor: int) -&gt; bool\n</code></pre> <p>Check if value is divisible by divisor.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef divisible_by(value: Any, divisor: int) -&gt; bool:\n    \"\"\"Check if value is divisible by divisor.\"\"\"\n    return value is not None and isinstance(value,\n                                            int) and value % divisor == 0\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.generator_or_list_generators <code>staticmethod</code> \u00b6 <pre><code>generator_or_list_generators(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a Generator or list of Generators.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef generator_or_list_generators(value: Any) -&gt; bool:\n    \"\"\"Check if value is a Generator or list of Generators.\"\"\"\n    if isinstance(value, torch.Generator):\n        return True\n    if isinstance(value, list):\n        return all(isinstance(item, torch.Generator) for item in value)\n    return False\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.is_list <code>staticmethod</code> \u00b6 <pre><code>is_list(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a list (can be empty).</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef is_list(value: Any) -&gt; bool:\n    \"\"\"Check if value is a list (can be empty).\"\"\"\n    return isinstance(value, list)\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.is_tensor <code>staticmethod</code> \u00b6 <pre><code>is_tensor(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a torch tensor and doesn't contain NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef is_tensor(value: Any) -&gt; bool:\n    \"\"\"Check if value is a torch tensor and doesn't contain NaN values.\"\"\"\n    if not isinstance(value, torch.Tensor):\n        return False\n    return not torch.isnan(value).any().item()\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.is_tuple <code>staticmethod</code> \u00b6 <pre><code>is_tuple(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a tuple.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef is_tuple(value: Any) -&gt; bool:\n    \"\"\"Check if value is a tuple.\"\"\"\n    return isinstance(value, tuple)\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.list_length <code>staticmethod</code> \u00b6 <pre><code>list_length(value: Any, length: int) -&gt; bool\n</code></pre> <p>Check if list has specific length.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef list_length(value: Any, length: int) -&gt; bool:\n    \"\"\"Check if list has specific length.\"\"\"\n    return isinstance(value, list) and len(value) == length\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.list_min_length <code>staticmethod</code> \u00b6 <pre><code>list_min_length(value: Any, min_length: int) -&gt; bool\n</code></pre> <p>Check if list has at least min_length items.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef list_min_length(value: Any, min_length: int) -&gt; bool:\n    \"\"\"Check if list has at least min_length items.\"\"\"\n    return isinstance(value, list) and len(value) &gt;= min_length\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.list_not_empty <code>staticmethod</code> \u00b6 <pre><code>list_not_empty(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a non-empty list.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef list_not_empty(value: Any) -&gt; bool:\n    \"\"\"Check if value is a non-empty list.\"\"\"\n    return isinstance(value, list) and len(value) &gt; 0\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.list_of_tensors <code>staticmethod</code> \u00b6 <pre><code>list_of_tensors(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a non-empty list where all items are tensors without NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef list_of_tensors(value: Any) -&gt; bool:\n    \"\"\"Check if value is a non-empty list where all items are tensors without NaN values.\"\"\"\n    if not isinstance(value, list) or len(value) == 0:\n        return False\n    for item in value:\n        if not isinstance(item, torch.Tensor):\n            return False\n        if torch.isnan(item).any().item():\n            return False\n    return True\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.list_of_tensors_dims <code>staticmethod</code> \u00b6 <pre><code>list_of_tensors_dims(dims: int) -&gt; Callable[[Any], bool]\n</code></pre> <p>Return a validator that checks if value is a list of tensors with specific dimensions and no NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef list_of_tensors_dims(dims: int) -&gt; Callable[[Any], bool]:\n    \"\"\"Return a validator that checks if value is a list of tensors with specific dimensions and no NaN values.\"\"\"\n\n    def validator(value: Any) -&gt; bool:\n        return StageValidators.list_of_tensors_with_dims(value, dims)\n\n    return validator\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.list_of_tensors_min_dims <code>staticmethod</code> \u00b6 <pre><code>list_of_tensors_min_dims(\n    min_dims: int,\n) -&gt; Callable[[Any], bool]\n</code></pre> <p>Return a validator that checks if value is a list of tensors with at least min_dims dimensions and no NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef list_of_tensors_min_dims(min_dims: int) -&gt; Callable[[Any], bool]:\n    \"\"\"Return a validator that checks if value is a list of tensors with at least min_dims dimensions and no NaN values.\"\"\"\n\n    def validator(value: Any) -&gt; bool:\n        return StageValidators.list_of_tensors_with_min_dims(\n            value, min_dims)\n\n    return validator\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.list_of_tensors_with_dims <code>staticmethod</code> \u00b6 <pre><code>list_of_tensors_with_dims(value: Any, dims: int) -&gt; bool\n</code></pre> <p>Check if value is a non-empty list where all items are tensors with specific dimensions and no NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef list_of_tensors_with_dims(value: Any, dims: int) -&gt; bool:\n    \"\"\"Check if value is a non-empty list where all items are tensors with specific dimensions and no NaN values.\"\"\"\n    if not isinstance(value, list) or len(value) == 0:\n        return False\n    for item in value:\n        if not isinstance(item, torch.Tensor):\n            return False\n        if item.dim() != dims:\n            return False\n        if torch.isnan(item).any().item():\n            return False\n    return True\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.list_of_tensors_with_min_dims <code>staticmethod</code> \u00b6 <pre><code>list_of_tensors_with_min_dims(\n    value: Any, min_dims: int\n) -&gt; bool\n</code></pre> <p>Check if value is a non-empty list where all items are tensors with at least min_dims dimensions and no NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef list_of_tensors_with_min_dims(value: Any, min_dims: int) -&gt; bool:\n    \"\"\"Check if value is a non-empty list where all items are tensors with at least min_dims dimensions and no NaN values.\"\"\"\n    if not isinstance(value, list) or len(value) == 0:\n        return False\n    for item in value:\n        if not isinstance(item, torch.Tensor):\n            return False\n        if item.dim() &lt; min_dims:\n            return False\n        if torch.isnan(item).any().item():\n            return False\n    return True\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.min_dims <code>staticmethod</code> \u00b6 <pre><code>min_dims(min_dims: int) -&gt; Callable[[Any], bool]\n</code></pre> <p>Return a validator that checks if tensor has at least min_dims dimensions and no NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef min_dims(min_dims: int) -&gt; Callable[[Any], bool]:\n    \"\"\"Return a validator that checks if tensor has at least min_dims dimensions and no NaN values.\"\"\"\n\n    def validator(value: Any) -&gt; bool:\n        return StageValidators.tensor_min_dims(value, min_dims)\n\n    return validator\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.non_negative_float <code>staticmethod</code> \u00b6 <pre><code>non_negative_float(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a non-negative float.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef non_negative_float(value: Any) -&gt; bool:\n    \"\"\"Check if value is a non-negative float.\"\"\"\n    return isinstance(value, int | float) and value &gt;= 0\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.none_or_list <code>staticmethod</code> \u00b6 <pre><code>none_or_list(value: Any) -&gt; bool\n</code></pre> <p>Check if value is None or a list.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef none_or_list(value: Any) -&gt; bool:\n    \"\"\"Check if value is None or a list.\"\"\"\n    return value is None or isinstance(value, list)\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.none_or_positive_int <code>staticmethod</code> \u00b6 <pre><code>none_or_positive_int(value: Any) -&gt; bool\n</code></pre> <p>Check if value is None or a positive integer.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef none_or_positive_int(value: Any) -&gt; bool:\n    \"\"\"Check if value is None or a positive integer.\"\"\"\n    return value is None or (isinstance(value, int) and value &gt; 0)\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.none_or_tensor <code>staticmethod</code> \u00b6 <pre><code>none_or_tensor(value: Any) -&gt; bool\n</code></pre> <p>Check if value is None or a tensor without NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef none_or_tensor(value: Any) -&gt; bool:\n    \"\"\"Check if value is None or a tensor without NaN values.\"\"\"\n    if value is None:\n        return True\n    if not isinstance(value, torch.Tensor):\n        return False\n    return not torch.isnan(value).any().item()\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.none_or_tensor_with_dims <code>staticmethod</code> \u00b6 <pre><code>none_or_tensor_with_dims(\n    dims: int,\n) -&gt; Callable[[Any], bool]\n</code></pre> <p>Return a validator that checks if value is None or a tensor with specific dimensions and no NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef none_or_tensor_with_dims(dims: int) -&gt; Callable[[Any], bool]:\n    \"\"\"Return a validator that checks if value is None or a tensor with specific dimensions and no NaN values.\"\"\"\n\n    def validator(value: Any) -&gt; bool:\n        if value is None:\n            return True\n        if not isinstance(value, torch.Tensor):\n            return False\n        if value.dim() != dims:\n            return False\n        return not torch.isnan(value).any().item()\n\n    return validator\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.not_none <code>staticmethod</code> \u00b6 <pre><code>not_none(value: Any) -&gt; bool\n</code></pre> <p>Check if value is not None.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef not_none(value: Any) -&gt; bool:\n    \"\"\"Check if value is not None.\"\"\"\n    return value is not None\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.positive_float <code>staticmethod</code> \u00b6 <pre><code>positive_float(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a positive float.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef positive_float(value: Any) -&gt; bool:\n    \"\"\"Check if value is a positive float.\"\"\"\n    return isinstance(value, int | float) and value &gt; 0\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.positive_int <code>staticmethod</code> \u00b6 <pre><code>positive_int(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a positive integer.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef positive_int(value: Any) -&gt; bool:\n    \"\"\"Check if value is a positive integer.\"\"\"\n    return isinstance(value, int) and value &gt; 0\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.positive_int_divisible <code>staticmethod</code> \u00b6 <pre><code>positive_int_divisible(\n    divisor: int,\n) -&gt; Callable[[Any], bool]\n</code></pre> <p>Return a validator that checks if value is a positive integer divisible by divisor.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef positive_int_divisible(divisor: int) -&gt; Callable[[Any], bool]:\n    \"\"\"Return a validator that checks if value is a positive integer divisible by divisor.\"\"\"\n\n    def validator(value: Any) -&gt; bool:\n        return (isinstance(value, int) and value &gt; 0\n                and StageValidators.divisible_by(value, divisor))\n\n    return validator\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.string_not_empty <code>staticmethod</code> \u00b6 <pre><code>string_not_empty(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a non-empty string.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef string_not_empty(value: Any) -&gt; bool:\n    \"\"\"Check if value is a non-empty string.\"\"\"\n    return isinstance(value, str) and len(value.strip()) &gt; 0\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.string_or_list_strings <code>staticmethod</code> \u00b6 <pre><code>string_or_list_strings(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a string or list of strings.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef string_or_list_strings(value: Any) -&gt; bool:\n    \"\"\"Check if value is a string or list of strings.\"\"\"\n    if isinstance(value, str):\n        return True\n    if isinstance(value, list):\n        return all(isinstance(item, str) for item in value)\n    return False\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.tensor_min_dims <code>staticmethod</code> \u00b6 <pre><code>tensor_min_dims(value: Any, min_dims: int) -&gt; bool\n</code></pre> <p>Check if value is a tensor with at least min_dims dimensions and no NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef tensor_min_dims(value: Any, min_dims: int) -&gt; bool:\n    \"\"\"Check if value is a tensor with at least min_dims dimensions and no NaN values.\"\"\"\n    if not isinstance(value, torch.Tensor):\n        return False\n    if value.dim() &lt; min_dims:\n        return False\n    return not torch.isnan(value).any().item()\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.tensor_shape_matches <code>staticmethod</code> \u00b6 <pre><code>tensor_shape_matches(\n    value: Any, expected_shape: tuple\n) -&gt; bool\n</code></pre> <p>Check if tensor shape matches expected shape (None for any size) and no NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef tensor_shape_matches(value: Any, expected_shape: tuple) -&gt; bool:\n    \"\"\"Check if tensor shape matches expected shape (None for any size) and no NaN values.\"\"\"\n    if not isinstance(value, torch.Tensor):\n        return False\n    if len(value.shape) != len(expected_shape):\n        return False\n    for actual, expected in zip(value.shape, expected_shape, strict=True):\n        if expected is not None and actual != expected:\n            return False\n    return not torch.isnan(value).any().item()\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.tensor_with_dims <code>staticmethod</code> \u00b6 <pre><code>tensor_with_dims(value: Any, dims: int) -&gt; bool\n</code></pre> <p>Check if value is a tensor with specific dimensions and no NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef tensor_with_dims(value: Any, dims: int) -&gt; bool:\n    \"\"\"Check if value is a tensor with specific dimensions and no NaN values.\"\"\"\n    if not isinstance(value, torch.Tensor):\n        return False\n    if value.dim() != dims:\n        return False\n    return not torch.isnan(value).any().item()\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.with_dims <code>staticmethod</code> \u00b6 <pre><code>with_dims(dims: int) -&gt; Callable[[Any], bool]\n</code></pre> <p>Return a validator that checks if tensor has specific dimensions and no NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef with_dims(dims: int) -&gt; Callable[[Any], bool]:\n    \"\"\"Return a validator that checks if tensor has specific dimensions and no NaN values.\"\"\"\n\n    def validator(value: Any) -&gt; bool:\n        return StageValidators.tensor_with_dims(value, dims)\n\n    return validator\n</code></pre> fastvideo.pipelines.stages.validators.ValidationFailure \u00b6 <pre><code>ValidationFailure(\n    validator_name: str,\n    actual_value: Any,\n    expected: str | None = None,\n    error_msg: str | None = None,\n)\n</code></pre> <p>Details about a specific validation failure.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>def __init__(self,\n             validator_name: str,\n             actual_value: Any,\n             expected: str | None = None,\n             error_msg: str | None = None):\n    self.validator_name = validator_name\n    self.actual_value = actual_value\n    self.expected = expected\n    self.error_msg = error_msg\n</code></pre> fastvideo.pipelines.stages.validators.VerificationResult \u00b6 <pre><code>VerificationResult()\n</code></pre> <p>Wrapper class for stage verification results.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>def __init__(self) -&gt; None:\n    self._checks: dict[str, bool] = {}\n    self._failures: dict[str, list[ValidationFailure]] = {}\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.validators.VerificationResult.add_check \u00b6 <pre><code>add_check(\n    field_name: str,\n    value: Any,\n    validators: Callable[[Any], bool]\n    | list[Callable[[Any], bool]],\n) -&gt; VerificationResult\n</code></pre> <p>Add a validation check for a field.</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the field being checked</p> required <code>value</code> <code>Any</code> <p>The actual value to validate</p> required <code>validators</code> <code>Callable[[Any], bool] | list[Callable[[Any], bool]]</code> <p>Single validation function or list of validation functions.        Each function will be called with the value as its first argument.</p> required <p>Returns:</p> Type Description <code>VerificationResult</code> <p>Self for method chaining</p> <p>Examples:</p> fastvideo.pipelines.stages.validators.VerificationResult.get_detailed_failures \u00b6 <pre><code>get_detailed_failures() -&gt; dict[\n    str, list[ValidationFailure]\n]\n</code></pre> <p>Get detailed failure information for each failed field.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>def get_detailed_failures(self) -&gt; dict[str, list[ValidationFailure]]:\n    \"\"\"Get detailed failure information for each failed field.\"\"\"\n    return self._failures.copy()\n</code></pre> fastvideo.pipelines.stages.validators.VerificationResult.get_failed_fields \u00b6 <pre><code>get_failed_fields() -&gt; list[str]\n</code></pre> <p>Get list of fields that failed validation.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>def get_failed_fields(self) -&gt; list[str]:\n    \"\"\"Get list of fields that failed validation.\"\"\"\n    return [field for field, passed in self._checks.items() if not passed]\n</code></pre> fastvideo.pipelines.stages.validators.VerificationResult.get_failure_summary \u00b6 <pre><code>get_failure_summary() -&gt; str\n</code></pre> <p>Get a comprehensive summary of all validation failures.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>def get_failure_summary(self) -&gt; str:\n    \"\"\"Get a comprehensive summary of all validation failures.\"\"\"\n    if self.is_valid():\n        return \"All validations passed\"\n\n    summary_parts = []\n    for field_name, failures in self._failures.items():\n        field_summary = f\"\\n  Field '{field_name}':\"\n        for i, failure in enumerate(failures, 1):\n            field_summary += f\"\\n    {i}. {failure}\"\n        summary_parts.append(field_summary)\n\n    return \"Validation failures:\" + \"\".join(summary_parts)\n</code></pre> fastvideo.pipelines.stages.validators.VerificationResult.is_valid \u00b6 <pre><code>is_valid() -&gt; bool\n</code></pre> <p>Check if all validations passed.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>def is_valid(self) -&gt; bool:\n    \"\"\"Check if all validations passed.\"\"\"\n    return all(self._checks.values())\n</code></pre> fastvideo.pipelines.stages.validators.VerificationResult.to_dict \u00b6 <pre><code>to_dict() -&gt; dict\n</code></pre> <p>Convert to dictionary for backward compatibility.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Convert to dictionary for backward compatibility.\"\"\"\n    return self._checks.copy()\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.validators.VerificationResult.add_check--single-validator","title":"Single validator","text":"<p>result.add_check(\"tensor\", my_tensor, V.is_tensor)</p>"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.validators.VerificationResult.add_check--multiple-validators-all-must-pass","title":"Multiple validators (all must pass)","text":"<p>result.add_check(\"latents\", batch.latents, [V.is_tensor, V.with_dims(5)])</p>"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.validators.VerificationResult.add_check--using-partial-functions-for-parameters","title":"Using partial functions for parameters","text":"<p>result.add_check(\"height\", batch.height, [V.not_none, V.divisible(8)])</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>def add_check(\n    self, field_name: str, value: Any,\n    validators: Callable[[Any], bool] | list[Callable[[Any], bool]]\n) -&gt; 'VerificationResult':\n    \"\"\"\n    Add a validation check for a field.\n\n    Args:\n        field_name: Name of the field being checked\n        value: The actual value to validate\n        validators: Single validation function or list of validation functions.\n                   Each function will be called with the value as its first argument.\n\n    Returns:\n        Self for method chaining\n\n    Examples:\n        # Single validator\n        result.add_check(\"tensor\", my_tensor, V.is_tensor)\n\n        # Multiple validators (all must pass)\n        result.add_check(\"latents\", batch.latents, [V.is_tensor, V.with_dims(5)])\n\n        # Using partial functions for parameters\n        result.add_check(\"height\", batch.height, [V.not_none, V.divisible(8)])\n    \"\"\"\n    if not isinstance(validators, list):\n        validators = [validators]\n\n    failures = []\n    all_passed = True\n\n    # Apply all validators and collect detailed failure info\n    for validator in validators:\n        try:\n            passed = validator(value)\n            if not passed:\n                all_passed = False\n                failure = self._create_validation_failure(validator, value)\n                failures.append(failure)\n        except Exception as e:\n            # If any validator raises an exception, consider the check failed\n            all_passed = False\n            validator_name = getattr(validator, '__name__', str(validator))\n            failure = ValidationFailure(\n                validator_name=validator_name,\n                actual_value=value,\n                error_msg=f\"Exception during validation: {str(e)}\")\n            failures.append(failure)\n\n    self._checks[field_name] = all_passed\n    if not all_passed:\n        self._failures[field_name] = failures\n\n    return self\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.training","title":"fastvideo.pipelines.training","text":"<p>Training pipelines for fastvideo.v1.</p> <p>This package contains pipelines for training diffusion models.</p>"},{"location":"api/fastvideo/#submodules_2","title":"Submodules","text":""},{"location":"api/fastvideo/#fastvideopipelinescomposed_pipeline_base","title":"fastvideo.pipelines.composed_pipeline_base","text":""},{"location":"api/fastvideo/#fastvideo.pipelines.composed_pipeline_base","title":"composed_pipeline_base","text":"<p>Base class for composed pipelines.</p> <p>This module defines the base class for pipelines that are composed of multiple stages.</p>"},{"location":"api/fastvideo/#fastvideo.pipelines.composed_pipeline_base-classes","title":"Classes","text":""},{"location":"api/fastvideo/#fastvideo.pipelines.composed_pipeline_base.ComposedPipelineBase","title":"fastvideo.pipelines.composed_pipeline_base.ComposedPipelineBase","text":"<pre><code>ComposedPipelineBase(\n    model_path: str,\n    fastvideo_args: FastVideoArgs | TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for pipelines composed of multiple stages.</p> <p>This class provides the framework for creating pipelines by composing multiple stages together. Each stage is responsible for a specific part of the diffusion process, and the pipeline orchestrates the execution of these stages.</p> <p>Initialize the pipeline. After init, the pipeline should be ready to use. The pipeline should be stateless and not hold any batch state.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def __init__(self,\n             model_path: str,\n             fastvideo_args: FastVideoArgs | TrainingArgs,\n             required_config_modules: list[str] | None = None,\n             loaded_modules: dict[str, torch.nn.Module] | None = None):\n    \"\"\"\n    Initialize the pipeline. After __init__, the pipeline should be ready to\n    use. The pipeline should be stateless and not hold any batch state.\n    \"\"\"\n    self.fastvideo_args = fastvideo_args\n\n    self.model_path: str = model_path\n    self._stages: list[PipelineStage] = []\n    self._stage_name_mapping: dict[str, PipelineStage] = {}\n\n    if required_config_modules is not None:\n        self._required_config_modules = required_config_modules\n\n    if self._required_config_modules is None:\n        raise NotImplementedError(\n            \"Subclass must set _required_config_modules\")\n\n    maybe_init_distributed_environment_and_model_parallel(\n        fastvideo_args.tp_size, fastvideo_args.sp_size)\n\n    # Torch profiler. Enabled and configured through env vars:\n    # FASTVIDEO_TORCH_PROFILER_DIR=/path/to/save/trace\n    trace_dir = envs.FASTVIDEO_TORCH_PROFILER_DIR\n    self.profiler_controller = get_or_create_profiler(trace_dir)\n    self.profiler = self.profiler_controller.profiler\n\n    self.local_rank = get_world_group().local_rank\n\n    # Load modules directly in initialization\n    logger.info(\"Loading pipeline modules...\")\n    with self.profiler_controller.region(\"profiler_region_model_loading\"):\n        self.modules = self.load_modules(fastvideo_args, loaded_modules)\n</code></pre> Attributes\u00b6 fastvideo.pipelines.composed_pipeline_base.ComposedPipelineBase.required_config_modules <code>property</code> \u00b6 <pre><code>required_config_modules: list[str]\n</code></pre> <p>List of modules that are required by the pipeline. The names should match the diffusers directory and model_index.json file. These modules will be loaded using the PipelineComponentLoader and made available in the modules dictionary. Access these modules using the get_module method.</p> <p>class ConcretePipeline(ComposedPipelineBase):     _required_config_modules = [\"vae\", \"text_encoder\", \"transformer\", \"scheduler\", \"tokenizer\"]</p> <pre><code>@property\ndef required_config_modules(self):\n    return self._required_config_modules\n</code></pre> fastvideo.pipelines.composed_pipeline_base.ComposedPipelineBase.stages <code>property</code> \u00b6 <pre><code>stages: list[PipelineStage]\n</code></pre> <p>List of stages in the pipeline.</p> Functions\u00b6 fastvideo.pipelines.composed_pipeline_base.ComposedPipelineBase.create_pipeline_stages <code>abstractmethod</code> \u00b6 <pre><code>create_pipeline_stages(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Create the inference pipeline stages.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>@abstractmethod\ndef create_pipeline_stages(self, fastvideo_args: FastVideoArgs):\n    \"\"\"\n    Create the inference pipeline stages.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.pipelines.composed_pipeline_base.ComposedPipelineBase.create_training_stages \u00b6 <pre><code>create_training_stages(training_args: TrainingArgs)\n</code></pre> <p>Create the training pipeline stages.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def create_training_stages(self, training_args: TrainingArgs):\n    \"\"\"\n    Create the training pipeline stages.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.pipelines.composed_pipeline_base.ComposedPipelineBase.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Generate a video or image using the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The batch to generate from.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:     ForwardBatch: The batch with the generated video or image.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Generate a video or image using the pipeline.\n\n    Args:\n        batch: The batch to generate from.\n        fastvideo_args: The inference arguments.\n    Returns:\n        ForwardBatch: The batch with the generated video or image.\n    \"\"\"\n    if not self.post_init_called:\n        self.post_init()\n\n    # Execute each stage\n    logger.info(\"Running pipeline stages: %s\",\n                self._stage_name_mapping.keys())\n    # logger.info(\"Batch: %s\", batch)\n    for stage in self.stages:\n        batch = stage(batch, fastvideo_args)\n\n    # Return the output\n    return batch\n</code></pre> fastvideo.pipelines.composed_pipeline_base.ComposedPipelineBase.from_pretrained <code>classmethod</code> \u00b6 <pre><code>from_pretrained(\n    model_path: str,\n    device: str | None = None,\n    torch_dtype: dtype | None = None,\n    pipeline_config: str | PipelineConfig | None = None,\n    args: Namespace | None = None,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n    **kwargs\n) -&gt; ComposedPipelineBase\n</code></pre> <p>Load a pipeline from a pretrained model. loaded_modules: Optional[Dict[str, torch.nn.Module]] = None, If provided, loaded_modules will be used instead of loading from config/pretrained weights.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>@classmethod\ndef from_pretrained(cls,\n                    model_path: str,\n                    device: str | None = None,\n                    torch_dtype: torch.dtype | None = None,\n                    pipeline_config: str | PipelineConfig | None = None,\n                    args: argparse.Namespace | None = None,\n                    required_config_modules: list[str] | None = None,\n                    loaded_modules: dict[str, torch.nn.Module]\n                    | None = None,\n                    **kwargs) -&gt; \"ComposedPipelineBase\":\n    \"\"\"\n    Load a pipeline from a pretrained model.\n    loaded_modules: Optional[Dict[str, torch.nn.Module]] = None,\n    If provided, loaded_modules will be used instead of loading from config/pretrained weights.\n    \"\"\"\n    if args is None or args.inference_mode:\n\n        kwargs['model_path'] = model_path\n        fastvideo_args = FastVideoArgs.from_kwargs(**kwargs)\n    else:\n        assert args is not None, \"args must be provided for training mode\"\n        fastvideo_args = TrainingArgs.from_cli_args(args)\n        # TODO(will): fix this so that its not so ugly\n        fastvideo_args.model_path = model_path\n        for key, value in kwargs.items():\n            setattr(fastvideo_args, key, value)\n\n        fastvideo_args.dit_cpu_offload = False\n        # we hijack the precision to be the master weight type so that the\n        # model is loaded with the correct precision. Subsequently we will\n        # use FSDP2's MixedPrecisionPolicy to set the precision for the\n        # fwd, bwd, and other operations' precision.\n        assert fastvideo_args.pipeline_config.dit_precision == 'fp32', 'only fp32 is supported for training'\n\n    logger.info(\"fastvideo_args in from_pretrained: %s\", fastvideo_args)\n\n    pipe = cls(model_path,\n               fastvideo_args,\n               required_config_modules=required_config_modules,\n               loaded_modules=loaded_modules)\n    pipe.post_init()\n    return pipe\n</code></pre> fastvideo.pipelines.composed_pipeline_base.ComposedPipelineBase.initialize_pipeline \u00b6 <pre><code>initialize_pipeline(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Initialize the pipeline.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def initialize_pipeline(self, fastvideo_args: FastVideoArgs):\n    \"\"\"\n    Initialize the pipeline.\n    \"\"\"\n    return\n</code></pre> fastvideo.pipelines.composed_pipeline_base.ComposedPipelineBase.load_modules \u00b6 <pre><code>load_modules(\n    fastvideo_args: FastVideoArgs,\n    loaded_modules: dict[str, Module] | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Load the modules from the config. loaded_modules: Optional[Dict[str, torch.nn.Module]] = None,  If provided, loaded_modules will be used instead of loading from config/pretrained weights.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def load_modules(\n    self,\n    fastvideo_args: FastVideoArgs,\n    loaded_modules: dict[str, torch.nn.Module] | None = None\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Load the modules from the config.\n    loaded_modules: Optional[Dict[str, torch.nn.Module]] = None, \n    If provided, loaded_modules will be used instead of loading from config/pretrained weights.\n    \"\"\"\n\n    model_index = self._load_config(self.model_path)\n    logger.info(\"Loading pipeline modules from config: %s\", model_index)\n\n    # remove keys that are not pipeline modules\n    model_index.pop(\"_class_name\")\n    model_index.pop(\"_diffusers_version\")\n    if \"boundary_ratio\" in model_index and model_index[\n            \"boundary_ratio\"] is not None:\n        logger.info(\n            \"MoE pipeline detected. Adding transformer_2 to self.required_config_modules...\"\n        )\n        self.required_config_modules.append(\"transformer_2\")\n        logger.info(\"MoE pipeline detected. Setting boundary ratio to %s\",\n                    model_index[\"boundary_ratio\"])\n        fastvideo_args.pipeline_config.dit_config.boundary_ratio = model_index[\n            \"boundary_ratio\"]\n\n    model_index.pop(\"boundary_ratio\", None)\n    # used by Wan2.2 ti2v\n    model_index.pop(\"expand_timesteps\", None)\n\n    # some sanity checks\n    assert len(\n        model_index\n    ) &gt; 1, \"model_index.json must contain at least one pipeline module\"\n\n    for module_name in self.required_config_modules:\n        if module_name not in model_index and module_name in self._extra_config_module_map:\n            extra_module_value = self._extra_config_module_map[module_name]\n            logger.warning(\n                \"model_index.json does not contain a %s module, but found {%s: %s} in _extra_config_module_map, adding to model_index.\",\n                module_name, module_name, extra_module_value)\n            if extra_module_value in model_index:\n                logger.info(\"Using module %s for %s\", extra_module_value,\n                            module_name)\n                model_index[module_name] = model_index[extra_module_value]\n                continue\n            else:\n                raise ValueError(\n                    f\"Required module key: {module_name} value: {model_index.get(module_name)} was not found in loaded modules {model_index.keys()}\"\n                )\n\n    # all the component models used by the pipeline\n    required_modules = self.required_config_modules\n    logger.info(\"Loading required modules: %s\", required_modules)\n\n    modules = {}\n    for module_name, (transformers_or_diffusers,\n                      architecture) in model_index.items():\n        if transformers_or_diffusers is None:\n            logger.warning(\n                \"Module %s in model_index.json has null value, removing from required_config_modules\",\n                module_name)\n            if module_name in self.required_config_modules:\n                self.required_config_modules.remove(module_name)\n            continue\n        if module_name not in required_modules:\n            logger.info(\"Skipping module %s\", module_name)\n            continue\n        if loaded_modules is not None and module_name in loaded_modules:\n            logger.info(\"Using module %s already provided\", module_name)\n            modules[module_name] = loaded_modules[module_name]\n            continue\n\n        # we load the module from the extra config module map if it exists\n        if module_name in self._extra_config_module_map:\n            load_module_name = self._extra_config_module_map[module_name]\n        else:\n            load_module_name = module_name\n\n        component_model_path = os.path.join(self.model_path,\n                                            load_module_name)\n        module = PipelineComponentLoader.load_module(\n            module_name=load_module_name,\n            component_model_path=component_model_path,\n            transformers_or_diffusers=transformers_or_diffusers,\n            fastvideo_args=fastvideo_args,\n        )\n        logger.info(\"Loaded module %s from %s\", module_name,\n                    component_model_path)\n\n        if module_name in modules:\n            logger.warning(\"Overwriting module %s\", module_name)\n        modules[module_name] = module\n\n    # Check if all required modules were loaded\n    for module_name in required_modules:\n        if module_name not in modules or modules[module_name] is None:\n            raise ValueError(\n                f\"Required module key: {module_name} value: {modules.get(module_name)} was not found in loaded modules {modules.keys()}\"\n            )\n\n    return modules\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.composed_pipeline_base-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideopipelineslora_pipeline","title":"fastvideo.pipelines.lora_pipeline","text":""},{"location":"api/fastvideo/#fastvideo.pipelines.lora_pipeline","title":"lora_pipeline","text":""},{"location":"api/fastvideo/#fastvideo.pipelines.lora_pipeline-classes","title":"Classes","text":""},{"location":"api/fastvideo/#fastvideo.pipelines.lora_pipeline.LoRAPipeline","title":"fastvideo.pipelines.lora_pipeline.LoRAPipeline","text":"<pre><code>LoRAPipeline(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ComposedPipelineBase</code></p> <p>Pipeline that supports injecting LoRA adapters into the diffusion transformer. TODO: support training.</p> Source code in <code>fastvideo/pipelines/lora_pipeline.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.device = get_local_torch_device()\n    self.exclude_lora_layers = self.modules[\n        \"transformer\"].config.arch_config.exclude_lora_layers\n    self.lora_target_modules = self.fastvideo_args.lora_target_modules\n    self.lora_path = self.fastvideo_args.lora_path\n    self.lora_nickname = self.fastvideo_args.lora_nickname\n    self.training_mode = self.fastvideo_args.training_mode\n    if self.training_mode and getattr(self.fastvideo_args, \"lora_training\",\n                                      False):\n        assert isinstance(self.fastvideo_args, TrainingArgs)\n        if self.fastvideo_args.lora_alpha is None:\n            self.fastvideo_args.lora_alpha = self.fastvideo_args.lora_rank\n        self.lora_rank = self.fastvideo_args.lora_rank  # type: ignore\n        self.lora_alpha = self.fastvideo_args.lora_alpha  # type: ignore\n        logger.info(\"Using LoRA training with rank %d and alpha %d\",\n                    self.lora_rank, self.lora_alpha)\n        if self.lora_target_modules is None:\n            self.lora_target_modules = [\n                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"to_q\", \"to_k\",\n                \"to_v\", \"to_out\", \"to_qkv\"\n            ]\n        self.convert_to_lora_layers()\n    # Inference\n    elif not self.training_mode and self.lora_path is not None:\n        self.convert_to_lora_layers()\n        self.set_lora_adapter(\n            self.lora_nickname,  # type: ignore\n            self.lora_path)  # type: ignore\n</code></pre> Functions\u00b6 fastvideo.pipelines.lora_pipeline.LoRAPipeline.convert_to_lora_layers \u00b6 <pre><code>convert_to_lora_layers() -&gt; None\n</code></pre> <p>Unified method to convert the transformer to a LoRA transformer.</p> Source code in <code>fastvideo/pipelines/lora_pipeline.py</code> <pre><code>def convert_to_lora_layers(self) -&gt; None:\n    \"\"\"\n    Unified method to convert the transformer to a LoRA transformer.\n    \"\"\"\n    if self.lora_initialized:\n        return\n    self.lora_initialized = True\n    converted_count = 0\n    for name, layer in self.modules[\"transformer\"].named_modules():\n        if not self.is_target_layer(name):\n            continue\n\n        excluded = False\n        for exclude_layer in self.exclude_lora_layers:\n            if exclude_layer in name:\n                excluded = True\n                break\n        if excluded:\n            continue\n\n        layer = get_lora_layer(layer,\n                               lora_rank=self.lora_rank,\n                               lora_alpha=self.lora_alpha,\n                               training_mode=self.training_mode)\n        if layer is not None:\n            self.lora_layers[name] = layer\n            replace_submodule(self.modules[\"transformer\"], name, layer)\n            converted_count += 1\n    logger.info(\"Converted %d layers to LoRA layers\", converted_count)\n\n    if \"fake_score_transformer\" in self.modules:\n        for name, layer in self.modules[\n                \"fake_score_transformer\"].named_modules():\n            if not self.is_target_layer(name):\n                continue\n            layer = get_lora_layer(layer,\n                                   lora_rank=self.lora_rank,\n                                   lora_alpha=self.lora_alpha,\n                                   training_mode=self.training_mode)\n            if layer is not None:\n                self.lora_layers_critic[name] = layer\n                replace_submodule(self.modules[\"fake_score_transformer\"],\n                                  name, layer)\n                converted_count += 1\n        logger.info(\n            \"Converted %d layers to LoRA layers in the critic model\",\n            converted_count)\n</code></pre> fastvideo.pipelines.lora_pipeline.LoRAPipeline.set_lora_adapter \u00b6 <pre><code>set_lora_adapter(\n    lora_nickname: str, lora_path: str | None = None\n)\n</code></pre> <p>Load a LoRA adapter into the pipeline and merge it into the transformer. Args:     lora_nickname: The \"nick name\" of the adapter when referenced in the pipeline.     lora_path: The path to the adapter, either a local path or a Hugging Face repo id.</p> Source code in <code>fastvideo/pipelines/lora_pipeline.py</code> <pre><code>def set_lora_adapter(self,\n                     lora_nickname: str,\n                     lora_path: str | None = None):  # type: ignore\n    \"\"\"\n    Load a LoRA adapter into the pipeline and merge it into the transformer.\n    Args:\n        lora_nickname: The \"nick name\" of the adapter when referenced in the pipeline.\n        lora_path: The path to the adapter, either a local path or a Hugging Face repo id.\n    \"\"\"\n\n    if lora_nickname not in self.lora_adapters and lora_path is None:\n        raise ValueError(\n            f\"Adapter {lora_nickname} not found in the pipeline. Please provide lora_path to load it.\"\n        )\n    if not self.lora_initialized:\n        self.convert_to_lora_layers()\n    adapter_updated = False\n    rank = dist.get_rank()\n    if lora_path is not None and lora_path != self.cur_adapter_path:\n        lora_local_path = maybe_download_lora(lora_path)\n        lora_state_dict = load_file(lora_local_path)\n\n        # Map the hf layer names to our custom layer names\n        param_names_mapping_fn = get_param_names_mapping(\n            self.modules[\"transformer\"].param_names_mapping)\n        lora_param_names_mapping_fn = get_param_names_mapping(\n            self.modules[\"transformer\"].lora_param_names_mapping)\n\n        to_merge_params: defaultdict[Hashable,\n                                     dict[Any, Any]] = defaultdict(dict)\n        for name, weight in lora_state_dict.items():\n            name = name.replace(\"diffusion_model.\", \"\")\n            name = name.replace(\".weight\", \"\")\n            name, _, _ = lora_param_names_mapping_fn(name)\n            target_name, merge_index, num_params_to_merge = param_names_mapping_fn(\n                name)\n            # for (in_dim, r) @ (r, out_dim), we only merge (r, out_dim * n) where n is the number of linear layers to fuse\n            # see param mapping in HunyuanVideoArchConfig\n            if merge_index is not None and \"lora_B\" in name:\n                to_merge_params[target_name][merge_index] = weight\n                if len(to_merge_params[target_name]) == num_params_to_merge:\n                    # cat at output dim according to the merge_index order\n                    sorted_tensors = [\n                        to_merge_params[target_name][i]\n                        for i in range(num_params_to_merge)\n                    ]\n                    weight = torch.cat(sorted_tensors, dim=1)\n                    del to_merge_params[target_name]\n                else:\n                    continue\n\n            if target_name in self.lora_adapters[lora_nickname]:\n                raise ValueError(\n                    f\"Target name {target_name} already exists in lora_adapters[{lora_nickname}]\"\n                )\n            self.lora_adapters[lora_nickname][target_name] = weight.to(\n                self.device)\n        adapter_updated = True\n        self.cur_adapter_path = lora_path\n        logger.info(\"Rank %d: loaded LoRA adapter %s\", rank, lora_path)\n\n    if not adapter_updated and self.cur_adapter_name == lora_nickname:\n        return\n    self.cur_adapter_name = lora_nickname\n\n    # Merge the new adapter\n    adapted_count = 0\n    for name, layer in self.lora_layers.items():\n        lora_A_name = name + \".lora_A\"\n        lora_B_name = name + \".lora_B\"\n        if lora_A_name in self.lora_adapters[lora_nickname]\\\n            and lora_B_name in self.lora_adapters[lora_nickname]:\n            layer.set_lora_weights(\n                self.lora_adapters[lora_nickname][lora_A_name],\n                self.lora_adapters[lora_nickname][lora_B_name],\n                training_mode=self.fastvideo_args.training_mode,\n                lora_path=lora_path)\n            adapted_count += 1\n        else:\n            if rank == 0:\n                logger.warning(\n                    \"LoRA adapter %s does not contain the weights for layer %s. LoRA will not be applied to it.\",\n                    lora_path, name)\n            layer.disable_lora = True\n    logger.info(\"Rank %d: LoRA adapter %s applied to %d layers\", rank,\n                lora_path, adapted_count)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.lora_pipeline-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideopipelinespipeline_batch_info","title":"fastvideo.pipelines.pipeline_batch_info","text":""},{"location":"api/fastvideo/#fastvideo.pipelines.pipeline_batch_info","title":"pipeline_batch_info","text":"<p>Data structures for functional pipeline processing.</p> <p>This module defines the dataclasses used to pass state between pipeline components in a functional manner, reducing the need for explicit parameter passing.</p>"},{"location":"api/fastvideo/#fastvideo.pipelines.pipeline_batch_info-classes","title":"Classes","text":""},{"location":"api/fastvideo/#fastvideo.pipelines.pipeline_batch_info.ForwardBatch","title":"fastvideo.pipelines.pipeline_batch_info.ForwardBatch  <code>dataclass</code>","text":"<pre><code>ForwardBatch(\n    data_type: str,\n    generator: Generator | list[Generator] | None = None,\n    image_path: str | None = None,\n    image_embeds: list[Tensor] = list(),\n    pil_image: Tensor | Image | None = None,\n    preprocessed_image: Tensor | None = None,\n    prompt: str | list[str] | None = None,\n    negative_prompt: str | list[str] | None = None,\n    prompt_path: str | None = None,\n    output_path: str = \"outputs/\",\n    output_video_name: str | None = None,\n    video_path: str | None = None,\n    video_latent: Tensor | None = None,\n    prompt_embeds: list[Tensor] = list(),\n    negative_prompt_embeds: list[Tensor] | None = None,\n    prompt_attention_mask: list[Tensor] | None = None,\n    negative_attention_mask: list[Tensor] | None = None,\n    clip_embedding_pos: list[Tensor] | None = None,\n    clip_embedding_neg: list[Tensor] | None = None,\n    max_sequence_length: int | None = None,\n    prompt_template: dict[str, Any] | None = None,\n    do_classifier_free_guidance: bool = False,\n    batch_size: int | None = None,\n    num_videos_per_prompt: int = 1,\n    seed: int | None = None,\n    seeds: list[int] | None = None,\n    is_prompt_processed: bool = False,\n    latents: Tensor | None = None,\n    raw_latent_shape: Tensor | None = None,\n    noise_pred: Tensor | None = None,\n    image_latent: Tensor | None = None,\n    height_latents: list[int] | int | None = None,\n    width_latents: list[int] | int | None = None,\n    num_frames: list[int] | int = 1,\n    num_frames_round_down: bool = False,\n    height: list[int] | int | None = None,\n    width: list[int] | int | None = None,\n    fps: list[int] | int | None = None,\n    timesteps: Tensor | None = None,\n    timestep: Tensor | float | int | None = None,\n    step_index: int | None = None,\n    boundary_ratio: float | None = None,\n    num_inference_steps: int = 50,\n    guidance_scale: float = 1.0,\n    guidance_scale_2: float | None = None,\n    guidance_rescale: float = 0.0,\n    eta: float = 0.0,\n    sigmas: list[float] | None = None,\n    n_tokens: int | None = None,\n    extra_step_kwargs: dict[str, Any] = dict(),\n    modules: dict[str, Any] = dict(),\n    output: Tensor | None = None,\n    return_trajectory_latents: bool = False,\n    return_trajectory_decoded: bool = False,\n    trajectory_timesteps: list[Tensor] | None = None,\n    trajectory_latents: Tensor | None = None,\n    trajectory_decoded: list[Tensor] | None = None,\n    extra: dict[str, Any] = dict(),\n    save_video: bool = True,\n    return_frames: bool = False,\n    enable_teacache: bool = False,\n    teacache_params: TeaCacheParams\n    | WanTeaCacheParams\n    | None = None,\n    STA_param: list | None = None,\n    is_cfg_negative: bool = False,\n    mask_search_final_result_pos: list[list] | None = None,\n    mask_search_final_result_neg: list[list] | None = None,\n    VSA_sparsity: float = 0.0,\n    logging_info: PipelineLoggingInfo = PipelineLoggingInfo(),\n)\n</code></pre> <p>Complete state passed through the pipeline execution.</p> <p>This dataclass contains all information needed during the diffusion pipeline execution, allowing methods to update specific components without needing to manage numerous individual parameters.</p> Functions\u00b6 fastvideo.pipelines.pipeline_batch_info.ForwardBatch.__post_init__ \u00b6 <pre><code>__post_init__()\n</code></pre> <p>Initialize dependent fields after dataclass initialization.</p> Source code in <code>fastvideo/pipelines/pipeline_batch_info.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Initialize dependent fields after dataclass initialization.\"\"\"\n\n    # Set do_classifier_free_guidance based on guidance scale and negative prompt\n    if self.guidance_scale &gt; 1.0:\n        self.do_classifier_free_guidance = True\n    if self.negative_prompt_embeds is None:\n        self.negative_prompt_embeds = []\n    if self.guidance_scale_2 is None:\n        self.guidance_scale_2 = self.guidance_scale\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.pipeline_batch_info.PipelineLoggingInfo","title":"fastvideo.pipelines.pipeline_batch_info.PipelineLoggingInfo","text":"<pre><code>PipelineLoggingInfo()\n</code></pre> <p>Simple approach using OrderedDict to track stage metrics.</p> Source code in <code>fastvideo/pipelines/pipeline_batch_info.py</code> <pre><code>def __init__(self):\n    # OrderedDict preserves insertion order and allows easy access\n    self.stages: OrderedDict[str, dict[str, Any]] = OrderedDict()\n</code></pre> Functions\u00b6 fastvideo.pipelines.pipeline_batch_info.PipelineLoggingInfo.add_stage_execution_time \u00b6 <pre><code>add_stage_execution_time(\n    stage_name: str, execution_time: float\n)\n</code></pre> <p>Add execution time for a stage.</p> Source code in <code>fastvideo/pipelines/pipeline_batch_info.py</code> <pre><code>def add_stage_execution_time(self, stage_name: str, execution_time: float):\n    \"\"\"Add execution time for a stage.\"\"\"\n    if stage_name not in self.stages:\n        self.stages[stage_name] = {}\n    self.stages[stage_name]['execution_time'] = execution_time\n    self.stages[stage_name]['timestamp'] = time.time()\n</code></pre> fastvideo.pipelines.pipeline_batch_info.PipelineLoggingInfo.add_stage_metric \u00b6 <pre><code>add_stage_metric(\n    stage_name: str, metric_name: str, value: Any\n)\n</code></pre> <p>Add any metric for a stage.</p> Source code in <code>fastvideo/pipelines/pipeline_batch_info.py</code> <pre><code>def add_stage_metric(self, stage_name: str, metric_name: str, value: Any):\n    \"\"\"Add any metric for a stage.\"\"\"\n    if stage_name not in self.stages:\n        self.stages[stage_name] = {}\n    self.stages[stage_name][metric_name] = value\n</code></pre> fastvideo.pipelines.pipeline_batch_info.PipelineLoggingInfo.get_execution_order \u00b6 <pre><code>get_execution_order() -&gt; list[str]\n</code></pre> <p>Get stages in execution order.</p> Source code in <code>fastvideo/pipelines/pipeline_batch_info.py</code> <pre><code>def get_execution_order(self) -&gt; list[str]:\n    \"\"\"Get stages in execution order.\"\"\"\n    return list(self.stages.keys())\n</code></pre> fastvideo.pipelines.pipeline_batch_info.PipelineLoggingInfo.get_stage_info \u00b6 <pre><code>get_stage_info(stage_name: str) -&gt; dict[str, Any]\n</code></pre> <p>Get all info for a specific stage.</p> Source code in <code>fastvideo/pipelines/pipeline_batch_info.py</code> <pre><code>def get_stage_info(self, stage_name: str) -&gt; dict[str, Any]:\n    \"\"\"Get all info for a specific stage.\"\"\"\n    return self.stages.get(stage_name, {})\n</code></pre> fastvideo.pipelines.pipeline_batch_info.PipelineLoggingInfo.get_total_execution_time \u00b6 <pre><code>get_total_execution_time() -&gt; float\n</code></pre> <p>Get total pipeline execution time.</p> Source code in <code>fastvideo/pipelines/pipeline_batch_info.py</code> <pre><code>def get_total_execution_time(self) -&gt; float:\n    \"\"\"Get total pipeline execution time.\"\"\"\n    return sum(\n        stage.get('execution_time', 0) for stage in self.stages.values())\n</code></pre>"},{"location":"api/fastvideo/#fastvideopipelinespipeline_registry","title":"fastvideo.pipelines.pipeline_registry","text":""},{"location":"api/fastvideo/#fastvideo.pipelines.pipeline_registry","title":"pipeline_registry","text":""},{"location":"api/fastvideo/#fastvideo.pipelines.pipeline_registry-classes","title":"Classes","text":""},{"location":"api/fastvideo/#fastvideo.pipelines.pipeline_registry.PipelineType","title":"fastvideo.pipelines.pipeline_registry.PipelineType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration for different pipeline types.</p> <p>Inherits from str to allow string comparison for backward compatibility.</p> Functions\u00b6 fastvideo.pipelines.pipeline_registry.PipelineType.choices <code>classmethod</code> \u00b6 <pre><code>choices() -&gt; list[str]\n</code></pre> <p>Get all available choices as strings.</p> Source code in <code>fastvideo/pipelines/pipeline_registry.py</code> <pre><code>@classmethod\ndef choices(cls) -&gt; list[str]:\n    \"\"\"Get all available choices as strings.\"\"\"\n    return [pipeline_type.value for pipeline_type in cls]\n</code></pre> fastvideo.pipelines.pipeline_registry.PipelineType.from_string <code>classmethod</code> \u00b6 <pre><code>from_string(value: str) -&gt; PipelineType\n</code></pre> <p>Convert string to PipelineType enum.</p> Source code in <code>fastvideo/pipelines/pipeline_registry.py</code> <pre><code>@classmethod\ndef from_string(cls, value: str) -&gt; \"PipelineType\":\n    \"\"\"Convert string to PipelineType enum.\"\"\"\n    try:\n        return cls(value.lower())\n    except ValueError:\n        raise ValueError(\n            f\"Invalid pipeline type: {value}. Must be one of: {', '.join([t.value for t in cls])}\"\n        ) from None\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.pipeline_registry-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideo.pipelines.pipeline_registry.get_pipeline_registry","title":"fastvideo.pipelines.pipeline_registry.get_pipeline_registry","text":"<pre><code>get_pipeline_registry(\n    pipeline_type: PipelineType | str | None = None,\n) -&gt; _PipelineRegistry\n</code></pre> <p>Get a pipeline registry for the specified mode, pipeline type, and workload type.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_type</code> <code>PipelineType | str | None</code> <p>Pipeline type to load. If None and mode is provided, will be derived from mode.</p> <code>None</code> <p>Returns:</p> Type Description <code>_PipelineRegistry</code> <p>A pipeline registry instance.</p> Source code in <code>fastvideo/pipelines/pipeline_registry.py</code> <pre><code>def get_pipeline_registry(\n        pipeline_type: PipelineType | str | None = None) -&gt; _PipelineRegistry:\n    \"\"\"\n    Get a pipeline registry for the specified mode, pipeline type, and workload type.\n\n    Args:\n        pipeline_type: Pipeline type to load. If None and mode is provided, will be derived from mode.\n\n    Returns:\n        A pipeline registry instance.\n    \"\"\"\n    if isinstance(pipeline_type, str):\n        pipeline_type = PipelineType.from_string(pipeline_type)\n\n    pipeline_classes = import_pipeline_classes(pipeline_type)\n    return _PipelineRegistry(pipeline_classes)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.pipeline_registry.import_pipeline_classes","title":"fastvideo.pipelines.pipeline_registry.import_pipeline_classes  <code>cached</code>","text":"<pre><code>import_pipeline_classes(\n    pipeline_types: list[PipelineType]\n    | PipelineType\n    | None = None,\n) -&gt; dict[\n    str,\n    dict[str, dict[str, type[ComposedPipelineBase] | None]],\n]\n</code></pre> <p>Import pipeline classes based on the pipeline type and workload type.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_types</code> <code>list[PipelineType] | PipelineType | None</code> <p>The pipeline types to load (basic, preprocess, training).            If None, loads all types.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, dict[str, dict[str, type[ComposedPipelineBase] | None]]]</code> <p>A three-level nested dictionary:</p> <code>dict[str, dict[str, dict[str, type[ComposedPipelineBase] | None]]]</code> <p>{pipeline_type: {architecture_name: {pipeline_name: pipeline_cls}}}</p> <code>dict[str, dict[str, dict[str, type[ComposedPipelineBase] | None]]]</code> <p>e.g., {\"basic\": {\"wan\": {\"WanPipeline\": WanPipeline}}}</p> Source code in <code>fastvideo/pipelines/pipeline_registry.py</code> <pre><code>@lru_cache\ndef import_pipeline_classes(\n    pipeline_types: list[PipelineType] | PipelineType | None = None\n) -&gt; dict[str, dict[str, dict[str, type[ComposedPipelineBase] | None]]]:\n    \"\"\"\n    Import pipeline classes based on the pipeline type and workload type.\n\n    Args:\n        pipeline_types: The pipeline types to load (basic, preprocess, training). \n                      If None, loads all types.\n\n    Returns:\n        A three-level nested dictionary:\n        {pipeline_type: {architecture_name: {pipeline_name: pipeline_cls}}}\n        e.g., {\"basic\": {\"wan\": {\"WanPipeline\": WanPipeline}}}\n    \"\"\"\n    type_to_arch_to_pipeline_dict: dict[str,\n                                        dict[str,\n                                             dict[str,\n                                                  type[ComposedPipelineBase]\n                                                  | None]]] = {}\n    package_name: str = \"fastvideo.pipelines\"\n\n    # Determine which pipeline types to scan\n    if isinstance(pipeline_types, list):\n        pipeline_types_to_scan = [\n            pipeline_type.value for pipeline_type in pipeline_types\n        ]\n    elif isinstance(pipeline_types, PipelineType):\n        pipeline_types_to_scan = [pipeline_types.value]\n    else:\n        pipeline_types_to_scan = [pt.value for pt in PipelineType]\n\n    logger.info(\"Loading pipelines for types: %s\", pipeline_types_to_scan)\n\n    for pipeline_type_str in pipeline_types_to_scan:\n        arch_to_pipeline_dict: dict[str, dict[str, type[ComposedPipelineBase]\n                                              | None]] = {}\n\n        # Try to load from pipeline-type-specific directory first\n        pipeline_type_package_name = f\"{package_name}.{pipeline_type_str}\"\n\n        try:\n            pipeline_type_package = importlib.import_module(\n                pipeline_type_package_name)\n            logger.debug(\"Successfully imported %s\", pipeline_type_package_name)\n\n            for _, arch, ispkg in pkgutil.iter_modules(\n                    pipeline_type_package.__path__):\n                pipeline_dict: dict[str, type[ComposedPipelineBase] | None] = {}\n\n                arch_package_name = f\"{pipeline_type_package_name}.{arch}\"\n                if ispkg:\n                    arch_package = importlib.import_module(arch_package_name)\n                    for _, module_name, ispkg in pkgutil.walk_packages(\n                            arch_package.__path__, arch_package_name + \".\"):\n                        if not ispkg:\n                            pipeline_module = importlib.import_module(\n                                module_name)\n                            if hasattr(pipeline_module, \"EntryClass\"):\n                                if isinstance(pipeline_module.EntryClass, list):\n                                    for pipeline in pipeline_module.EntryClass:\n                                        pipeline_name = pipeline.__name__\n                                        assert (\n                                            pipeline_name not in pipeline_dict\n                                        ), f\"Duplicated pipeline implementation for {pipeline_name} in {pipeline_type_str}.{arch_package_name}\"\n                                        pipeline_dict[pipeline_name] = pipeline\n                                else:\n                                    pipeline_name = pipeline_module.EntryClass.__name__\n                                    assert (\n                                        pipeline_name not in pipeline_dict\n                                    ), f\"Duplicated pipeline implementation for {pipeline_name} in {pipeline_type_str}.{arch_package_name}\"\n                                    pipeline_dict[\n                                        pipeline_name] = pipeline_module.EntryClass\n\n                arch_to_pipeline_dict[arch] = pipeline_dict\n\n        except ImportError as e:\n            raise ImportError(\n                f\"Could not import {pipeline_type_package_name} when importing pipeline classes: {e}\"\n            ) from None\n\n        type_to_arch_to_pipeline_dict[pipeline_type_str] = arch_to_pipeline_dict\n\n    # Log summary\n    total_pipelines = sum(\n        len(pipeline_dict)\n        for arch_to_pipeline_dict in type_to_arch_to_pipeline_dict.values()\n        for pipeline_dict in arch_to_pipeline_dict.values())\n    logger.info(\"Loaded %d pipeline classes across %d types\", total_pipelines,\n                len(pipeline_types_to_scan))\n\n    return type_to_arch_to_pipeline_dict\n</code></pre>"},{"location":"api/fastvideo/#fastvideopipelinesstages","title":"fastvideo.pipelines.stages","text":""},{"location":"api/fastvideo/#fastvideo.pipelines.stages","title":"stages","text":"<p>Pipeline stages for diffusion models.</p> <p>This package contains the various stages that can be composed to create complete diffusion pipelines.</p>"},{"location":"api/fastvideo/#fastvideo.pipelines.stages-classes","title":"Classes","text":""},{"location":"api/fastvideo/#fastvideo.pipelines.stages.CausalDMDDenosingStage","title":"fastvideo.pipelines.stages.CausalDMDDenosingStage","text":"<pre><code>CausalDMDDenosingStage(\n    transformer, scheduler, transformer_2=None\n)\n</code></pre> <p>               Bases: <code>DenoisingStage</code></p> <p>Denoising stage for causal diffusion.</p> Source code in <code>fastvideo/pipelines/stages/causal_denoising.py</code> <pre><code>def __init__(self, transformer, scheduler, transformer_2=None) -&gt; None:\n    super().__init__(transformer, scheduler, transformer_2)\n    # KV and cross-attention cache state (initialized on first forward)\n    self.transformer = transformer\n    self.transformer_2 = transformer_2\n    self.kv_cache1: list | None = None\n    self.crossattn_cache: list | None = None\n    # Model-dependent constants (aligned with causal_inference.py assumptions)\n    self.num_transformer_blocks = len(self.transformer.blocks)\n    self.num_frames_per_block = self.transformer.config.arch_config.num_frames_per_block\n    self.sliding_window_num_frames = self.transformer.config.arch_config.sliding_window_num_frames\n\n    try:\n        self.local_attn_size = getattr(self.transformer.model,\n                                       \"local_attn_size\",\n                                       -1)  # type: ignore\n    except Exception:\n        self.local_attn_size = -1\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.CausalDMDDenosingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify denoising stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/causal_denoising.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify denoising stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds, V.list_not_empty)\n    result.add_check(\"image_embeds\", batch.image_embeds, V.is_list)\n    result.add_check(\"image_latent\", batch.image_latent,\n                     V.none_or_tensor_with_dims(5))\n    result.add_check(\"num_inference_steps\", batch.num_inference_steps,\n                     V.positive_int)\n    result.add_check(\"guidance_scale\", batch.guidance_scale,\n                     V.positive_float)\n    result.add_check(\"eta\", batch.eta, V.non_negative_float)\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    result.add_check(\"do_classifier_free_guidance\",\n                     batch.do_classifier_free_guidance, V.bool_value)\n    result.add_check(\n        \"negative_prompt_embeds\", batch.negative_prompt_embeds, lambda x:\n        not batch.do_classifier_free_guidance or V.list_not_empty(x))\n    return result\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.ConditioningStage","title":"fastvideo.pipelines.stages.ConditioningStage","text":"<p>               Bases: <code>PipelineStage</code></p> <p>Stage for applying conditioning to the diffusion process.</p> <p>This stage handles the application of conditioning, such as classifier-free guidance, to the diffusion process.</p> Functions\u00b6 fastvideo.pipelines.stages.ConditioningStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Apply conditioning to the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with applied conditioning.</p> Source code in <code>fastvideo/pipelines/stages/conditioning.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Apply conditioning to the diffusion process.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with applied conditioning.\n    \"\"\"\n    # TODO!!\n    if not batch.do_classifier_free_guidance:\n        return batch\n    else:\n        return batch\n\n    logger.info(\"batch.negative_prompt_embeds: %s\",\n                batch.negative_prompt_embeds)\n    logger.info(\"do_classifier_free_guidance: %s\",\n                batch.do_classifier_free_guidance)\n    logger.info(\"cfg_scale: %s\", batch.guidance_scale)\n\n    # Ensure negative prompt embeddings are available\n    assert batch.negative_prompt_embeds is not None, (\n        \"Negative prompt embeddings are required for classifier-free guidance\"\n    )\n\n    # Concatenate primary embeddings and masks\n    batch.prompt_embeds = torch.cat(\n        [batch.negative_prompt_embeds, batch.prompt_embeds])\n    if batch.attention_mask is not None:\n        batch.attention_mask = torch.cat(\n            [batch.negative_attention_mask, batch.attention_mask])\n\n    # Concatenate secondary embeddings and masks if present\n    if batch.prompt_embeds_2 is not None:\n        batch.prompt_embeds_2 = torch.cat(\n            [batch.negative_prompt_embeds_2, batch.prompt_embeds_2])\n    if batch.attention_mask_2 is not None:\n        batch.attention_mask_2 = torch.cat(\n            [batch.negative_attention_mask_2, batch.attention_mask_2])\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.ConditioningStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify conditioning stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/conditioning.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify conditioning stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"do_classifier_free_guidance\",\n                     batch.do_classifier_free_guidance, V.bool_value)\n    result.add_check(\"guidance_scale\", batch.guidance_scale,\n                     V.positive_float)\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds, V.list_not_empty)\n    result.add_check(\n        \"negative_prompt_embeds\", batch.negative_prompt_embeds, lambda x:\n        not batch.do_classifier_free_guidance or V.list_not_empty(x))\n    return result\n</code></pre> fastvideo.pipelines.stages.ConditioningStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify conditioning stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/conditioning.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify conditioning stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds, V.list_not_empty)\n    return result\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.DecodingStage","title":"fastvideo.pipelines.stages.DecodingStage","text":"<pre><code>DecodingStage(vae, pipeline=None)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for decoding latent representations into pixel space.</p> <p>This stage handles the decoding of latent representations into the final output format (e.g., pixel values).</p> Source code in <code>fastvideo/pipelines/stages/decoding.py</code> <pre><code>def __init__(self, vae, pipeline=None) -&gt; None:\n    self.vae: ParallelTiledVAE = vae\n    self.pipeline = weakref.ref(pipeline) if pipeline else None\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.DecodingStage.decode \u00b6 <pre><code>decode(\n    latents: Tensor, fastvideo_args: FastVideoArgs\n) -&gt; torch.Tensor\n</code></pre> <p>Decode latent representations into pixel space using VAE.</p> <p>Parameters:</p> Name Type Description Default <code>latents</code> <code>Tensor</code> <p>Input latent tensor with shape (batch, channels, frames, height_latents, width_latents)</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>Configuration containing: - disable_autocast: Whether to disable automatic mixed precision (default: False) - pipeline_config.vae_precision: VAE computation precision (\"fp32\", \"fp16\", \"bf16\") - pipeline_config.vae_tiling: Whether to enable VAE tiling for memory efficiency</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Decoded video tensor with shape (batch, channels, frames, height, width), </p> <code>Tensor</code> <p>normalized to [0, 1] range and moved to CPU as float32</p> Source code in <code>fastvideo/pipelines/stages/decoding.py</code> <pre><code>@torch.no_grad()\ndef decode(self, latents: torch.Tensor,\n           fastvideo_args: FastVideoArgs) -&gt; torch.Tensor:\n    \"\"\"\n    Decode latent representations into pixel space using VAE.\n\n    Args:\n        latents: Input latent tensor with shape (batch, channels, frames, height_latents, width_latents)\n        fastvideo_args: Configuration containing:\n            - disable_autocast: Whether to disable automatic mixed precision (default: False)\n            - pipeline_config.vae_precision: VAE computation precision (\"fp32\", \"fp16\", \"bf16\")\n            - pipeline_config.vae_tiling: Whether to enable VAE tiling for memory efficiency\n\n    Returns:\n        Decoded video tensor with shape (batch, channels, frames, height, width), \n        normalized to [0, 1] range and moved to CPU as float32\n    \"\"\"\n    self.vae = self.vae.to(get_local_torch_device())\n    latents = latents.to(get_local_torch_device())\n\n    # Setup VAE precision\n    vae_dtype = PRECISION_TO_TYPE[\n        fastvideo_args.pipeline_config.vae_precision]\n    vae_autocast_enabled = (\n        vae_dtype != torch.float32) and not fastvideo_args.disable_autocast\n\n    if isinstance(self.vae.scaling_factor, torch.Tensor):\n        latents = latents / self.vae.scaling_factor.to(\n            latents.device, latents.dtype)\n    else:\n        latents = latents / self.vae.scaling_factor\n\n    # Apply shifting if needed\n    if (hasattr(self.vae, \"shift_factor\")\n            and self.vae.shift_factor is not None):\n        if isinstance(self.vae.shift_factor, torch.Tensor):\n            latents += self.vae.shift_factor.to(latents.device,\n                                                latents.dtype)\n        else:\n            latents += self.vae.shift_factor\n\n    # Decode latents\n    with torch.autocast(device_type=\"cuda\",\n                        dtype=vae_dtype,\n                        enabled=vae_autocast_enabled):\n        if fastvideo_args.pipeline_config.vae_tiling:\n            self.vae.enable_tiling()\n        # if fastvideo_args.vae_sp:\n        #     self.vae.enable_parallel()\n        if not vae_autocast_enabled:\n            latents = latents.to(vae_dtype)\n        image = self.vae.decode(latents)\n\n    # Normalize image to [0, 1] range\n    image = (image / 2 + 0.5).clamp(0, 1)\n    return image\n</code></pre> fastvideo.pipelines.stages.DecodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Decode latent representations into pixel space.</p> <p>This method processes the batch through the VAE decoder, converting latent representations to pixel-space video/images. It also optionally decodes trajectory latents for visualization purposes.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch containing: - latents: Tensor to decode (batch, channels, frames, height_latents, width_latents) - return_trajectory_decoded (optional): Flag to decode trajectory latents - trajectory_latents (optional): Latents at different timesteps - trajectory_timesteps (optional): Corresponding timesteps</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>Configuration containing: - output_type: \"latent\" to skip decoding, otherwise decode to pixels - vae_cpu_offload: Whether to offload VAE to CPU after decoding - model_loaded: Track VAE loading state - model_paths: Path to VAE model if loading needed</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>Modified batch with: - output: Decoded frames (batch, channels, frames, height, width) as CPU float32 - trajectory_decoded (if requested): List of decoded frames per timestep</p> Source code in <code>fastvideo/pipelines/stages/decoding.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Decode latent representations into pixel space.\n\n    This method processes the batch through the VAE decoder, converting latent\n    representations to pixel-space video/images. It also optionally decodes\n    trajectory latents for visualization purposes.\n\n    Args:\n        batch: The current batch containing:\n            - latents: Tensor to decode (batch, channels, frames, height_latents, width_latents)\n            - return_trajectory_decoded (optional): Flag to decode trajectory latents\n            - trajectory_latents (optional): Latents at different timesteps\n            - trajectory_timesteps (optional): Corresponding timesteps\n        fastvideo_args: Configuration containing:\n            - output_type: \"latent\" to skip decoding, otherwise decode to pixels\n            - vae_cpu_offload: Whether to offload VAE to CPU after decoding\n            - model_loaded: Track VAE loading state\n            - model_paths: Path to VAE model if loading needed\n\n    Returns:\n        Modified batch with:\n            - output: Decoded frames (batch, channels, frames, height, width) as CPU float32\n            - trajectory_decoded (if requested): List of decoded frames per timestep\n    \"\"\"\n    # load vae if not already loaded (used for memory constrained devices)\n    pipeline = self.pipeline() if self.pipeline else None\n    if not fastvideo_args.model_loaded[\"vae\"]:\n        loader = VAELoader()\n        self.vae = loader.load(fastvideo_args.model_paths[\"vae\"],\n                               fastvideo_args)\n        if pipeline:\n            pipeline.add_module(\"vae\", self.vae)\n        fastvideo_args.model_loaded[\"vae\"] = True\n\n    if fastvideo_args.output_type == \"latent\":\n        frames = batch.latents\n    else:\n        frames = self.decode(batch.latents, fastvideo_args)\n\n    # decode trajectory latents if needed\n    if batch.return_trajectory_decoded:\n        batch.trajectory_decoded = []\n        assert batch.trajectory_latents is not None, \"batch should have trajectory latents\"\n        for idx in range(batch.trajectory_latents.shape[1]):\n            # batch.trajectory_latents is [batch_size, timesteps, channels, frames, height, width]\n            cur_latent = batch.trajectory_latents[:, idx, :, :, :, :]\n            cur_timestep = batch.trajectory_timesteps[idx]\n            logger.info(\"decoding trajectory latent for timestep: %s\",\n                        cur_timestep)\n            decoded_frames = self.decode(cur_latent, fastvideo_args)\n            batch.trajectory_decoded.append(decoded_frames.cpu().float())\n\n    # Convert to CPU float32 for compatibility\n    frames = frames.cpu().float()\n\n    # Update batch with decoded image\n    batch.output = frames\n\n    # Offload models if needed\n    if hasattr(self, 'maybe_free_model_hooks'):\n        self.maybe_free_model_hooks()\n\n    if fastvideo_args.vae_cpu_offload:\n        self.vae.to(\"cpu\")\n\n    if torch.backends.mps.is_available():\n        del self.vae\n        if pipeline is not None and \"vae\" in pipeline.modules:\n            del pipeline.modules[\"vae\"]\n        fastvideo_args.model_loaded[\"vae\"] = False\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.DecodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify decoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/decoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify decoding stage inputs.\"\"\"\n    result = VerificationResult()\n    # Denoised latents for VAE decoding: [batch_size, channels, frames, height_latents, width_latents]\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre> fastvideo.pipelines.stages.DecodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify decoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/decoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify decoding stage outputs.\"\"\"\n    result = VerificationResult()\n    # Decoded video/images: [batch_size, channels, frames, height, width]\n    result.add_check(\"output\", batch.output, [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.DenoisingStage","title":"fastvideo.pipelines.stages.DenoisingStage","text":"<pre><code>DenoisingStage(\n    transformer,\n    scheduler,\n    pipeline=None,\n    transformer_2=None,\n    vae=None,\n)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for running the denoising loop in diffusion pipelines.</p> <p>This stage handles the iterative denoising process that transforms the initial noise into the final output.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def __init__(self,\n             transformer,\n             scheduler,\n             pipeline=None,\n             transformer_2=None,\n             vae=None) -&gt; None:\n    super().__init__()\n    self.transformer = transformer\n    self.transformer_2 = transformer_2\n    self.scheduler = scheduler\n    self.vae = vae\n    self.pipeline = weakref.ref(pipeline) if pipeline else None\n    attn_head_size = self.transformer.hidden_size // self.transformer.num_attention_heads\n    self.attn_backend = get_attn_backend(\n        head_size=attn_head_size,\n        dtype=torch.float16,  # TODO(will): hack\n        supported_attention_backends=(\n            AttentionBackendEnum.SLIDING_TILE_ATTN,\n            AttentionBackendEnum.VIDEO_SPARSE_ATTN,\n            AttentionBackendEnum.VMOBA_ATTN,\n            AttentionBackendEnum.FLASH_ATTN,\n            AttentionBackendEnum.TORCH_SDPA,\n            AttentionBackendEnum.SAGE_ATTN_THREE)  # hack\n    )\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.DenoisingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Run the denoising loop.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with denoised latents.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Run the denoising loop.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with denoised latents.\n    \"\"\"\n    pipeline = self.pipeline() if self.pipeline else None\n    if not fastvideo_args.model_loaded[\"transformer\"]:\n        loader = TransformerLoader()\n        self.transformer = loader.load(\n            fastvideo_args.model_paths[\"transformer\"], fastvideo_args)\n        if pipeline:\n            pipeline.add_module(\"transformer\", self.transformer)\n        fastvideo_args.model_loaded[\"transformer\"] = True\n\n    # Prepare extra step kwargs for scheduler\n    extra_step_kwargs = self.prepare_extra_func_kwargs(\n        self.scheduler.step,\n        {\n            \"generator\": batch.generator,\n            \"eta\": batch.eta\n        },\n    )\n\n    # Setup precision and autocast settings\n    # TODO(will): make the precision configurable for inference\n    # target_dtype = PRECISION_TO_TYPE[fastvideo_args.precision]\n    target_dtype = torch.bfloat16\n    autocast_enabled = (target_dtype != torch.float32\n                        ) and not fastvideo_args.disable_autocast\n\n    # Handle sequence parallelism if enabled\n    sp_world_size, rank_in_sp_group = get_sp_world_size(\n    ), get_sp_parallel_rank()\n    sp_group = sp_world_size &gt; 1\n    if sp_group:\n        latents = rearrange(batch.latents,\n                            \"b c (n t) h w -&gt; b c n t h w\",\n                            n=sp_world_size).contiguous()\n        latents = latents[:, :, rank_in_sp_group, :, :, :]\n        batch.latents = latents\n        if batch.image_latent is not None:\n            image_latent = rearrange(batch.image_latent,\n                                     \"b c (n t) h w -&gt; b c n t h w\",\n                                     n=sp_world_size).contiguous()\n            image_latent = image_latent[:, :, rank_in_sp_group, :, :, :]\n            batch.image_latent = image_latent\n    # Get timesteps and calculate warmup steps\n    timesteps = batch.timesteps\n    # TODO(will): remove this once we add input/output validation for stages\n    if timesteps is None:\n        raise ValueError(\"Timesteps must be provided\")\n    num_inference_steps = batch.num_inference_steps\n    num_warmup_steps = len(\n        timesteps) - num_inference_steps * self.scheduler.order\n\n    # Prepare image latents and embeddings for I2V generation\n    image_embeds = batch.image_embeds\n    if len(image_embeds) &gt; 0:\n        assert not torch.isnan(\n            image_embeds[0]).any(), \"image_embeds contains nan\"\n        image_embeds = [\n            image_embed.to(target_dtype) for image_embed in image_embeds\n        ]\n\n    image_kwargs = self.prepare_extra_func_kwargs(\n        self.transformer.forward,\n        {\n            \"encoder_hidden_states_image\": image_embeds,\n            \"mask_strategy\": dict_to_3d_list(\n                None, t_max=50, l_max=60, h_max=24)\n        },\n    )\n\n    pos_cond_kwargs = self.prepare_extra_func_kwargs(\n        self.transformer.forward,\n        {\n            \"encoder_hidden_states_2\": batch.clip_embedding_pos,\n            \"encoder_attention_mask\": batch.prompt_attention_mask,\n        },\n    )\n\n    neg_cond_kwargs = self.prepare_extra_func_kwargs(\n        self.transformer.forward,\n        {\n            \"encoder_hidden_states_2\": batch.clip_embedding_neg,\n            \"encoder_attention_mask\": batch.negative_attention_mask,\n        },\n    )\n\n    # Prepare STA parameters\n    if st_attn_available and self.attn_backend == SlidingTileAttentionBackend:\n        self.prepare_sta_param(batch, fastvideo_args)\n\n    # Get latents and embeddings\n    latents = batch.latents\n    prompt_embeds = batch.prompt_embeds\n    assert not torch.isnan(\n        prompt_embeds[0]).any(), \"prompt_embeds contains nan\"\n    if batch.do_classifier_free_guidance:\n        neg_prompt_embeds = batch.negative_prompt_embeds\n        assert neg_prompt_embeds is not None\n        assert not torch.isnan(\n            neg_prompt_embeds[0]).any(), \"neg_prompt_embeds contains nan\"\n\n    # (Wan2.2) Calculate timestep to switch from high noise expert to low noise expert\n    boundary_ratio = fastvideo_args.pipeline_config.dit_config.boundary_ratio\n    if batch.boundary_ratio is not None:\n        logger.info(\"Overriding boundary ratio from %s to %s\",\n                    boundary_ratio, batch.boundary_ratio)\n        boundary_ratio = batch.boundary_ratio\n\n    if boundary_ratio is not None:\n        boundary_timestep = boundary_ratio * self.scheduler.num_train_timesteps\n    else:\n        boundary_timestep = None\n    latent_model_input = latents.to(target_dtype)\n    assert latent_model_input.shape[0] == 1, \"only support batch size 1\"\n\n    if fastvideo_args.pipeline_config.ti2v_task and batch.pil_image is not None:\n        # TI2V directly replaces the first frame of the latent with\n        # the image latent instead of appending along the channel dim\n        assert batch.image_latent is None, \"TI2V task should not have image latents\"\n        assert self.vae is not None, \"VAE is not provided for TI2V task\"\n        z = self.vae.encode(batch.pil_image).mean.float()\n        if (hasattr(self.vae, \"shift_factor\")\n                and self.vae.shift_factor is not None):\n            if isinstance(self.vae.shift_factor, torch.Tensor):\n                z -= self.vae.shift_factor.to(z.device, z.dtype)\n            else:\n                z -= self.vae.shift_factor\n\n        if isinstance(self.vae.scaling_factor, torch.Tensor):\n            z = z * self.vae.scaling_factor.to(z.device, z.dtype)\n        else:\n            z = z * self.vae.scaling_factor\n\n        latent_model_input = latent_model_input.squeeze(0)\n        _, mask2 = masks_like([latent_model_input], zero=True)\n\n        latent_model_input = (1. -\n                              mask2[0]) * z + mask2[0] * latent_model_input\n        # latent_model_input = latent_model_input.unsqueeze(0)\n        latent_model_input = latent_model_input.to(get_local_torch_device())\n        latents = latent_model_input\n        F = batch.num_frames\n        temporal_scale = fastvideo_args.pipeline_config.vae_config.arch_config.scale_factor_temporal\n        spatial_scale = fastvideo_args.pipeline_config.vae_config.arch_config.scale_factor_spatial\n        patch_size = fastvideo_args.pipeline_config.dit_config.arch_config.patch_size\n        seq_len = ((F - 1) // temporal_scale +\n                   1) * (batch.height // spatial_scale) * (\n                       batch.width // spatial_scale) // (patch_size[1] *\n                                                         patch_size[2])\n        seq_len = int(math.ceil(seq_len / sp_world_size)) * sp_world_size\n\n    # Initialize lists for ODE trajectory\n    trajectory_timesteps: list[torch.Tensor] = []\n    trajectory_latents: list[torch.Tensor] = []\n\n    # Run denoising loop\n    with self.progress_bar(total=num_inference_steps) as progress_bar:\n        for i, t in enumerate(timesteps):\n            # Skip if interrupted\n            if hasattr(self, 'interrupt') and self.interrupt:\n                continue\n\n            if boundary_timestep is None or t &gt;= boundary_timestep:\n                if (fastvideo_args.dit_cpu_offload\n                        and self.transformer_2 is not None and next(\n                            self.transformer_2.parameters()).device.type\n                        == 'cuda'):\n                    self.transformer_2.to('cpu')\n                current_model = self.transformer\n                current_guidance_scale = batch.guidance_scale\n            else:\n                # low-noise stage in wan2.2\n                if fastvideo_args.dit_cpu_offload and next(\n                        self.transformer.parameters(\n                        )).device.type == 'cuda':\n                    self.transformer.to('cpu')\n                current_model = self.transformer_2\n                current_guidance_scale = batch.guidance_scale_2\n            assert current_model is not None, \"current_model is None\"\n\n            # Expand latents for V2V/I2V\n            latent_model_input = latents.to(target_dtype)\n            if batch.video_latent is not None:\n                latent_model_input = torch.cat([\n                    latent_model_input, batch.video_latent,\n                    torch.zeros_like(latents)\n                ],\n                                               dim=1).to(target_dtype)\n            elif batch.image_latent is not None:\n                assert not fastvideo_args.pipeline_config.ti2v_task, \"image latents should not be provided for TI2V task\"\n                latent_model_input = torch.cat(\n                    [latent_model_input, batch.image_latent],\n                    dim=1).to(target_dtype)\n\n            assert not torch.isnan(\n                latent_model_input).any(), \"latent_model_input contains nan\"\n            if fastvideo_args.pipeline_config.ti2v_task and batch.pil_image is not None:\n                timestep = torch.stack([t]).to(get_local_torch_device())\n                temp_ts = (mask2[0][0][:, ::2, ::2] * timestep).flatten()\n                temp_ts = torch.cat([\n                    temp_ts,\n                    temp_ts.new_ones(seq_len - temp_ts.size(0)) * timestep\n                ])\n                timestep = temp_ts.unsqueeze(0)\n                t_expand = timestep.repeat(latent_model_input.shape[0], 1)\n            else:\n                t_expand = t.repeat(latent_model_input.shape[0])\n\n            latent_model_input = self.scheduler.scale_model_input(\n                latent_model_input, t)\n\n            # Prepare inputs for transformer\n            guidance_expand = (\n                torch.tensor(\n                    [fastvideo_args.pipeline_config.embedded_cfg_scale] *\n                    latent_model_input.shape[0],\n                    dtype=torch.float32,\n                    device=get_local_torch_device(),\n                ).to(target_dtype) *\n                1000.0 if fastvideo_args.pipeline_config.embedded_cfg_scale\n                is not None else None)\n\n            # Predict noise residual\n            with torch.autocast(device_type=\"cuda\",\n                                dtype=target_dtype,\n                                enabled=autocast_enabled):\n                if (st_attn_available\n                        and self.attn_backend == SlidingTileAttentionBackend\n                    ) or (vsa_available and self.attn_backend\n                          == VideoSparseAttentionBackend):\n                    self.attn_metadata_builder_cls = self.attn_backend.get_builder_cls(\n                    )\n\n                    if self.attn_metadata_builder_cls is not None:\n                        self.attn_metadata_builder = self.attn_metadata_builder_cls(\n                        )\n                        # TODO(will): clean this up\n                        attn_metadata = self.attn_metadata_builder.build(  # type: ignore\n                            current_timestep=i,  # type: ignore\n                            raw_latent_shape=batch.\n                            raw_latent_shape[2:5],  # type: ignore\n                            patch_size=fastvideo_args.\n                            pipeline_config.  # type: ignore\n                            dit_config.patch_size,  # type: ignore\n                            STA_param=batch.STA_param,  # type: ignore\n                            VSA_sparsity=fastvideo_args.\n                            VSA_sparsity,  # type: ignore\n                            device=get_local_torch_device(),\n                        )\n                        assert attn_metadata is not None, \"attn_metadata cannot be None\"\n                    else:\n                        attn_metadata = None\n                elif (vmoba_attn_available\n                      and self.attn_backend == VMOBAAttentionBackend):\n                    self.attn_metadata_builder_cls = self.attn_backend.get_builder_cls(\n                    )\n                    if self.attn_metadata_builder_cls is not None:\n                        self.attn_metadata_builder = self.attn_metadata_builder_cls(\n                        )\n                        # Prepare V-MoBA parameters from config\n                        moba_params = fastvideo_args.moba_config.copy()\n                        moba_params.update({\n                            \"current_timestep\":\n                            i,\n                            \"raw_latent_shape\":\n                            batch.raw_latent_shape[2:5],\n                            \"patch_size\":\n                            fastvideo_args.pipeline_config.dit_config.\n                            patch_size,\n                            \"device\":\n                            get_local_torch_device(),\n                        })\n                        attn_metadata = self.attn_metadata_builder.build(\n                            **moba_params)\n                        assert attn_metadata is not None, \"attn_metadata cannot be None\"\n                    else:\n                        attn_metadata = None\n                else:\n                    attn_metadata = None\n                # TODO(will): finalize the interface. vLLM uses this to\n                # support torch dynamo compilation. They pass in\n                # attn_metadata, vllm_config, and num_tokens. We can pass in\n                # fastvideo_args or training_args, and attn_metadata.\n                batch.is_cfg_negative = False\n                with set_forward_context(\n                        current_timestep=i,\n                        attn_metadata=attn_metadata,\n                        forward_batch=batch,\n                        # fastvideo_args=fastvideo_args\n                ):\n                    # Run transformer\n                    noise_pred = current_model(\n                        latent_model_input,\n                        prompt_embeds,\n                        t_expand,\n                        guidance=guidance_expand,\n                        **image_kwargs,\n                        **pos_cond_kwargs,\n                    )\n\n                # Apply guidance\n                if batch.do_classifier_free_guidance:\n                    batch.is_cfg_negative = True\n                    with set_forward_context(\n                            current_timestep=i,\n                            attn_metadata=attn_metadata,\n                            forward_batch=batch,\n                            # fastvideo_args=fastvideo_args\n                    ):\n                        # Run transformer\n                        noise_pred_uncond = current_model(\n                            latent_model_input,\n                            neg_prompt_embeds,\n                            t_expand,\n                            guidance=guidance_expand,\n                            **image_kwargs,\n                            **neg_cond_kwargs,\n                        )\n                    noise_pred_text = noise_pred\n                    noise_pred = noise_pred_uncond + current_guidance_scale * (\n                        noise_pred_text - noise_pred_uncond)\n\n                    # Apply guidance rescale if needed\n                    if batch.guidance_rescale &gt; 0.0:\n                        # Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf\n                        noise_pred = self.rescale_noise_cfg(\n                            noise_pred,\n                            noise_pred_text,\n                            guidance_rescale=batch.guidance_rescale,\n                        )\n                # Compute the previous noisy sample\n                latents = self.scheduler.step(noise_pred,\n                                              t,\n                                              latents,\n                                              **extra_step_kwargs,\n                                              return_dict=False)[0]\n                if fastvideo_args.pipeline_config.ti2v_task and batch.pil_image is not None:\n                    latents = latents.squeeze(0)\n                    latents = (1. - mask2[0]) * z + mask2[0] * latents\n                    # latents = latents.unsqueeze(0)\n\n            # save trajectory latents if needed\n            if batch.return_trajectory_latents:\n                trajectory_timesteps.append(t)\n                trajectory_latents.append(latents)\n\n            # Update progress bar\n            if i == len(timesteps) - 1 or (\n                (i + 1) &gt; num_warmup_steps and\n                (i + 1) % self.scheduler.order == 0\n                    and progress_bar is not None):\n                progress_bar.update()\n\n    # Gather results if using sequence parallelism\n    trajectory_tensor: torch.Tensor | None = None\n    if trajectory_latents:\n        trajectory_tensor = torch.stack(trajectory_latents, dim=1)\n        trajectory_timesteps_tensor = torch.stack(trajectory_timesteps,\n                                                  dim=0)\n    else:\n        trajectory_tensor = None\n        trajectory_timesteps_tensor = None\n\n    # Gather results if using sequence parallelism\n    if sp_group:\n        latents = sequence_model_parallel_all_gather(latents, dim=2)\n        if batch.return_trajectory_latents:\n            trajectory_tensor = trajectory_tensor.to(\n                get_local_torch_device())\n            trajectory_tensor = sequence_model_parallel_all_gather(\n                trajectory_tensor, dim=3)\n\n    if trajectory_tensor is not None and trajectory_timesteps_tensor is not None:\n        batch.trajectory_timesteps = trajectory_timesteps_tensor.cpu()\n        batch.trajectory_latents = trajectory_tensor.cpu()\n\n    # Update batch with final latents\n    batch.latents = latents\n\n    # Save STA mask search results if needed\n    if st_attn_available and self.attn_backend == SlidingTileAttentionBackend and fastvideo_args.STA_mode == STA_Mode.STA_SEARCHING:\n        self.save_sta_search_results(batch)\n\n    # deallocate transformer if on mps\n    if torch.backends.mps.is_available():\n        logger.info(\"Memory before deallocating transformer: %s\",\n                    torch.mps.current_allocated_memory())\n        del self.transformer\n        if pipeline is not None and \"transformer\" in pipeline.modules:\n            del pipeline.modules[\"transformer\"]\n        fastvideo_args.model_loaded[\"transformer\"] = False\n        logger.info(\"Memory after deallocating transformer: %s\",\n                    torch.mps.current_allocated_memory())\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.DenoisingStage.prepare_extra_func_kwargs \u00b6 <pre><code>prepare_extra_func_kwargs(func, kwargs) -&gt; dict[str, Any]\n</code></pre> <p>Prepare extra kwargs for the scheduler step / denoise step.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <p>The function to prepare kwargs for.</p> required <code>kwargs</code> <p>The kwargs to prepare.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The prepared kwargs.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def prepare_extra_func_kwargs(self, func, kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Prepare extra kwargs for the scheduler step / denoise step.\n\n    Args:\n        func: The function to prepare kwargs for.\n        kwargs: The kwargs to prepare.\n\n    Returns:\n        The prepared kwargs.\n    \"\"\"\n    extra_step_kwargs = {}\n    for k, v in kwargs.items():\n        accepts = k in set(inspect.signature(func).parameters.keys())\n        if accepts:\n            extra_step_kwargs[k] = v\n    return extra_step_kwargs\n</code></pre> fastvideo.pipelines.stages.DenoisingStage.prepare_sta_param \u00b6 <pre><code>prepare_sta_param(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n)\n</code></pre> <p>Prepare Sliding Tile Attention (STA) parameters and settings.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def prepare_sta_param(self, batch: ForwardBatch,\n                      fastvideo_args: FastVideoArgs):\n    \"\"\"\n    Prepare Sliding Tile Attention (STA) parameters and settings.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n    \"\"\"\n    # TODO(kevin): STA mask search, currently only support Wan2.1 with 69x768x1280\n    from fastvideo.STA_configuration import configure_sta\n    STA_mode = fastvideo_args.STA_mode\n    skip_time_steps = fastvideo_args.skip_time_steps\n    if batch.timesteps is None:\n        raise ValueError(\"Timesteps must be provided\")\n    timesteps_num = batch.timesteps.shape[0]\n\n    logger.info(\"STA_mode: %s\", STA_mode)\n    if (batch.num_frames, batch.height,\n            batch.width) != (69, 768, 1280) and STA_mode != \"STA_inference\":\n        raise NotImplementedError(\n            \"STA mask search/tuning is not supported for this resolution\")\n\n    if STA_mode == STA_Mode.STA_SEARCHING or STA_mode == STA_Mode.STA_TUNING or STA_mode == STA_Mode.STA_TUNING_CFG:\n        size = (batch.width, batch.height)\n        if size == (1280, 768):\n            # TODO: make it configurable\n            sparse_mask_candidates_searching = [\n                \"3, 1, 10\", \"1, 5, 7\", \"3, 3, 3\", \"1, 6, 5\", \"1, 3, 10\",\n                \"3, 6, 1\"\n            ]\n            sparse_mask_candidates_tuning = [\n                \"3, 1, 10\", \"1, 5, 7\", \"3, 3, 3\", \"1, 6, 5\", \"1, 3, 10\",\n                \"3, 6, 1\"\n            ]\n            full_mask = [\"3,6,10\"]\n        else:\n            raise NotImplementedError(\n                \"STA mask search is not supported for this resolution\")\n    layer_num = self.transformer.config.num_layers\n    # specific for HunyuanVideo\n    if hasattr(self.transformer.config, \"num_single_layers\"):\n        layer_num += self.transformer.config.num_single_layers\n    head_num = self.transformer.config.num_attention_heads\n\n    if STA_mode == STA_Mode.STA_SEARCHING:\n        STA_param = configure_sta(\n            mode=STA_Mode.STA_SEARCHING,\n            layer_num=layer_num,\n            head_num=head_num,\n            time_step_num=timesteps_num,\n            mask_candidates=sparse_mask_candidates_searching +\n            full_mask,  # last is full mask; Can add more sparse masks while keep last one as full mask\n        )\n    elif STA_mode == STA_Mode.STA_TUNING:\n        STA_param = configure_sta(\n            mode=STA_Mode.STA_TUNING,\n            layer_num=layer_num,\n            head_num=head_num,\n            time_step_num=timesteps_num,\n            mask_search_files_path=\n            f'output/mask_search_result_pos_{size[0]}x{size[1]}/',\n            mask_candidates=sparse_mask_candidates_tuning,\n            full_attention_mask=[int(x) for x in full_mask[0].split(',')],\n            skip_time_steps=\n            skip_time_steps,  # Use full attention for first 12 steps\n            save_dir=\n            f'output/mask_search_strategy_{size[0]}x{size[1]}/',  # Custom save directory\n            timesteps=timesteps_num)\n    elif STA_mode == STA_Mode.STA_TUNING_CFG:\n        STA_param = configure_sta(\n            mode=STA_Mode.STA_TUNING_CFG,\n            layer_num=layer_num,\n            head_num=head_num,\n            time_step_num=timesteps_num,\n            mask_search_files_path_pos=\n            f'output/mask_search_result_pos_{size[0]}x{size[1]}/',\n            mask_search_files_path_neg=\n            f'output/mask_search_result_neg_{size[0]}x{size[1]}/',\n            mask_candidates=sparse_mask_candidates_tuning,\n            full_attention_mask=[int(x) for x in full_mask[0].split(',')],\n            skip_time_steps=skip_time_steps,\n            save_dir=f'output/mask_search_strategy_{size[0]}x{size[1]}/',\n            timesteps=timesteps_num)\n    elif STA_mode == STA_Mode.STA_INFERENCE:\n        import fastvideo.envs as envs\n        config_file = envs.FASTVIDEO_ATTENTION_CONFIG\n        if config_file is None:\n            raise ValueError(\"FASTVIDEO_ATTENTION_CONFIG is not set\")\n        STA_param = configure_sta(mode=STA_Mode.STA_INFERENCE,\n                                  layer_num=layer_num,\n                                  head_num=head_num,\n                                  time_step_num=timesteps_num,\n                                  load_path=config_file)\n\n    batch.STA_param = STA_param\n    batch.mask_search_final_result_pos = [[] for _ in range(timesteps_num)]\n    batch.mask_search_final_result_neg = [[] for _ in range(timesteps_num)]\n</code></pre> fastvideo.pipelines.stages.DenoisingStage.progress_bar \u00b6 <pre><code>progress_bar(\n    iterable: Iterable | None = None,\n    total: int | None = None,\n) -&gt; tqdm\n</code></pre> <p>Create a progress bar for the denoising process.</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>Iterable | None</code> <p>The iterable to iterate over.</p> <code>None</code> <code>total</code> <code>int | None</code> <p>The total number of items.</p> <code>None</code> <p>Returns:</p> Type Description <code>tqdm</code> <p>A tqdm progress bar.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def progress_bar(self,\n                 iterable: Iterable | None = None,\n                 total: int | None = None) -&gt; tqdm:\n    \"\"\"\n    Create a progress bar for the denoising process.\n\n    Args:\n        iterable: The iterable to iterate over.\n        total: The total number of items.\n\n    Returns:\n        A tqdm progress bar.\n    \"\"\"\n    local_rank = get_world_group().local_rank\n    if local_rank == 0:\n        return tqdm(iterable=iterable, total=total)\n    else:\n        return tqdm(iterable=iterable, total=total, disable=True)\n</code></pre> fastvideo.pipelines.stages.DenoisingStage.rescale_noise_cfg \u00b6 <pre><code>rescale_noise_cfg(\n    noise_cfg, noise_pred_text, guidance_rescale=0.0\n) -&gt; torch.Tensor\n</code></pre> <p>Rescale noise prediction according to guidance_rescale.</p> <p>Based on findings of \"Common Diffusion Noise Schedules and Sample Steps are Flawed\" (https://arxiv.org/pdf/2305.08891.pdf), Section 3.4.</p> <p>Parameters:</p> Name Type Description Default <code>noise_cfg</code> <p>The noise prediction with guidance.</p> required <code>noise_pred_text</code> <p>The text-conditioned noise prediction.</p> required <code>guidance_rescale</code> <p>The guidance rescale factor.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The rescaled noise prediction.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def rescale_noise_cfg(self,\n                      noise_cfg,\n                      noise_pred_text,\n                      guidance_rescale=0.0) -&gt; torch.Tensor:\n    \"\"\"\n    Rescale noise prediction according to guidance_rescale.\n\n    Based on findings of \"Common Diffusion Noise Schedules and Sample Steps are Flawed\"\n    (https://arxiv.org/pdf/2305.08891.pdf), Section 3.4.\n\n    Args:\n        noise_cfg: The noise prediction with guidance.\n        noise_pred_text: The text-conditioned noise prediction.\n        guidance_rescale: The guidance rescale factor.\n\n    Returns:\n        The rescaled noise prediction.\n    \"\"\"\n    std_text = noise_pred_text.std(dim=list(range(1, noise_pred_text.ndim)),\n                                   keepdim=True)\n    std_cfg = noise_cfg.std(dim=list(range(1, noise_cfg.ndim)),\n                            keepdim=True)\n    # Rescale the results from guidance (fixes overexposure)\n    noise_pred_rescaled = noise_cfg * (std_text / std_cfg)\n    # Mix with the original results from guidance by factor guidance_rescale\n    noise_cfg = (guidance_rescale * noise_pred_rescaled +\n                 (1 - guidance_rescale) * noise_cfg)\n    return noise_cfg\n</code></pre> fastvideo.pipelines.stages.DenoisingStage.save_sta_search_results \u00b6 <pre><code>save_sta_search_results(batch: ForwardBatch)\n</code></pre> <p>Save the STA mask search results.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def save_sta_search_results(self, batch: ForwardBatch):\n    \"\"\"\n    Save the STA mask search results.\n\n    Args:\n        batch: The current batch information.\n    \"\"\"\n    size = (batch.width, batch.height)\n    if size == (1280, 768):\n        # TODO: make it configurable\n        sparse_mask_candidates_searching = [\n            \"3, 1, 10\", \"1, 5, 7\", \"3, 3, 3\", \"1, 6, 5\", \"1, 3, 10\",\n            \"3, 6, 1\"\n        ]\n    else:\n        raise NotImplementedError(\n            \"STA mask search is not supported for this resolution\")\n\n    from fastvideo.STA_configuration import save_mask_search_results\n    if batch.mask_search_final_result_pos is not None and batch.prompt is not None:\n        save_mask_search_results(\n            [\n                dict(layer_data)\n                for layer_data in batch.mask_search_final_result_pos\n            ],\n            prompt=str(batch.prompt),\n            mask_strategies=sparse_mask_candidates_searching,\n            output_dir=f'output/mask_search_result_pos_{size[0]}x{size[1]}/'\n        )\n    if batch.mask_search_final_result_neg is not None and batch.prompt is not None:\n        save_mask_search_results(\n            [\n                dict(layer_data)\n                for layer_data in batch.mask_search_final_result_neg\n            ],\n            prompt=str(batch.prompt),\n            mask_strategies=sparse_mask_candidates_searching,\n            output_dir=f'output/mask_search_result_neg_{size[0]}x{size[1]}/'\n        )\n</code></pre> fastvideo.pipelines.stages.DenoisingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify denoising stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify denoising stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"timesteps\", batch.timesteps,\n                     [V.is_tensor, V.min_dims(1)])\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds, V.list_not_empty)\n    result.add_check(\"image_embeds\", batch.image_embeds, V.is_list)\n    result.add_check(\"image_latent\", batch.image_latent,\n                     V.none_or_tensor_with_dims(5))\n    result.add_check(\"num_inference_steps\", batch.num_inference_steps,\n                     V.positive_int)\n    result.add_check(\"guidance_scale\", batch.guidance_scale,\n                     V.positive_float)\n    result.add_check(\"eta\", batch.eta, V.non_negative_float)\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    result.add_check(\"do_classifier_free_guidance\",\n                     batch.do_classifier_free_guidance, V.bool_value)\n    result.add_check(\n        \"negative_prompt_embeds\", batch.negative_prompt_embeds, lambda x:\n        not batch.do_classifier_free_guidance or V.list_not_empty(x))\n    return result\n</code></pre> fastvideo.pipelines.stages.DenoisingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify denoising stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify denoising stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.DmdDenoisingStage","title":"fastvideo.pipelines.stages.DmdDenoisingStage","text":"<pre><code>DmdDenoisingStage(transformer, scheduler)\n</code></pre> <p>               Bases: <code>DenoisingStage</code></p> <p>Denoising stage for DMD.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def __init__(self, transformer, scheduler) -&gt; None:\n    super().__init__(transformer, scheduler)\n    self.scheduler = FlowMatchEulerDiscreteScheduler(shift=8.0)\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.DmdDenoisingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Run the denoising loop.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with denoised latents.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Run the denoising loop.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with denoised latents.\n    \"\"\"\n    # Setup precision and autocast settings\n    # TODO(will): make the precision configurable for inference\n    # target_dtype = PRECISION_TO_TYPE[fastvideo_args.precision]\n    target_dtype = torch.bfloat16\n    autocast_enabled = (target_dtype != torch.float32\n                        ) and not fastvideo_args.disable_autocast\n\n    # Get timesteps and calculate warmup steps\n    timesteps = batch.timesteps\n\n    # TODO(will): remove this once we add input/output validation for stages\n    if timesteps is None:\n        raise ValueError(\"Timesteps must be provided\")\n    num_inference_steps = batch.num_inference_steps\n    num_warmup_steps = len(\n        timesteps) - num_inference_steps * self.scheduler.order\n\n    # Prepare image latents and embeddings for I2V generation\n    image_embeds = batch.image_embeds\n    if len(image_embeds) &gt; 0:\n        assert torch.isnan(image_embeds[0]).sum() == 0\n        image_embeds = [\n            image_embed.to(target_dtype) for image_embed in image_embeds\n        ]\n\n    image_kwargs = self.prepare_extra_func_kwargs(\n        self.transformer.forward,\n        {\n            \"encoder_hidden_states_image\": image_embeds,\n            \"mask_strategy\": dict_to_3d_list(\n                None, t_max=50, l_max=60, h_max=24)\n        },\n    )\n\n    pos_cond_kwargs = self.prepare_extra_func_kwargs(\n        self.transformer.forward,\n        {\n            \"encoder_hidden_states_2\": batch.clip_embedding_pos,\n            \"encoder_attention_mask\": batch.prompt_attention_mask,\n        },\n    )\n\n    # Prepare STA parameters\n    if st_attn_available and self.attn_backend == SlidingTileAttentionBackend:\n        self.prepare_sta_param(batch, fastvideo_args)\n\n    # Get latents and embeddings\n    assert batch.latents is not None, \"latents must be provided\"\n    latents = batch.latents\n    latents = latents.permute(0, 2, 1, 3, 4)\n\n    video_raw_latent_shape = latents.shape\n    prompt_embeds = batch.prompt_embeds\n    assert not torch.isnan(\n        prompt_embeds[0]).any(), \"prompt_embeds contains nan\"\n    timesteps = torch.tensor(\n        fastvideo_args.pipeline_config.dmd_denoising_steps,\n        dtype=torch.long,\n        device=get_local_torch_device())\n\n    # Handle sequence parallelism if enabled\n    sp_world_size, rank_in_sp_group = get_sp_world_size(\n    ), get_sp_parallel_rank()\n    sp_group = sp_world_size &gt; 1\n    if sp_group:\n        latents = rearrange(latents,\n                            \"b (n t) c h w -&gt; b n t c h w\",\n                            n=sp_world_size).contiguous()\n        latents = latents[:, rank_in_sp_group, :, :, :, :]\n        if batch.image_latent is not None:\n            image_latent = rearrange(batch.image_latent,\n                                     \"b c (n t) h w -&gt; b c n t h w\",\n                                     n=sp_world_size).contiguous()\n\n            image_latent = image_latent[:, :, rank_in_sp_group, :, :, :]\n            batch.image_latent = image_latent\n\n    # Run denoising loop\n    with self.progress_bar(total=len(timesteps)) as progress_bar:\n        for i, t in enumerate(timesteps):\n            # Skip if interrupted\n            if hasattr(self, 'interrupt') and self.interrupt:\n                continue\n            # Expand latents for I2V\n            noise_latents = latents.clone()\n            latent_model_input = latents.to(target_dtype)\n\n            if batch.image_latent is not None:\n                latent_model_input = torch.cat([\n                    latent_model_input,\n                    batch.image_latent.permute(0, 2, 1, 3, 4)\n                ],\n                                               dim=2).to(target_dtype)\n            assert not torch.isnan(\n                latent_model_input).any(), \"latent_model_input contains nan\"\n\n            # Prepare inputs for transformer\n            t_expand = t.repeat(latent_model_input.shape[0])\n            guidance_expand = (\n                torch.tensor(\n                    [fastvideo_args.pipeline_config.embedded_cfg_scale] *\n                    latent_model_input.shape[0],\n                    dtype=torch.float32,\n                    device=get_local_torch_device(),\n                ).to(target_dtype) *\n                1000.0 if fastvideo_args.pipeline_config.embedded_cfg_scale\n                is not None else None)\n\n            # Predict noise residual\n            with torch.autocast(device_type=\"cuda\",\n                                dtype=target_dtype,\n                                enabled=autocast_enabled):\n                if (vsa_available and self.attn_backend\n                        == VideoSparseAttentionBackend):\n                    self.attn_metadata_builder_cls = self.attn_backend.get_builder_cls(\n                    )\n\n                    if self.attn_metadata_builder_cls is not None:\n                        self.attn_metadata_builder = self.attn_metadata_builder_cls(\n                        )\n                        # TODO(will): clean this up\n                        attn_metadata = self.attn_metadata_builder.build(  # type: ignore\n                            current_timestep=i,  # type: ignore\n                            raw_latent_shape=batch.\n                            raw_latent_shape[2:5],  # type: ignore\n                            patch_size=fastvideo_args.\n                            pipeline_config.  # type: ignore\n                            dit_config.patch_size,  # type: ignore\n                            STA_param=batch.STA_param,  # type: ignore\n                            VSA_sparsity=fastvideo_args.\n                            VSA_sparsity,  # type: ignore\n                            device=get_local_torch_device(),  # type: ignore\n                        )  # type: ignore\n                        assert attn_metadata is not None, \"attn_metadata cannot be None\"\n                    else:\n                        attn_metadata = None\n                else:\n                    attn_metadata = None\n\n                batch.is_cfg_negative = False\n                with set_forward_context(\n                        current_timestep=i,\n                        attn_metadata=attn_metadata,\n                        forward_batch=batch,\n                        # fastvideo_args=fastvideo_args\n                ):\n                    # Run transformer\n                    pred_noise = self.transformer(\n                        latent_model_input.permute(0, 2, 1, 3, 4),\n                        prompt_embeds,\n                        t_expand,\n                        guidance=guidance_expand,\n                        **image_kwargs,\n                        **pos_cond_kwargs,\n                    ).permute(0, 2, 1, 3, 4)\n\n                pred_video = pred_noise_to_pred_video(\n                    pred_noise=pred_noise.flatten(0, 1),\n                    noise_input_latent=noise_latents.flatten(0, 1),\n                    timestep=t_expand,\n                    scheduler=self.scheduler).unflatten(\n                        0, pred_noise.shape[:2])\n\n                if i &lt; len(timesteps) - 1:\n                    next_timestep = timesteps[i + 1] * torch.ones(\n                        [1], dtype=torch.long, device=pred_video.device)\n                    noise = torch.randn(video_raw_latent_shape,\n                                        dtype=pred_video.dtype,\n                                        generator=batch.generator[0]).to(\n                                            self.device)\n                    if sp_group:\n                        noise = rearrange(noise,\n                                          \"b (n t) c h w -&gt; b n t c h w\",\n                                          n=sp_world_size).contiguous()\n                        noise = noise[:, rank_in_sp_group, :, :, :, :]\n                    latents = self.scheduler.add_noise(\n                        pred_video.flatten(0, 1), noise.flatten(0, 1),\n                        next_timestep).unflatten(0, pred_video.shape[:2])\n                else:\n                    latents = pred_video\n\n                # Update progress bar\n                if i == len(timesteps) - 1 or (\n                    (i + 1) &gt; num_warmup_steps and\n                    (i + 1) % self.scheduler.order == 0\n                        and progress_bar is not None):\n                    progress_bar.update()\n\n    # Gather results if using sequence parallelism\n    if sp_group:\n        latents = sequence_model_parallel_all_gather(latents, dim=1)\n    latents = latents.permute(0, 2, 1, 3, 4)\n    # Update batch with final latents\n    batch.latents = latents\n\n    return batch\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.EncodingStage","title":"fastvideo.pipelines.stages.EncodingStage","text":"<pre><code>EncodingStage(vae: ParallelTiledVAE)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for encoding pixel space representations into latent space.</p> <p>This stage handles the encoding of pixel-space video/images into latent representations for further processing in the diffusion pipeline.</p> Source code in <code>fastvideo/pipelines/stages/encoding.py</code> <pre><code>def __init__(self, vae: ParallelTiledVAE) -&gt; None:\n    self.vae: ParallelTiledVAE = vae\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.EncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode pixel space representations into latent space.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded latents.</p> Source code in <code>fastvideo/pipelines/stages/encoding.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode pixel space representations into latent space.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded latents.\n    \"\"\"\n    assert batch.latents is not None and isinstance(batch.latents,\n                                                    torch.Tensor)\n\n    self.vae = self.vae.to(get_local_torch_device())\n\n    # Setup VAE precision\n    vae_dtype = PRECISION_TO_TYPE[\n        fastvideo_args.pipeline_config.vae_precision]\n    vae_autocast_enabled = (\n        vae_dtype != torch.float32) and not fastvideo_args.disable_autocast\n\n    # Normalize input to [-1, 1] range (reverse of decoding normalization)\n    latents = (batch.latents * 2.0 - 1.0).clamp(-1, 1)\n\n    # Move to appropriate device and dtype\n    latents = latents.to(get_local_torch_device())\n\n    # Encode image to latents\n    with torch.autocast(device_type=\"cuda\",\n                        dtype=vae_dtype,\n                        enabled=vae_autocast_enabled):\n        if fastvideo_args.pipeline_config.vae_tiling:\n            self.vae.enable_tiling()\n        # if fastvideo_args.vae_sp:\n        #     self.vae.enable_parallel()\n        if not vae_autocast_enabled:\n            latents = latents.to(vae_dtype)\n        latents = self.vae.encode(latents).mean\n\n    # Update batch with encoded latents\n    batch.latents = latents\n\n    # Offload models if needed\n    if hasattr(self, 'maybe_free_model_hooks'):\n        self.maybe_free_model_hooks()\n\n    if fastvideo_args.vae_cpu_offload:\n        self.vae.to(\"cpu\")\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.EncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/encoding.py</code> <pre><code>@torch.no_grad()\ndef verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    # Input video/images for VAE encoding: [batch_size, channels, frames, height, width]\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre> fastvideo.pipelines.stages.EncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    # Encoded latents: [batch_size, channels, frames, height_latents, width_latents]\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.ImageEncodingStage","title":"fastvideo.pipelines.stages.ImageEncodingStage","text":"<pre><code>ImageEncodingStage(image_encoder, image_processor)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for encoding image prompts into embeddings for diffusion models.</p> <p>This stage handles the encoding of image prompts into the embedding space expected by the diffusion model.</p> <p>Initialize the prompt encoding stage.</p> <p>Parameters:</p> Name Type Description Default <code>enable_logging</code> <p>Whether to enable logging for this stage.</p> required <code>is_secondary</code> <p>Whether this is a secondary image encoder.</p> required Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def __init__(self, image_encoder, image_processor) -&gt; None:\n    \"\"\"\n    Initialize the prompt encoding stage.\n\n    Args:\n        enable_logging: Whether to enable logging for this stage.\n        is_secondary: Whether this is a secondary image encoder.\n    \"\"\"\n    super().__init__()\n    self.image_processor = image_processor\n    self.image_encoder = image_encoder\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.ImageEncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode the prompt into image encoder hidden states.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded prompt embeddings.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode the prompt into image encoder hidden states.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded prompt embeddings.\n    \"\"\"\n    self.image_encoder = self.image_encoder.to(get_local_torch_device())\n\n    image = batch.pil_image\n\n    image_inputs = self.image_processor(\n        images=image, return_tensors=\"pt\").to(get_local_torch_device())\n    with set_forward_context(current_timestep=0, attn_metadata=None):\n        outputs = self.image_encoder(**image_inputs)\n        image_embeds = outputs.last_hidden_state\n\n    batch.image_embeds.append(image_embeds)\n\n    if fastvideo_args.image_encoder_cpu_offload:\n        self.image_encoder.to('cpu')\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.ImageEncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify image encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify image encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"pil_image\", batch.pil_image, V.not_none)\n    result.add_check(\"image_embeds\", batch.image_embeds, V.is_list)\n    return result\n</code></pre> fastvideo.pipelines.stages.ImageEncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify image encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify image encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"image_embeds\", batch.image_embeds,\n                     V.list_of_tensors_dims(3))\n    return result\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.ImageVAEEncodingStage","title":"fastvideo.pipelines.stages.ImageVAEEncodingStage","text":"<pre><code>ImageVAEEncodingStage(vae: ParallelTiledVAE)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for encoding image pixel representations into latent space.</p> <p>This stage handles the encoding of image pixel representations into the final input format (e.g., latents) for image-to-video generation.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def __init__(self, vae: ParallelTiledVAE) -&gt; None:\n    self.vae: ParallelTiledVAE = vae\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.ImageVAEEncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode pixel representations into latent space.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded outputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode pixel representations into latent space.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded outputs.\n    \"\"\"\n    assert batch.pil_image is not None\n    if fastvideo_args.mode == ExecutionMode.INFERENCE:\n        assert batch.pil_image is not None and isinstance(\n            batch.pil_image, PIL.Image.Image)\n        assert batch.height is not None and isinstance(batch.height, int)\n        assert batch.width is not None and isinstance(batch.width, int)\n        assert batch.num_frames is not None and isinstance(\n            batch.num_frames, int)\n        height = batch.height\n        width = batch.width\n        num_frames = batch.num_frames\n    elif fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        assert batch.pil_image is not None and isinstance(\n            batch.pil_image, torch.Tensor)\n        assert batch.height is not None and isinstance(batch.height, list)\n        assert batch.width is not None and isinstance(batch.width, list)\n        assert batch.num_frames is not None and isinstance(\n            batch.num_frames, list)\n        num_frames = batch.num_frames[0]\n        height = batch.height[0]\n        width = batch.width[0]\n\n    self.vae = self.vae.to(get_local_torch_device())\n\n    # Process single image for I2V\n    latent_height = height // self.vae.spatial_compression_ratio\n    latent_width = width // self.vae.spatial_compression_ratio\n    image = batch.pil_image\n    image = self.preprocess(\n        image,\n        vae_scale_factor=self.vae.spatial_compression_ratio,\n        height=height,\n        width=width).to(get_local_torch_device(), dtype=torch.float32)\n\n    # (B, C, H, W) -&gt; (B, C, 1, H, W)\n    image = image.unsqueeze(2)\n\n    video_condition = torch.cat([\n        image,\n        image.new_zeros(image.shape[0], image.shape[1], num_frames - 1,\n                        image.shape[3], image.shape[4])\n    ],\n                                dim=2)\n    video_condition = video_condition.to(device=get_local_torch_device(),\n                                         dtype=torch.float32)\n\n    # Setup VAE precision\n    vae_dtype = PRECISION_TO_TYPE[\n        fastvideo_args.pipeline_config.vae_precision]\n    vae_autocast_enabled = (\n        vae_dtype != torch.float32) and not fastvideo_args.disable_autocast\n\n    # Encode Image\n    with torch.autocast(device_type=\"cuda\",\n                        dtype=vae_dtype,\n                        enabled=vae_autocast_enabled):\n        if fastvideo_args.pipeline_config.vae_tiling:\n            self.vae.enable_tiling()\n        # if fastvideo_args.vae_sp:\n        #     self.vae.enable_parallel()\n        if not vae_autocast_enabled:\n            video_condition = video_condition.to(vae_dtype)\n        encoder_output = self.vae.encode(video_condition)\n\n    if fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        latent_condition = encoder_output.mean\n    else:\n        generator = batch.generator\n        if generator is None:\n            raise ValueError(\"Generator must be provided\")\n        latent_condition = self.retrieve_latents(encoder_output, generator)\n\n    # Apply shifting if needed\n    if (hasattr(self.vae, \"shift_factor\")\n            and self.vae.shift_factor is not None):\n        if isinstance(self.vae.shift_factor, torch.Tensor):\n            latent_condition -= self.vae.shift_factor.to(\n                latent_condition.device, latent_condition.dtype)\n        else:\n            latent_condition -= self.vae.shift_factor\n\n    if isinstance(self.vae.scaling_factor, torch.Tensor):\n        latent_condition = latent_condition * self.vae.scaling_factor.to(\n            latent_condition.device, latent_condition.dtype)\n    else:\n        latent_condition = latent_condition * self.vae.scaling_factor\n\n    if fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        batch.image_latent = latent_condition\n    else:\n        mask_lat_size = torch.ones(1, 1, num_frames, latent_height,\n                                   latent_width)\n        mask_lat_size[:, :, list(range(1, num_frames))] = 0\n        first_frame_mask = mask_lat_size[:, :, 0:1]\n        first_frame_mask = torch.repeat_interleave(\n            first_frame_mask,\n            dim=2,\n            repeats=self.vae.temporal_compression_ratio)\n        mask_lat_size = torch.concat(\n            [first_frame_mask, mask_lat_size[:, :, 1:, :]], dim=2)\n        mask_lat_size = mask_lat_size.view(\n            1, -1, self.vae.temporal_compression_ratio, latent_height,\n            latent_width)\n        mask_lat_size = mask_lat_size.transpose(1, 2)\n        mask_lat_size = mask_lat_size.to(latent_condition.device)\n\n        batch.image_latent = torch.concat([mask_lat_size, latent_condition],\n                                          dim=1)\n\n    # Offload models if needed\n    if hasattr(self, 'maybe_free_model_hooks'):\n        self.maybe_free_model_hooks()\n\n    self.vae.to(\"cpu\")\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.ImageVAEEncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    if fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        result.add_check(\"height\", batch.height, V.list_not_empty)\n        result.add_check(\"width\", batch.width, V.list_not_empty)\n        result.add_check(\"num_frames\", batch.num_frames, V.list_not_empty)\n    else:\n        result.add_check(\"height\", batch.height, V.positive_int)\n        result.add_check(\"width\", batch.width, V.positive_int)\n        result.add_check(\"num_frames\", batch.num_frames, V.positive_int)\n    return result\n</code></pre> fastvideo.pipelines.stages.ImageVAEEncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"image_latent\", batch.image_latent,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.InputValidationStage","title":"fastvideo.pipelines.stages.InputValidationStage","text":"<p>               Bases: <code>PipelineStage</code></p> <p>Stage for validating and preparing inputs for diffusion pipelines.</p> <p>This stage validates that all required inputs are present and properly formatted before proceeding with the diffusion process.</p> Functions\u00b6 fastvideo.pipelines.stages.InputValidationStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Validate and prepare inputs.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The validated batch information.</p> Source code in <code>fastvideo/pipelines/stages/input_validation.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Validate and prepare inputs.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The validated batch information.\n    \"\"\"\n\n    self._generate_seeds(batch, fastvideo_args)\n\n    # Ensure prompt is properly formatted\n    if batch.prompt is None and batch.prompt_embeds is None:\n        raise ValueError(\n            \"Either `prompt` or `prompt_embeds` must be provided\")\n\n    # Ensure negative prompt is properly formatted if using classifier-free guidance\n    if (batch.do_classifier_free_guidance and batch.negative_prompt is None\n            and batch.negative_prompt_embeds is None):\n        raise ValueError(\n            \"For classifier-free guidance, either `negative_prompt` or \"\n            \"`negative_prompt_embeds` must be provided\")\n\n    # Validate height and width\n    if batch.height is None or batch.width is None:\n        raise ValueError(\n            \"Height and width must be provided. Please set `height` and `width`.\"\n        )\n    if batch.height % 8 != 0 or batch.width % 8 != 0:\n        raise ValueError(\n            f\"Height and width must be divisible by 8 but are {batch.height} and {batch.width}.\"\n        )\n\n    # Validate number of inference steps\n    if batch.num_inference_steps &lt;= 0:\n        raise ValueError(\n            f\"Number of inference steps must be positive, but got {batch.num_inference_steps}\"\n        )\n\n    # Validate guidance scale if using classifier-free guidance\n    if batch.do_classifier_free_guidance and batch.guidance_scale &lt;= 0:\n        raise ValueError(\n            f\"Guidance scale must be positive, but got {batch.guidance_scale}\"\n        )\n\n    # for i2v, get image from image_path\n    # @TODO(Wei) hard-coded for wan2.2 5b ti2v for now. Should put this in image_encoding stage\n    if batch.image_path is not None:\n        if batch.image_path.endswith(\".mp4\"):\n            image = load_video(batch.image_path)[0]\n        else:\n            image = load_image(batch.image_path)\n        batch.pil_image = image\n\n    # further processing for ti2v task\n    if fastvideo_args.pipeline_config.ti2v_task and batch.pil_image is not None:\n        img = batch.pil_image\n        ih, iw = img.height, img.width\n        patch_size = fastvideo_args.pipeline_config.dit_config.arch_config.patch_size\n        vae_stride = fastvideo_args.pipeline_config.vae_config.arch_config.scale_factor_spatial\n        dh, dw = patch_size[1] * vae_stride, patch_size[2] * vae_stride\n        max_area = 704 * 1280\n        ow, oh = best_output_size(iw, ih, dw, dh, max_area)\n\n        scale = max(ow / iw, oh / ih)\n        img = img.resize((round(iw * scale), round(ih * scale)),\n                         Image.LANCZOS)\n        logger.info(\"resized img height: %s, img width: %s\", img.height,\n                    img.width)\n\n        # center-crop\n        x1 = (img.width - ow) // 2\n        y1 = (img.height - oh) // 2\n        img = img.crop((x1, y1, x1 + ow, y1 + oh))\n        assert img.width == ow and img.height == oh\n\n        # to tensor\n        img = TF.to_tensor(img).sub_(0.5).div_(0.5).to(\n            self.device).unsqueeze(1)\n        img = img.unsqueeze(0)\n        batch.height = oh\n        batch.width = ow\n        batch.pil_image = img\n\n    # for v2v, get control video from video path\n    if batch.video_path is not None:\n        pil_images, original_fps = load_video(batch.video_path,\n                                              return_fps=True)\n        logger.info(\"Loaded video with %s frames, original FPS: %s\",\n                    len(pil_images), original_fps)\n\n        # Get target parameters from batch\n        target_fps = batch.fps\n        target_num_frames = batch.num_frames\n        target_height = batch.height\n        target_width = batch.width\n\n        if target_fps is not None and original_fps is not None:\n            frame_skip = max(1, int(original_fps // target_fps))\n            if frame_skip &gt; 1:\n                pil_images = pil_images[::frame_skip]\n                effective_fps = original_fps / frame_skip\n                logger.info(\n                    \"Resampled video from %.1f fps to %.1f fps (skip=%s)\",\n                    original_fps, effective_fps, frame_skip)\n\n        # Limit to target number of frames\n        if target_num_frames is not None and len(\n                pil_images) &gt; target_num_frames:\n            pil_images = pil_images[:target_num_frames]\n            logger.info(\"Limited video to %s frames (from %s total)\",\n                        target_num_frames, len(pil_images))\n\n        # Resize each PIL image to target dimensions\n        resized_images = []\n        for pil_img in pil_images:\n            resized_img = resize(pil_img,\n                                 target_height,\n                                 target_width,\n                                 resize_mode=\"default\",\n                                 resample=\"lanczos\")\n            resized_images.append(resized_img)\n\n        # Convert PIL images to numpy array\n        video_numpy = pil_to_numpy(resized_images)\n        video_numpy = normalize(video_numpy)\n        video_tensor = numpy_to_pt(video_numpy)\n\n        # Rearrange to [C, T, H, W] and add batch dimension -&gt; [B, C, T, H, W]\n        input_video = video_tensor.permute(1, 0, 2, 3).unsqueeze(0)\n\n        batch.video_latent = input_video\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.InputValidationStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify input validation stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/input_validation.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify input validation stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"seed\", batch.seed, [V.not_none, V.positive_int])\n    result.add_check(\"num_videos_per_prompt\", batch.num_videos_per_prompt,\n                     V.positive_int)\n    result.add_check(\n        \"prompt_or_embeds\", None, lambda _: V.string_or_list_strings(\n            batch.prompt) or V.list_not_empty(batch.prompt_embeds))\n    result.add_check(\"height\", batch.height, V.positive_int)\n    result.add_check(\"width\", batch.width, V.positive_int)\n    result.add_check(\"num_inference_steps\", batch.num_inference_steps,\n                     V.positive_int)\n    result.add_check(\n        \"guidance_scale\", batch.guidance_scale, lambda x: not batch.\n        do_classifier_free_guidance or V.positive_float(x))\n    return result\n</code></pre> fastvideo.pipelines.stages.InputValidationStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify input validation stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/input_validation.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify input validation stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"seeds\", batch.seeds, V.list_not_empty)\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    return result\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.LatentPreparationStage","title":"fastvideo.pipelines.stages.LatentPreparationStage","text":"<pre><code>LatentPreparationStage(scheduler, transformer)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for preparing initial latent variables for the diffusion process.</p> <p>This stage handles the preparation of the initial latent variables that will be denoised during the diffusion process.</p> Source code in <code>fastvideo/pipelines/stages/latent_preparation.py</code> <pre><code>def __init__(self, scheduler, transformer) -&gt; None:\n    super().__init__()\n    self.scheduler = scheduler\n    self.transformer = transformer\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.LatentPreparationStage.adjust_video_length \u00b6 <pre><code>adjust_video_length(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; int\n</code></pre> <p>Adjust video length based on VAE version.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The batch with adjusted video length.</p> Source code in <code>fastvideo/pipelines/stages/latent_preparation.py</code> <pre><code>def adjust_video_length(self, batch: ForwardBatch,\n                        fastvideo_args: FastVideoArgs) -&gt; int:\n    \"\"\"\n    Adjust video length based on VAE version.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with adjusted video length.\n    \"\"\"\n\n    video_length = batch.num_frames\n    use_temporal_scaling_frames = fastvideo_args.pipeline_config.vae_config.use_temporal_scaling_frames\n    if use_temporal_scaling_frames:\n        temporal_scale_factor = fastvideo_args.pipeline_config.vae_config.arch_config.temporal_compression_ratio\n        latent_num_frames = (video_length - 1) // temporal_scale_factor + 1\n    else:  # stepvideo only\n        latent_num_frames = video_length // 17 * 3\n    return int(latent_num_frames)\n</code></pre> fastvideo.pipelines.stages.LatentPreparationStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Prepare initial latent variables for the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with prepared latent variables.</p> Source code in <code>fastvideo/pipelines/stages/latent_preparation.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Prepare initial latent variables for the diffusion process.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with prepared latent variables.\n    \"\"\"\n\n    latent_num_frames = None\n    # Adjust video length based on VAE version if needed\n    if hasattr(self, 'adjust_video_length'):\n        latent_num_frames = self.adjust_video_length(batch, fastvideo_args)\n    # Determine batch size\n    if isinstance(batch.prompt, list):\n        batch_size = len(batch.prompt)\n    elif batch.prompt is not None:\n        batch_size = 1\n    else:\n        batch_size = batch.prompt_embeds[0].shape[0]\n\n    # Adjust batch size for number of videos per prompt\n    batch_size *= batch.num_videos_per_prompt\n\n    # Get required parameters\n    dtype = batch.prompt_embeds[0].dtype\n    device = get_local_torch_device()\n    generator = batch.generator\n    latents = batch.latents\n    num_frames = latent_num_frames if latent_num_frames is not None else batch.num_frames\n    height = batch.height\n    width = batch.width\n\n    # TODO(will): remove this once we add input/output validation for stages\n    if height is None or width is None:\n        raise ValueError(\"Height and width must be provided\")\n\n    # Calculate latent shape\n    shape = (\n        batch_size,\n        self.transformer.num_channels_latents,\n        num_frames,\n        height // fastvideo_args.pipeline_config.vae_config.arch_config.\n        spatial_compression_ratio,\n        width // fastvideo_args.pipeline_config.vae_config.arch_config.\n        spatial_compression_ratio,\n    )\n\n    # Validate generator if it's a list\n    if isinstance(generator, list) and len(generator) != batch_size:\n        raise ValueError(\n            f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n            f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n        )\n    # Generate or use provided latents\n    if latents is None:\n        latents = randn_tensor(shape,\n                               generator=generator,\n                               device=device,\n                               dtype=dtype)\n    else:\n        latents = latents.to(device)\n\n    # Scale the initial noise if needed\n    if hasattr(self.scheduler, \"init_noise_sigma\"):\n        latents = latents * self.scheduler.init_noise_sigma\n    # Update batch with prepared latents\n    batch.latents = latents\n    batch.raw_latent_shape = latents.shape\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.LatentPreparationStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify latent preparation stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/latent_preparation.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify latent preparation stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\n        \"prompt_or_embeds\", None, lambda _: V.string_or_list_strings(\n            batch.prompt) or V.list_not_empty(batch.prompt_embeds))\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds,\n                     V.list_of_tensors)\n    result.add_check(\"num_videos_per_prompt\", batch.num_videos_per_prompt,\n                     V.positive_int)\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    result.add_check(\"num_frames\", batch.num_frames, V.positive_int)\n    result.add_check(\"height\", batch.height, V.positive_int)\n    result.add_check(\"width\", batch.width, V.positive_int)\n    result.add_check(\"latents\", batch.latents, V.none_or_tensor)\n    return result\n</code></pre> fastvideo.pipelines.stages.LatentPreparationStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify latent preparation stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/latent_preparation.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify latent preparation stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    result.add_check(\"raw_latent_shape\", batch.raw_latent_shape, V.is_tuple)\n    return result\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.PipelineStage","title":"fastvideo.pipelines.stages.PipelineStage","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all pipeline stages.</p> <p>A pipeline stage represents a discrete step in the diffusion process that can be composed with other stages to create a complete pipeline. Each stage is responsible for a specific part of the process, such as prompt encoding, latent preparation, etc.</p> Attributes\u00b6 fastvideo.pipelines.stages.PipelineStage.device <code>property</code> \u00b6 <pre><code>device: device\n</code></pre> <p>Get the device for this stage.</p> Functions\u00b6 fastvideo.pipelines.stages.PipelineStage.__call__ \u00b6 <pre><code>__call__(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Execute the stage's processing on the batch with optional verification and logging. Should not be overridden by subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The updated batch information after this stage's processing.</p> Source code in <code>fastvideo/pipelines/stages/base.py</code> <pre><code>def __call__(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Execute the stage's processing on the batch with optional verification and logging.\n    Should not be overridden by subclasses.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The updated batch information after this stage's processing.\n    \"\"\"\n    stage_name = self.__class__.__name__\n\n    # Check if verification is enabled (simple approach for prototype)\n    enable_verification = getattr(fastvideo_args,\n                                  'enable_stage_verification', False)\n\n    if enable_verification:\n        # Pre-execution input verification\n        try:\n            input_result = self.verify_input(batch, fastvideo_args)\n            self._run_verification(input_result, stage_name, \"input\")\n        except Exception as e:\n            logger.error(\"Input verification failed for %s: %s\", stage_name,\n                         str(e))\n            raise\n\n    # Execute the actual stage logic\n    if envs.FASTVIDEO_STAGE_LOGGING:\n        logger.info(\"[%s] Starting execution\", stage_name)\n        start_time = time.perf_counter()\n\n        try:\n            result = self.forward(batch, fastvideo_args)\n            execution_time = time.perf_counter() - start_time\n            logger.info(\"[%s] Execution completed in %s ms\", stage_name,\n                        execution_time * 1000)\n            batch.logging_info.add_stage_execution_time(\n                stage_name, execution_time)\n        except Exception as e:\n            execution_time = time.perf_counter() - start_time\n            logger.error(\"[%s] Error during execution after %s ms: %s\",\n                         stage_name, execution_time * 1000, e)\n            logger.error(\"[%s] Traceback: %s\", stage_name,\n                         traceback.format_exc())\n            raise\n    else:\n        # Direct execution (current behavior)\n        result = self.forward(batch, fastvideo_args)\n\n    if enable_verification:\n        # Post-execution output verification\n        try:\n            output_result = self.verify_output(result, fastvideo_args)\n            self._run_verification(output_result, stage_name, \"output\")\n        except Exception as e:\n            logger.error(\"Output verification failed for %s: %s\",\n                         stage_name, str(e))\n            raise\n\n    return result\n</code></pre> fastvideo.pipelines.stages.PipelineStage.forward <code>abstractmethod</code> \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Forward pass of the stage's processing.</p> <p>This method should be implemented by subclasses to provide the forward processing logic for the stage.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The updated batch information after this stage's processing.</p> Source code in <code>fastvideo/pipelines/stages/base.py</code> <pre><code>@abstractmethod\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Forward pass of the stage's processing.\n\n    This method should be implemented by subclasses to provide the forward\n    processing logic for the stage.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The updated batch information after this stage's processing.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.pipelines.stages.PipelineStage.set_logging \u00b6 <pre><code>set_logging(enable: bool)\n</code></pre> <p>Enable or disable logging for this stage.</p> <p>Parameters:</p> Name Type Description Default <code>enable</code> <code>bool</code> <p>Whether to enable logging.</p> required Source code in <code>fastvideo/pipelines/stages/base.py</code> <pre><code>def set_logging(self, enable: bool):\n    \"\"\"\n    Enable or disable logging for this stage.\n\n    Args:\n        enable: Whether to enable logging.\n    \"\"\"\n    self._enable_logging = enable\n</code></pre> fastvideo.pipelines.stages.PipelineStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify the input for the stage.</p> Example <p>from fastvideo.pipelines.stages.validators import V, VerificationResult</p> <p>def verify_input(self, batch, fastvideo_args):     result = VerificationResult()     result.add_check(\"height\", batch.height, V.positive_int_divisible(8))     result.add_check(\"width\", batch.width, V.positive_int_divisible(8))     result.add_check(\"image_latent\", batch.image_latent, V.is_tensor)     return result</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>VerificationResult</code> <p>A VerificationResult containing the verification status.</p> Source code in <code>fastvideo/pipelines/stages/base.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"\n    Verify the input for the stage.\n\n    Example:\n        from fastvideo.pipelines.stages.validators import V, VerificationResult\n\n        def verify_input(self, batch, fastvideo_args):\n            result = VerificationResult()\n            result.add_check(\"height\", batch.height, V.positive_int_divisible(8))\n            result.add_check(\"width\", batch.width, V.positive_int_divisible(8))\n            result.add_check(\"image_latent\", batch.image_latent, V.is_tensor)\n            return result\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        A VerificationResult containing the verification status.\n\n    \"\"\"\n    # Default implementation - no verification\n    return VerificationResult()\n</code></pre> fastvideo.pipelines.stages.PipelineStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify the output for the stage.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>VerificationResult</code> <p>A VerificationResult containing the verification status.</p> Source code in <code>fastvideo/pipelines/stages/base.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"\n    Verify the output for the stage.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        A VerificationResult containing the verification status.\n    \"\"\"\n    # Default implementation - no verification\n    return VerificationResult()\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.RefImageEncodingStage","title":"fastvideo.pipelines.stages.RefImageEncodingStage","text":"<pre><code>RefImageEncodingStage(image_encoder, image_processor)\n</code></pre> <p>               Bases: <code>ImageEncodingStage</code></p> <p>Stage for encoding reference image prompts into embeddings for Wan2.1 Control models.</p> <p>This stage extends ImageEncodingStage with specialized preprocessing for reference images.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def __init__(self, image_encoder, image_processor) -&gt; None:\n    \"\"\"\n    Initialize the prompt encoding stage.\n\n    Args:\n        enable_logging: Whether to enable logging for this stage.\n        is_secondary: Whether this is a secondary image encoder.\n    \"\"\"\n    super().__init__()\n    self.image_processor = image_processor\n    self.image_encoder = image_encoder\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.RefImageEncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode the prompt into image encoder hidden states.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded prompt embeddings.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode the prompt into image encoder hidden states.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded prompt embeddings.\n    \"\"\"\n    self.image_encoder = self.image_encoder.to(get_local_torch_device())\n\n    image = batch.pil_image\n    if image is None:\n        image = create_default_image()\n    # Preprocess reference image for CLIP encoder\n    image_tensor = preprocess_reference_image_for_clip(\n        image, get_local_torch_device())\n\n    image_inputs = self.image_processor(images=image_tensor,\n                                        return_tensors=\"pt\").to(\n                                            get_local_torch_device())\n    with set_forward_context(current_timestep=0, attn_metadata=None):\n        outputs = self.image_encoder(**image_inputs)\n        image_embeds = outputs.last_hidden_state\n    batch.image_embeds.append(image_embeds)\n\n    if batch.pil_image is None:\n        batch.image_embeds = [\n            torch.zeros_like(x) for x in batch.image_embeds\n        ]\n\n    return batch\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.StepvideoPromptEncodingStage","title":"fastvideo.pipelines.stages.StepvideoPromptEncodingStage","text":"<pre><code>StepvideoPromptEncodingStage(stepllm, clip)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for encoding prompts using the remote caption API.</p> <p>This stage applies the magic string transformations and calls the remote caption service asynchronously to get:   - primary prompt embeddings,   - an attention mask,   - and a clip embedding.</p> Source code in <code>fastvideo/pipelines/stages/stepvideo_encoding.py</code> <pre><code>def __init__(self, stepllm, clip) -&gt; None:\n    super().__init__()\n    # self.caption_client = caption_client  # This should have a call_caption(prompts: List[str]) method.\n    self.stepllm = stepllm\n    self.clip = clip\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.StepvideoPromptEncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify stepvideo encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/stepvideo_encoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify stepvideo encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"prompt\", batch.prompt, V.string_not_empty)\n    return result\n</code></pre> fastvideo.pipelines.stages.StepvideoPromptEncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify stepvideo encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/stepvideo_encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify stepvideo encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds,\n                     [V.is_tensor, V.with_dims(3)])\n    result.add_check(\"negative_prompt_embeds\", batch.negative_prompt_embeds,\n                     [V.is_tensor, V.with_dims(3)])\n    result.add_check(\"prompt_attention_mask\", batch.prompt_attention_mask,\n                     [V.is_tensor, V.with_dims(2)])\n    result.add_check(\"negative_attention_mask\",\n                     batch.negative_attention_mask,\n                     [V.is_tensor, V.with_dims(2)])\n    result.add_check(\"clip_embedding_pos\", batch.clip_embedding_pos,\n                     [V.is_tensor, V.with_dims(2)])\n    result.add_check(\"clip_embedding_neg\", batch.clip_embedding_neg,\n                     [V.is_tensor, V.with_dims(2)])\n    return result\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.TextEncodingStage","title":"fastvideo.pipelines.stages.TextEncodingStage","text":"<pre><code>TextEncodingStage(text_encoders, tokenizers)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for encoding text prompts into embeddings for diffusion models.</p> <p>This stage handles the encoding of text prompts into the embedding space expected by the diffusion model.</p> <p>Initialize the prompt encoding stage.</p> <p>Parameters:</p> Name Type Description Default <code>enable_logging</code> <p>Whether to enable logging for this stage.</p> required <code>is_secondary</code> <p>Whether this is a secondary text encoder.</p> required Source code in <code>fastvideo/pipelines/stages/text_encoding.py</code> <pre><code>def __init__(self, text_encoders, tokenizers) -&gt; None:\n    \"\"\"\n    Initialize the prompt encoding stage.\n\n    Args:\n        enable_logging: Whether to enable logging for this stage.\n        is_secondary: Whether this is a secondary text encoder.\n    \"\"\"\n    super().__init__()\n    self.tokenizers = tokenizers\n    self.text_encoders = text_encoders\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.TextEncodingStage.encode_text \u00b6 <pre><code>encode_text(\n    text: str | list[str],\n    fastvideo_args: FastVideoArgs,\n    encoder_index: int | list[int] | None = None,\n    return_attention_mask: bool = False,\n    return_type: str = \"list\",\n    device: device | str | None = None,\n    dtype: dtype | None = None,\n    max_length: int | None = None,\n    truncation: bool | None = None,\n    padding: bool | str | None = None,\n)\n</code></pre> <p>Encode plain text using selected text encoder(s) and return embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str | list[str]</code> <p>A single string or a list of strings to encode.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments providing pipeline config, including tokenizer and encoder settings, preprocess and postprocess functions.</p> required <code>encoder_index</code> <code>int | list[int] | None</code> <p>Encoder selector by index. Accepts an int or list of ints.</p> <code>None</code> <code>return_attention_mask</code> <code>bool</code> <p>If True, also return attention masks for each selected encoder.</p> <code>False</code> <code>return_type</code> <code>str</code> <p>\"list\" (default) returns a list aligned with selection; \"dict\" returns a dict keyed by encoder index as a string; \"stack\" stacks along a new first dimension (requires matching shapes).</p> <code>'list'</code> <code>device</code> <code>device | str | None</code> <p>Optional device override for inputs; defaults to local torch device.</p> <code>None</code> <code>dtype</code> <code>dtype | None</code> <p>Optional dtype to cast returned embeddings to.</p> <code>None</code> <code>max_length</code> <code>int | None</code> <p>Optional per-call tokenizer override.</p> <code>None</code> <code>truncation</code> <code>bool | None</code> <p>Optional per-call tokenizer override.</p> <code>None</code> <code>padding</code> <code>bool | str | None</code> <p>Optional per-call tokenizer override.</p> <code>None</code> <p>Returns:</p> Type Description <p>Depending on return_type and return_attention_mask:</p> <ul> <li>list: List[Tensor] or (List[Tensor], List[Tensor])</li> </ul> <ul> <li>dict: Dict[str, Tensor] or (Dict[str, Tensor], Dict[str, Tensor])</li> </ul> <ul> <li>stack: Tensor of shape [num_encoders, ...] or a tuple with stacked attention masks</li> </ul> Source code in <code>fastvideo/pipelines/stages/text_encoding.py</code> <pre><code>@torch.no_grad()\ndef encode_text(\n    self,\n    text: str | list[str],\n    fastvideo_args: FastVideoArgs,\n    encoder_index: int | list[int] | None = None,\n    return_attention_mask: bool = False,\n    return_type: str = \"list\",  # one of: \"list\", \"dict\", \"stack\"\n    device: torch.device | str | None = None,\n    dtype: torch.dtype | None = None,\n    max_length: int | None = None,\n    truncation: bool | None = None,\n    padding: bool | str | None = None,\n):\n    \"\"\"\n    Encode plain text using selected text encoder(s) and return embeddings.\n\n    Args:\n        text: A single string or a list of strings to encode.\n        fastvideo_args: The inference arguments providing pipeline config,\n            including tokenizer and encoder settings, preprocess and postprocess\n            functions.\n        encoder_index: Encoder selector by index. Accepts an int or list of ints.\n        return_attention_mask: If True, also return attention masks for each\n            selected encoder.\n        return_type: \"list\" (default) returns a list aligned with selection;\n            \"dict\" returns a dict keyed by encoder index as a string; \"stack\" stacks along a\n            new first dimension (requires matching shapes).\n        device: Optional device override for inputs; defaults to local torch device.\n        dtype: Optional dtype to cast returned embeddings to.\n        max_length: Optional per-call tokenizer override.\n        truncation: Optional per-call tokenizer override.\n        padding: Optional per-call tokenizer override.\n\n    Returns:\n        Depending on return_type and return_attention_mask:\n        - list: List[Tensor] or (List[Tensor], List[Tensor])\n        - dict: Dict[str, Tensor] or (Dict[str, Tensor], Dict[str, Tensor])\n        - stack: Tensor of shape [num_encoders, ...] or a tuple with stacked\n          attention masks\n    \"\"\"\n\n    assert len(self.tokenizers) == len(self.text_encoders)\n    assert len(self.text_encoders) == len(\n        fastvideo_args.pipeline_config.text_encoder_configs)\n\n    # Resolve selection into indices\n    encoder_cfgs = fastvideo_args.pipeline_config.text_encoder_configs\n    if encoder_index is None:\n        indices: list[int] = [0]\n    elif isinstance(encoder_index, int):\n        indices = [encoder_index]\n    else:\n        indices = list(encoder_index)\n    # validate range\n    num_encoders = len(self.text_encoders)\n    for idx in indices:\n        if idx &lt; 0 or idx &gt;= num_encoders:\n            raise IndexError(\n                f\"encoder index {idx} out of range [0, {num_encoders-1}]\")\n\n    # Validate indices are within range\n    num_encoders = len(self.text_encoders)\n\n    # Normalize input to list[str]\n    assert isinstance(text, str | list)\n    if isinstance(text, str):\n        texts: list[str] = [text]\n    else:\n        texts = text\n\n    embeds_list: list[torch.Tensor] = []\n    attn_masks_list: list[torch.Tensor] = []\n\n    preprocess_funcs = fastvideo_args.pipeline_config.preprocess_text_funcs\n    postprocess_funcs = fastvideo_args.pipeline_config.postprocess_text_funcs\n    encoder_cfgs = fastvideo_args.pipeline_config.text_encoder_configs\n\n    if return_type not in (\"list\", \"dict\", \"stack\"):\n        raise ValueError(\n            f\"Invalid return_type '{return_type}'. Expected one of: 'list', 'dict', 'stack'\"\n        )\n\n    target_device = device if device is not None else get_local_torch_device(\n    )\n\n    for i in indices:\n        tokenizer = self.tokenizers[i]\n        text_encoder = self.text_encoders[i]\n        encoder_config = encoder_cfgs[i]\n        preprocess_func = preprocess_funcs[i]\n        postprocess_func = postprocess_funcs[i]\n\n        processed_texts: list[str] = []\n        for prompt_str in texts:\n            processed_texts.append(preprocess_func(prompt_str))\n\n        tok_kwargs = dict(encoder_config.tokenizer_kwargs)\n        if max_length is not None:\n            tok_kwargs[\"max_length\"] = max_length\n        if truncation is not None:\n            tok_kwargs[\"truncation\"] = truncation\n        if padding is not None:\n            tok_kwargs[\"padding\"] = padding\n\n        text_inputs = tokenizer(processed_texts,\n                                **tok_kwargs).to(target_device)\n\n        input_ids = text_inputs[\"input_ids\"]\n        attention_mask = text_inputs[\"attention_mask\"]\n\n        with set_forward_context(current_timestep=0, attn_metadata=None):\n            outputs = text_encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                output_hidden_states=True,\n            )\n\n        prompt_embeds = postprocess_func(outputs)\n        if dtype is not None:\n            prompt_embeds = prompt_embeds.to(dtype=dtype)\n        embeds_list.append(prompt_embeds)\n        if return_attention_mask:\n            attn_masks_list.append(attention_mask)\n\n    # Shape results according to return_type\n    if return_type == \"list\":\n        if return_attention_mask:\n            return embeds_list, attn_masks_list\n        return embeds_list\n\n    if return_type == \"dict\":\n        key_strs = [str(i) for i in indices]\n        embeds_dict = {\n            k: v\n            for k, v in zip(key_strs, embeds_list, strict=False)\n        }\n        if return_attention_mask:\n            attn_dict = {\n                k: v\n                for k, v in zip(key_strs, attn_masks_list, strict=False)\n            }\n            return embeds_dict, attn_dict\n        return embeds_dict\n\n    # return_type == \"stack\"\n    # Validate shapes are compatible\n    base_shape = list(embeds_list[0].shape)\n    for t in embeds_list[1:]:\n        if list(t.shape) != base_shape:\n            raise ValueError(\n                f\"Cannot stack embeddings with differing shapes: {[list(t.shape) for t in embeds_list]}\"\n            )\n    stacked_embeds = torch.stack(embeds_list, dim=0)\n    if return_attention_mask:\n        base_mask_shape = list(attn_masks_list[0].shape)\n        for m in attn_masks_list[1:]:\n            if list(m.shape) != base_mask_shape:\n                raise ValueError(\n                    f\"Cannot stack attention masks with differing shapes: {[list(m.shape) for m in attn_masks_list]}\"\n                )\n        stacked_masks = torch.stack(attn_masks_list, dim=0)\n        return stacked_embeds, stacked_masks\n    return stacked_embeds\n</code></pre> fastvideo.pipelines.stages.TextEncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode the prompt into text encoder hidden states.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded prompt embeddings.</p> Source code in <code>fastvideo/pipelines/stages/text_encoding.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode the prompt into text encoder hidden states.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded prompt embeddings.\n    \"\"\"\n    assert len(self.tokenizers) == len(self.text_encoders)\n    assert len(self.text_encoders) == len(\n        fastvideo_args.pipeline_config.text_encoder_configs)\n\n    # Encode positive prompt with all available encoders\n    assert batch.prompt is not None\n    prompt_text: str | list[str] = batch.prompt\n    all_indices: list[int] = list(range(len(self.text_encoders)))\n    prompt_embeds_list, prompt_masks_list = self.encode_text(\n        prompt_text,\n        fastvideo_args,\n        encoder_index=all_indices,\n        return_attention_mask=True,\n    )\n    for pe in prompt_embeds_list:\n        batch.prompt_embeds.append(pe)\n    if batch.prompt_attention_mask is not None:\n        for am in prompt_masks_list:\n            batch.prompt_attention_mask.append(am)\n\n    # Encode negative prompt if CFG is enabled\n    if batch.do_classifier_free_guidance:\n        assert isinstance(batch.negative_prompt, str)\n        neg_embeds_list, neg_masks_list = self.encode_text(\n            batch.negative_prompt,\n            fastvideo_args,\n            encoder_index=all_indices,\n            return_attention_mask=True,\n        )\n        assert batch.negative_prompt_embeds is not None\n        for ne in neg_embeds_list:\n            batch.negative_prompt_embeds.append(ne)\n        if batch.negative_attention_mask is not None:\n            for nm in neg_masks_list:\n                batch.negative_attention_mask.append(nm)\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.TextEncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify text encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/text_encoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify text encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"prompt\", batch.prompt, V.string_or_list_strings)\n    result.add_check(\n        \"negative_prompt\", batch.negative_prompt, lambda x: not batch.\n        do_classifier_free_guidance or V.string_not_empty(x))\n    result.add_check(\"do_classifier_free_guidance\",\n                     batch.do_classifier_free_guidance, V.bool_value)\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds, V.is_list)\n    result.add_check(\"negative_prompt_embeds\", batch.negative_prompt_embeds,\n                     V.none_or_list)\n    return result\n</code></pre> fastvideo.pipelines.stages.TextEncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify text encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/text_encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify text encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds,\n                     V.list_of_tensors_min_dims(2))\n    result.add_check(\n        \"negative_prompt_embeds\", batch.negative_prompt_embeds,\n        lambda x: not batch.do_classifier_free_guidance or V.\n        list_of_tensors_with_min_dims(x, 2))\n    return result\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.TimestepPreparationStage","title":"fastvideo.pipelines.stages.TimestepPreparationStage","text":"<pre><code>TimestepPreparationStage(scheduler)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for preparing timesteps for the diffusion process.</p> <p>This stage handles the preparation of the timestep sequence that will be used during the diffusion process.</p> Source code in <code>fastvideo/pipelines/stages/timestep_preparation.py</code> <pre><code>def __init__(self, scheduler) -&gt; None:\n    self.scheduler = scheduler\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.TimestepPreparationStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Prepare timesteps for the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with prepared timesteps.</p> Source code in <code>fastvideo/pipelines/stages/timestep_preparation.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Prepare timesteps for the diffusion process.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with prepared timesteps.\n    \"\"\"\n    scheduler = self.scheduler\n    device = get_local_torch_device()\n    num_inference_steps = batch.num_inference_steps\n    timesteps = batch.timesteps\n    sigmas = batch.sigmas\n    n_tokens = batch.n_tokens\n\n    # Prepare extra kwargs for set_timesteps\n    extra_set_timesteps_kwargs = {}\n    if n_tokens is not None and \"n_tokens\" in inspect.signature(\n            scheduler.set_timesteps).parameters:\n        extra_set_timesteps_kwargs[\"n_tokens\"] = n_tokens\n\n    # Handle custom timesteps or sigmas\n    if timesteps is not None and sigmas is not None:\n        raise ValueError(\n            \"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\"\n        )\n\n    if timesteps is not None:\n        accepts_timesteps = \"timesteps\" in inspect.signature(\n            scheduler.set_timesteps).parameters\n        if not accepts_timesteps:\n            raise ValueError(\n                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n            )\n        scheduler.set_timesteps(timesteps=timesteps,\n                                device=device,\n                                **extra_set_timesteps_kwargs)\n        timesteps = scheduler.timesteps\n    elif sigmas is not None:\n        accept_sigmas = \"sigmas\" in inspect.signature(\n            scheduler.set_timesteps).parameters\n        if not accept_sigmas:\n            raise ValueError(\n                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n                f\" sigmas schedules. Please check whether you are using the correct scheduler.\"\n            )\n        scheduler.set_timesteps(sigmas=sigmas,\n                                device=device,\n                                **extra_set_timesteps_kwargs)\n        timesteps = scheduler.timesteps\n    else:\n        scheduler.set_timesteps(num_inference_steps,\n                                device=device,\n                                **extra_set_timesteps_kwargs)\n        timesteps = scheduler.timesteps\n\n    # Update batch with prepared timesteps\n    batch.timesteps = timesteps\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.TimestepPreparationStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify timestep preparation stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/timestep_preparation.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify timestep preparation stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"num_inference_steps\", batch.num_inference_steps,\n                     V.positive_int)\n    result.add_check(\"timesteps\", batch.timesteps, V.none_or_tensor)\n    result.add_check(\"sigmas\", batch.sigmas, V.none_or_list)\n    result.add_check(\"n_tokens\", batch.n_tokens, V.none_or_positive_int)\n    return result\n</code></pre> fastvideo.pipelines.stages.TimestepPreparationStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify timestep preparation stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/timestep_preparation.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify timestep preparation stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"timesteps\", batch.timesteps,\n                     [V.is_tensor, V.with_dims(1)])\n    return result\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.VideoVAEEncodingStage","title":"fastvideo.pipelines.stages.VideoVAEEncodingStage","text":"<pre><code>VideoVAEEncodingStage(vae: ParallelTiledVAE)\n</code></pre> <p>               Bases: <code>ImageVAEEncodingStage</code></p> <p>Stage for encoding video pixel representations into latent space.</p> <p>This stage handles the encoding of video pixel representations for video-to-video generation and control. Inherits from ImageVAEEncodingStage to reuse common functionality.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def __init__(self, vae: ParallelTiledVAE) -&gt; None:\n    self.vae: ParallelTiledVAE = vae\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.VideoVAEEncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode video pixel representations into latent space.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded outputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode video pixel representations into latent space.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded outputs.\n    \"\"\"\n    assert batch.video_latent is not None, \"Video latent input is required for VideoVAEEncodingStage\"\n\n    if fastvideo_args.mode == ExecutionMode.INFERENCE:\n        assert batch.height is not None and isinstance(batch.height, int)\n        assert batch.width is not None and isinstance(batch.width, int)\n        assert batch.num_frames is not None and isinstance(\n            batch.num_frames, int)\n        height = batch.height\n        width = batch.width\n        num_frames = batch.num_frames\n    elif fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        assert batch.height is not None and isinstance(batch.height, list)\n        assert batch.width is not None and isinstance(batch.width, list)\n        assert batch.num_frames is not None and isinstance(\n            batch.num_frames, list)\n        num_frames = batch.num_frames[0]\n        height = batch.height[0]\n        width = batch.width[0]\n\n    self.vae = self.vae.to(get_local_torch_device())\n\n    # Prepare video tensor from control video\n    video_condition = self._prepare_control_video_tensor(\n        batch.video_latent, num_frames, height,\n        width).to(get_local_torch_device(), dtype=torch.float32)\n\n    # Setup VAE precision\n    vae_dtype = PRECISION_TO_TYPE[\n        fastvideo_args.pipeline_config.vae_precision]\n    vae_autocast_enabled = (\n        vae_dtype != torch.float32) and not fastvideo_args.disable_autocast\n\n    # Encode control video\n    with torch.autocast(device_type=\"cuda\",\n                        dtype=vae_dtype,\n                        enabled=vae_autocast_enabled):\n        if fastvideo_args.pipeline_config.vae_tiling:\n            self.vae.enable_tiling()\n        if not vae_autocast_enabled:\n            video_condition = video_condition.to(vae_dtype)\n        encoder_output = self.vae.encode(video_condition)\n\n    generator = batch.generator\n    if generator is None:\n        raise ValueError(\"Generator must be provided\")\n    latent_condition = self.retrieve_latents(encoder_output, generator)\n\n    if (hasattr(self.vae, \"shift_factor\")\n            and self.vae.shift_factor is not None):\n        if isinstance(self.vae.shift_factor, torch.Tensor):\n            latent_condition -= self.vae.shift_factor.to(\n                latent_condition.device, latent_condition.dtype)\n        else:\n            latent_condition -= self.vae.shift_factor\n\n    if isinstance(self.vae.scaling_factor, torch.Tensor):\n        latent_condition = latent_condition * self.vae.scaling_factor.to(\n            latent_condition.device, latent_condition.dtype)\n    else:\n        latent_condition = latent_condition * self.vae.scaling_factor\n\n    batch.video_latent = latent_condition\n\n    # Offload models if needed\n    if hasattr(self, 'maybe_free_model_hooks'):\n        self.maybe_free_model_hooks()\n\n    self.vae.to(\"cpu\")\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.VideoVAEEncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify video encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify video encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"video_latent\", batch.video_latent, V.not_none)\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    if fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        result.add_check(\"height\", batch.height, V.list_not_empty)\n        result.add_check(\"width\", batch.width, V.list_not_empty)\n        result.add_check(\"num_frames\", batch.num_frames, V.list_not_empty)\n    else:\n        result.add_check(\"height\", batch.height, V.positive_int)\n        result.add_check(\"width\", batch.width, V.positive_int)\n        result.add_check(\"num_frames\", batch.num_frames, V.positive_int)\n    return result\n</code></pre> fastvideo.pipelines.stages.VideoVAEEncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify video encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify video encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"video_latent\", batch.video_latent,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.stages-modules","title":"Modules","text":""},{"location":"api/fastvideo/#fastvideo.pipelines.stages.base","title":"fastvideo.pipelines.stages.base","text":"<p>Base classes for pipeline stages.</p> <p>This module defines the abstract base classes for pipeline stages that can be composed to create complete diffusion pipelines.</p> Classes\u00b6 fastvideo.pipelines.stages.base.PipelineStage \u00b6 <p>               Bases: <code>ABC</code></p> <p>Abstract base class for all pipeline stages.</p> <p>A pipeline stage represents a discrete step in the diffusion process that can be composed with other stages to create a complete pipeline. Each stage is responsible for a specific part of the process, such as prompt encoding, latent preparation, etc.</p> Attributes\u00b6 fastvideo.pipelines.stages.base.PipelineStage.device <code>property</code> \u00b6 <pre><code>device: device\n</code></pre> <p>Get the device for this stage.</p> Functions\u00b6 fastvideo.pipelines.stages.base.PipelineStage.__call__ \u00b6 <pre><code>__call__(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Execute the stage's processing on the batch with optional verification and logging. Should not be overridden by subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The updated batch information after this stage's processing.</p> Source code in <code>fastvideo/pipelines/stages/base.py</code> <pre><code>def __call__(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Execute the stage's processing on the batch with optional verification and logging.\n    Should not be overridden by subclasses.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The updated batch information after this stage's processing.\n    \"\"\"\n    stage_name = self.__class__.__name__\n\n    # Check if verification is enabled (simple approach for prototype)\n    enable_verification = getattr(fastvideo_args,\n                                  'enable_stage_verification', False)\n\n    if enable_verification:\n        # Pre-execution input verification\n        try:\n            input_result = self.verify_input(batch, fastvideo_args)\n            self._run_verification(input_result, stage_name, \"input\")\n        except Exception as e:\n            logger.error(\"Input verification failed for %s: %s\", stage_name,\n                         str(e))\n            raise\n\n    # Execute the actual stage logic\n    if envs.FASTVIDEO_STAGE_LOGGING:\n        logger.info(\"[%s] Starting execution\", stage_name)\n        start_time = time.perf_counter()\n\n        try:\n            result = self.forward(batch, fastvideo_args)\n            execution_time = time.perf_counter() - start_time\n            logger.info(\"[%s] Execution completed in %s ms\", stage_name,\n                        execution_time * 1000)\n            batch.logging_info.add_stage_execution_time(\n                stage_name, execution_time)\n        except Exception as e:\n            execution_time = time.perf_counter() - start_time\n            logger.error(\"[%s] Error during execution after %s ms: %s\",\n                         stage_name, execution_time * 1000, e)\n            logger.error(\"[%s] Traceback: %s\", stage_name,\n                         traceback.format_exc())\n            raise\n    else:\n        # Direct execution (current behavior)\n        result = self.forward(batch, fastvideo_args)\n\n    if enable_verification:\n        # Post-execution output verification\n        try:\n            output_result = self.verify_output(result, fastvideo_args)\n            self._run_verification(output_result, stage_name, \"output\")\n        except Exception as e:\n            logger.error(\"Output verification failed for %s: %s\",\n                         stage_name, str(e))\n            raise\n\n    return result\n</code></pre> fastvideo.pipelines.stages.base.PipelineStage.forward <code>abstractmethod</code> \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Forward pass of the stage's processing.</p> <p>This method should be implemented by subclasses to provide the forward processing logic for the stage.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The updated batch information after this stage's processing.</p> Source code in <code>fastvideo/pipelines/stages/base.py</code> <pre><code>@abstractmethod\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Forward pass of the stage's processing.\n\n    This method should be implemented by subclasses to provide the forward\n    processing logic for the stage.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The updated batch information after this stage's processing.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.pipelines.stages.base.PipelineStage.set_logging \u00b6 <pre><code>set_logging(enable: bool)\n</code></pre> <p>Enable or disable logging for this stage.</p> <p>Parameters:</p> Name Type Description Default <code>enable</code> <code>bool</code> <p>Whether to enable logging.</p> required Source code in <code>fastvideo/pipelines/stages/base.py</code> <pre><code>def set_logging(self, enable: bool):\n    \"\"\"\n    Enable or disable logging for this stage.\n\n    Args:\n        enable: Whether to enable logging.\n    \"\"\"\n    self._enable_logging = enable\n</code></pre> fastvideo.pipelines.stages.base.PipelineStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify the input for the stage.</p> Example <p>from fastvideo.pipelines.stages.validators import V, VerificationResult</p> <p>def verify_input(self, batch, fastvideo_args):     result = VerificationResult()     result.add_check(\"height\", batch.height, V.positive_int_divisible(8))     result.add_check(\"width\", batch.width, V.positive_int_divisible(8))     result.add_check(\"image_latent\", batch.image_latent, V.is_tensor)     return result</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>VerificationResult</code> <p>A VerificationResult containing the verification status.</p> Source code in <code>fastvideo/pipelines/stages/base.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"\n    Verify the input for the stage.\n\n    Example:\n        from fastvideo.pipelines.stages.validators import V, VerificationResult\n\n        def verify_input(self, batch, fastvideo_args):\n            result = VerificationResult()\n            result.add_check(\"height\", batch.height, V.positive_int_divisible(8))\n            result.add_check(\"width\", batch.width, V.positive_int_divisible(8))\n            result.add_check(\"image_latent\", batch.image_latent, V.is_tensor)\n            return result\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        A VerificationResult containing the verification status.\n\n    \"\"\"\n    # Default implementation - no verification\n    return VerificationResult()\n</code></pre> fastvideo.pipelines.stages.base.PipelineStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify the output for the stage.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>VerificationResult</code> <p>A VerificationResult containing the verification status.</p> Source code in <code>fastvideo/pipelines/stages/base.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"\n    Verify the output for the stage.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        A VerificationResult containing the verification status.\n    \"\"\"\n    # Default implementation - no verification\n    return VerificationResult()\n</code></pre> fastvideo.pipelines.stages.base.StageVerificationError \u00b6 <p>               Bases: <code>Exception</code></p> <p>Exception raised when stage verification fails.</p> Functions\u00b6"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.causal_denoising","title":"fastvideo.pipelines.stages.causal_denoising","text":"Classes\u00b6 fastvideo.pipelines.stages.causal_denoising.CausalDMDDenosingStage \u00b6 <pre><code>CausalDMDDenosingStage(\n    transformer, scheduler, transformer_2=None\n)\n</code></pre> <p>               Bases: <code>DenoisingStage</code></p> <p>Denoising stage for causal diffusion.</p> Source code in <code>fastvideo/pipelines/stages/causal_denoising.py</code> <pre><code>def __init__(self, transformer, scheduler, transformer_2=None) -&gt; None:\n    super().__init__(transformer, scheduler, transformer_2)\n    # KV and cross-attention cache state (initialized on first forward)\n    self.transformer = transformer\n    self.transformer_2 = transformer_2\n    self.kv_cache1: list | None = None\n    self.crossattn_cache: list | None = None\n    # Model-dependent constants (aligned with causal_inference.py assumptions)\n    self.num_transformer_blocks = len(self.transformer.blocks)\n    self.num_frames_per_block = self.transformer.config.arch_config.num_frames_per_block\n    self.sliding_window_num_frames = self.transformer.config.arch_config.sliding_window_num_frames\n\n    try:\n        self.local_attn_size = getattr(self.transformer.model,\n                                       \"local_attn_size\",\n                                       -1)  # type: ignore\n    except Exception:\n        self.local_attn_size = -1\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.causal_denoising.CausalDMDDenosingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify denoising stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/causal_denoising.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify denoising stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds, V.list_not_empty)\n    result.add_check(\"image_embeds\", batch.image_embeds, V.is_list)\n    result.add_check(\"image_latent\", batch.image_latent,\n                     V.none_or_tensor_with_dims(5))\n    result.add_check(\"num_inference_steps\", batch.num_inference_steps,\n                     V.positive_int)\n    result.add_check(\"guidance_scale\", batch.guidance_scale,\n                     V.positive_float)\n    result.add_check(\"eta\", batch.eta, V.non_negative_float)\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    result.add_check(\"do_classifier_free_guidance\",\n                     batch.do_classifier_free_guidance, V.bool_value)\n    result.add_check(\n        \"negative_prompt_embeds\", batch.negative_prompt_embeds, lambda x:\n        not batch.do_classifier_free_guidance or V.list_not_empty(x))\n    return result\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.conditioning","title":"fastvideo.pipelines.stages.conditioning","text":"<p>Conditioning stage for diffusion pipelines.</p> Classes\u00b6 fastvideo.pipelines.stages.conditioning.ConditioningStage \u00b6 <p>               Bases: <code>PipelineStage</code></p> <p>Stage for applying conditioning to the diffusion process.</p> <p>This stage handles the application of conditioning, such as classifier-free guidance, to the diffusion process.</p> Functions\u00b6 fastvideo.pipelines.stages.conditioning.ConditioningStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Apply conditioning to the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with applied conditioning.</p> Source code in <code>fastvideo/pipelines/stages/conditioning.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Apply conditioning to the diffusion process.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with applied conditioning.\n    \"\"\"\n    # TODO!!\n    if not batch.do_classifier_free_guidance:\n        return batch\n    else:\n        return batch\n\n    logger.info(\"batch.negative_prompt_embeds: %s\",\n                batch.negative_prompt_embeds)\n    logger.info(\"do_classifier_free_guidance: %s\",\n                batch.do_classifier_free_guidance)\n    logger.info(\"cfg_scale: %s\", batch.guidance_scale)\n\n    # Ensure negative prompt embeddings are available\n    assert batch.negative_prompt_embeds is not None, (\n        \"Negative prompt embeddings are required for classifier-free guidance\"\n    )\n\n    # Concatenate primary embeddings and masks\n    batch.prompt_embeds = torch.cat(\n        [batch.negative_prompt_embeds, batch.prompt_embeds])\n    if batch.attention_mask is not None:\n        batch.attention_mask = torch.cat(\n            [batch.negative_attention_mask, batch.attention_mask])\n\n    # Concatenate secondary embeddings and masks if present\n    if batch.prompt_embeds_2 is not None:\n        batch.prompt_embeds_2 = torch.cat(\n            [batch.negative_prompt_embeds_2, batch.prompt_embeds_2])\n    if batch.attention_mask_2 is not None:\n        batch.attention_mask_2 = torch.cat(\n            [batch.negative_attention_mask_2, batch.attention_mask_2])\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.conditioning.ConditioningStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify conditioning stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/conditioning.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify conditioning stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"do_classifier_free_guidance\",\n                     batch.do_classifier_free_guidance, V.bool_value)\n    result.add_check(\"guidance_scale\", batch.guidance_scale,\n                     V.positive_float)\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds, V.list_not_empty)\n    result.add_check(\n        \"negative_prompt_embeds\", batch.negative_prompt_embeds, lambda x:\n        not batch.do_classifier_free_guidance or V.list_not_empty(x))\n    return result\n</code></pre> fastvideo.pipelines.stages.conditioning.ConditioningStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify conditioning stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/conditioning.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify conditioning stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds, V.list_not_empty)\n    return result\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.decoding","title":"fastvideo.pipelines.stages.decoding","text":"<p>Decoding stage for diffusion pipelines.</p> Classes\u00b6 fastvideo.pipelines.stages.decoding.DecodingStage \u00b6 <pre><code>DecodingStage(vae, pipeline=None)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for decoding latent representations into pixel space.</p> <p>This stage handles the decoding of latent representations into the final output format (e.g., pixel values).</p> Source code in <code>fastvideo/pipelines/stages/decoding.py</code> <pre><code>def __init__(self, vae, pipeline=None) -&gt; None:\n    self.vae: ParallelTiledVAE = vae\n    self.pipeline = weakref.ref(pipeline) if pipeline else None\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.decoding.DecodingStage.decode \u00b6 <pre><code>decode(\n    latents: Tensor, fastvideo_args: FastVideoArgs\n) -&gt; torch.Tensor\n</code></pre> <p>Decode latent representations into pixel space using VAE.</p> <p>Parameters:</p> Name Type Description Default <code>latents</code> <code>Tensor</code> <p>Input latent tensor with shape (batch, channels, frames, height_latents, width_latents)</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>Configuration containing: - disable_autocast: Whether to disable automatic mixed precision (default: False) - pipeline_config.vae_precision: VAE computation precision (\"fp32\", \"fp16\", \"bf16\") - pipeline_config.vae_tiling: Whether to enable VAE tiling for memory efficiency</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Decoded video tensor with shape (batch, channels, frames, height, width), </p> <code>Tensor</code> <p>normalized to [0, 1] range and moved to CPU as float32</p> Source code in <code>fastvideo/pipelines/stages/decoding.py</code> <pre><code>@torch.no_grad()\ndef decode(self, latents: torch.Tensor,\n           fastvideo_args: FastVideoArgs) -&gt; torch.Tensor:\n    \"\"\"\n    Decode latent representations into pixel space using VAE.\n\n    Args:\n        latents: Input latent tensor with shape (batch, channels, frames, height_latents, width_latents)\n        fastvideo_args: Configuration containing:\n            - disable_autocast: Whether to disable automatic mixed precision (default: False)\n            - pipeline_config.vae_precision: VAE computation precision (\"fp32\", \"fp16\", \"bf16\")\n            - pipeline_config.vae_tiling: Whether to enable VAE tiling for memory efficiency\n\n    Returns:\n        Decoded video tensor with shape (batch, channels, frames, height, width), \n        normalized to [0, 1] range and moved to CPU as float32\n    \"\"\"\n    self.vae = self.vae.to(get_local_torch_device())\n    latents = latents.to(get_local_torch_device())\n\n    # Setup VAE precision\n    vae_dtype = PRECISION_TO_TYPE[\n        fastvideo_args.pipeline_config.vae_precision]\n    vae_autocast_enabled = (\n        vae_dtype != torch.float32) and not fastvideo_args.disable_autocast\n\n    if isinstance(self.vae.scaling_factor, torch.Tensor):\n        latents = latents / self.vae.scaling_factor.to(\n            latents.device, latents.dtype)\n    else:\n        latents = latents / self.vae.scaling_factor\n\n    # Apply shifting if needed\n    if (hasattr(self.vae, \"shift_factor\")\n            and self.vae.shift_factor is not None):\n        if isinstance(self.vae.shift_factor, torch.Tensor):\n            latents += self.vae.shift_factor.to(latents.device,\n                                                latents.dtype)\n        else:\n            latents += self.vae.shift_factor\n\n    # Decode latents\n    with torch.autocast(device_type=\"cuda\",\n                        dtype=vae_dtype,\n                        enabled=vae_autocast_enabled):\n        if fastvideo_args.pipeline_config.vae_tiling:\n            self.vae.enable_tiling()\n        # if fastvideo_args.vae_sp:\n        #     self.vae.enable_parallel()\n        if not vae_autocast_enabled:\n            latents = latents.to(vae_dtype)\n        image = self.vae.decode(latents)\n\n    # Normalize image to [0, 1] range\n    image = (image / 2 + 0.5).clamp(0, 1)\n    return image\n</code></pre> fastvideo.pipelines.stages.decoding.DecodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Decode latent representations into pixel space.</p> <p>This method processes the batch through the VAE decoder, converting latent representations to pixel-space video/images. It also optionally decodes trajectory latents for visualization purposes.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch containing: - latents: Tensor to decode (batch, channels, frames, height_latents, width_latents) - return_trajectory_decoded (optional): Flag to decode trajectory latents - trajectory_latents (optional): Latents at different timesteps - trajectory_timesteps (optional): Corresponding timesteps</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>Configuration containing: - output_type: \"latent\" to skip decoding, otherwise decode to pixels - vae_cpu_offload: Whether to offload VAE to CPU after decoding - model_loaded: Track VAE loading state - model_paths: Path to VAE model if loading needed</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>Modified batch with: - output: Decoded frames (batch, channels, frames, height, width) as CPU float32 - trajectory_decoded (if requested): List of decoded frames per timestep</p> Source code in <code>fastvideo/pipelines/stages/decoding.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Decode latent representations into pixel space.\n\n    This method processes the batch through the VAE decoder, converting latent\n    representations to pixel-space video/images. It also optionally decodes\n    trajectory latents for visualization purposes.\n\n    Args:\n        batch: The current batch containing:\n            - latents: Tensor to decode (batch, channels, frames, height_latents, width_latents)\n            - return_trajectory_decoded (optional): Flag to decode trajectory latents\n            - trajectory_latents (optional): Latents at different timesteps\n            - trajectory_timesteps (optional): Corresponding timesteps\n        fastvideo_args: Configuration containing:\n            - output_type: \"latent\" to skip decoding, otherwise decode to pixels\n            - vae_cpu_offload: Whether to offload VAE to CPU after decoding\n            - model_loaded: Track VAE loading state\n            - model_paths: Path to VAE model if loading needed\n\n    Returns:\n        Modified batch with:\n            - output: Decoded frames (batch, channels, frames, height, width) as CPU float32\n            - trajectory_decoded (if requested): List of decoded frames per timestep\n    \"\"\"\n    # load vae if not already loaded (used for memory constrained devices)\n    pipeline = self.pipeline() if self.pipeline else None\n    if not fastvideo_args.model_loaded[\"vae\"]:\n        loader = VAELoader()\n        self.vae = loader.load(fastvideo_args.model_paths[\"vae\"],\n                               fastvideo_args)\n        if pipeline:\n            pipeline.add_module(\"vae\", self.vae)\n        fastvideo_args.model_loaded[\"vae\"] = True\n\n    if fastvideo_args.output_type == \"latent\":\n        frames = batch.latents\n    else:\n        frames = self.decode(batch.latents, fastvideo_args)\n\n    # decode trajectory latents if needed\n    if batch.return_trajectory_decoded:\n        batch.trajectory_decoded = []\n        assert batch.trajectory_latents is not None, \"batch should have trajectory latents\"\n        for idx in range(batch.trajectory_latents.shape[1]):\n            # batch.trajectory_latents is [batch_size, timesteps, channels, frames, height, width]\n            cur_latent = batch.trajectory_latents[:, idx, :, :, :, :]\n            cur_timestep = batch.trajectory_timesteps[idx]\n            logger.info(\"decoding trajectory latent for timestep: %s\",\n                        cur_timestep)\n            decoded_frames = self.decode(cur_latent, fastvideo_args)\n            batch.trajectory_decoded.append(decoded_frames.cpu().float())\n\n    # Convert to CPU float32 for compatibility\n    frames = frames.cpu().float()\n\n    # Update batch with decoded image\n    batch.output = frames\n\n    # Offload models if needed\n    if hasattr(self, 'maybe_free_model_hooks'):\n        self.maybe_free_model_hooks()\n\n    if fastvideo_args.vae_cpu_offload:\n        self.vae.to(\"cpu\")\n\n    if torch.backends.mps.is_available():\n        del self.vae\n        if pipeline is not None and \"vae\" in pipeline.modules:\n            del pipeline.modules[\"vae\"]\n        fastvideo_args.model_loaded[\"vae\"] = False\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.decoding.DecodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify decoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/decoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify decoding stage inputs.\"\"\"\n    result = VerificationResult()\n    # Denoised latents for VAE decoding: [batch_size, channels, frames, height_latents, width_latents]\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre> fastvideo.pipelines.stages.decoding.DecodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify decoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/decoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify decoding stage outputs.\"\"\"\n    result = VerificationResult()\n    # Decoded video/images: [batch_size, channels, frames, height, width]\n    result.add_check(\"output\", batch.output, [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.denoising","title":"fastvideo.pipelines.stages.denoising","text":"<p>Denoising stage for diffusion pipelines.</p> Classes\u00b6 fastvideo.pipelines.stages.denoising.DenoisingStage \u00b6 <pre><code>DenoisingStage(\n    transformer,\n    scheduler,\n    pipeline=None,\n    transformer_2=None,\n    vae=None,\n)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for running the denoising loop in diffusion pipelines.</p> <p>This stage handles the iterative denoising process that transforms the initial noise into the final output.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def __init__(self,\n             transformer,\n             scheduler,\n             pipeline=None,\n             transformer_2=None,\n             vae=None) -&gt; None:\n    super().__init__()\n    self.transformer = transformer\n    self.transformer_2 = transformer_2\n    self.scheduler = scheduler\n    self.vae = vae\n    self.pipeline = weakref.ref(pipeline) if pipeline else None\n    attn_head_size = self.transformer.hidden_size // self.transformer.num_attention_heads\n    self.attn_backend = get_attn_backend(\n        head_size=attn_head_size,\n        dtype=torch.float16,  # TODO(will): hack\n        supported_attention_backends=(\n            AttentionBackendEnum.SLIDING_TILE_ATTN,\n            AttentionBackendEnum.VIDEO_SPARSE_ATTN,\n            AttentionBackendEnum.VMOBA_ATTN,\n            AttentionBackendEnum.FLASH_ATTN,\n            AttentionBackendEnum.TORCH_SDPA,\n            AttentionBackendEnum.SAGE_ATTN_THREE)  # hack\n    )\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.denoising.DenoisingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Run the denoising loop.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with denoised latents.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Run the denoising loop.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with denoised latents.\n    \"\"\"\n    pipeline = self.pipeline() if self.pipeline else None\n    if not fastvideo_args.model_loaded[\"transformer\"]:\n        loader = TransformerLoader()\n        self.transformer = loader.load(\n            fastvideo_args.model_paths[\"transformer\"], fastvideo_args)\n        if pipeline:\n            pipeline.add_module(\"transformer\", self.transformer)\n        fastvideo_args.model_loaded[\"transformer\"] = True\n\n    # Prepare extra step kwargs for scheduler\n    extra_step_kwargs = self.prepare_extra_func_kwargs(\n        self.scheduler.step,\n        {\n            \"generator\": batch.generator,\n            \"eta\": batch.eta\n        },\n    )\n\n    # Setup precision and autocast settings\n    # TODO(will): make the precision configurable for inference\n    # target_dtype = PRECISION_TO_TYPE[fastvideo_args.precision]\n    target_dtype = torch.bfloat16\n    autocast_enabled = (target_dtype != torch.float32\n                        ) and not fastvideo_args.disable_autocast\n\n    # Handle sequence parallelism if enabled\n    sp_world_size, rank_in_sp_group = get_sp_world_size(\n    ), get_sp_parallel_rank()\n    sp_group = sp_world_size &gt; 1\n    if sp_group:\n        latents = rearrange(batch.latents,\n                            \"b c (n t) h w -&gt; b c n t h w\",\n                            n=sp_world_size).contiguous()\n        latents = latents[:, :, rank_in_sp_group, :, :, :]\n        batch.latents = latents\n        if batch.image_latent is not None:\n            image_latent = rearrange(batch.image_latent,\n                                     \"b c (n t) h w -&gt; b c n t h w\",\n                                     n=sp_world_size).contiguous()\n            image_latent = image_latent[:, :, rank_in_sp_group, :, :, :]\n            batch.image_latent = image_latent\n    # Get timesteps and calculate warmup steps\n    timesteps = batch.timesteps\n    # TODO(will): remove this once we add input/output validation for stages\n    if timesteps is None:\n        raise ValueError(\"Timesteps must be provided\")\n    num_inference_steps = batch.num_inference_steps\n    num_warmup_steps = len(\n        timesteps) - num_inference_steps * self.scheduler.order\n\n    # Prepare image latents and embeddings for I2V generation\n    image_embeds = batch.image_embeds\n    if len(image_embeds) &gt; 0:\n        assert not torch.isnan(\n            image_embeds[0]).any(), \"image_embeds contains nan\"\n        image_embeds = [\n            image_embed.to(target_dtype) for image_embed in image_embeds\n        ]\n\n    image_kwargs = self.prepare_extra_func_kwargs(\n        self.transformer.forward,\n        {\n            \"encoder_hidden_states_image\": image_embeds,\n            \"mask_strategy\": dict_to_3d_list(\n                None, t_max=50, l_max=60, h_max=24)\n        },\n    )\n\n    pos_cond_kwargs = self.prepare_extra_func_kwargs(\n        self.transformer.forward,\n        {\n            \"encoder_hidden_states_2\": batch.clip_embedding_pos,\n            \"encoder_attention_mask\": batch.prompt_attention_mask,\n        },\n    )\n\n    neg_cond_kwargs = self.prepare_extra_func_kwargs(\n        self.transformer.forward,\n        {\n            \"encoder_hidden_states_2\": batch.clip_embedding_neg,\n            \"encoder_attention_mask\": batch.negative_attention_mask,\n        },\n    )\n\n    # Prepare STA parameters\n    if st_attn_available and self.attn_backend == SlidingTileAttentionBackend:\n        self.prepare_sta_param(batch, fastvideo_args)\n\n    # Get latents and embeddings\n    latents = batch.latents\n    prompt_embeds = batch.prompt_embeds\n    assert not torch.isnan(\n        prompt_embeds[0]).any(), \"prompt_embeds contains nan\"\n    if batch.do_classifier_free_guidance:\n        neg_prompt_embeds = batch.negative_prompt_embeds\n        assert neg_prompt_embeds is not None\n        assert not torch.isnan(\n            neg_prompt_embeds[0]).any(), \"neg_prompt_embeds contains nan\"\n\n    # (Wan2.2) Calculate timestep to switch from high noise expert to low noise expert\n    boundary_ratio = fastvideo_args.pipeline_config.dit_config.boundary_ratio\n    if batch.boundary_ratio is not None:\n        logger.info(\"Overriding boundary ratio from %s to %s\",\n                    boundary_ratio, batch.boundary_ratio)\n        boundary_ratio = batch.boundary_ratio\n\n    if boundary_ratio is not None:\n        boundary_timestep = boundary_ratio * self.scheduler.num_train_timesteps\n    else:\n        boundary_timestep = None\n    latent_model_input = latents.to(target_dtype)\n    assert latent_model_input.shape[0] == 1, \"only support batch size 1\"\n\n    if fastvideo_args.pipeline_config.ti2v_task and batch.pil_image is not None:\n        # TI2V directly replaces the first frame of the latent with\n        # the image latent instead of appending along the channel dim\n        assert batch.image_latent is None, \"TI2V task should not have image latents\"\n        assert self.vae is not None, \"VAE is not provided for TI2V task\"\n        z = self.vae.encode(batch.pil_image).mean.float()\n        if (hasattr(self.vae, \"shift_factor\")\n                and self.vae.shift_factor is not None):\n            if isinstance(self.vae.shift_factor, torch.Tensor):\n                z -= self.vae.shift_factor.to(z.device, z.dtype)\n            else:\n                z -= self.vae.shift_factor\n\n        if isinstance(self.vae.scaling_factor, torch.Tensor):\n            z = z * self.vae.scaling_factor.to(z.device, z.dtype)\n        else:\n            z = z * self.vae.scaling_factor\n\n        latent_model_input = latent_model_input.squeeze(0)\n        _, mask2 = masks_like([latent_model_input], zero=True)\n\n        latent_model_input = (1. -\n                              mask2[0]) * z + mask2[0] * latent_model_input\n        # latent_model_input = latent_model_input.unsqueeze(0)\n        latent_model_input = latent_model_input.to(get_local_torch_device())\n        latents = latent_model_input\n        F = batch.num_frames\n        temporal_scale = fastvideo_args.pipeline_config.vae_config.arch_config.scale_factor_temporal\n        spatial_scale = fastvideo_args.pipeline_config.vae_config.arch_config.scale_factor_spatial\n        patch_size = fastvideo_args.pipeline_config.dit_config.arch_config.patch_size\n        seq_len = ((F - 1) // temporal_scale +\n                   1) * (batch.height // spatial_scale) * (\n                       batch.width // spatial_scale) // (patch_size[1] *\n                                                         patch_size[2])\n        seq_len = int(math.ceil(seq_len / sp_world_size)) * sp_world_size\n\n    # Initialize lists for ODE trajectory\n    trajectory_timesteps: list[torch.Tensor] = []\n    trajectory_latents: list[torch.Tensor] = []\n\n    # Run denoising loop\n    with self.progress_bar(total=num_inference_steps) as progress_bar:\n        for i, t in enumerate(timesteps):\n            # Skip if interrupted\n            if hasattr(self, 'interrupt') and self.interrupt:\n                continue\n\n            if boundary_timestep is None or t &gt;= boundary_timestep:\n                if (fastvideo_args.dit_cpu_offload\n                        and self.transformer_2 is not None and next(\n                            self.transformer_2.parameters()).device.type\n                        == 'cuda'):\n                    self.transformer_2.to('cpu')\n                current_model = self.transformer\n                current_guidance_scale = batch.guidance_scale\n            else:\n                # low-noise stage in wan2.2\n                if fastvideo_args.dit_cpu_offload and next(\n                        self.transformer.parameters(\n                        )).device.type == 'cuda':\n                    self.transformer.to('cpu')\n                current_model = self.transformer_2\n                current_guidance_scale = batch.guidance_scale_2\n            assert current_model is not None, \"current_model is None\"\n\n            # Expand latents for V2V/I2V\n            latent_model_input = latents.to(target_dtype)\n            if batch.video_latent is not None:\n                latent_model_input = torch.cat([\n                    latent_model_input, batch.video_latent,\n                    torch.zeros_like(latents)\n                ],\n                                               dim=1).to(target_dtype)\n            elif batch.image_latent is not None:\n                assert not fastvideo_args.pipeline_config.ti2v_task, \"image latents should not be provided for TI2V task\"\n                latent_model_input = torch.cat(\n                    [latent_model_input, batch.image_latent],\n                    dim=1).to(target_dtype)\n\n            assert not torch.isnan(\n                latent_model_input).any(), \"latent_model_input contains nan\"\n            if fastvideo_args.pipeline_config.ti2v_task and batch.pil_image is not None:\n                timestep = torch.stack([t]).to(get_local_torch_device())\n                temp_ts = (mask2[0][0][:, ::2, ::2] * timestep).flatten()\n                temp_ts = torch.cat([\n                    temp_ts,\n                    temp_ts.new_ones(seq_len - temp_ts.size(0)) * timestep\n                ])\n                timestep = temp_ts.unsqueeze(0)\n                t_expand = timestep.repeat(latent_model_input.shape[0], 1)\n            else:\n                t_expand = t.repeat(latent_model_input.shape[0])\n\n            latent_model_input = self.scheduler.scale_model_input(\n                latent_model_input, t)\n\n            # Prepare inputs for transformer\n            guidance_expand = (\n                torch.tensor(\n                    [fastvideo_args.pipeline_config.embedded_cfg_scale] *\n                    latent_model_input.shape[0],\n                    dtype=torch.float32,\n                    device=get_local_torch_device(),\n                ).to(target_dtype) *\n                1000.0 if fastvideo_args.pipeline_config.embedded_cfg_scale\n                is not None else None)\n\n            # Predict noise residual\n            with torch.autocast(device_type=\"cuda\",\n                                dtype=target_dtype,\n                                enabled=autocast_enabled):\n                if (st_attn_available\n                        and self.attn_backend == SlidingTileAttentionBackend\n                    ) or (vsa_available and self.attn_backend\n                          == VideoSparseAttentionBackend):\n                    self.attn_metadata_builder_cls = self.attn_backend.get_builder_cls(\n                    )\n\n                    if self.attn_metadata_builder_cls is not None:\n                        self.attn_metadata_builder = self.attn_metadata_builder_cls(\n                        )\n                        # TODO(will): clean this up\n                        attn_metadata = self.attn_metadata_builder.build(  # type: ignore\n                            current_timestep=i,  # type: ignore\n                            raw_latent_shape=batch.\n                            raw_latent_shape[2:5],  # type: ignore\n                            patch_size=fastvideo_args.\n                            pipeline_config.  # type: ignore\n                            dit_config.patch_size,  # type: ignore\n                            STA_param=batch.STA_param,  # type: ignore\n                            VSA_sparsity=fastvideo_args.\n                            VSA_sparsity,  # type: ignore\n                            device=get_local_torch_device(),\n                        )\n                        assert attn_metadata is not None, \"attn_metadata cannot be None\"\n                    else:\n                        attn_metadata = None\n                elif (vmoba_attn_available\n                      and self.attn_backend == VMOBAAttentionBackend):\n                    self.attn_metadata_builder_cls = self.attn_backend.get_builder_cls(\n                    )\n                    if self.attn_metadata_builder_cls is not None:\n                        self.attn_metadata_builder = self.attn_metadata_builder_cls(\n                        )\n                        # Prepare V-MoBA parameters from config\n                        moba_params = fastvideo_args.moba_config.copy()\n                        moba_params.update({\n                            \"current_timestep\":\n                            i,\n                            \"raw_latent_shape\":\n                            batch.raw_latent_shape[2:5],\n                            \"patch_size\":\n                            fastvideo_args.pipeline_config.dit_config.\n                            patch_size,\n                            \"device\":\n                            get_local_torch_device(),\n                        })\n                        attn_metadata = self.attn_metadata_builder.build(\n                            **moba_params)\n                        assert attn_metadata is not None, \"attn_metadata cannot be None\"\n                    else:\n                        attn_metadata = None\n                else:\n                    attn_metadata = None\n                # TODO(will): finalize the interface. vLLM uses this to\n                # support torch dynamo compilation. They pass in\n                # attn_metadata, vllm_config, and num_tokens. We can pass in\n                # fastvideo_args or training_args, and attn_metadata.\n                batch.is_cfg_negative = False\n                with set_forward_context(\n                        current_timestep=i,\n                        attn_metadata=attn_metadata,\n                        forward_batch=batch,\n                        # fastvideo_args=fastvideo_args\n                ):\n                    # Run transformer\n                    noise_pred = current_model(\n                        latent_model_input,\n                        prompt_embeds,\n                        t_expand,\n                        guidance=guidance_expand,\n                        **image_kwargs,\n                        **pos_cond_kwargs,\n                    )\n\n                # Apply guidance\n                if batch.do_classifier_free_guidance:\n                    batch.is_cfg_negative = True\n                    with set_forward_context(\n                            current_timestep=i,\n                            attn_metadata=attn_metadata,\n                            forward_batch=batch,\n                            # fastvideo_args=fastvideo_args\n                    ):\n                        # Run transformer\n                        noise_pred_uncond = current_model(\n                            latent_model_input,\n                            neg_prompt_embeds,\n                            t_expand,\n                            guidance=guidance_expand,\n                            **image_kwargs,\n                            **neg_cond_kwargs,\n                        )\n                    noise_pred_text = noise_pred\n                    noise_pred = noise_pred_uncond + current_guidance_scale * (\n                        noise_pred_text - noise_pred_uncond)\n\n                    # Apply guidance rescale if needed\n                    if batch.guidance_rescale &gt; 0.0:\n                        # Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf\n                        noise_pred = self.rescale_noise_cfg(\n                            noise_pred,\n                            noise_pred_text,\n                            guidance_rescale=batch.guidance_rescale,\n                        )\n                # Compute the previous noisy sample\n                latents = self.scheduler.step(noise_pred,\n                                              t,\n                                              latents,\n                                              **extra_step_kwargs,\n                                              return_dict=False)[0]\n                if fastvideo_args.pipeline_config.ti2v_task and batch.pil_image is not None:\n                    latents = latents.squeeze(0)\n                    latents = (1. - mask2[0]) * z + mask2[0] * latents\n                    # latents = latents.unsqueeze(0)\n\n            # save trajectory latents if needed\n            if batch.return_trajectory_latents:\n                trajectory_timesteps.append(t)\n                trajectory_latents.append(latents)\n\n            # Update progress bar\n            if i == len(timesteps) - 1 or (\n                (i + 1) &gt; num_warmup_steps and\n                (i + 1) % self.scheduler.order == 0\n                    and progress_bar is not None):\n                progress_bar.update()\n\n    # Gather results if using sequence parallelism\n    trajectory_tensor: torch.Tensor | None = None\n    if trajectory_latents:\n        trajectory_tensor = torch.stack(trajectory_latents, dim=1)\n        trajectory_timesteps_tensor = torch.stack(trajectory_timesteps,\n                                                  dim=0)\n    else:\n        trajectory_tensor = None\n        trajectory_timesteps_tensor = None\n\n    # Gather results if using sequence parallelism\n    if sp_group:\n        latents = sequence_model_parallel_all_gather(latents, dim=2)\n        if batch.return_trajectory_latents:\n            trajectory_tensor = trajectory_tensor.to(\n                get_local_torch_device())\n            trajectory_tensor = sequence_model_parallel_all_gather(\n                trajectory_tensor, dim=3)\n\n    if trajectory_tensor is not None and trajectory_timesteps_tensor is not None:\n        batch.trajectory_timesteps = trajectory_timesteps_tensor.cpu()\n        batch.trajectory_latents = trajectory_tensor.cpu()\n\n    # Update batch with final latents\n    batch.latents = latents\n\n    # Save STA mask search results if needed\n    if st_attn_available and self.attn_backend == SlidingTileAttentionBackend and fastvideo_args.STA_mode == STA_Mode.STA_SEARCHING:\n        self.save_sta_search_results(batch)\n\n    # deallocate transformer if on mps\n    if torch.backends.mps.is_available():\n        logger.info(\"Memory before deallocating transformer: %s\",\n                    torch.mps.current_allocated_memory())\n        del self.transformer\n        if pipeline is not None and \"transformer\" in pipeline.modules:\n            del pipeline.modules[\"transformer\"]\n        fastvideo_args.model_loaded[\"transformer\"] = False\n        logger.info(\"Memory after deallocating transformer: %s\",\n                    torch.mps.current_allocated_memory())\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.denoising.DenoisingStage.prepare_extra_func_kwargs \u00b6 <pre><code>prepare_extra_func_kwargs(func, kwargs) -&gt; dict[str, Any]\n</code></pre> <p>Prepare extra kwargs for the scheduler step / denoise step.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <p>The function to prepare kwargs for.</p> required <code>kwargs</code> <p>The kwargs to prepare.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The prepared kwargs.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def prepare_extra_func_kwargs(self, func, kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Prepare extra kwargs for the scheduler step / denoise step.\n\n    Args:\n        func: The function to prepare kwargs for.\n        kwargs: The kwargs to prepare.\n\n    Returns:\n        The prepared kwargs.\n    \"\"\"\n    extra_step_kwargs = {}\n    for k, v in kwargs.items():\n        accepts = k in set(inspect.signature(func).parameters.keys())\n        if accepts:\n            extra_step_kwargs[k] = v\n    return extra_step_kwargs\n</code></pre> fastvideo.pipelines.stages.denoising.DenoisingStage.prepare_sta_param \u00b6 <pre><code>prepare_sta_param(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n)\n</code></pre> <p>Prepare Sliding Tile Attention (STA) parameters and settings.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def prepare_sta_param(self, batch: ForwardBatch,\n                      fastvideo_args: FastVideoArgs):\n    \"\"\"\n    Prepare Sliding Tile Attention (STA) parameters and settings.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n    \"\"\"\n    # TODO(kevin): STA mask search, currently only support Wan2.1 with 69x768x1280\n    from fastvideo.STA_configuration import configure_sta\n    STA_mode = fastvideo_args.STA_mode\n    skip_time_steps = fastvideo_args.skip_time_steps\n    if batch.timesteps is None:\n        raise ValueError(\"Timesteps must be provided\")\n    timesteps_num = batch.timesteps.shape[0]\n\n    logger.info(\"STA_mode: %s\", STA_mode)\n    if (batch.num_frames, batch.height,\n            batch.width) != (69, 768, 1280) and STA_mode != \"STA_inference\":\n        raise NotImplementedError(\n            \"STA mask search/tuning is not supported for this resolution\")\n\n    if STA_mode == STA_Mode.STA_SEARCHING or STA_mode == STA_Mode.STA_TUNING or STA_mode == STA_Mode.STA_TUNING_CFG:\n        size = (batch.width, batch.height)\n        if size == (1280, 768):\n            # TODO: make it configurable\n            sparse_mask_candidates_searching = [\n                \"3, 1, 10\", \"1, 5, 7\", \"3, 3, 3\", \"1, 6, 5\", \"1, 3, 10\",\n                \"3, 6, 1\"\n            ]\n            sparse_mask_candidates_tuning = [\n                \"3, 1, 10\", \"1, 5, 7\", \"3, 3, 3\", \"1, 6, 5\", \"1, 3, 10\",\n                \"3, 6, 1\"\n            ]\n            full_mask = [\"3,6,10\"]\n        else:\n            raise NotImplementedError(\n                \"STA mask search is not supported for this resolution\")\n    layer_num = self.transformer.config.num_layers\n    # specific for HunyuanVideo\n    if hasattr(self.transformer.config, \"num_single_layers\"):\n        layer_num += self.transformer.config.num_single_layers\n    head_num = self.transformer.config.num_attention_heads\n\n    if STA_mode == STA_Mode.STA_SEARCHING:\n        STA_param = configure_sta(\n            mode=STA_Mode.STA_SEARCHING,\n            layer_num=layer_num,\n            head_num=head_num,\n            time_step_num=timesteps_num,\n            mask_candidates=sparse_mask_candidates_searching +\n            full_mask,  # last is full mask; Can add more sparse masks while keep last one as full mask\n        )\n    elif STA_mode == STA_Mode.STA_TUNING:\n        STA_param = configure_sta(\n            mode=STA_Mode.STA_TUNING,\n            layer_num=layer_num,\n            head_num=head_num,\n            time_step_num=timesteps_num,\n            mask_search_files_path=\n            f'output/mask_search_result_pos_{size[0]}x{size[1]}/',\n            mask_candidates=sparse_mask_candidates_tuning,\n            full_attention_mask=[int(x) for x in full_mask[0].split(',')],\n            skip_time_steps=\n            skip_time_steps,  # Use full attention for first 12 steps\n            save_dir=\n            f'output/mask_search_strategy_{size[0]}x{size[1]}/',  # Custom save directory\n            timesteps=timesteps_num)\n    elif STA_mode == STA_Mode.STA_TUNING_CFG:\n        STA_param = configure_sta(\n            mode=STA_Mode.STA_TUNING_CFG,\n            layer_num=layer_num,\n            head_num=head_num,\n            time_step_num=timesteps_num,\n            mask_search_files_path_pos=\n            f'output/mask_search_result_pos_{size[0]}x{size[1]}/',\n            mask_search_files_path_neg=\n            f'output/mask_search_result_neg_{size[0]}x{size[1]}/',\n            mask_candidates=sparse_mask_candidates_tuning,\n            full_attention_mask=[int(x) for x in full_mask[0].split(',')],\n            skip_time_steps=skip_time_steps,\n            save_dir=f'output/mask_search_strategy_{size[0]}x{size[1]}/',\n            timesteps=timesteps_num)\n    elif STA_mode == STA_Mode.STA_INFERENCE:\n        import fastvideo.envs as envs\n        config_file = envs.FASTVIDEO_ATTENTION_CONFIG\n        if config_file is None:\n            raise ValueError(\"FASTVIDEO_ATTENTION_CONFIG is not set\")\n        STA_param = configure_sta(mode=STA_Mode.STA_INFERENCE,\n                                  layer_num=layer_num,\n                                  head_num=head_num,\n                                  time_step_num=timesteps_num,\n                                  load_path=config_file)\n\n    batch.STA_param = STA_param\n    batch.mask_search_final_result_pos = [[] for _ in range(timesteps_num)]\n    batch.mask_search_final_result_neg = [[] for _ in range(timesteps_num)]\n</code></pre> fastvideo.pipelines.stages.denoising.DenoisingStage.progress_bar \u00b6 <pre><code>progress_bar(\n    iterable: Iterable | None = None,\n    total: int | None = None,\n) -&gt; tqdm\n</code></pre> <p>Create a progress bar for the denoising process.</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>Iterable | None</code> <p>The iterable to iterate over.</p> <code>None</code> <code>total</code> <code>int | None</code> <p>The total number of items.</p> <code>None</code> <p>Returns:</p> Type Description <code>tqdm</code> <p>A tqdm progress bar.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def progress_bar(self,\n                 iterable: Iterable | None = None,\n                 total: int | None = None) -&gt; tqdm:\n    \"\"\"\n    Create a progress bar for the denoising process.\n\n    Args:\n        iterable: The iterable to iterate over.\n        total: The total number of items.\n\n    Returns:\n        A tqdm progress bar.\n    \"\"\"\n    local_rank = get_world_group().local_rank\n    if local_rank == 0:\n        return tqdm(iterable=iterable, total=total)\n    else:\n        return tqdm(iterable=iterable, total=total, disable=True)\n</code></pre> fastvideo.pipelines.stages.denoising.DenoisingStage.rescale_noise_cfg \u00b6 <pre><code>rescale_noise_cfg(\n    noise_cfg, noise_pred_text, guidance_rescale=0.0\n) -&gt; torch.Tensor\n</code></pre> <p>Rescale noise prediction according to guidance_rescale.</p> <p>Based on findings of \"Common Diffusion Noise Schedules and Sample Steps are Flawed\" (https://arxiv.org/pdf/2305.08891.pdf), Section 3.4.</p> <p>Parameters:</p> Name Type Description Default <code>noise_cfg</code> <p>The noise prediction with guidance.</p> required <code>noise_pred_text</code> <p>The text-conditioned noise prediction.</p> required <code>guidance_rescale</code> <p>The guidance rescale factor.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The rescaled noise prediction.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def rescale_noise_cfg(self,\n                      noise_cfg,\n                      noise_pred_text,\n                      guidance_rescale=0.0) -&gt; torch.Tensor:\n    \"\"\"\n    Rescale noise prediction according to guidance_rescale.\n\n    Based on findings of \"Common Diffusion Noise Schedules and Sample Steps are Flawed\"\n    (https://arxiv.org/pdf/2305.08891.pdf), Section 3.4.\n\n    Args:\n        noise_cfg: The noise prediction with guidance.\n        noise_pred_text: The text-conditioned noise prediction.\n        guidance_rescale: The guidance rescale factor.\n\n    Returns:\n        The rescaled noise prediction.\n    \"\"\"\n    std_text = noise_pred_text.std(dim=list(range(1, noise_pred_text.ndim)),\n                                   keepdim=True)\n    std_cfg = noise_cfg.std(dim=list(range(1, noise_cfg.ndim)),\n                            keepdim=True)\n    # Rescale the results from guidance (fixes overexposure)\n    noise_pred_rescaled = noise_cfg * (std_text / std_cfg)\n    # Mix with the original results from guidance by factor guidance_rescale\n    noise_cfg = (guidance_rescale * noise_pred_rescaled +\n                 (1 - guidance_rescale) * noise_cfg)\n    return noise_cfg\n</code></pre> fastvideo.pipelines.stages.denoising.DenoisingStage.save_sta_search_results \u00b6 <pre><code>save_sta_search_results(batch: ForwardBatch)\n</code></pre> <p>Save the STA mask search results.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def save_sta_search_results(self, batch: ForwardBatch):\n    \"\"\"\n    Save the STA mask search results.\n\n    Args:\n        batch: The current batch information.\n    \"\"\"\n    size = (batch.width, batch.height)\n    if size == (1280, 768):\n        # TODO: make it configurable\n        sparse_mask_candidates_searching = [\n            \"3, 1, 10\", \"1, 5, 7\", \"3, 3, 3\", \"1, 6, 5\", \"1, 3, 10\",\n            \"3, 6, 1\"\n        ]\n    else:\n        raise NotImplementedError(\n            \"STA mask search is not supported for this resolution\")\n\n    from fastvideo.STA_configuration import save_mask_search_results\n    if batch.mask_search_final_result_pos is not None and batch.prompt is not None:\n        save_mask_search_results(\n            [\n                dict(layer_data)\n                for layer_data in batch.mask_search_final_result_pos\n            ],\n            prompt=str(batch.prompt),\n            mask_strategies=sparse_mask_candidates_searching,\n            output_dir=f'output/mask_search_result_pos_{size[0]}x{size[1]}/'\n        )\n    if batch.mask_search_final_result_neg is not None and batch.prompt is not None:\n        save_mask_search_results(\n            [\n                dict(layer_data)\n                for layer_data in batch.mask_search_final_result_neg\n            ],\n            prompt=str(batch.prompt),\n            mask_strategies=sparse_mask_candidates_searching,\n            output_dir=f'output/mask_search_result_neg_{size[0]}x{size[1]}/'\n        )\n</code></pre> fastvideo.pipelines.stages.denoising.DenoisingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify denoising stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify denoising stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"timesteps\", batch.timesteps,\n                     [V.is_tensor, V.min_dims(1)])\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds, V.list_not_empty)\n    result.add_check(\"image_embeds\", batch.image_embeds, V.is_list)\n    result.add_check(\"image_latent\", batch.image_latent,\n                     V.none_or_tensor_with_dims(5))\n    result.add_check(\"num_inference_steps\", batch.num_inference_steps,\n                     V.positive_int)\n    result.add_check(\"guidance_scale\", batch.guidance_scale,\n                     V.positive_float)\n    result.add_check(\"eta\", batch.eta, V.non_negative_float)\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    result.add_check(\"do_classifier_free_guidance\",\n                     batch.do_classifier_free_guidance, V.bool_value)\n    result.add_check(\n        \"negative_prompt_embeds\", batch.negative_prompt_embeds, lambda x:\n        not batch.do_classifier_free_guidance or V.list_not_empty(x))\n    return result\n</code></pre> fastvideo.pipelines.stages.denoising.DenoisingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify denoising stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify denoising stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre> fastvideo.pipelines.stages.denoising.DmdDenoisingStage \u00b6 <pre><code>DmdDenoisingStage(transformer, scheduler)\n</code></pre> <p>               Bases: <code>DenoisingStage</code></p> <p>Denoising stage for DMD.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def __init__(self, transformer, scheduler) -&gt; None:\n    super().__init__(transformer, scheduler)\n    self.scheduler = FlowMatchEulerDiscreteScheduler(shift=8.0)\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.denoising.DmdDenoisingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Run the denoising loop.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with denoised latents.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Run the denoising loop.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with denoised latents.\n    \"\"\"\n    # Setup precision and autocast settings\n    # TODO(will): make the precision configurable for inference\n    # target_dtype = PRECISION_TO_TYPE[fastvideo_args.precision]\n    target_dtype = torch.bfloat16\n    autocast_enabled = (target_dtype != torch.float32\n                        ) and not fastvideo_args.disable_autocast\n\n    # Get timesteps and calculate warmup steps\n    timesteps = batch.timesteps\n\n    # TODO(will): remove this once we add input/output validation for stages\n    if timesteps is None:\n        raise ValueError(\"Timesteps must be provided\")\n    num_inference_steps = batch.num_inference_steps\n    num_warmup_steps = len(\n        timesteps) - num_inference_steps * self.scheduler.order\n\n    # Prepare image latents and embeddings for I2V generation\n    image_embeds = batch.image_embeds\n    if len(image_embeds) &gt; 0:\n        assert torch.isnan(image_embeds[0]).sum() == 0\n        image_embeds = [\n            image_embed.to(target_dtype) for image_embed in image_embeds\n        ]\n\n    image_kwargs = self.prepare_extra_func_kwargs(\n        self.transformer.forward,\n        {\n            \"encoder_hidden_states_image\": image_embeds,\n            \"mask_strategy\": dict_to_3d_list(\n                None, t_max=50, l_max=60, h_max=24)\n        },\n    )\n\n    pos_cond_kwargs = self.prepare_extra_func_kwargs(\n        self.transformer.forward,\n        {\n            \"encoder_hidden_states_2\": batch.clip_embedding_pos,\n            \"encoder_attention_mask\": batch.prompt_attention_mask,\n        },\n    )\n\n    # Prepare STA parameters\n    if st_attn_available and self.attn_backend == SlidingTileAttentionBackend:\n        self.prepare_sta_param(batch, fastvideo_args)\n\n    # Get latents and embeddings\n    assert batch.latents is not None, \"latents must be provided\"\n    latents = batch.latents\n    latents = latents.permute(0, 2, 1, 3, 4)\n\n    video_raw_latent_shape = latents.shape\n    prompt_embeds = batch.prompt_embeds\n    assert not torch.isnan(\n        prompt_embeds[0]).any(), \"prompt_embeds contains nan\"\n    timesteps = torch.tensor(\n        fastvideo_args.pipeline_config.dmd_denoising_steps,\n        dtype=torch.long,\n        device=get_local_torch_device())\n\n    # Handle sequence parallelism if enabled\n    sp_world_size, rank_in_sp_group = get_sp_world_size(\n    ), get_sp_parallel_rank()\n    sp_group = sp_world_size &gt; 1\n    if sp_group:\n        latents = rearrange(latents,\n                            \"b (n t) c h w -&gt; b n t c h w\",\n                            n=sp_world_size).contiguous()\n        latents = latents[:, rank_in_sp_group, :, :, :, :]\n        if batch.image_latent is not None:\n            image_latent = rearrange(batch.image_latent,\n                                     \"b c (n t) h w -&gt; b c n t h w\",\n                                     n=sp_world_size).contiguous()\n\n            image_latent = image_latent[:, :, rank_in_sp_group, :, :, :]\n            batch.image_latent = image_latent\n\n    # Run denoising loop\n    with self.progress_bar(total=len(timesteps)) as progress_bar:\n        for i, t in enumerate(timesteps):\n            # Skip if interrupted\n            if hasattr(self, 'interrupt') and self.interrupt:\n                continue\n            # Expand latents for I2V\n            noise_latents = latents.clone()\n            latent_model_input = latents.to(target_dtype)\n\n            if batch.image_latent is not None:\n                latent_model_input = torch.cat([\n                    latent_model_input,\n                    batch.image_latent.permute(0, 2, 1, 3, 4)\n                ],\n                                               dim=2).to(target_dtype)\n            assert not torch.isnan(\n                latent_model_input).any(), \"latent_model_input contains nan\"\n\n            # Prepare inputs for transformer\n            t_expand = t.repeat(latent_model_input.shape[0])\n            guidance_expand = (\n                torch.tensor(\n                    [fastvideo_args.pipeline_config.embedded_cfg_scale] *\n                    latent_model_input.shape[0],\n                    dtype=torch.float32,\n                    device=get_local_torch_device(),\n                ).to(target_dtype) *\n                1000.0 if fastvideo_args.pipeline_config.embedded_cfg_scale\n                is not None else None)\n\n            # Predict noise residual\n            with torch.autocast(device_type=\"cuda\",\n                                dtype=target_dtype,\n                                enabled=autocast_enabled):\n                if (vsa_available and self.attn_backend\n                        == VideoSparseAttentionBackend):\n                    self.attn_metadata_builder_cls = self.attn_backend.get_builder_cls(\n                    )\n\n                    if self.attn_metadata_builder_cls is not None:\n                        self.attn_metadata_builder = self.attn_metadata_builder_cls(\n                        )\n                        # TODO(will): clean this up\n                        attn_metadata = self.attn_metadata_builder.build(  # type: ignore\n                            current_timestep=i,  # type: ignore\n                            raw_latent_shape=batch.\n                            raw_latent_shape[2:5],  # type: ignore\n                            patch_size=fastvideo_args.\n                            pipeline_config.  # type: ignore\n                            dit_config.patch_size,  # type: ignore\n                            STA_param=batch.STA_param,  # type: ignore\n                            VSA_sparsity=fastvideo_args.\n                            VSA_sparsity,  # type: ignore\n                            device=get_local_torch_device(),  # type: ignore\n                        )  # type: ignore\n                        assert attn_metadata is not None, \"attn_metadata cannot be None\"\n                    else:\n                        attn_metadata = None\n                else:\n                    attn_metadata = None\n\n                batch.is_cfg_negative = False\n                with set_forward_context(\n                        current_timestep=i,\n                        attn_metadata=attn_metadata,\n                        forward_batch=batch,\n                        # fastvideo_args=fastvideo_args\n                ):\n                    # Run transformer\n                    pred_noise = self.transformer(\n                        latent_model_input.permute(0, 2, 1, 3, 4),\n                        prompt_embeds,\n                        t_expand,\n                        guidance=guidance_expand,\n                        **image_kwargs,\n                        **pos_cond_kwargs,\n                    ).permute(0, 2, 1, 3, 4)\n\n                pred_video = pred_noise_to_pred_video(\n                    pred_noise=pred_noise.flatten(0, 1),\n                    noise_input_latent=noise_latents.flatten(0, 1),\n                    timestep=t_expand,\n                    scheduler=self.scheduler).unflatten(\n                        0, pred_noise.shape[:2])\n\n                if i &lt; len(timesteps) - 1:\n                    next_timestep = timesteps[i + 1] * torch.ones(\n                        [1], dtype=torch.long, device=pred_video.device)\n                    noise = torch.randn(video_raw_latent_shape,\n                                        dtype=pred_video.dtype,\n                                        generator=batch.generator[0]).to(\n                                            self.device)\n                    if sp_group:\n                        noise = rearrange(noise,\n                                          \"b (n t) c h w -&gt; b n t c h w\",\n                                          n=sp_world_size).contiguous()\n                        noise = noise[:, rank_in_sp_group, :, :, :, :]\n                    latents = self.scheduler.add_noise(\n                        pred_video.flatten(0, 1), noise.flatten(0, 1),\n                        next_timestep).unflatten(0, pred_video.shape[:2])\n                else:\n                    latents = pred_video\n\n                # Update progress bar\n                if i == len(timesteps) - 1 or (\n                    (i + 1) &gt; num_warmup_steps and\n                    (i + 1) % self.scheduler.order == 0\n                        and progress_bar is not None):\n                    progress_bar.update()\n\n    # Gather results if using sequence parallelism\n    if sp_group:\n        latents = sequence_model_parallel_all_gather(latents, dim=1)\n    latents = latents.permute(0, 2, 1, 3, 4)\n    # Update batch with final latents\n    batch.latents = latents\n\n    return batch\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.encoding","title":"fastvideo.pipelines.stages.encoding","text":"<p>Encoding stage for diffusion pipelines.</p> Classes\u00b6 fastvideo.pipelines.stages.encoding.EncodingStage \u00b6 <pre><code>EncodingStage(vae: ParallelTiledVAE)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for encoding pixel space representations into latent space.</p> <p>This stage handles the encoding of pixel-space video/images into latent representations for further processing in the diffusion pipeline.</p> Source code in <code>fastvideo/pipelines/stages/encoding.py</code> <pre><code>def __init__(self, vae: ParallelTiledVAE) -&gt; None:\n    self.vae: ParallelTiledVAE = vae\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.encoding.EncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode pixel space representations into latent space.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded latents.</p> Source code in <code>fastvideo/pipelines/stages/encoding.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode pixel space representations into latent space.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded latents.\n    \"\"\"\n    assert batch.latents is not None and isinstance(batch.latents,\n                                                    torch.Tensor)\n\n    self.vae = self.vae.to(get_local_torch_device())\n\n    # Setup VAE precision\n    vae_dtype = PRECISION_TO_TYPE[\n        fastvideo_args.pipeline_config.vae_precision]\n    vae_autocast_enabled = (\n        vae_dtype != torch.float32) and not fastvideo_args.disable_autocast\n\n    # Normalize input to [-1, 1] range (reverse of decoding normalization)\n    latents = (batch.latents * 2.0 - 1.0).clamp(-1, 1)\n\n    # Move to appropriate device and dtype\n    latents = latents.to(get_local_torch_device())\n\n    # Encode image to latents\n    with torch.autocast(device_type=\"cuda\",\n                        dtype=vae_dtype,\n                        enabled=vae_autocast_enabled):\n        if fastvideo_args.pipeline_config.vae_tiling:\n            self.vae.enable_tiling()\n        # if fastvideo_args.vae_sp:\n        #     self.vae.enable_parallel()\n        if not vae_autocast_enabled:\n            latents = latents.to(vae_dtype)\n        latents = self.vae.encode(latents).mean\n\n    # Update batch with encoded latents\n    batch.latents = latents\n\n    # Offload models if needed\n    if hasattr(self, 'maybe_free_model_hooks'):\n        self.maybe_free_model_hooks()\n\n    if fastvideo_args.vae_cpu_offload:\n        self.vae.to(\"cpu\")\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.encoding.EncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/encoding.py</code> <pre><code>@torch.no_grad()\ndef verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    # Input video/images for VAE encoding: [batch_size, channels, frames, height, width]\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre> fastvideo.pipelines.stages.encoding.EncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    # Encoded latents: [batch_size, channels, frames, height_latents, width_latents]\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.image_encoding","title":"fastvideo.pipelines.stages.image_encoding","text":"<p>Image and video encoding stages for diffusion pipelines.</p> <p>This module contains implementations of encoding stages for diffusion pipelines: - ImageEncodingStage: Encodes images using image encoders (e.g., CLIP) - RefImageEncodingStage: Encodes reference image for Wan2.1 control pipeline - ImageVAEEncodingStage: Encodes images to latent space using VAE for I2V generation - VideoVAEEncodingStage: Encodes videos to latent space using VAE for V2V and control tasks</p> Classes\u00b6 fastvideo.pipelines.stages.image_encoding.ImageEncodingStage \u00b6 <pre><code>ImageEncodingStage(image_encoder, image_processor)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for encoding image prompts into embeddings for diffusion models.</p> <p>This stage handles the encoding of image prompts into the embedding space expected by the diffusion model.</p> <p>Initialize the prompt encoding stage.</p> <p>Parameters:</p> Name Type Description Default <code>enable_logging</code> <p>Whether to enable logging for this stage.</p> required <code>is_secondary</code> <p>Whether this is a secondary image encoder.</p> required Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def __init__(self, image_encoder, image_processor) -&gt; None:\n    \"\"\"\n    Initialize the prompt encoding stage.\n\n    Args:\n        enable_logging: Whether to enable logging for this stage.\n        is_secondary: Whether this is a secondary image encoder.\n    \"\"\"\n    super().__init__()\n    self.image_processor = image_processor\n    self.image_encoder = image_encoder\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.image_encoding.ImageEncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode the prompt into image encoder hidden states.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded prompt embeddings.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode the prompt into image encoder hidden states.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded prompt embeddings.\n    \"\"\"\n    self.image_encoder = self.image_encoder.to(get_local_torch_device())\n\n    image = batch.pil_image\n\n    image_inputs = self.image_processor(\n        images=image, return_tensors=\"pt\").to(get_local_torch_device())\n    with set_forward_context(current_timestep=0, attn_metadata=None):\n        outputs = self.image_encoder(**image_inputs)\n        image_embeds = outputs.last_hidden_state\n\n    batch.image_embeds.append(image_embeds)\n\n    if fastvideo_args.image_encoder_cpu_offload:\n        self.image_encoder.to('cpu')\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.image_encoding.ImageEncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify image encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify image encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"pil_image\", batch.pil_image, V.not_none)\n    result.add_check(\"image_embeds\", batch.image_embeds, V.is_list)\n    return result\n</code></pre> fastvideo.pipelines.stages.image_encoding.ImageEncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify image encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify image encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"image_embeds\", batch.image_embeds,\n                     V.list_of_tensors_dims(3))\n    return result\n</code></pre> fastvideo.pipelines.stages.image_encoding.ImageVAEEncodingStage \u00b6 <pre><code>ImageVAEEncodingStage(vae: ParallelTiledVAE)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for encoding image pixel representations into latent space.</p> <p>This stage handles the encoding of image pixel representations into the final input format (e.g., latents) for image-to-video generation.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def __init__(self, vae: ParallelTiledVAE) -&gt; None:\n    self.vae: ParallelTiledVAE = vae\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.image_encoding.ImageVAEEncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode pixel representations into latent space.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded outputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode pixel representations into latent space.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded outputs.\n    \"\"\"\n    assert batch.pil_image is not None\n    if fastvideo_args.mode == ExecutionMode.INFERENCE:\n        assert batch.pil_image is not None and isinstance(\n            batch.pil_image, PIL.Image.Image)\n        assert batch.height is not None and isinstance(batch.height, int)\n        assert batch.width is not None and isinstance(batch.width, int)\n        assert batch.num_frames is not None and isinstance(\n            batch.num_frames, int)\n        height = batch.height\n        width = batch.width\n        num_frames = batch.num_frames\n    elif fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        assert batch.pil_image is not None and isinstance(\n            batch.pil_image, torch.Tensor)\n        assert batch.height is not None and isinstance(batch.height, list)\n        assert batch.width is not None and isinstance(batch.width, list)\n        assert batch.num_frames is not None and isinstance(\n            batch.num_frames, list)\n        num_frames = batch.num_frames[0]\n        height = batch.height[0]\n        width = batch.width[0]\n\n    self.vae = self.vae.to(get_local_torch_device())\n\n    # Process single image for I2V\n    latent_height = height // self.vae.spatial_compression_ratio\n    latent_width = width // self.vae.spatial_compression_ratio\n    image = batch.pil_image\n    image = self.preprocess(\n        image,\n        vae_scale_factor=self.vae.spatial_compression_ratio,\n        height=height,\n        width=width).to(get_local_torch_device(), dtype=torch.float32)\n\n    # (B, C, H, W) -&gt; (B, C, 1, H, W)\n    image = image.unsqueeze(2)\n\n    video_condition = torch.cat([\n        image,\n        image.new_zeros(image.shape[0], image.shape[1], num_frames - 1,\n                        image.shape[3], image.shape[4])\n    ],\n                                dim=2)\n    video_condition = video_condition.to(device=get_local_torch_device(),\n                                         dtype=torch.float32)\n\n    # Setup VAE precision\n    vae_dtype = PRECISION_TO_TYPE[\n        fastvideo_args.pipeline_config.vae_precision]\n    vae_autocast_enabled = (\n        vae_dtype != torch.float32) and not fastvideo_args.disable_autocast\n\n    # Encode Image\n    with torch.autocast(device_type=\"cuda\",\n                        dtype=vae_dtype,\n                        enabled=vae_autocast_enabled):\n        if fastvideo_args.pipeline_config.vae_tiling:\n            self.vae.enable_tiling()\n        # if fastvideo_args.vae_sp:\n        #     self.vae.enable_parallel()\n        if not vae_autocast_enabled:\n            video_condition = video_condition.to(vae_dtype)\n        encoder_output = self.vae.encode(video_condition)\n\n    if fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        latent_condition = encoder_output.mean\n    else:\n        generator = batch.generator\n        if generator is None:\n            raise ValueError(\"Generator must be provided\")\n        latent_condition = self.retrieve_latents(encoder_output, generator)\n\n    # Apply shifting if needed\n    if (hasattr(self.vae, \"shift_factor\")\n            and self.vae.shift_factor is not None):\n        if isinstance(self.vae.shift_factor, torch.Tensor):\n            latent_condition -= self.vae.shift_factor.to(\n                latent_condition.device, latent_condition.dtype)\n        else:\n            latent_condition -= self.vae.shift_factor\n\n    if isinstance(self.vae.scaling_factor, torch.Tensor):\n        latent_condition = latent_condition * self.vae.scaling_factor.to(\n            latent_condition.device, latent_condition.dtype)\n    else:\n        latent_condition = latent_condition * self.vae.scaling_factor\n\n    if fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        batch.image_latent = latent_condition\n    else:\n        mask_lat_size = torch.ones(1, 1, num_frames, latent_height,\n                                   latent_width)\n        mask_lat_size[:, :, list(range(1, num_frames))] = 0\n        first_frame_mask = mask_lat_size[:, :, 0:1]\n        first_frame_mask = torch.repeat_interleave(\n            first_frame_mask,\n            dim=2,\n            repeats=self.vae.temporal_compression_ratio)\n        mask_lat_size = torch.concat(\n            [first_frame_mask, mask_lat_size[:, :, 1:, :]], dim=2)\n        mask_lat_size = mask_lat_size.view(\n            1, -1, self.vae.temporal_compression_ratio, latent_height,\n            latent_width)\n        mask_lat_size = mask_lat_size.transpose(1, 2)\n        mask_lat_size = mask_lat_size.to(latent_condition.device)\n\n        batch.image_latent = torch.concat([mask_lat_size, latent_condition],\n                                          dim=1)\n\n    # Offload models if needed\n    if hasattr(self, 'maybe_free_model_hooks'):\n        self.maybe_free_model_hooks()\n\n    self.vae.to(\"cpu\")\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.image_encoding.ImageVAEEncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    if fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        result.add_check(\"height\", batch.height, V.list_not_empty)\n        result.add_check(\"width\", batch.width, V.list_not_empty)\n        result.add_check(\"num_frames\", batch.num_frames, V.list_not_empty)\n    else:\n        result.add_check(\"height\", batch.height, V.positive_int)\n        result.add_check(\"width\", batch.width, V.positive_int)\n        result.add_check(\"num_frames\", batch.num_frames, V.positive_int)\n    return result\n</code></pre> fastvideo.pipelines.stages.image_encoding.ImageVAEEncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"image_latent\", batch.image_latent,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre> fastvideo.pipelines.stages.image_encoding.RefImageEncodingStage \u00b6 <pre><code>RefImageEncodingStage(image_encoder, image_processor)\n</code></pre> <p>               Bases: <code>ImageEncodingStage</code></p> <p>Stage for encoding reference image prompts into embeddings for Wan2.1 Control models.</p> <p>This stage extends ImageEncodingStage with specialized preprocessing for reference images.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def __init__(self, image_encoder, image_processor) -&gt; None:\n    \"\"\"\n    Initialize the prompt encoding stage.\n\n    Args:\n        enable_logging: Whether to enable logging for this stage.\n        is_secondary: Whether this is a secondary image encoder.\n    \"\"\"\n    super().__init__()\n    self.image_processor = image_processor\n    self.image_encoder = image_encoder\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.image_encoding.RefImageEncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode the prompt into image encoder hidden states.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded prompt embeddings.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode the prompt into image encoder hidden states.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded prompt embeddings.\n    \"\"\"\n    self.image_encoder = self.image_encoder.to(get_local_torch_device())\n\n    image = batch.pil_image\n    if image is None:\n        image = create_default_image()\n    # Preprocess reference image for CLIP encoder\n    image_tensor = preprocess_reference_image_for_clip(\n        image, get_local_torch_device())\n\n    image_inputs = self.image_processor(images=image_tensor,\n                                        return_tensors=\"pt\").to(\n                                            get_local_torch_device())\n    with set_forward_context(current_timestep=0, attn_metadata=None):\n        outputs = self.image_encoder(**image_inputs)\n        image_embeds = outputs.last_hidden_state\n    batch.image_embeds.append(image_embeds)\n\n    if batch.pil_image is None:\n        batch.image_embeds = [\n            torch.zeros_like(x) for x in batch.image_embeds\n        ]\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.image_encoding.VideoVAEEncodingStage \u00b6 <pre><code>VideoVAEEncodingStage(vae: ParallelTiledVAE)\n</code></pre> <p>               Bases: <code>ImageVAEEncodingStage</code></p> <p>Stage for encoding video pixel representations into latent space.</p> <p>This stage handles the encoding of video pixel representations for video-to-video generation and control. Inherits from ImageVAEEncodingStage to reuse common functionality.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def __init__(self, vae: ParallelTiledVAE) -&gt; None:\n    self.vae: ParallelTiledVAE = vae\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.image_encoding.VideoVAEEncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode video pixel representations into latent space.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded outputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode video pixel representations into latent space.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded outputs.\n    \"\"\"\n    assert batch.video_latent is not None, \"Video latent input is required for VideoVAEEncodingStage\"\n\n    if fastvideo_args.mode == ExecutionMode.INFERENCE:\n        assert batch.height is not None and isinstance(batch.height, int)\n        assert batch.width is not None and isinstance(batch.width, int)\n        assert batch.num_frames is not None and isinstance(\n            batch.num_frames, int)\n        height = batch.height\n        width = batch.width\n        num_frames = batch.num_frames\n    elif fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        assert batch.height is not None and isinstance(batch.height, list)\n        assert batch.width is not None and isinstance(batch.width, list)\n        assert batch.num_frames is not None and isinstance(\n            batch.num_frames, list)\n        num_frames = batch.num_frames[0]\n        height = batch.height[0]\n        width = batch.width[0]\n\n    self.vae = self.vae.to(get_local_torch_device())\n\n    # Prepare video tensor from control video\n    video_condition = self._prepare_control_video_tensor(\n        batch.video_latent, num_frames, height,\n        width).to(get_local_torch_device(), dtype=torch.float32)\n\n    # Setup VAE precision\n    vae_dtype = PRECISION_TO_TYPE[\n        fastvideo_args.pipeline_config.vae_precision]\n    vae_autocast_enabled = (\n        vae_dtype != torch.float32) and not fastvideo_args.disable_autocast\n\n    # Encode control video\n    with torch.autocast(device_type=\"cuda\",\n                        dtype=vae_dtype,\n                        enabled=vae_autocast_enabled):\n        if fastvideo_args.pipeline_config.vae_tiling:\n            self.vae.enable_tiling()\n        if not vae_autocast_enabled:\n            video_condition = video_condition.to(vae_dtype)\n        encoder_output = self.vae.encode(video_condition)\n\n    generator = batch.generator\n    if generator is None:\n        raise ValueError(\"Generator must be provided\")\n    latent_condition = self.retrieve_latents(encoder_output, generator)\n\n    if (hasattr(self.vae, \"shift_factor\")\n            and self.vae.shift_factor is not None):\n        if isinstance(self.vae.shift_factor, torch.Tensor):\n            latent_condition -= self.vae.shift_factor.to(\n                latent_condition.device, latent_condition.dtype)\n        else:\n            latent_condition -= self.vae.shift_factor\n\n    if isinstance(self.vae.scaling_factor, torch.Tensor):\n        latent_condition = latent_condition * self.vae.scaling_factor.to(\n            latent_condition.device, latent_condition.dtype)\n    else:\n        latent_condition = latent_condition * self.vae.scaling_factor\n\n    batch.video_latent = latent_condition\n\n    # Offload models if needed\n    if hasattr(self, 'maybe_free_model_hooks'):\n        self.maybe_free_model_hooks()\n\n    self.vae.to(\"cpu\")\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.image_encoding.VideoVAEEncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify video encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify video encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"video_latent\", batch.video_latent, V.not_none)\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    if fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        result.add_check(\"height\", batch.height, V.list_not_empty)\n        result.add_check(\"width\", batch.width, V.list_not_empty)\n        result.add_check(\"num_frames\", batch.num_frames, V.list_not_empty)\n    else:\n        result.add_check(\"height\", batch.height, V.positive_int)\n        result.add_check(\"width\", batch.width, V.positive_int)\n        result.add_check(\"num_frames\", batch.num_frames, V.positive_int)\n    return result\n</code></pre> fastvideo.pipelines.stages.image_encoding.VideoVAEEncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify video encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify video encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"video_latent\", batch.video_latent,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.input_validation","title":"fastvideo.pipelines.stages.input_validation","text":"<p>Input validation stage for diffusion pipelines.</p> Classes\u00b6 fastvideo.pipelines.stages.input_validation.InputValidationStage \u00b6 <p>               Bases: <code>PipelineStage</code></p> <p>Stage for validating and preparing inputs for diffusion pipelines.</p> <p>This stage validates that all required inputs are present and properly formatted before proceeding with the diffusion process.</p> Functions\u00b6 fastvideo.pipelines.stages.input_validation.InputValidationStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Validate and prepare inputs.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The validated batch information.</p> Source code in <code>fastvideo/pipelines/stages/input_validation.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Validate and prepare inputs.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The validated batch information.\n    \"\"\"\n\n    self._generate_seeds(batch, fastvideo_args)\n\n    # Ensure prompt is properly formatted\n    if batch.prompt is None and batch.prompt_embeds is None:\n        raise ValueError(\n            \"Either `prompt` or `prompt_embeds` must be provided\")\n\n    # Ensure negative prompt is properly formatted if using classifier-free guidance\n    if (batch.do_classifier_free_guidance and batch.negative_prompt is None\n            and batch.negative_prompt_embeds is None):\n        raise ValueError(\n            \"For classifier-free guidance, either `negative_prompt` or \"\n            \"`negative_prompt_embeds` must be provided\")\n\n    # Validate height and width\n    if batch.height is None or batch.width is None:\n        raise ValueError(\n            \"Height and width must be provided. Please set `height` and `width`.\"\n        )\n    if batch.height % 8 != 0 or batch.width % 8 != 0:\n        raise ValueError(\n            f\"Height and width must be divisible by 8 but are {batch.height} and {batch.width}.\"\n        )\n\n    # Validate number of inference steps\n    if batch.num_inference_steps &lt;= 0:\n        raise ValueError(\n            f\"Number of inference steps must be positive, but got {batch.num_inference_steps}\"\n        )\n\n    # Validate guidance scale if using classifier-free guidance\n    if batch.do_classifier_free_guidance and batch.guidance_scale &lt;= 0:\n        raise ValueError(\n            f\"Guidance scale must be positive, but got {batch.guidance_scale}\"\n        )\n\n    # for i2v, get image from image_path\n    # @TODO(Wei) hard-coded for wan2.2 5b ti2v for now. Should put this in image_encoding stage\n    if batch.image_path is not None:\n        if batch.image_path.endswith(\".mp4\"):\n            image = load_video(batch.image_path)[0]\n        else:\n            image = load_image(batch.image_path)\n        batch.pil_image = image\n\n    # further processing for ti2v task\n    if fastvideo_args.pipeline_config.ti2v_task and batch.pil_image is not None:\n        img = batch.pil_image\n        ih, iw = img.height, img.width\n        patch_size = fastvideo_args.pipeline_config.dit_config.arch_config.patch_size\n        vae_stride = fastvideo_args.pipeline_config.vae_config.arch_config.scale_factor_spatial\n        dh, dw = patch_size[1] * vae_stride, patch_size[2] * vae_stride\n        max_area = 704 * 1280\n        ow, oh = best_output_size(iw, ih, dw, dh, max_area)\n\n        scale = max(ow / iw, oh / ih)\n        img = img.resize((round(iw * scale), round(ih * scale)),\n                         Image.LANCZOS)\n        logger.info(\"resized img height: %s, img width: %s\", img.height,\n                    img.width)\n\n        # center-crop\n        x1 = (img.width - ow) // 2\n        y1 = (img.height - oh) // 2\n        img = img.crop((x1, y1, x1 + ow, y1 + oh))\n        assert img.width == ow and img.height == oh\n\n        # to tensor\n        img = TF.to_tensor(img).sub_(0.5).div_(0.5).to(\n            self.device).unsqueeze(1)\n        img = img.unsqueeze(0)\n        batch.height = oh\n        batch.width = ow\n        batch.pil_image = img\n\n    # for v2v, get control video from video path\n    if batch.video_path is not None:\n        pil_images, original_fps = load_video(batch.video_path,\n                                              return_fps=True)\n        logger.info(\"Loaded video with %s frames, original FPS: %s\",\n                    len(pil_images), original_fps)\n\n        # Get target parameters from batch\n        target_fps = batch.fps\n        target_num_frames = batch.num_frames\n        target_height = batch.height\n        target_width = batch.width\n\n        if target_fps is not None and original_fps is not None:\n            frame_skip = max(1, int(original_fps // target_fps))\n            if frame_skip &gt; 1:\n                pil_images = pil_images[::frame_skip]\n                effective_fps = original_fps / frame_skip\n                logger.info(\n                    \"Resampled video from %.1f fps to %.1f fps (skip=%s)\",\n                    original_fps, effective_fps, frame_skip)\n\n        # Limit to target number of frames\n        if target_num_frames is not None and len(\n                pil_images) &gt; target_num_frames:\n            pil_images = pil_images[:target_num_frames]\n            logger.info(\"Limited video to %s frames (from %s total)\",\n                        target_num_frames, len(pil_images))\n\n        # Resize each PIL image to target dimensions\n        resized_images = []\n        for pil_img in pil_images:\n            resized_img = resize(pil_img,\n                                 target_height,\n                                 target_width,\n                                 resize_mode=\"default\",\n                                 resample=\"lanczos\")\n            resized_images.append(resized_img)\n\n        # Convert PIL images to numpy array\n        video_numpy = pil_to_numpy(resized_images)\n        video_numpy = normalize(video_numpy)\n        video_tensor = numpy_to_pt(video_numpy)\n\n        # Rearrange to [C, T, H, W] and add batch dimension -&gt; [B, C, T, H, W]\n        input_video = video_tensor.permute(1, 0, 2, 3).unsqueeze(0)\n\n        batch.video_latent = input_video\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.input_validation.InputValidationStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify input validation stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/input_validation.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify input validation stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"seed\", batch.seed, [V.not_none, V.positive_int])\n    result.add_check(\"num_videos_per_prompt\", batch.num_videos_per_prompt,\n                     V.positive_int)\n    result.add_check(\n        \"prompt_or_embeds\", None, lambda _: V.string_or_list_strings(\n            batch.prompt) or V.list_not_empty(batch.prompt_embeds))\n    result.add_check(\"height\", batch.height, V.positive_int)\n    result.add_check(\"width\", batch.width, V.positive_int)\n    result.add_check(\"num_inference_steps\", batch.num_inference_steps,\n                     V.positive_int)\n    result.add_check(\n        \"guidance_scale\", batch.guidance_scale, lambda x: not batch.\n        do_classifier_free_guidance or V.positive_float(x))\n    return result\n</code></pre> fastvideo.pipelines.stages.input_validation.InputValidationStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify input validation stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/input_validation.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify input validation stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"seeds\", batch.seeds, V.list_not_empty)\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    return result\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.latent_preparation","title":"fastvideo.pipelines.stages.latent_preparation","text":"<p>Latent preparation stage for diffusion pipelines.</p> Classes\u00b6 fastvideo.pipelines.stages.latent_preparation.LatentPreparationStage \u00b6 <pre><code>LatentPreparationStage(scheduler, transformer)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for preparing initial latent variables for the diffusion process.</p> <p>This stage handles the preparation of the initial latent variables that will be denoised during the diffusion process.</p> Source code in <code>fastvideo/pipelines/stages/latent_preparation.py</code> <pre><code>def __init__(self, scheduler, transformer) -&gt; None:\n    super().__init__()\n    self.scheduler = scheduler\n    self.transformer = transformer\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.latent_preparation.LatentPreparationStage.adjust_video_length \u00b6 <pre><code>adjust_video_length(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; int\n</code></pre> <p>Adjust video length based on VAE version.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The batch with adjusted video length.</p> Source code in <code>fastvideo/pipelines/stages/latent_preparation.py</code> <pre><code>def adjust_video_length(self, batch: ForwardBatch,\n                        fastvideo_args: FastVideoArgs) -&gt; int:\n    \"\"\"\n    Adjust video length based on VAE version.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with adjusted video length.\n    \"\"\"\n\n    video_length = batch.num_frames\n    use_temporal_scaling_frames = fastvideo_args.pipeline_config.vae_config.use_temporal_scaling_frames\n    if use_temporal_scaling_frames:\n        temporal_scale_factor = fastvideo_args.pipeline_config.vae_config.arch_config.temporal_compression_ratio\n        latent_num_frames = (video_length - 1) // temporal_scale_factor + 1\n    else:  # stepvideo only\n        latent_num_frames = video_length // 17 * 3\n    return int(latent_num_frames)\n</code></pre> fastvideo.pipelines.stages.latent_preparation.LatentPreparationStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Prepare initial latent variables for the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with prepared latent variables.</p> Source code in <code>fastvideo/pipelines/stages/latent_preparation.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Prepare initial latent variables for the diffusion process.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with prepared latent variables.\n    \"\"\"\n\n    latent_num_frames = None\n    # Adjust video length based on VAE version if needed\n    if hasattr(self, 'adjust_video_length'):\n        latent_num_frames = self.adjust_video_length(batch, fastvideo_args)\n    # Determine batch size\n    if isinstance(batch.prompt, list):\n        batch_size = len(batch.prompt)\n    elif batch.prompt is not None:\n        batch_size = 1\n    else:\n        batch_size = batch.prompt_embeds[0].shape[0]\n\n    # Adjust batch size for number of videos per prompt\n    batch_size *= batch.num_videos_per_prompt\n\n    # Get required parameters\n    dtype = batch.prompt_embeds[0].dtype\n    device = get_local_torch_device()\n    generator = batch.generator\n    latents = batch.latents\n    num_frames = latent_num_frames if latent_num_frames is not None else batch.num_frames\n    height = batch.height\n    width = batch.width\n\n    # TODO(will): remove this once we add input/output validation for stages\n    if height is None or width is None:\n        raise ValueError(\"Height and width must be provided\")\n\n    # Calculate latent shape\n    shape = (\n        batch_size,\n        self.transformer.num_channels_latents,\n        num_frames,\n        height // fastvideo_args.pipeline_config.vae_config.arch_config.\n        spatial_compression_ratio,\n        width // fastvideo_args.pipeline_config.vae_config.arch_config.\n        spatial_compression_ratio,\n    )\n\n    # Validate generator if it's a list\n    if isinstance(generator, list) and len(generator) != batch_size:\n        raise ValueError(\n            f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n            f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n        )\n    # Generate or use provided latents\n    if latents is None:\n        latents = randn_tensor(shape,\n                               generator=generator,\n                               device=device,\n                               dtype=dtype)\n    else:\n        latents = latents.to(device)\n\n    # Scale the initial noise if needed\n    if hasattr(self.scheduler, \"init_noise_sigma\"):\n        latents = latents * self.scheduler.init_noise_sigma\n    # Update batch with prepared latents\n    batch.latents = latents\n    batch.raw_latent_shape = latents.shape\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.latent_preparation.LatentPreparationStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify latent preparation stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/latent_preparation.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify latent preparation stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\n        \"prompt_or_embeds\", None, lambda _: V.string_or_list_strings(\n            batch.prompt) or V.list_not_empty(batch.prompt_embeds))\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds,\n                     V.list_of_tensors)\n    result.add_check(\"num_videos_per_prompt\", batch.num_videos_per_prompt,\n                     V.positive_int)\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    result.add_check(\"num_frames\", batch.num_frames, V.positive_int)\n    result.add_check(\"height\", batch.height, V.positive_int)\n    result.add_check(\"width\", batch.width, V.positive_int)\n    result.add_check(\"latents\", batch.latents, V.none_or_tensor)\n    return result\n</code></pre> fastvideo.pipelines.stages.latent_preparation.LatentPreparationStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify latent preparation stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/latent_preparation.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify latent preparation stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    result.add_check(\"raw_latent_shape\", batch.raw_latent_shape, V.is_tuple)\n    return result\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.stepvideo_encoding","title":"fastvideo.pipelines.stages.stepvideo_encoding","text":"Classes\u00b6 fastvideo.pipelines.stages.stepvideo_encoding.StepvideoPromptEncodingStage \u00b6 <pre><code>StepvideoPromptEncodingStage(stepllm, clip)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for encoding prompts using the remote caption API.</p> <p>This stage applies the magic string transformations and calls the remote caption service asynchronously to get:   - primary prompt embeddings,   - an attention mask,   - and a clip embedding.</p> Source code in <code>fastvideo/pipelines/stages/stepvideo_encoding.py</code> <pre><code>def __init__(self, stepllm, clip) -&gt; None:\n    super().__init__()\n    # self.caption_client = caption_client  # This should have a call_caption(prompts: List[str]) method.\n    self.stepllm = stepllm\n    self.clip = clip\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.stepvideo_encoding.StepvideoPromptEncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify stepvideo encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/stepvideo_encoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify stepvideo encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"prompt\", batch.prompt, V.string_not_empty)\n    return result\n</code></pre> fastvideo.pipelines.stages.stepvideo_encoding.StepvideoPromptEncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify stepvideo encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/stepvideo_encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify stepvideo encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds,\n                     [V.is_tensor, V.with_dims(3)])\n    result.add_check(\"negative_prompt_embeds\", batch.negative_prompt_embeds,\n                     [V.is_tensor, V.with_dims(3)])\n    result.add_check(\"prompt_attention_mask\", batch.prompt_attention_mask,\n                     [V.is_tensor, V.with_dims(2)])\n    result.add_check(\"negative_attention_mask\",\n                     batch.negative_attention_mask,\n                     [V.is_tensor, V.with_dims(2)])\n    result.add_check(\"clip_embedding_pos\", batch.clip_embedding_pos,\n                     [V.is_tensor, V.with_dims(2)])\n    result.add_check(\"clip_embedding_neg\", batch.clip_embedding_neg,\n                     [V.is_tensor, V.with_dims(2)])\n    return result\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.text_encoding","title":"fastvideo.pipelines.stages.text_encoding","text":"<p>Prompt encoding stages for diffusion pipelines.</p> <p>This module contains implementations of prompt encoding stages for diffusion pipelines.</p> Classes\u00b6 fastvideo.pipelines.stages.text_encoding.TextEncodingStage \u00b6 <pre><code>TextEncodingStage(text_encoders, tokenizers)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for encoding text prompts into embeddings for diffusion models.</p> <p>This stage handles the encoding of text prompts into the embedding space expected by the diffusion model.</p> <p>Initialize the prompt encoding stage.</p> <p>Parameters:</p> Name Type Description Default <code>enable_logging</code> <p>Whether to enable logging for this stage.</p> required <code>is_secondary</code> <p>Whether this is a secondary text encoder.</p> required Source code in <code>fastvideo/pipelines/stages/text_encoding.py</code> <pre><code>def __init__(self, text_encoders, tokenizers) -&gt; None:\n    \"\"\"\n    Initialize the prompt encoding stage.\n\n    Args:\n        enable_logging: Whether to enable logging for this stage.\n        is_secondary: Whether this is a secondary text encoder.\n    \"\"\"\n    super().__init__()\n    self.tokenizers = tokenizers\n    self.text_encoders = text_encoders\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.text_encoding.TextEncodingStage.encode_text \u00b6 <pre><code>encode_text(\n    text: str | list[str],\n    fastvideo_args: FastVideoArgs,\n    encoder_index: int | list[int] | None = None,\n    return_attention_mask: bool = False,\n    return_type: str = \"list\",\n    device: device | str | None = None,\n    dtype: dtype | None = None,\n    max_length: int | None = None,\n    truncation: bool | None = None,\n    padding: bool | str | None = None,\n)\n</code></pre> <p>Encode plain text using selected text encoder(s) and return embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str | list[str]</code> <p>A single string or a list of strings to encode.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments providing pipeline config, including tokenizer and encoder settings, preprocess and postprocess functions.</p> required <code>encoder_index</code> <code>int | list[int] | None</code> <p>Encoder selector by index. Accepts an int or list of ints.</p> <code>None</code> <code>return_attention_mask</code> <code>bool</code> <p>If True, also return attention masks for each selected encoder.</p> <code>False</code> <code>return_type</code> <code>str</code> <p>\"list\" (default) returns a list aligned with selection; \"dict\" returns a dict keyed by encoder index as a string; \"stack\" stacks along a new first dimension (requires matching shapes).</p> <code>'list'</code> <code>device</code> <code>device | str | None</code> <p>Optional device override for inputs; defaults to local torch device.</p> <code>None</code> <code>dtype</code> <code>dtype | None</code> <p>Optional dtype to cast returned embeddings to.</p> <code>None</code> <code>max_length</code> <code>int | None</code> <p>Optional per-call tokenizer override.</p> <code>None</code> <code>truncation</code> <code>bool | None</code> <p>Optional per-call tokenizer override.</p> <code>None</code> <code>padding</code> <code>bool | str | None</code> <p>Optional per-call tokenizer override.</p> <code>None</code> <p>Returns:</p> Type Description <p>Depending on return_type and return_attention_mask:</p> <ul> <li>list: List[Tensor] or (List[Tensor], List[Tensor])</li> </ul> <ul> <li>dict: Dict[str, Tensor] or (Dict[str, Tensor], Dict[str, Tensor])</li> </ul> <ul> <li>stack: Tensor of shape [num_encoders, ...] or a tuple with stacked attention masks</li> </ul> Source code in <code>fastvideo/pipelines/stages/text_encoding.py</code> <pre><code>@torch.no_grad()\ndef encode_text(\n    self,\n    text: str | list[str],\n    fastvideo_args: FastVideoArgs,\n    encoder_index: int | list[int] | None = None,\n    return_attention_mask: bool = False,\n    return_type: str = \"list\",  # one of: \"list\", \"dict\", \"stack\"\n    device: torch.device | str | None = None,\n    dtype: torch.dtype | None = None,\n    max_length: int | None = None,\n    truncation: bool | None = None,\n    padding: bool | str | None = None,\n):\n    \"\"\"\n    Encode plain text using selected text encoder(s) and return embeddings.\n\n    Args:\n        text: A single string or a list of strings to encode.\n        fastvideo_args: The inference arguments providing pipeline config,\n            including tokenizer and encoder settings, preprocess and postprocess\n            functions.\n        encoder_index: Encoder selector by index. Accepts an int or list of ints.\n        return_attention_mask: If True, also return attention masks for each\n            selected encoder.\n        return_type: \"list\" (default) returns a list aligned with selection;\n            \"dict\" returns a dict keyed by encoder index as a string; \"stack\" stacks along a\n            new first dimension (requires matching shapes).\n        device: Optional device override for inputs; defaults to local torch device.\n        dtype: Optional dtype to cast returned embeddings to.\n        max_length: Optional per-call tokenizer override.\n        truncation: Optional per-call tokenizer override.\n        padding: Optional per-call tokenizer override.\n\n    Returns:\n        Depending on return_type and return_attention_mask:\n        - list: List[Tensor] or (List[Tensor], List[Tensor])\n        - dict: Dict[str, Tensor] or (Dict[str, Tensor], Dict[str, Tensor])\n        - stack: Tensor of shape [num_encoders, ...] or a tuple with stacked\n          attention masks\n    \"\"\"\n\n    assert len(self.tokenizers) == len(self.text_encoders)\n    assert len(self.text_encoders) == len(\n        fastvideo_args.pipeline_config.text_encoder_configs)\n\n    # Resolve selection into indices\n    encoder_cfgs = fastvideo_args.pipeline_config.text_encoder_configs\n    if encoder_index is None:\n        indices: list[int] = [0]\n    elif isinstance(encoder_index, int):\n        indices = [encoder_index]\n    else:\n        indices = list(encoder_index)\n    # validate range\n    num_encoders = len(self.text_encoders)\n    for idx in indices:\n        if idx &lt; 0 or idx &gt;= num_encoders:\n            raise IndexError(\n                f\"encoder index {idx} out of range [0, {num_encoders-1}]\")\n\n    # Validate indices are within range\n    num_encoders = len(self.text_encoders)\n\n    # Normalize input to list[str]\n    assert isinstance(text, str | list)\n    if isinstance(text, str):\n        texts: list[str] = [text]\n    else:\n        texts = text\n\n    embeds_list: list[torch.Tensor] = []\n    attn_masks_list: list[torch.Tensor] = []\n\n    preprocess_funcs = fastvideo_args.pipeline_config.preprocess_text_funcs\n    postprocess_funcs = fastvideo_args.pipeline_config.postprocess_text_funcs\n    encoder_cfgs = fastvideo_args.pipeline_config.text_encoder_configs\n\n    if return_type not in (\"list\", \"dict\", \"stack\"):\n        raise ValueError(\n            f\"Invalid return_type '{return_type}'. Expected one of: 'list', 'dict', 'stack'\"\n        )\n\n    target_device = device if device is not None else get_local_torch_device(\n    )\n\n    for i in indices:\n        tokenizer = self.tokenizers[i]\n        text_encoder = self.text_encoders[i]\n        encoder_config = encoder_cfgs[i]\n        preprocess_func = preprocess_funcs[i]\n        postprocess_func = postprocess_funcs[i]\n\n        processed_texts: list[str] = []\n        for prompt_str in texts:\n            processed_texts.append(preprocess_func(prompt_str))\n\n        tok_kwargs = dict(encoder_config.tokenizer_kwargs)\n        if max_length is not None:\n            tok_kwargs[\"max_length\"] = max_length\n        if truncation is not None:\n            tok_kwargs[\"truncation\"] = truncation\n        if padding is not None:\n            tok_kwargs[\"padding\"] = padding\n\n        text_inputs = tokenizer(processed_texts,\n                                **tok_kwargs).to(target_device)\n\n        input_ids = text_inputs[\"input_ids\"]\n        attention_mask = text_inputs[\"attention_mask\"]\n\n        with set_forward_context(current_timestep=0, attn_metadata=None):\n            outputs = text_encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                output_hidden_states=True,\n            )\n\n        prompt_embeds = postprocess_func(outputs)\n        if dtype is not None:\n            prompt_embeds = prompt_embeds.to(dtype=dtype)\n        embeds_list.append(prompt_embeds)\n        if return_attention_mask:\n            attn_masks_list.append(attention_mask)\n\n    # Shape results according to return_type\n    if return_type == \"list\":\n        if return_attention_mask:\n            return embeds_list, attn_masks_list\n        return embeds_list\n\n    if return_type == \"dict\":\n        key_strs = [str(i) for i in indices]\n        embeds_dict = {\n            k: v\n            for k, v in zip(key_strs, embeds_list, strict=False)\n        }\n        if return_attention_mask:\n            attn_dict = {\n                k: v\n                for k, v in zip(key_strs, attn_masks_list, strict=False)\n            }\n            return embeds_dict, attn_dict\n        return embeds_dict\n\n    # return_type == \"stack\"\n    # Validate shapes are compatible\n    base_shape = list(embeds_list[0].shape)\n    for t in embeds_list[1:]:\n        if list(t.shape) != base_shape:\n            raise ValueError(\n                f\"Cannot stack embeddings with differing shapes: {[list(t.shape) for t in embeds_list]}\"\n            )\n    stacked_embeds = torch.stack(embeds_list, dim=0)\n    if return_attention_mask:\n        base_mask_shape = list(attn_masks_list[0].shape)\n        for m in attn_masks_list[1:]:\n            if list(m.shape) != base_mask_shape:\n                raise ValueError(\n                    f\"Cannot stack attention masks with differing shapes: {[list(m.shape) for m in attn_masks_list]}\"\n                )\n        stacked_masks = torch.stack(attn_masks_list, dim=0)\n        return stacked_embeds, stacked_masks\n    return stacked_embeds\n</code></pre> fastvideo.pipelines.stages.text_encoding.TextEncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode the prompt into text encoder hidden states.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded prompt embeddings.</p> Source code in <code>fastvideo/pipelines/stages/text_encoding.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode the prompt into text encoder hidden states.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded prompt embeddings.\n    \"\"\"\n    assert len(self.tokenizers) == len(self.text_encoders)\n    assert len(self.text_encoders) == len(\n        fastvideo_args.pipeline_config.text_encoder_configs)\n\n    # Encode positive prompt with all available encoders\n    assert batch.prompt is not None\n    prompt_text: str | list[str] = batch.prompt\n    all_indices: list[int] = list(range(len(self.text_encoders)))\n    prompt_embeds_list, prompt_masks_list = self.encode_text(\n        prompt_text,\n        fastvideo_args,\n        encoder_index=all_indices,\n        return_attention_mask=True,\n    )\n    for pe in prompt_embeds_list:\n        batch.prompt_embeds.append(pe)\n    if batch.prompt_attention_mask is not None:\n        for am in prompt_masks_list:\n            batch.prompt_attention_mask.append(am)\n\n    # Encode negative prompt if CFG is enabled\n    if batch.do_classifier_free_guidance:\n        assert isinstance(batch.negative_prompt, str)\n        neg_embeds_list, neg_masks_list = self.encode_text(\n            batch.negative_prompt,\n            fastvideo_args,\n            encoder_index=all_indices,\n            return_attention_mask=True,\n        )\n        assert batch.negative_prompt_embeds is not None\n        for ne in neg_embeds_list:\n            batch.negative_prompt_embeds.append(ne)\n        if batch.negative_attention_mask is not None:\n            for nm in neg_masks_list:\n                batch.negative_attention_mask.append(nm)\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.text_encoding.TextEncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify text encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/text_encoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify text encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"prompt\", batch.prompt, V.string_or_list_strings)\n    result.add_check(\n        \"negative_prompt\", batch.negative_prompt, lambda x: not batch.\n        do_classifier_free_guidance or V.string_not_empty(x))\n    result.add_check(\"do_classifier_free_guidance\",\n                     batch.do_classifier_free_guidance, V.bool_value)\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds, V.is_list)\n    result.add_check(\"negative_prompt_embeds\", batch.negative_prompt_embeds,\n                     V.none_or_list)\n    return result\n</code></pre> fastvideo.pipelines.stages.text_encoding.TextEncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify text encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/text_encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify text encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds,\n                     V.list_of_tensors_min_dims(2))\n    result.add_check(\n        \"negative_prompt_embeds\", batch.negative_prompt_embeds,\n        lambda x: not batch.do_classifier_free_guidance or V.\n        list_of_tensors_with_min_dims(x, 2))\n    return result\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.timestep_preparation","title":"fastvideo.pipelines.stages.timestep_preparation","text":"<p>Timestep preparation stages for diffusion pipelines.</p> <p>This module contains implementations of timestep preparation stages for diffusion pipelines.</p> Classes\u00b6 fastvideo.pipelines.stages.timestep_preparation.TimestepPreparationStage \u00b6 <pre><code>TimestepPreparationStage(scheduler)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for preparing timesteps for the diffusion process.</p> <p>This stage handles the preparation of the timestep sequence that will be used during the diffusion process.</p> Source code in <code>fastvideo/pipelines/stages/timestep_preparation.py</code> <pre><code>def __init__(self, scheduler) -&gt; None:\n    self.scheduler = scheduler\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.timestep_preparation.TimestepPreparationStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Prepare timesteps for the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with prepared timesteps.</p> Source code in <code>fastvideo/pipelines/stages/timestep_preparation.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Prepare timesteps for the diffusion process.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with prepared timesteps.\n    \"\"\"\n    scheduler = self.scheduler\n    device = get_local_torch_device()\n    num_inference_steps = batch.num_inference_steps\n    timesteps = batch.timesteps\n    sigmas = batch.sigmas\n    n_tokens = batch.n_tokens\n\n    # Prepare extra kwargs for set_timesteps\n    extra_set_timesteps_kwargs = {}\n    if n_tokens is not None and \"n_tokens\" in inspect.signature(\n            scheduler.set_timesteps).parameters:\n        extra_set_timesteps_kwargs[\"n_tokens\"] = n_tokens\n\n    # Handle custom timesteps or sigmas\n    if timesteps is not None and sigmas is not None:\n        raise ValueError(\n            \"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\"\n        )\n\n    if timesteps is not None:\n        accepts_timesteps = \"timesteps\" in inspect.signature(\n            scheduler.set_timesteps).parameters\n        if not accepts_timesteps:\n            raise ValueError(\n                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n            )\n        scheduler.set_timesteps(timesteps=timesteps,\n                                device=device,\n                                **extra_set_timesteps_kwargs)\n        timesteps = scheduler.timesteps\n    elif sigmas is not None:\n        accept_sigmas = \"sigmas\" in inspect.signature(\n            scheduler.set_timesteps).parameters\n        if not accept_sigmas:\n            raise ValueError(\n                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n                f\" sigmas schedules. Please check whether you are using the correct scheduler.\"\n            )\n        scheduler.set_timesteps(sigmas=sigmas,\n                                device=device,\n                                **extra_set_timesteps_kwargs)\n        timesteps = scheduler.timesteps\n    else:\n        scheduler.set_timesteps(num_inference_steps,\n                                device=device,\n                                **extra_set_timesteps_kwargs)\n        timesteps = scheduler.timesteps\n\n    # Update batch with prepared timesteps\n    batch.timesteps = timesteps\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.timestep_preparation.TimestepPreparationStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify timestep preparation stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/timestep_preparation.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify timestep preparation stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"num_inference_steps\", batch.num_inference_steps,\n                     V.positive_int)\n    result.add_check(\"timesteps\", batch.timesteps, V.none_or_tensor)\n    result.add_check(\"sigmas\", batch.sigmas, V.none_or_list)\n    result.add_check(\"n_tokens\", batch.n_tokens, V.none_or_positive_int)\n    return result\n</code></pre> fastvideo.pipelines.stages.timestep_preparation.TimestepPreparationStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify timestep preparation stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/timestep_preparation.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify timestep preparation stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"timesteps\", batch.timesteps,\n                     [V.is_tensor, V.with_dims(1)])\n    return result\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.validators","title":"fastvideo.pipelines.stages.validators","text":"<p>Common validators for pipeline stage verification.</p> <p>This module provides reusable validation functions that can be used across all pipeline stages for input/output verification.</p> Classes\u00b6 fastvideo.pipelines.stages.validators.StageValidators \u00b6 <p>Common validators for pipeline stages.</p> Functions\u00b6 fastvideo.pipelines.stages.validators.StageValidators.bool_value <code>staticmethod</code> \u00b6 <pre><code>bool_value(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a boolean.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef bool_value(value: Any) -&gt; bool:\n    \"\"\"Check if value is a boolean.\"\"\"\n    return isinstance(value, bool)\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.divisible <code>staticmethod</code> \u00b6 <pre><code>divisible(divisor: int) -&gt; Callable[[Any], bool]\n</code></pre> <p>Return a validator that checks if value is divisible by divisor.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef divisible(divisor: int) -&gt; Callable[[Any], bool]:\n    \"\"\"Return a validator that checks if value is divisible by divisor.\"\"\"\n\n    def validator(value: Any) -&gt; bool:\n        return StageValidators.divisible_by(value, divisor)\n\n    return validator\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.divisible_by <code>staticmethod</code> \u00b6 <pre><code>divisible_by(value: Any, divisor: int) -&gt; bool\n</code></pre> <p>Check if value is divisible by divisor.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef divisible_by(value: Any, divisor: int) -&gt; bool:\n    \"\"\"Check if value is divisible by divisor.\"\"\"\n    return value is not None and isinstance(value,\n                                            int) and value % divisor == 0\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.generator_or_list_generators <code>staticmethod</code> \u00b6 <pre><code>generator_or_list_generators(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a Generator or list of Generators.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef generator_or_list_generators(value: Any) -&gt; bool:\n    \"\"\"Check if value is a Generator or list of Generators.\"\"\"\n    if isinstance(value, torch.Generator):\n        return True\n    if isinstance(value, list):\n        return all(isinstance(item, torch.Generator) for item in value)\n    return False\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.is_list <code>staticmethod</code> \u00b6 <pre><code>is_list(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a list (can be empty).</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef is_list(value: Any) -&gt; bool:\n    \"\"\"Check if value is a list (can be empty).\"\"\"\n    return isinstance(value, list)\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.is_tensor <code>staticmethod</code> \u00b6 <pre><code>is_tensor(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a torch tensor and doesn't contain NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef is_tensor(value: Any) -&gt; bool:\n    \"\"\"Check if value is a torch tensor and doesn't contain NaN values.\"\"\"\n    if not isinstance(value, torch.Tensor):\n        return False\n    return not torch.isnan(value).any().item()\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.is_tuple <code>staticmethod</code> \u00b6 <pre><code>is_tuple(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a tuple.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef is_tuple(value: Any) -&gt; bool:\n    \"\"\"Check if value is a tuple.\"\"\"\n    return isinstance(value, tuple)\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.list_length <code>staticmethod</code> \u00b6 <pre><code>list_length(value: Any, length: int) -&gt; bool\n</code></pre> <p>Check if list has specific length.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef list_length(value: Any, length: int) -&gt; bool:\n    \"\"\"Check if list has specific length.\"\"\"\n    return isinstance(value, list) and len(value) == length\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.list_min_length <code>staticmethod</code> \u00b6 <pre><code>list_min_length(value: Any, min_length: int) -&gt; bool\n</code></pre> <p>Check if list has at least min_length items.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef list_min_length(value: Any, min_length: int) -&gt; bool:\n    \"\"\"Check if list has at least min_length items.\"\"\"\n    return isinstance(value, list) and len(value) &gt;= min_length\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.list_not_empty <code>staticmethod</code> \u00b6 <pre><code>list_not_empty(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a non-empty list.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef list_not_empty(value: Any) -&gt; bool:\n    \"\"\"Check if value is a non-empty list.\"\"\"\n    return isinstance(value, list) and len(value) &gt; 0\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.list_of_tensors <code>staticmethod</code> \u00b6 <pre><code>list_of_tensors(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a non-empty list where all items are tensors without NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef list_of_tensors(value: Any) -&gt; bool:\n    \"\"\"Check if value is a non-empty list where all items are tensors without NaN values.\"\"\"\n    if not isinstance(value, list) or len(value) == 0:\n        return False\n    for item in value:\n        if not isinstance(item, torch.Tensor):\n            return False\n        if torch.isnan(item).any().item():\n            return False\n    return True\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.list_of_tensors_dims <code>staticmethod</code> \u00b6 <pre><code>list_of_tensors_dims(dims: int) -&gt; Callable[[Any], bool]\n</code></pre> <p>Return a validator that checks if value is a list of tensors with specific dimensions and no NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef list_of_tensors_dims(dims: int) -&gt; Callable[[Any], bool]:\n    \"\"\"Return a validator that checks if value is a list of tensors with specific dimensions and no NaN values.\"\"\"\n\n    def validator(value: Any) -&gt; bool:\n        return StageValidators.list_of_tensors_with_dims(value, dims)\n\n    return validator\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.list_of_tensors_min_dims <code>staticmethod</code> \u00b6 <pre><code>list_of_tensors_min_dims(\n    min_dims: int,\n) -&gt; Callable[[Any], bool]\n</code></pre> <p>Return a validator that checks if value is a list of tensors with at least min_dims dimensions and no NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef list_of_tensors_min_dims(min_dims: int) -&gt; Callable[[Any], bool]:\n    \"\"\"Return a validator that checks if value is a list of tensors with at least min_dims dimensions and no NaN values.\"\"\"\n\n    def validator(value: Any) -&gt; bool:\n        return StageValidators.list_of_tensors_with_min_dims(\n            value, min_dims)\n\n    return validator\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.list_of_tensors_with_dims <code>staticmethod</code> \u00b6 <pre><code>list_of_tensors_with_dims(value: Any, dims: int) -&gt; bool\n</code></pre> <p>Check if value is a non-empty list where all items are tensors with specific dimensions and no NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef list_of_tensors_with_dims(value: Any, dims: int) -&gt; bool:\n    \"\"\"Check if value is a non-empty list where all items are tensors with specific dimensions and no NaN values.\"\"\"\n    if not isinstance(value, list) or len(value) == 0:\n        return False\n    for item in value:\n        if not isinstance(item, torch.Tensor):\n            return False\n        if item.dim() != dims:\n            return False\n        if torch.isnan(item).any().item():\n            return False\n    return True\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.list_of_tensors_with_min_dims <code>staticmethod</code> \u00b6 <pre><code>list_of_tensors_with_min_dims(\n    value: Any, min_dims: int\n) -&gt; bool\n</code></pre> <p>Check if value is a non-empty list where all items are tensors with at least min_dims dimensions and no NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef list_of_tensors_with_min_dims(value: Any, min_dims: int) -&gt; bool:\n    \"\"\"Check if value is a non-empty list where all items are tensors with at least min_dims dimensions and no NaN values.\"\"\"\n    if not isinstance(value, list) or len(value) == 0:\n        return False\n    for item in value:\n        if not isinstance(item, torch.Tensor):\n            return False\n        if item.dim() &lt; min_dims:\n            return False\n        if torch.isnan(item).any().item():\n            return False\n    return True\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.min_dims <code>staticmethod</code> \u00b6 <pre><code>min_dims(min_dims: int) -&gt; Callable[[Any], bool]\n</code></pre> <p>Return a validator that checks if tensor has at least min_dims dimensions and no NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef min_dims(min_dims: int) -&gt; Callable[[Any], bool]:\n    \"\"\"Return a validator that checks if tensor has at least min_dims dimensions and no NaN values.\"\"\"\n\n    def validator(value: Any) -&gt; bool:\n        return StageValidators.tensor_min_dims(value, min_dims)\n\n    return validator\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.non_negative_float <code>staticmethod</code> \u00b6 <pre><code>non_negative_float(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a non-negative float.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef non_negative_float(value: Any) -&gt; bool:\n    \"\"\"Check if value is a non-negative float.\"\"\"\n    return isinstance(value, int | float) and value &gt;= 0\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.none_or_list <code>staticmethod</code> \u00b6 <pre><code>none_or_list(value: Any) -&gt; bool\n</code></pre> <p>Check if value is None or a list.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef none_or_list(value: Any) -&gt; bool:\n    \"\"\"Check if value is None or a list.\"\"\"\n    return value is None or isinstance(value, list)\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.none_or_positive_int <code>staticmethod</code> \u00b6 <pre><code>none_or_positive_int(value: Any) -&gt; bool\n</code></pre> <p>Check if value is None or a positive integer.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef none_or_positive_int(value: Any) -&gt; bool:\n    \"\"\"Check if value is None or a positive integer.\"\"\"\n    return value is None or (isinstance(value, int) and value &gt; 0)\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.none_or_tensor <code>staticmethod</code> \u00b6 <pre><code>none_or_tensor(value: Any) -&gt; bool\n</code></pre> <p>Check if value is None or a tensor without NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef none_or_tensor(value: Any) -&gt; bool:\n    \"\"\"Check if value is None or a tensor without NaN values.\"\"\"\n    if value is None:\n        return True\n    if not isinstance(value, torch.Tensor):\n        return False\n    return not torch.isnan(value).any().item()\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.none_or_tensor_with_dims <code>staticmethod</code> \u00b6 <pre><code>none_or_tensor_with_dims(\n    dims: int,\n) -&gt; Callable[[Any], bool]\n</code></pre> <p>Return a validator that checks if value is None or a tensor with specific dimensions and no NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef none_or_tensor_with_dims(dims: int) -&gt; Callable[[Any], bool]:\n    \"\"\"Return a validator that checks if value is None or a tensor with specific dimensions and no NaN values.\"\"\"\n\n    def validator(value: Any) -&gt; bool:\n        if value is None:\n            return True\n        if not isinstance(value, torch.Tensor):\n            return False\n        if value.dim() != dims:\n            return False\n        return not torch.isnan(value).any().item()\n\n    return validator\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.not_none <code>staticmethod</code> \u00b6 <pre><code>not_none(value: Any) -&gt; bool\n</code></pre> <p>Check if value is not None.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef not_none(value: Any) -&gt; bool:\n    \"\"\"Check if value is not None.\"\"\"\n    return value is not None\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.positive_float <code>staticmethod</code> \u00b6 <pre><code>positive_float(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a positive float.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef positive_float(value: Any) -&gt; bool:\n    \"\"\"Check if value is a positive float.\"\"\"\n    return isinstance(value, int | float) and value &gt; 0\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.positive_int <code>staticmethod</code> \u00b6 <pre><code>positive_int(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a positive integer.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef positive_int(value: Any) -&gt; bool:\n    \"\"\"Check if value is a positive integer.\"\"\"\n    return isinstance(value, int) and value &gt; 0\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.positive_int_divisible <code>staticmethod</code> \u00b6 <pre><code>positive_int_divisible(\n    divisor: int,\n) -&gt; Callable[[Any], bool]\n</code></pre> <p>Return a validator that checks if value is a positive integer divisible by divisor.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef positive_int_divisible(divisor: int) -&gt; Callable[[Any], bool]:\n    \"\"\"Return a validator that checks if value is a positive integer divisible by divisor.\"\"\"\n\n    def validator(value: Any) -&gt; bool:\n        return (isinstance(value, int) and value &gt; 0\n                and StageValidators.divisible_by(value, divisor))\n\n    return validator\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.string_not_empty <code>staticmethod</code> \u00b6 <pre><code>string_not_empty(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a non-empty string.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef string_not_empty(value: Any) -&gt; bool:\n    \"\"\"Check if value is a non-empty string.\"\"\"\n    return isinstance(value, str) and len(value.strip()) &gt; 0\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.string_or_list_strings <code>staticmethod</code> \u00b6 <pre><code>string_or_list_strings(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a string or list of strings.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef string_or_list_strings(value: Any) -&gt; bool:\n    \"\"\"Check if value is a string or list of strings.\"\"\"\n    if isinstance(value, str):\n        return True\n    if isinstance(value, list):\n        return all(isinstance(item, str) for item in value)\n    return False\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.tensor_min_dims <code>staticmethod</code> \u00b6 <pre><code>tensor_min_dims(value: Any, min_dims: int) -&gt; bool\n</code></pre> <p>Check if value is a tensor with at least min_dims dimensions and no NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef tensor_min_dims(value: Any, min_dims: int) -&gt; bool:\n    \"\"\"Check if value is a tensor with at least min_dims dimensions and no NaN values.\"\"\"\n    if not isinstance(value, torch.Tensor):\n        return False\n    if value.dim() &lt; min_dims:\n        return False\n    return not torch.isnan(value).any().item()\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.tensor_shape_matches <code>staticmethod</code> \u00b6 <pre><code>tensor_shape_matches(\n    value: Any, expected_shape: tuple\n) -&gt; bool\n</code></pre> <p>Check if tensor shape matches expected shape (None for any size) and no NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef tensor_shape_matches(value: Any, expected_shape: tuple) -&gt; bool:\n    \"\"\"Check if tensor shape matches expected shape (None for any size) and no NaN values.\"\"\"\n    if not isinstance(value, torch.Tensor):\n        return False\n    if len(value.shape) != len(expected_shape):\n        return False\n    for actual, expected in zip(value.shape, expected_shape, strict=True):\n        if expected is not None and actual != expected:\n            return False\n    return not torch.isnan(value).any().item()\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.tensor_with_dims <code>staticmethod</code> \u00b6 <pre><code>tensor_with_dims(value: Any, dims: int) -&gt; bool\n</code></pre> <p>Check if value is a tensor with specific dimensions and no NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef tensor_with_dims(value: Any, dims: int) -&gt; bool:\n    \"\"\"Check if value is a tensor with specific dimensions and no NaN values.\"\"\"\n    if not isinstance(value, torch.Tensor):\n        return False\n    if value.dim() != dims:\n        return False\n    return not torch.isnan(value).any().item()\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.with_dims <code>staticmethod</code> \u00b6 <pre><code>with_dims(dims: int) -&gt; Callable[[Any], bool]\n</code></pre> <p>Return a validator that checks if tensor has specific dimensions and no NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef with_dims(dims: int) -&gt; Callable[[Any], bool]:\n    \"\"\"Return a validator that checks if tensor has specific dimensions and no NaN values.\"\"\"\n\n    def validator(value: Any) -&gt; bool:\n        return StageValidators.tensor_with_dims(value, dims)\n\n    return validator\n</code></pre> fastvideo.pipelines.stages.validators.ValidationFailure \u00b6 <pre><code>ValidationFailure(\n    validator_name: str,\n    actual_value: Any,\n    expected: str | None = None,\n    error_msg: str | None = None,\n)\n</code></pre> <p>Details about a specific validation failure.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>def __init__(self,\n             validator_name: str,\n             actual_value: Any,\n             expected: str | None = None,\n             error_msg: str | None = None):\n    self.validator_name = validator_name\n    self.actual_value = actual_value\n    self.expected = expected\n    self.error_msg = error_msg\n</code></pre> fastvideo.pipelines.stages.validators.VerificationResult \u00b6 <pre><code>VerificationResult()\n</code></pre> <p>Wrapper class for stage verification results.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>def __init__(self) -&gt; None:\n    self._checks: dict[str, bool] = {}\n    self._failures: dict[str, list[ValidationFailure]] = {}\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.validators.VerificationResult.add_check \u00b6 <pre><code>add_check(\n    field_name: str,\n    value: Any,\n    validators: Callable[[Any], bool]\n    | list[Callable[[Any], bool]],\n) -&gt; VerificationResult\n</code></pre> <p>Add a validation check for a field.</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the field being checked</p> required <code>value</code> <code>Any</code> <p>The actual value to validate</p> required <code>validators</code> <code>Callable[[Any], bool] | list[Callable[[Any], bool]]</code> <p>Single validation function or list of validation functions.        Each function will be called with the value as its first argument.</p> required <p>Returns:</p> Type Description <code>VerificationResult</code> <p>Self for method chaining</p> <p>Examples:</p> fastvideo.pipelines.stages.validators.VerificationResult.get_detailed_failures \u00b6 <pre><code>get_detailed_failures() -&gt; dict[\n    str, list[ValidationFailure]\n]\n</code></pre> <p>Get detailed failure information for each failed field.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>def get_detailed_failures(self) -&gt; dict[str, list[ValidationFailure]]:\n    \"\"\"Get detailed failure information for each failed field.\"\"\"\n    return self._failures.copy()\n</code></pre> fastvideo.pipelines.stages.validators.VerificationResult.get_failed_fields \u00b6 <pre><code>get_failed_fields() -&gt; list[str]\n</code></pre> <p>Get list of fields that failed validation.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>def get_failed_fields(self) -&gt; list[str]:\n    \"\"\"Get list of fields that failed validation.\"\"\"\n    return [field for field, passed in self._checks.items() if not passed]\n</code></pre> fastvideo.pipelines.stages.validators.VerificationResult.get_failure_summary \u00b6 <pre><code>get_failure_summary() -&gt; str\n</code></pre> <p>Get a comprehensive summary of all validation failures.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>def get_failure_summary(self) -&gt; str:\n    \"\"\"Get a comprehensive summary of all validation failures.\"\"\"\n    if self.is_valid():\n        return \"All validations passed\"\n\n    summary_parts = []\n    for field_name, failures in self._failures.items():\n        field_summary = f\"\\n  Field '{field_name}':\"\n        for i, failure in enumerate(failures, 1):\n            field_summary += f\"\\n    {i}. {failure}\"\n        summary_parts.append(field_summary)\n\n    return \"Validation failures:\" + \"\".join(summary_parts)\n</code></pre> fastvideo.pipelines.stages.validators.VerificationResult.is_valid \u00b6 <pre><code>is_valid() -&gt; bool\n</code></pre> <p>Check if all validations passed.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>def is_valid(self) -&gt; bool:\n    \"\"\"Check if all validations passed.\"\"\"\n    return all(self._checks.values())\n</code></pre> fastvideo.pipelines.stages.validators.VerificationResult.to_dict \u00b6 <pre><code>to_dict() -&gt; dict\n</code></pre> <p>Convert to dictionary for backward compatibility.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Convert to dictionary for backward compatibility.\"\"\"\n    return self._checks.copy()\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.validators.VerificationResult.add_check--single-validator","title":"Single validator","text":"<p>result.add_check(\"tensor\", my_tensor, V.is_tensor)</p>"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.validators.VerificationResult.add_check--multiple-validators-all-must-pass","title":"Multiple validators (all must pass)","text":"<p>result.add_check(\"latents\", batch.latents, [V.is_tensor, V.with_dims(5)])</p>"},{"location":"api/fastvideo/#fastvideo.pipelines.stages.validators.VerificationResult.add_check--using-partial-functions-for-parameters","title":"Using partial functions for parameters","text":"<p>result.add_check(\"height\", batch.height, [V.not_none, V.divisible(8)])</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>def add_check(\n    self, field_name: str, value: Any,\n    validators: Callable[[Any], bool] | list[Callable[[Any], bool]]\n) -&gt; 'VerificationResult':\n    \"\"\"\n    Add a validation check for a field.\n\n    Args:\n        field_name: Name of the field being checked\n        value: The actual value to validate\n        validators: Single validation function or list of validation functions.\n                   Each function will be called with the value as its first argument.\n\n    Returns:\n        Self for method chaining\n\n    Examples:\n        # Single validator\n        result.add_check(\"tensor\", my_tensor, V.is_tensor)\n\n        # Multiple validators (all must pass)\n        result.add_check(\"latents\", batch.latents, [V.is_tensor, V.with_dims(5)])\n\n        # Using partial functions for parameters\n        result.add_check(\"height\", batch.height, [V.not_none, V.divisible(8)])\n    \"\"\"\n    if not isinstance(validators, list):\n        validators = [validators]\n\n    failures = []\n    all_passed = True\n\n    # Apply all validators and collect detailed failure info\n    for validator in validators:\n        try:\n            passed = validator(value)\n            if not passed:\n                all_passed = False\n                failure = self._create_validation_failure(validator, value)\n                failures.append(failure)\n        except Exception as e:\n            # If any validator raises an exception, consider the check failed\n            all_passed = False\n            validator_name = getattr(validator, '__name__', str(validator))\n            failure = ValidationFailure(\n                validator_name=validator_name,\n                actual_value=value,\n                error_msg=f\"Exception during validation: {str(e)}\")\n            failures.append(failure)\n\n    self._checks[field_name] = all_passed\n    if not all_passed:\n        self._failures[field_name] = failures\n\n    return self\n</code></pre>"},{"location":"api/fastvideo/#fastvideotraining","title":"fastvideo.training","text":""},{"location":"api/fastvideo/#fastvideo.training","title":"training","text":""},{"location":"api/fastvideo/#fastvideo.training-classes","title":"Classes","text":""},{"location":"api/fastvideo/#fastvideo.training.DistillationPipeline","title":"fastvideo.training.DistillationPipeline","text":"<pre><code>DistillationPipeline(\n    model_path: str,\n    fastvideo_args: TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>TrainingPipeline</code></p> <p>A distillation pipeline for training a 3 step model. Inherits from TrainingPipeline to reuse training infrastructure.</p> Source code in <code>fastvideo/training/training_pipeline.py</code> <pre><code>def __init__(\n        self,\n        model_path: str,\n        fastvideo_args: TrainingArgs,\n        required_config_modules: list[str] | None = None,\n        loaded_modules: dict[str, torch.nn.Module] | None = None) -&gt; None:\n    fastvideo_args.inference_mode = False\n    self.lora_training = fastvideo_args.lora_training\n    if self.lora_training and fastvideo_args.lora_rank is None:\n        raise ValueError(\"lora rank must be set when using lora training\")\n\n    set_random_seed(fastvideo_args.seed)  # for lora param init\n    super().__init__(model_path, fastvideo_args, required_config_modules,\n                     loaded_modules)  # type: ignore\n    self.tracker = DummyTracker()\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.training.DistillationPipeline-functions","title":"Functions","text":"fastvideo.training.DistillationPipeline.apply_ema_to_model \u00b6 <pre><code>apply_ema_to_model(model)\n</code></pre> <p>Apply EMA weights to the model for validation or inference.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def apply_ema_to_model(self, model):\n    \"\"\"Apply EMA weights to the model for validation or inference.\"\"\"\n    if model is self.transformer and self.generator_ema is not None:\n        with self.generator_ema.apply_to_model(model):\n            return model\n    elif model is self.transformer_2 and self.generator_ema_2 is not None:\n        with self.generator_ema_2.apply_to_model(model):\n            return model\n    return model\n</code></pre> fastvideo.training.DistillationPipeline.get_ema_2_model_copy \u00b6 <pre><code>get_ema_2_model_copy() -&gt; torch.nn.Module | None\n</code></pre> <p>Get a copy of the transformer_2 model with EMA weights applied.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def get_ema_2_model_copy(self) -&gt; torch.nn.Module | None:\n    \"\"\"Get a copy of the transformer_2 model with EMA weights applied.\"\"\"\n    if self.generator_ema_2 is not None and self.transformer_2 is not None:\n        ema_2_model = copy.deepcopy(self.transformer_2)\n        self.generator_ema_2.copy_to_unwrapped(ema_2_model)\n        return ema_2_model\n    return None\n</code></pre> fastvideo.training.DistillationPipeline.get_ema_model_copy \u00b6 <pre><code>get_ema_model_copy() -&gt; torch.nn.Module | None\n</code></pre> <p>Get a copy of the model with EMA weights applied.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def get_ema_model_copy(self) -&gt; torch.nn.Module | None:\n    \"\"\"Get a copy of the model with EMA weights applied.\"\"\"\n    if self.generator_ema is not None:\n        ema_model = copy.deepcopy(self.transformer)\n        self.generator_ema.copy_to_unwrapped(ema_model)\n        return ema_model\n    return None\n</code></pre> fastvideo.training.DistillationPipeline.get_ema_stats \u00b6 <pre><code>get_ema_stats() -&gt; dict[str, Any]\n</code></pre> <p>Get EMA statistics for monitoring.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def get_ema_stats(self) -&gt; dict[str, Any]:\n    \"\"\"Get EMA statistics for monitoring.\"\"\"\n    ema_enabled = self.generator_ema is not None\n    ema_2_enabled = self.generator_ema_2 is not None\n\n    if not ema_enabled and not ema_2_enabled:\n        return {\n            \"ema_enabled\": False,\n            \"ema_2_enabled\": False,\n            \"ema_decay\": None,\n            \"ema_start_step\": self.training_args.ema_start_step,\n            \"ema_ready\": False,\n            \"ema_2_ready\": False,\n            \"ema_step\": self.current_trainstep,\n        }\n\n    return {\n        \"ema_enabled\": ema_enabled,\n        \"ema_2_enabled\": ema_2_enabled,\n        \"ema_decay\": self.training_args.ema_decay,\n        \"ema_start_step\": self.training_args.ema_start_step,\n        \"ema_ready\": self.is_ema_ready() if ema_enabled else False,\n        \"ema_2_ready\": self.is_ema_ready() if ema_2_enabled else False,\n        \"ema_step\": self.current_trainstep,\n    }\n</code></pre> fastvideo.training.DistillationPipeline.initialize_training_pipeline \u00b6 <pre><code>initialize_training_pipeline(training_args: TrainingArgs)\n</code></pre> <p>Initialize the distillation training pipeline with multiple models.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def initialize_training_pipeline(self, training_args: TrainingArgs):\n    \"\"\"Initialize the distillation training pipeline with multiple models.\"\"\"\n    logger.info(\"Initializing distillation pipeline...\")\n\n    super().initialize_training_pipeline(training_args)\n\n    self.noise_scheduler = self.get_module(\"scheduler\")\n    self.vae = self.get_module(\"vae\")\n    self.vae.requires_grad_(False)\n\n    self.timestep_shift = self.training_args.pipeline_config.flow_shift\n    self.noise_scheduler = FlowMatchEulerDiscreteScheduler(\n        shift=self.timestep_shift)\n\n    if self.training_args.boundary_ratio is not None:\n        self.boundary_timestep = self.training_args.boundary_ratio * self.noise_scheduler.num_train_timesteps\n    else:\n        self.boundary_timestep = None\n\n    if training_args.real_score_model_path:\n        logger.info(\"Loading real score transformer from: %s\",\n                    training_args.real_score_model_path)\n        training_args.override_transformer_cls_name = \"WanTransformer3DModel\"\n        self.real_score_transformer = self.load_module_from_path(\n            training_args.real_score_model_path, \"transformer\",\n            training_args)\n        try:\n            self.real_score_transformer_2 = self.load_module_from_path(\n                training_args.real_score_model_path, \"transformer_2\",\n                training_args)\n            logger.info(\"Loaded real score transformer_2 for MoE support\")\n        except Exception:\n            logger.info(\n                \"real score transformer_2 not found, using single transformer\"\n            )\n            self.real_score_transformer_2 = None\n    else:\n        self.real_score_transformer = self.get_module(\n            \"real_score_transformer\")\n        self.real_score_transformer_2 = self.get_module(\n            \"real_score_transformer_2\")\n\n    if training_args.fake_score_model_path:\n        logger.info(\"Loading fake score transformer from: %s\",\n                    training_args.fake_score_model_path)\n        training_args.override_transformer_cls_name = \"WanTransformer3DModel\"\n        self.fake_score_transformer = self.load_module_from_path(\n            training_args.fake_score_model_path, \"transformer\",\n            training_args)\n        try:\n            self.fake_score_transformer_2 = self.load_module_from_path(\n                training_args.fake_score_model_path, \"transformer_2\",\n                training_args)\n            logger.info(\"Loaded fake score transformer_2 for MoE support\")\n        except Exception:\n            logger.info(\n                \"fake score transformer_2 not found, using single transformer\"\n            )\n            self.fake_score_transformer_2 = None\n    else:\n        self.fake_score_transformer = self.get_module(\n            \"fake_score_transformer\")\n        self.fake_score_transformer_2 = self.get_module(\n            \"fake_score_transformer_2\")\n\n    self.real_score_transformer.requires_grad_(False)\n    self.real_score_transformer.eval()\n    if self.real_score_transformer_2 is not None:\n        self.real_score_transformer_2.requires_grad_(False)\n        self.real_score_transformer_2.eval()\n\n    # Set training modes for fake score transformers (trainable)\n    self.fake_score_transformer.requires_grad_(True)\n    self.fake_score_transformer.train()\n    if self.fake_score_transformer_2 is not None:\n        self.fake_score_transformer_2.requires_grad_(True)\n        self.fake_score_transformer_2.train()\n\n    if training_args.enable_gradient_checkpointing_type is not None:\n        self.fake_score_transformer = apply_activation_checkpointing(\n            self.fake_score_transformer,\n            checkpointing_type=training_args.\n            enable_gradient_checkpointing_type)\n        if self.fake_score_transformer_2 is not None:\n            self.fake_score_transformer_2 = apply_activation_checkpointing(\n                self.fake_score_transformer_2,\n                checkpointing_type=training_args.\n                enable_gradient_checkpointing_type)\n\n        self.real_score_transformer = apply_activation_checkpointing(\n            self.real_score_transformer,\n            checkpointing_type=training_args.\n            enable_gradient_checkpointing_type)\n        if self.real_score_transformer_2 is not None:\n            self.real_score_transformer_2 = apply_activation_checkpointing(\n                self.real_score_transformer_2,\n                checkpointing_type=training_args.\n                enable_gradient_checkpointing_type)\n\n    # Initialize optimizers\n    fake_score_params = list(\n        filter(lambda p: p.requires_grad,\n               self.fake_score_transformer.parameters()))\n\n    # Use separate learning rate for fake_score_transformer if specified\n    fake_score_lr = training_args.fake_score_learning_rate\n    if fake_score_lr == 0.0:\n        fake_score_lr = training_args.learning_rate\n\n    betas_str = training_args.fake_score_betas\n    betas = tuple(float(x.strip()) for x in betas_str.split(\",\"))\n\n    self.fake_score_optimizer = torch.optim.AdamW(\n        fake_score_params,\n        lr=fake_score_lr,\n        betas=betas,\n        weight_decay=training_args.weight_decay,\n        eps=1e-8,\n    )\n\n    self.fake_score_lr_scheduler = get_scheduler(\n        training_args.fake_score_lr_scheduler,\n        optimizer=self.fake_score_optimizer,\n        num_warmup_steps=training_args.lr_warmup_steps,\n        num_training_steps=training_args.max_train_steps,\n        num_cycles=training_args.lr_num_cycles,\n        power=training_args.lr_power,\n        min_lr_ratio=training_args.min_lr_ratio,\n        last_epoch=self.init_steps - 1,\n    )\n\n    if self.fake_score_transformer_2 is not None:\n        fake_score_params_2 = list(\n            filter(lambda p: p.requires_grad,\n                   self.fake_score_transformer_2.parameters()))\n        self.fake_score_optimizer_2 = torch.optim.AdamW(\n            fake_score_params_2,\n            lr=fake_score_lr,\n            betas=betas,\n            weight_decay=training_args.weight_decay,\n            eps=1e-8,\n        )\n        self.fake_score_lr_scheduler_2 = get_scheduler(\n            training_args.fake_score_lr_scheduler,\n            optimizer=self.fake_score_optimizer_2,\n            num_warmup_steps=training_args.lr_warmup_steps,\n            num_training_steps=training_args.max_train_steps,\n            num_cycles=training_args.lr_num_cycles,\n            power=training_args.lr_power,\n            min_lr_ratio=training_args.min_lr_ratio,\n            last_epoch=self.init_steps - 1,\n        )\n\n    logger.info(\n        \"Distillation optimizers initialized: generator and fake_score\")\n\n    self.generator_update_interval = self.training_args.generator_update_interval\n    logger.info(\n        \"Distillation pipeline initialized with generator_update_interval=%s\",\n        self.generator_update_interval)\n\n    self.denoising_step_list = torch.tensor(\n        self.training_args.pipeline_config.dmd_denoising_steps,\n        dtype=torch.long,\n        device=get_local_torch_device())\n\n    if training_args.warp_denoising_step:  # Warp the denoising step according to the scheduler time shift\n        timesteps = torch.cat((self.noise_scheduler.timesteps.cpu(),\n                               torch.tensor([0],\n                                            dtype=torch.float32))).cuda()\n        self.denoising_step_list = timesteps[1000 -\n                                             self.denoising_step_list]\n        logger.info(\"Warping denoising_step_list\")\n\n    self.denoising_step_list = self.denoising_step_list.to(\n        get_local_torch_device())\n    logger.info(\"Distillation generator model to %s denoising steps: %s\",\n                len(self.denoising_step_list), self.denoising_step_list)\n    self.num_train_timestep = self.noise_scheduler.num_train_timesteps\n\n    self.min_timestep = int(self.training_args.min_timestep_ratio *\n                            self.num_train_timestep)\n    self.max_timestep = int(self.training_args.max_timestep_ratio *\n                            self.num_train_timestep)\n\n    self.real_score_guidance_scale = self.training_args.real_score_guidance_scale\n\n    self.generator_ema: EMA_FSDP | None = None\n    self.generator_ema_2: EMA_FSDP | None = None\n    if (self.training_args.ema_decay\n            is not None) and (self.training_args.ema_decay &gt; 0.0):\n        self.generator_ema = EMA_FSDP(self.transformer,\n                                      decay=self.training_args.ema_decay)\n        logger.info(\"Initialized generator EMA with decay=%s\",\n                    self.training_args.ema_decay)\n\n        # Initialize EMA for transformer_2 if it exists\n        if self.transformer_2 is not None:\n            self.generator_ema_2 = EMA_FSDP(\n                self.transformer_2, decay=self.training_args.ema_decay)\n            logger.info(\"Initialized generator EMA_2 with decay=%s\",\n                        self.training_args.ema_decay)\n    else:\n        logger.info(\"Generator EMA disabled (ema_decay &lt;= 0.0)\")\n</code></pre> fastvideo.training.DistillationPipeline.initialize_validation_pipeline <code>abstractmethod</code> \u00b6 <pre><code>initialize_validation_pipeline(training_args: TrainingArgs)\n</code></pre> <p>Initialize validation pipeline - must be implemented by subclasses.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>@abstractmethod\ndef initialize_validation_pipeline(self, training_args: TrainingArgs):\n    \"\"\"Initialize validation pipeline - must be implemented by subclasses.\"\"\"\n    raise NotImplementedError(\n        \"Distillation pipelines must implement this method\")\n</code></pre> fastvideo.training.DistillationPipeline.is_ema_ready \u00b6 <pre><code>is_ema_ready(current_step: int | None = None)\n</code></pre> <p>Check if EMA is ready for use (after ema_start_step).</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def is_ema_ready(self, current_step: int | None = None):\n    \"\"\"Check if EMA is ready for use (after ema_start_step).\"\"\"\n    if current_step is None:\n        current_step = getattr(self, 'current_trainstep', 0)\n    return (self.generator_ema is not None\n            and current_step &gt;= self.training_args.ema_start_step)\n</code></pre> fastvideo.training.DistillationPipeline.load_module_from_path \u00b6 <pre><code>load_module_from_path(\n    model_path: str,\n    module_type: str,\n    training_args: TrainingArgs,\n)\n</code></pre> <p>Load a module from a specific path using the same loading logic as the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the model</p> required <code>module_type</code> <code>str</code> <p>Type of module to load (e.g., \"transformer\")</p> required <code>training_args</code> <code>TrainingArgs</code> <p>Training arguments</p> required <p>Returns:</p> Type Description <p>The loaded module</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def load_module_from_path(self, model_path: str, module_type: str,\n                          training_args: \"TrainingArgs\"):\n    \"\"\"\n    Load a module from a specific path using the same loading logic as the pipeline.\n\n    Args:\n        model_path: Path to the model\n        module_type: Type of module to load (e.g., \"transformer\")\n        training_args: Training arguments\n\n    Returns:\n        The loaded module\n    \"\"\"\n    logger.info(\"Loading %s from custom path: %s\", module_type, model_path)\n    # Set flag to prevent custom weight loading for teacher/critic models\n    training_args._loading_teacher_critic_model = True\n\n    try:\n        from fastvideo.models.loader.component_loader import (\n            PipelineComponentLoader)\n\n        # Download the model if it's a Hugging Face model ID\n        local_model_path = maybe_download_model(model_path)\n        logger.info(\"Model downloaded/found at: %s\", local_model_path)\n        config = verify_model_config_and_directory(local_model_path)\n\n        if module_type not in config:\n            if hasattr(self, '_extra_config_module_map'\n                       ) and module_type in self._extra_config_module_map:\n                extra_module = self._extra_config_module_map[module_type]\n                if extra_module in config:\n                    module_type = extra_module\n                    logger.info(\"Using %s for %s\", extra_module,\n                                module_type)\n                else:\n                    raise ValueError(\n                        f\"Module {module_type} not found in config at {local_model_path}\"\n                    )\n            else:\n                raise ValueError(\n                    f\"Module {module_type} not found in config at {local_model_path}\"\n                )\n\n        module_info = config[module_type]\n        if module_info is None:\n            raise ValueError(\n                f\"Module {module_type} has null value in config at {local_model_path}\"\n            )\n\n        transformers_or_diffusers, architecture = module_info\n        component_path = os.path.join(local_model_path, module_type)\n        module = PipelineComponentLoader.load_module(\n            module_name=module_type,\n            component_model_path=component_path,\n            transformers_or_diffusers=transformers_or_diffusers,\n            fastvideo_args=training_args,\n        )\n\n        logger.info(\"Successfully loaded %s from %s\", module_type,\n                    component_path)\n        return module\n    finally:\n        # Always clean up the flag\n        if hasattr(training_args, '_loading_teacher_critic_model'):\n            delattr(training_args, '_loading_teacher_critic_model')\n</code></pre> fastvideo.training.DistillationPipeline.reset_ema \u00b6 <pre><code>reset_ema()\n</code></pre> <p>Reset EMA to current model weights.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def reset_ema(self):\n    \"\"\"Reset EMA to current model weights.\"\"\"\n    if self.generator_ema is not None:\n        logger.info(\"Resetting EMA to current model weights\")\n        self.generator_ema.update(self.transformer)\n        # Force update to current weights by setting decay to 0 temporarily\n        original_decay = self.generator_ema.decay\n        self.generator_ema.decay = 0.0\n        self.generator_ema.update(self.transformer)\n        self.generator_ema.decay = original_decay\n        logger.info(\"EMA reset completed\")\n    else:\n        logger.warning(\"Cannot reset EMA: EMA not initialized\")\n\n    if self.generator_ema_2 is not None:\n        logger.info(\"Resetting EMA_2 to current model weights\")\n        self.generator_ema_2.update(self.transformer_2)\n        # Force update to current weights by setting decay to 0 temporarily\n        original_decay_2 = self.generator_ema_2.decay\n        self.generator_ema_2.decay = 0.0\n        self.generator_ema_2.update(self.transformer_2)\n        self.generator_ema_2.decay = original_decay_2\n        logger.info(\"EMA_2 reset completed\")\n</code></pre> fastvideo.training.DistillationPipeline.save_ema_weights \u00b6 <pre><code>save_ema_weights(output_dir: str, step: int)\n</code></pre> <p>Save EMA weights separately for inference purposes.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def save_ema_weights(self, output_dir: str, step: int):\n    \"\"\"Save EMA weights separately for inference purposes.\"\"\"\n    if self.generator_ema is None and self.generator_ema_2 is None:\n        logger.warning(\"Cannot save EMA weights: No EMA initialized\")\n        return\n\n    if not self.is_ema_ready():\n        logger.warning(\n            \"Cannot save EMA weights: EMA not ready yet (step &lt; ema_start_step)\"\n        )\n        return\n\n    try:\n        # Save main transformer EMA\n        if self.generator_ema is not None:\n            ema_model = self.get_ema_model_copy()\n            if ema_model is None:\n                logger.warning(\"Failed to create EMA model copy\")\n            else:\n                ema_save_dir = os.path.join(output_dir,\n                                            f\"ema_checkpoint-{step}\")\n                os.makedirs(ema_save_dir, exist_ok=True)\n\n                # save as diffusers format\n                from safetensors.torch import save_file\n\n                from fastvideo.training.training_utils import (\n                    custom_to_hf_state_dict, gather_state_dict_on_cpu_rank0)\n                cpu_state = gather_state_dict_on_cpu_rank0(ema_model,\n                                                           device=None)\n\n                if self.global_rank == 0:\n                    weight_path = os.path.join(\n                        ema_save_dir, \"diffusion_pytorch_model.safetensors\")\n                    diffusers_state_dict = custom_to_hf_state_dict(\n                        cpu_state, ema_model.reverse_param_names_mapping)\n                    save_file(diffusers_state_dict, weight_path)\n\n                    config_dict = ema_model.hf_config\n                    if \"dtype\" in config_dict:\n                        del config_dict[\"dtype\"]\n                    config_path = os.path.join(ema_save_dir, \"config.json\")\n                    with open(config_path, \"w\") as f:\n                        json.dump(config_dict, f, indent=4)\n\n                    logger.info(\"EMA weights saved to %s\", weight_path)\n\n                del ema_model\n\n        # Save transformer_2 EMA\n        if self.generator_ema_2 is not None:\n            ema_2_model = self.get_ema_2_model_copy()\n            if ema_2_model is None:\n                logger.warning(\"Failed to create EMA_2 model copy\")\n            else:\n                ema_2_save_dir = os.path.join(output_dir,\n                                              f\"ema_2_checkpoint-{step}\")\n                os.makedirs(ema_2_save_dir, exist_ok=True)\n\n                # save as diffusers format\n                from safetensors.torch import save_file\n\n                from fastvideo.training.training_utils import (\n                    custom_to_hf_state_dict, gather_state_dict_on_cpu_rank0)\n                cpu_state_2 = gather_state_dict_on_cpu_rank0(ema_2_model,\n                                                             device=None)\n\n                if self.global_rank == 0:\n                    weight_path_2 = os.path.join(\n                        ema_2_save_dir,\n                        \"diffusion_pytorch_model.safetensors\")\n                    diffusers_state_dict_2 = custom_to_hf_state_dict(\n                        cpu_state_2,\n                        ema_2_model.reverse_param_names_mapping)\n                    save_file(diffusers_state_dict_2, weight_path_2)\n\n                    config_dict_2 = ema_2_model.hf_config\n                    if \"dtype\" in config_dict_2:\n                        del config_dict_2[\"dtype\"]\n                    config_path_2 = os.path.join(ema_2_save_dir,\n                                                 \"config.json\")\n                    with open(config_path_2, \"w\") as f:\n                        json.dump(config_dict_2, f, indent=4)\n\n                    logger.info(\"EMA_2 weights saved to %s\", weight_path_2)\n\n                del ema_2_model\n\n    except Exception as e:\n        logger.error(\"Failed to save EMA weights: %s\", str(e))\n</code></pre> fastvideo.training.DistillationPipeline.train \u00b6 <pre><code>train() -&gt; None\n</code></pre> <p>Main training loop with distillation-specific logging.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def train(self) -&gt; None:\n    \"\"\"Main training loop with distillation-specific logging.\"\"\"\n    assert self.training_args.seed is not None, \"seed must be set\"\n    seed = self.training_args.seed\n\n    # Set the same seed within each SP group to ensure reproducibility\n    if self.sp_world_size &gt; 1:\n        # Use the same seed for all processes within the same SP group\n        sp_group_seed = seed + (self.global_rank // self.sp_world_size)\n        set_random_seed(sp_group_seed)\n        logger.info(\"Rank %s: Using SP group seed %s\", self.global_rank,\n                    sp_group_seed)\n    else:\n        set_random_seed(seed + self.global_rank)\n\n    # Set random seeds for deterministic training\n    self.noise_random_generator = torch.Generator(device=\"cpu\").manual_seed(\n        self.seed)\n    self.noise_gen_cuda = torch.Generator(device=\"cuda\").manual_seed(\n        self.seed)\n    self.validation_random_generator = torch.Generator(\n        device=\"cpu\").manual_seed(self.seed)\n    logger.info(\"Initialized random seeds with seed: %s\", seed)\n\n    # Initialize current_trainstep for EMA ready checks\n    #TODO: check if needed\n    self.current_trainstep = self.init_steps\n\n    # Resume from checkpoint if specified (this will restore random states)\n    if self.training_args.resume_from_checkpoint:\n        self._resume_from_checkpoint()\n        logger.info(\"Resumed from checkpoint, random states restored\")\n    else:\n        logger.info(\"Starting training from scratch\")\n\n    self.train_loader_iter = iter(self.train_dataloader)\n\n    step_times: deque[float] = deque(maxlen=100)\n\n    self._log_training_info()\n    self._log_validation(self.transformer, self.training_args,\n                         self.init_steps)\n\n    progress_bar = tqdm(\n        range(0, self.training_args.max_train_steps),\n        initial=self.init_steps,\n        desc=\"Steps\",\n        disable=self.local_rank &gt; 0,\n    )\n\n    use_vsa = vsa_available and envs.FASTVIDEO_ATTENTION_BACKEND == \"VIDEO_SPARSE_ATTN\"\n    for step in range(self.init_steps + 1,\n                      self.training_args.max_train_steps + 1):\n        start_time = time.perf_counter()\n        if use_vsa:\n            vsa_sparsity = self.training_args.VSA_sparsity\n            vsa_decay_rate = self.training_args.VSA_decay_rate\n            vsa_decay_interval_steps = self.training_args.VSA_decay_interval_steps\n            if vsa_decay_interval_steps &gt; 1:\n                current_decay_times = min(step // vsa_decay_interval_steps,\n                                          vsa_sparsity // vsa_decay_rate)\n                current_vsa_sparsity = current_decay_times * vsa_decay_rate\n            else:\n                current_vsa_sparsity = vsa_sparsity\n        else:\n            current_vsa_sparsity = 0.0\n\n        training_batch = TrainingBatch()\n        self.current_trainstep = step\n        training_batch.current_vsa_sparsity = current_vsa_sparsity\n\n        if (step &gt;= self.training_args.ema_start_step) and \\\n                (self.generator_ema is None) and (self.training_args.ema_decay &gt; 0):\n            self.generator_ema = EMA_FSDP(\n                self.transformer, decay=self.training_args.ema_decay)\n            logger.info(\"Created generator EMA at step %s with decay=%s\",\n                        step, self.training_args.ema_decay)\n\n            # Create EMA for transformer_2 if it exists\n            if self.transformer_2 is not None and self.generator_ema_2 is None:\n                self.generator_ema_2 = EMA_FSDP(\n                    self.transformer_2, decay=self.training_args.ema_decay)\n                logger.info(\n                    \"Created generator EMA_2 at step %s with decay=%s\",\n                    step, self.training_args.ema_decay)\n\n        with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n            training_batch = self.train_one_step(training_batch)\n\n        total_loss = training_batch.total_loss\n        generator_loss = training_batch.generator_loss\n        fake_score_loss = training_batch.fake_score_loss\n        grad_norm = training_batch.grad_norm\n\n        step_time = time.perf_counter() - start_time\n        step_times.append(step_time)\n        avg_step_time = sum(step_times) / len(step_times)\n\n        progress_bar.set_postfix({\n            \"total_loss\":\n            f\"{total_loss:.4f}\",\n            \"generator_loss\":\n            f\"{generator_loss:.4f}\",\n            \"fake_score_loss\":\n            f\"{fake_score_loss:.4f}\",\n            \"step_time\":\n            f\"{step_time:.2f}s\",\n            \"grad_norm\":\n            grad_norm,\n            \"ema\":\n            \"\u2713\" if (self.generator_ema is not None and self.is_ema_ready())\n            else \"\u2717\",\n            \"ema2\":\n            \"\u2713\" if (self.generator_ema_2 is not None\n                    and self.is_ema_ready()) else \"\u2717\",\n        })\n        progress_bar.update(1)\n\n        if self.global_rank == 0:\n            # Prepare logging data\n            log_data = {\n                \"train_total_loss\":\n                total_loss,\n                \"train_fake_score_loss\":\n                fake_score_loss,\n                \"learning_rate\":\n                self.lr_scheduler.get_last_lr()[0],\n                \"fake_score_learning_rate\":\n                self.fake_score_lr_scheduler.get_last_lr()[0],\n                \"step_time\":\n                step_time,\n                \"avg_step_time\":\n                avg_step_time,\n                \"grad_norm\":\n                grad_norm,\n            }\n            # Only log generator loss when generator is actually trained\n            if (step % self.generator_update_interval == 0):\n                log_data[\"train_generator_loss\"] = generator_loss\n            if use_vsa:\n                log_data[\"VSA_train_sparsity\"] = current_vsa_sparsity\n\n            if self.generator_ema is not None or self.generator_ema_2 is not None:\n                log_data[\"ema_enabled\"] = self.generator_ema is not None\n                log_data[\"ema_2_enabled\"] = self.generator_ema_2 is not None\n                log_data[\"ema_decay\"] = self.training_args.ema_decay\n            else:\n                log_data[\"ema_enabled\"] = False\n                log_data[\"ema_2_enabled\"] = False\n\n            ema_stats = self.get_ema_stats()\n            log_data.update(ema_stats)\n\n            if training_batch.dmd_latent_vis_dict:\n                dmd_additional_logs = {\n                    \"generator_timestep\":\n                    training_batch.\n                    dmd_latent_vis_dict[\"generator_timestep\"].item(),\n                    \"dmd_timestep\":\n                    training_batch.dmd_latent_vis_dict[\"dmd_timestep\"].item(\n                    ),\n                }\n                log_data.update(dmd_additional_logs)\n\n            faker_score_additional_logs = {\n                \"fake_score_timestep\":\n                training_batch.\n                fake_score_latent_vis_dict[\"fake_score_timestep\"].item(),\n            }\n            log_data.update(faker_score_additional_logs)\n\n            self.tracker.log(log_data, step)\n\n        # Save training state checkpoint (for resuming training)\n        if (self.training_args.training_state_checkpointing_steps &gt; 0\n                and step %\n                self.training_args.training_state_checkpointing_steps == 0):\n            print(\"rank\", self.global_rank,\n                  \"save training state checkpoint at step\", step)\n            save_distillation_checkpoint(\n                self.transformer,\n                self.fake_score_transformer,\n                self.global_rank,\n                self.training_args.output_dir,\n                step,\n                self.optimizer,\n                self.fake_score_optimizer,\n                self.train_dataloader,\n                self.lr_scheduler,\n                self.fake_score_lr_scheduler,\n                self.noise_random_generator,\n                self.generator_ema,\n                # MoE support\n                generator_transformer_2=getattr(self, 'transformer_2',\n                                                None),\n                real_score_transformer_2=getattr(\n                    self, 'real_score_transformer_2', None),\n                fake_score_transformer_2=getattr(\n                    self, 'fake_score_transformer_2', None),\n                generator_optimizer_2=getattr(self, 'optimizer_2', None),\n                fake_score_optimizer_2=getattr(self,\n                                               'fake_score_optimizer_2',\n                                               None),\n                generator_scheduler_2=getattr(self, 'lr_scheduler_2', None),\n                fake_score_scheduler_2=getattr(self,\n                                               'fake_score_lr_scheduler_2',\n                                               None),\n                generator_ema_2=getattr(self, 'generator_ema_2', None))\n\n            if self.transformer:\n                self.transformer.train()\n            self.sp_group.barrier()\n\n        # Save weight-only checkpoint\n        if (self.training_args.weight_only_checkpointing_steps &gt; 0\n                and step %\n                self.training_args.weight_only_checkpointing_steps == 0):\n            print(\"rank\", self.global_rank,\n                  \"save weight-only checkpoint at step\", step)\n            save_distillation_checkpoint(\n                self.transformer,\n                self.fake_score_transformer,\n                self.global_rank,\n                self.training_args.output_dir,\n                f\"{step}_weight_only\",\n                only_save_generator_weight=True,\n                generator_ema=self.generator_ema,\n                # MoE support\n                generator_transformer_2=getattr(self, 'transformer_2',\n                                                None),\n                real_score_transformer_2=getattr(\n                    self, 'real_score_transformer_2', None),\n                fake_score_transformer_2=getattr(\n                    self, 'fake_score_transformer_2', None),\n                generator_optimizer_2=getattr(self, 'optimizer_2', None),\n                fake_score_optimizer_2=getattr(self,\n                                               'fake_score_optimizer_2',\n                                               None),\n                generator_scheduler_2=getattr(self, 'lr_scheduler_2', None),\n                fake_score_scheduler_2=getattr(self,\n                                               'fake_score_lr_scheduler_2',\n                                               None),\n                generator_ema_2=getattr(self, 'generator_ema_2', None))\n\n            if self.training_args.use_ema and self.is_ema_ready():\n                self.save_ema_weights(self.training_args.output_dir, step)\n\n        if self.training_args.log_validation and step % self.training_args.validation_steps == 0:\n            if self.training_args.log_visualization:\n                self.visualize_intermediate_latents(training_batch,\n                                                    self.training_args,\n                                                    step)\n            self._log_validation(self.transformer, self.training_args, step)\n\n    self.tracker.finish()\n\n    # Save final training state checkpoint\n    print(\"rank\", self.global_rank,\n          \"save final training state checkpoint at step\",\n          self.training_args.max_train_steps)\n    save_distillation_checkpoint(\n        self.transformer,\n        self.fake_score_transformer,\n        self.global_rank,\n        self.training_args.output_dir,\n        self.training_args.max_train_steps,\n        self.optimizer,\n        self.fake_score_optimizer,\n        self.train_dataloader,\n        self.lr_scheduler,\n        self.fake_score_lr_scheduler,\n        self.noise_random_generator,\n        self.generator_ema,\n        # MoE support\n        generator_transformer_2=getattr(self, 'transformer_2', None),\n        real_score_transformer_2=getattr(self, 'real_score_transformer_2',\n                                         None),\n        fake_score_transformer_2=getattr(self, 'fake_score_transformer_2',\n                                         None),\n        generator_optimizer_2=getattr(self, 'optimizer_2', None),\n        fake_score_optimizer_2=getattr(self, 'fake_score_optimizer_2',\n                                       None),\n        generator_scheduler_2=getattr(self, 'lr_scheduler_2', None),\n        fake_score_scheduler_2=getattr(self, 'fake_score_lr_scheduler_2',\n                                       None),\n        generator_ema_2=getattr(self, 'generator_ema_2', None))\n\n    if self.training_args.use_ema and self.is_ema_ready():\n        self.save_ema_weights(self.training_args.output_dir,\n                              self.training_args.max_train_steps)\n\n    if envs.FASTVIDEO_TORCH_PROFILER_DIR:\n        logger.info(\"Stopping profiler...\")\n        self.profiler_controller.stop()\n        logger.info(\"Profiler stopped.\")\n\n    if get_sp_group():\n        cleanup_dist_env_and_memory()\n</code></pre> fastvideo.training.DistillationPipeline.visualize_intermediate_latents \u00b6 <pre><code>visualize_intermediate_latents(\n    training_batch: TrainingBatch,\n    training_args: TrainingArgs,\n    step: int,\n)\n</code></pre> <p>Add visualization data to tracker logging and save frames to disk.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def visualize_intermediate_latents(self, training_batch: TrainingBatch,\n                                   training_args: TrainingArgs, step: int):\n    \"\"\"Add visualization data to tracker logging and save frames to disk.\"\"\"\n    tracker_loss_dict: dict[str, Any] = {}\n    dmd_latents_vis_dict = training_batch.dmd_latent_vis_dict\n    fake_score_latents_vis_dict = training_batch.fake_score_latent_vis_dict\n    fake_score_log_keys = ['generator_pred_video']\n    dmd_log_keys = ['faker_score_pred_video', 'real_score_pred_video']\n\n    for latent_key in fake_score_log_keys:\n        latents = fake_score_latents_vis_dict[latent_key]\n        latents = latents.permute(0, 2, 1, 3, 4)\n\n        if isinstance(self.vae.scaling_factor, torch.Tensor):\n            latents = latents / self.vae.scaling_factor.to(\n                latents.device, latents.dtype)\n        else:\n            latents = latents / self.vae.scaling_factor\n\n        # Apply shifting if needed\n        if (hasattr(self.vae, \"shift_factor\")\n                and self.vae.shift_factor is not None):\n            if isinstance(self.vae.shift_factor, torch.Tensor):\n                latents += self.vae.shift_factor.to(latents.device,\n                                                    latents.dtype)\n            else:\n                latents += self.vae.shift_factor\n            with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n                video = self.vae.decode(latents)\n            video = (video / 2 + 0.5).clamp(0, 1)\n            video = video.cpu().float()\n            video = video.permute(0, 2, 1, 3, 4)\n            video = (video * 255).numpy().astype(np.uint8)\n            video_artifact = self.tracker.video(\n                video, fps=24, format=\"mp4\")  # change to 16 for Wan2.1\n            if video_artifact is not None:\n                tracker_loss_dict[latent_key] = video_artifact\n            # Clean up references\n            del video, latents\n\n    # Process DMD training data if available - use decode_stage instead of self.vae.decode\n    if 'generator_pred_video' in dmd_latents_vis_dict:\n        for latent_key in dmd_log_keys:\n            latents = dmd_latents_vis_dict[latent_key]\n            latents = latents.permute(0, 2, 1, 3, 4)\n            # decoded_latent = decode_stage(ForwardBatch(data_type=\"video\", latents=latents), training_args)\n            if isinstance(self.vae.scaling_factor, torch.Tensor):\n                latents = latents / self.vae.scaling_factor.to(\n                    latents.device, latents.dtype)\n            else:\n                latents = latents / self.vae.scaling_factor\n\n            # Apply shifting if needed\n            if (hasattr(self.vae, \"shift_factor\")\n                    and self.vae.shift_factor is not None):\n                if isinstance(self.vae.shift_factor, torch.Tensor):\n                    latents += self.vae.shift_factor.to(\n                        latents.device, latents.dtype)\n                else:\n                    latents += self.vae.shift_factor\n            with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n                video = self.vae.decode(latents)\n            video = (video / 2 + 0.5).clamp(0, 1)\n            video = video.cpu().float()\n            video = video.permute(0, 2, 1, 3, 4)\n            video = (video * 255).numpy().astype(np.uint8)\n            video_artifact = self.tracker.video(\n                video, fps=24, format=\"mp4\")  # change to 16 for Wan2.1\n            if video_artifact is not None:\n                tracker_loss_dict[latent_key] = video_artifact\n            # Clean up references\n            del video, latents\n\n    # Log to tracker\n    if self.global_rank == 0 and tracker_loss_dict:\n        self.tracker.log_artifacts(tracker_loss_dict, step)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.training.TrainingPipeline","title":"fastvideo.training.TrainingPipeline","text":"<pre><code>TrainingPipeline(\n    model_path: str,\n    fastvideo_args: TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>LoRAPipeline</code>, <code>ABC</code></p> <p>A pipeline for training a model. All training pipelines should inherit from this class. All reusable components and code should be implemented in this class.</p> Source code in <code>fastvideo/training/training_pipeline.py</code> <pre><code>def __init__(\n        self,\n        model_path: str,\n        fastvideo_args: TrainingArgs,\n        required_config_modules: list[str] | None = None,\n        loaded_modules: dict[str, torch.nn.Module] | None = None) -&gt; None:\n    fastvideo_args.inference_mode = False\n    self.lora_training = fastvideo_args.lora_training\n    if self.lora_training and fastvideo_args.lora_rank is None:\n        raise ValueError(\"lora rank must be set when using lora training\")\n\n    set_random_seed(fastvideo_args.seed)  # for lora param init\n    super().__init__(model_path, fastvideo_args, required_config_modules,\n                     loaded_modules)  # type: ignore\n    self.tracker = DummyTracker()\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.training.TrainingPipeline-functions","title":"Functions","text":"fastvideo.training.TrainingPipeline.visualize_intermediate_latents \u00b6 <pre><code>visualize_intermediate_latents(\n    training_batch: TrainingBatch,\n    training_args: TrainingArgs,\n    step: int,\n)\n</code></pre> <p>Add visualization data to tracker logging and save frames to disk.</p> Source code in <code>fastvideo/training/training_pipeline.py</code> <pre><code>def visualize_intermediate_latents(self, training_batch: TrainingBatch,\n                                   training_args: TrainingArgs, step: int):\n    \"\"\"Add visualization data to tracker logging and save frames to disk.\"\"\"\n    raise NotImplementedError(\n        \"Visualize intermediate latents is not implemented for training pipeline\"\n    )\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.training.WanTrainingPipeline","title":"fastvideo.training.WanTrainingPipeline","text":"<pre><code>WanTrainingPipeline(\n    model_path: str,\n    fastvideo_args: TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>TrainingPipeline</code></p> <p>A training pipeline for Wan.</p> Source code in <code>fastvideo/training/training_pipeline.py</code> <pre><code>def __init__(\n        self,\n        model_path: str,\n        fastvideo_args: TrainingArgs,\n        required_config_modules: list[str] | None = None,\n        loaded_modules: dict[str, torch.nn.Module] | None = None) -&gt; None:\n    fastvideo_args.inference_mode = False\n    self.lora_training = fastvideo_args.lora_training\n    if self.lora_training and fastvideo_args.lora_rank is None:\n        raise ValueError(\"lora rank must be set when using lora training\")\n\n    set_random_seed(fastvideo_args.seed)  # for lora param init\n    super().__init__(model_path, fastvideo_args, required_config_modules,\n                     loaded_modules)  # type: ignore\n    self.tracker = DummyTracker()\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.training.WanTrainingPipeline-functions","title":"Functions","text":"fastvideo.training.WanTrainingPipeline.create_training_stages \u00b6 <pre><code>create_training_stages(training_args: TrainingArgs)\n</code></pre> <p>May be used in future refactors.</p> Source code in <code>fastvideo/training/wan_training_pipeline.py</code> <pre><code>def create_training_stages(self, training_args: TrainingArgs):\n    \"\"\"\n    May be used in future refactors.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.training-modules","title":"Modules","text":""},{"location":"api/fastvideo/#fastvideo.training.distillation_pipeline","title":"fastvideo.training.distillation_pipeline","text":""},{"location":"api/fastvideo/#fastvideo.training.distillation_pipeline-classes","title":"Classes","text":"fastvideo.training.distillation_pipeline.DistillationPipeline \u00b6 <pre><code>DistillationPipeline(\n    model_path: str,\n    fastvideo_args: TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>TrainingPipeline</code></p> <p>A distillation pipeline for training a 3 step model. Inherits from TrainingPipeline to reuse training infrastructure.</p> Source code in <code>fastvideo/training/training_pipeline.py</code> <pre><code>def __init__(\n        self,\n        model_path: str,\n        fastvideo_args: TrainingArgs,\n        required_config_modules: list[str] | None = None,\n        loaded_modules: dict[str, torch.nn.Module] | None = None) -&gt; None:\n    fastvideo_args.inference_mode = False\n    self.lora_training = fastvideo_args.lora_training\n    if self.lora_training and fastvideo_args.lora_rank is None:\n        raise ValueError(\"lora rank must be set when using lora training\")\n\n    set_random_seed(fastvideo_args.seed)  # for lora param init\n    super().__init__(model_path, fastvideo_args, required_config_modules,\n                     loaded_modules)  # type: ignore\n    self.tracker = DummyTracker()\n</code></pre> Functions\u00b6 fastvideo.training.distillation_pipeline.DistillationPipeline.apply_ema_to_model \u00b6 <pre><code>apply_ema_to_model(model)\n</code></pre> <p>Apply EMA weights to the model for validation or inference.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def apply_ema_to_model(self, model):\n    \"\"\"Apply EMA weights to the model for validation or inference.\"\"\"\n    if model is self.transformer and self.generator_ema is not None:\n        with self.generator_ema.apply_to_model(model):\n            return model\n    elif model is self.transformer_2 and self.generator_ema_2 is not None:\n        with self.generator_ema_2.apply_to_model(model):\n            return model\n    return model\n</code></pre> fastvideo.training.distillation_pipeline.DistillationPipeline.get_ema_2_model_copy \u00b6 <pre><code>get_ema_2_model_copy() -&gt; torch.nn.Module | None\n</code></pre> <p>Get a copy of the transformer_2 model with EMA weights applied.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def get_ema_2_model_copy(self) -&gt; torch.nn.Module | None:\n    \"\"\"Get a copy of the transformer_2 model with EMA weights applied.\"\"\"\n    if self.generator_ema_2 is not None and self.transformer_2 is not None:\n        ema_2_model = copy.deepcopy(self.transformer_2)\n        self.generator_ema_2.copy_to_unwrapped(ema_2_model)\n        return ema_2_model\n    return None\n</code></pre> fastvideo.training.distillation_pipeline.DistillationPipeline.get_ema_model_copy \u00b6 <pre><code>get_ema_model_copy() -&gt; torch.nn.Module | None\n</code></pre> <p>Get a copy of the model with EMA weights applied.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def get_ema_model_copy(self) -&gt; torch.nn.Module | None:\n    \"\"\"Get a copy of the model with EMA weights applied.\"\"\"\n    if self.generator_ema is not None:\n        ema_model = copy.deepcopy(self.transformer)\n        self.generator_ema.copy_to_unwrapped(ema_model)\n        return ema_model\n    return None\n</code></pre> fastvideo.training.distillation_pipeline.DistillationPipeline.get_ema_stats \u00b6 <pre><code>get_ema_stats() -&gt; dict[str, Any]\n</code></pre> <p>Get EMA statistics for monitoring.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def get_ema_stats(self) -&gt; dict[str, Any]:\n    \"\"\"Get EMA statistics for monitoring.\"\"\"\n    ema_enabled = self.generator_ema is not None\n    ema_2_enabled = self.generator_ema_2 is not None\n\n    if not ema_enabled and not ema_2_enabled:\n        return {\n            \"ema_enabled\": False,\n            \"ema_2_enabled\": False,\n            \"ema_decay\": None,\n            \"ema_start_step\": self.training_args.ema_start_step,\n            \"ema_ready\": False,\n            \"ema_2_ready\": False,\n            \"ema_step\": self.current_trainstep,\n        }\n\n    return {\n        \"ema_enabled\": ema_enabled,\n        \"ema_2_enabled\": ema_2_enabled,\n        \"ema_decay\": self.training_args.ema_decay,\n        \"ema_start_step\": self.training_args.ema_start_step,\n        \"ema_ready\": self.is_ema_ready() if ema_enabled else False,\n        \"ema_2_ready\": self.is_ema_ready() if ema_2_enabled else False,\n        \"ema_step\": self.current_trainstep,\n    }\n</code></pre> fastvideo.training.distillation_pipeline.DistillationPipeline.initialize_training_pipeline \u00b6 <pre><code>initialize_training_pipeline(training_args: TrainingArgs)\n</code></pre> <p>Initialize the distillation training pipeline with multiple models.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def initialize_training_pipeline(self, training_args: TrainingArgs):\n    \"\"\"Initialize the distillation training pipeline with multiple models.\"\"\"\n    logger.info(\"Initializing distillation pipeline...\")\n\n    super().initialize_training_pipeline(training_args)\n\n    self.noise_scheduler = self.get_module(\"scheduler\")\n    self.vae = self.get_module(\"vae\")\n    self.vae.requires_grad_(False)\n\n    self.timestep_shift = self.training_args.pipeline_config.flow_shift\n    self.noise_scheduler = FlowMatchEulerDiscreteScheduler(\n        shift=self.timestep_shift)\n\n    if self.training_args.boundary_ratio is not None:\n        self.boundary_timestep = self.training_args.boundary_ratio * self.noise_scheduler.num_train_timesteps\n    else:\n        self.boundary_timestep = None\n\n    if training_args.real_score_model_path:\n        logger.info(\"Loading real score transformer from: %s\",\n                    training_args.real_score_model_path)\n        training_args.override_transformer_cls_name = \"WanTransformer3DModel\"\n        self.real_score_transformer = self.load_module_from_path(\n            training_args.real_score_model_path, \"transformer\",\n            training_args)\n        try:\n            self.real_score_transformer_2 = self.load_module_from_path(\n                training_args.real_score_model_path, \"transformer_2\",\n                training_args)\n            logger.info(\"Loaded real score transformer_2 for MoE support\")\n        except Exception:\n            logger.info(\n                \"real score transformer_2 not found, using single transformer\"\n            )\n            self.real_score_transformer_2 = None\n    else:\n        self.real_score_transformer = self.get_module(\n            \"real_score_transformer\")\n        self.real_score_transformer_2 = self.get_module(\n            \"real_score_transformer_2\")\n\n    if training_args.fake_score_model_path:\n        logger.info(\"Loading fake score transformer from: %s\",\n                    training_args.fake_score_model_path)\n        training_args.override_transformer_cls_name = \"WanTransformer3DModel\"\n        self.fake_score_transformer = self.load_module_from_path(\n            training_args.fake_score_model_path, \"transformer\",\n            training_args)\n        try:\n            self.fake_score_transformer_2 = self.load_module_from_path(\n                training_args.fake_score_model_path, \"transformer_2\",\n                training_args)\n            logger.info(\"Loaded fake score transformer_2 for MoE support\")\n        except Exception:\n            logger.info(\n                \"fake score transformer_2 not found, using single transformer\"\n            )\n            self.fake_score_transformer_2 = None\n    else:\n        self.fake_score_transformer = self.get_module(\n            \"fake_score_transformer\")\n        self.fake_score_transformer_2 = self.get_module(\n            \"fake_score_transformer_2\")\n\n    self.real_score_transformer.requires_grad_(False)\n    self.real_score_transformer.eval()\n    if self.real_score_transformer_2 is not None:\n        self.real_score_transformer_2.requires_grad_(False)\n        self.real_score_transformer_2.eval()\n\n    # Set training modes for fake score transformers (trainable)\n    self.fake_score_transformer.requires_grad_(True)\n    self.fake_score_transformer.train()\n    if self.fake_score_transformer_2 is not None:\n        self.fake_score_transformer_2.requires_grad_(True)\n        self.fake_score_transformer_2.train()\n\n    if training_args.enable_gradient_checkpointing_type is not None:\n        self.fake_score_transformer = apply_activation_checkpointing(\n            self.fake_score_transformer,\n            checkpointing_type=training_args.\n            enable_gradient_checkpointing_type)\n        if self.fake_score_transformer_2 is not None:\n            self.fake_score_transformer_2 = apply_activation_checkpointing(\n                self.fake_score_transformer_2,\n                checkpointing_type=training_args.\n                enable_gradient_checkpointing_type)\n\n        self.real_score_transformer = apply_activation_checkpointing(\n            self.real_score_transformer,\n            checkpointing_type=training_args.\n            enable_gradient_checkpointing_type)\n        if self.real_score_transformer_2 is not None:\n            self.real_score_transformer_2 = apply_activation_checkpointing(\n                self.real_score_transformer_2,\n                checkpointing_type=training_args.\n                enable_gradient_checkpointing_type)\n\n    # Initialize optimizers\n    fake_score_params = list(\n        filter(lambda p: p.requires_grad,\n               self.fake_score_transformer.parameters()))\n\n    # Use separate learning rate for fake_score_transformer if specified\n    fake_score_lr = training_args.fake_score_learning_rate\n    if fake_score_lr == 0.0:\n        fake_score_lr = training_args.learning_rate\n\n    betas_str = training_args.fake_score_betas\n    betas = tuple(float(x.strip()) for x in betas_str.split(\",\"))\n\n    self.fake_score_optimizer = torch.optim.AdamW(\n        fake_score_params,\n        lr=fake_score_lr,\n        betas=betas,\n        weight_decay=training_args.weight_decay,\n        eps=1e-8,\n    )\n\n    self.fake_score_lr_scheduler = get_scheduler(\n        training_args.fake_score_lr_scheduler,\n        optimizer=self.fake_score_optimizer,\n        num_warmup_steps=training_args.lr_warmup_steps,\n        num_training_steps=training_args.max_train_steps,\n        num_cycles=training_args.lr_num_cycles,\n        power=training_args.lr_power,\n        min_lr_ratio=training_args.min_lr_ratio,\n        last_epoch=self.init_steps - 1,\n    )\n\n    if self.fake_score_transformer_2 is not None:\n        fake_score_params_2 = list(\n            filter(lambda p: p.requires_grad,\n                   self.fake_score_transformer_2.parameters()))\n        self.fake_score_optimizer_2 = torch.optim.AdamW(\n            fake_score_params_2,\n            lr=fake_score_lr,\n            betas=betas,\n            weight_decay=training_args.weight_decay,\n            eps=1e-8,\n        )\n        self.fake_score_lr_scheduler_2 = get_scheduler(\n            training_args.fake_score_lr_scheduler,\n            optimizer=self.fake_score_optimizer_2,\n            num_warmup_steps=training_args.lr_warmup_steps,\n            num_training_steps=training_args.max_train_steps,\n            num_cycles=training_args.lr_num_cycles,\n            power=training_args.lr_power,\n            min_lr_ratio=training_args.min_lr_ratio,\n            last_epoch=self.init_steps - 1,\n        )\n\n    logger.info(\n        \"Distillation optimizers initialized: generator and fake_score\")\n\n    self.generator_update_interval = self.training_args.generator_update_interval\n    logger.info(\n        \"Distillation pipeline initialized with generator_update_interval=%s\",\n        self.generator_update_interval)\n\n    self.denoising_step_list = torch.tensor(\n        self.training_args.pipeline_config.dmd_denoising_steps,\n        dtype=torch.long,\n        device=get_local_torch_device())\n\n    if training_args.warp_denoising_step:  # Warp the denoising step according to the scheduler time shift\n        timesteps = torch.cat((self.noise_scheduler.timesteps.cpu(),\n                               torch.tensor([0],\n                                            dtype=torch.float32))).cuda()\n        self.denoising_step_list = timesteps[1000 -\n                                             self.denoising_step_list]\n        logger.info(\"Warping denoising_step_list\")\n\n    self.denoising_step_list = self.denoising_step_list.to(\n        get_local_torch_device())\n    logger.info(\"Distillation generator model to %s denoising steps: %s\",\n                len(self.denoising_step_list), self.denoising_step_list)\n    self.num_train_timestep = self.noise_scheduler.num_train_timesteps\n\n    self.min_timestep = int(self.training_args.min_timestep_ratio *\n                            self.num_train_timestep)\n    self.max_timestep = int(self.training_args.max_timestep_ratio *\n                            self.num_train_timestep)\n\n    self.real_score_guidance_scale = self.training_args.real_score_guidance_scale\n\n    self.generator_ema: EMA_FSDP | None = None\n    self.generator_ema_2: EMA_FSDP | None = None\n    if (self.training_args.ema_decay\n            is not None) and (self.training_args.ema_decay &gt; 0.0):\n        self.generator_ema = EMA_FSDP(self.transformer,\n                                      decay=self.training_args.ema_decay)\n        logger.info(\"Initialized generator EMA with decay=%s\",\n                    self.training_args.ema_decay)\n\n        # Initialize EMA for transformer_2 if it exists\n        if self.transformer_2 is not None:\n            self.generator_ema_2 = EMA_FSDP(\n                self.transformer_2, decay=self.training_args.ema_decay)\n            logger.info(\"Initialized generator EMA_2 with decay=%s\",\n                        self.training_args.ema_decay)\n    else:\n        logger.info(\"Generator EMA disabled (ema_decay &lt;= 0.0)\")\n</code></pre> fastvideo.training.distillation_pipeline.DistillationPipeline.initialize_validation_pipeline <code>abstractmethod</code> \u00b6 <pre><code>initialize_validation_pipeline(training_args: TrainingArgs)\n</code></pre> <p>Initialize validation pipeline - must be implemented by subclasses.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>@abstractmethod\ndef initialize_validation_pipeline(self, training_args: TrainingArgs):\n    \"\"\"Initialize validation pipeline - must be implemented by subclasses.\"\"\"\n    raise NotImplementedError(\n        \"Distillation pipelines must implement this method\")\n</code></pre> fastvideo.training.distillation_pipeline.DistillationPipeline.is_ema_ready \u00b6 <pre><code>is_ema_ready(current_step: int | None = None)\n</code></pre> <p>Check if EMA is ready for use (after ema_start_step).</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def is_ema_ready(self, current_step: int | None = None):\n    \"\"\"Check if EMA is ready for use (after ema_start_step).\"\"\"\n    if current_step is None:\n        current_step = getattr(self, 'current_trainstep', 0)\n    return (self.generator_ema is not None\n            and current_step &gt;= self.training_args.ema_start_step)\n</code></pre> fastvideo.training.distillation_pipeline.DistillationPipeline.load_module_from_path \u00b6 <pre><code>load_module_from_path(\n    model_path: str,\n    module_type: str,\n    training_args: TrainingArgs,\n)\n</code></pre> <p>Load a module from a specific path using the same loading logic as the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the model</p> required <code>module_type</code> <code>str</code> <p>Type of module to load (e.g., \"transformer\")</p> required <code>training_args</code> <code>TrainingArgs</code> <p>Training arguments</p> required <p>Returns:</p> Type Description <p>The loaded module</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def load_module_from_path(self, model_path: str, module_type: str,\n                          training_args: \"TrainingArgs\"):\n    \"\"\"\n    Load a module from a specific path using the same loading logic as the pipeline.\n\n    Args:\n        model_path: Path to the model\n        module_type: Type of module to load (e.g., \"transformer\")\n        training_args: Training arguments\n\n    Returns:\n        The loaded module\n    \"\"\"\n    logger.info(\"Loading %s from custom path: %s\", module_type, model_path)\n    # Set flag to prevent custom weight loading for teacher/critic models\n    training_args._loading_teacher_critic_model = True\n\n    try:\n        from fastvideo.models.loader.component_loader import (\n            PipelineComponentLoader)\n\n        # Download the model if it's a Hugging Face model ID\n        local_model_path = maybe_download_model(model_path)\n        logger.info(\"Model downloaded/found at: %s\", local_model_path)\n        config = verify_model_config_and_directory(local_model_path)\n\n        if module_type not in config:\n            if hasattr(self, '_extra_config_module_map'\n                       ) and module_type in self._extra_config_module_map:\n                extra_module = self._extra_config_module_map[module_type]\n                if extra_module in config:\n                    module_type = extra_module\n                    logger.info(\"Using %s for %s\", extra_module,\n                                module_type)\n                else:\n                    raise ValueError(\n                        f\"Module {module_type} not found in config at {local_model_path}\"\n                    )\n            else:\n                raise ValueError(\n                    f\"Module {module_type} not found in config at {local_model_path}\"\n                )\n\n        module_info = config[module_type]\n        if module_info is None:\n            raise ValueError(\n                f\"Module {module_type} has null value in config at {local_model_path}\"\n            )\n\n        transformers_or_diffusers, architecture = module_info\n        component_path = os.path.join(local_model_path, module_type)\n        module = PipelineComponentLoader.load_module(\n            module_name=module_type,\n            component_model_path=component_path,\n            transformers_or_diffusers=transformers_or_diffusers,\n            fastvideo_args=training_args,\n        )\n\n        logger.info(\"Successfully loaded %s from %s\", module_type,\n                    component_path)\n        return module\n    finally:\n        # Always clean up the flag\n        if hasattr(training_args, '_loading_teacher_critic_model'):\n            delattr(training_args, '_loading_teacher_critic_model')\n</code></pre> fastvideo.training.distillation_pipeline.DistillationPipeline.reset_ema \u00b6 <pre><code>reset_ema()\n</code></pre> <p>Reset EMA to current model weights.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def reset_ema(self):\n    \"\"\"Reset EMA to current model weights.\"\"\"\n    if self.generator_ema is not None:\n        logger.info(\"Resetting EMA to current model weights\")\n        self.generator_ema.update(self.transformer)\n        # Force update to current weights by setting decay to 0 temporarily\n        original_decay = self.generator_ema.decay\n        self.generator_ema.decay = 0.0\n        self.generator_ema.update(self.transformer)\n        self.generator_ema.decay = original_decay\n        logger.info(\"EMA reset completed\")\n    else:\n        logger.warning(\"Cannot reset EMA: EMA not initialized\")\n\n    if self.generator_ema_2 is not None:\n        logger.info(\"Resetting EMA_2 to current model weights\")\n        self.generator_ema_2.update(self.transformer_2)\n        # Force update to current weights by setting decay to 0 temporarily\n        original_decay_2 = self.generator_ema_2.decay\n        self.generator_ema_2.decay = 0.0\n        self.generator_ema_2.update(self.transformer_2)\n        self.generator_ema_2.decay = original_decay_2\n        logger.info(\"EMA_2 reset completed\")\n</code></pre> fastvideo.training.distillation_pipeline.DistillationPipeline.save_ema_weights \u00b6 <pre><code>save_ema_weights(output_dir: str, step: int)\n</code></pre> <p>Save EMA weights separately for inference purposes.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def save_ema_weights(self, output_dir: str, step: int):\n    \"\"\"Save EMA weights separately for inference purposes.\"\"\"\n    if self.generator_ema is None and self.generator_ema_2 is None:\n        logger.warning(\"Cannot save EMA weights: No EMA initialized\")\n        return\n\n    if not self.is_ema_ready():\n        logger.warning(\n            \"Cannot save EMA weights: EMA not ready yet (step &lt; ema_start_step)\"\n        )\n        return\n\n    try:\n        # Save main transformer EMA\n        if self.generator_ema is not None:\n            ema_model = self.get_ema_model_copy()\n            if ema_model is None:\n                logger.warning(\"Failed to create EMA model copy\")\n            else:\n                ema_save_dir = os.path.join(output_dir,\n                                            f\"ema_checkpoint-{step}\")\n                os.makedirs(ema_save_dir, exist_ok=True)\n\n                # save as diffusers format\n                from safetensors.torch import save_file\n\n                from fastvideo.training.training_utils import (\n                    custom_to_hf_state_dict, gather_state_dict_on_cpu_rank0)\n                cpu_state = gather_state_dict_on_cpu_rank0(ema_model,\n                                                           device=None)\n\n                if self.global_rank == 0:\n                    weight_path = os.path.join(\n                        ema_save_dir, \"diffusion_pytorch_model.safetensors\")\n                    diffusers_state_dict = custom_to_hf_state_dict(\n                        cpu_state, ema_model.reverse_param_names_mapping)\n                    save_file(diffusers_state_dict, weight_path)\n\n                    config_dict = ema_model.hf_config\n                    if \"dtype\" in config_dict:\n                        del config_dict[\"dtype\"]\n                    config_path = os.path.join(ema_save_dir, \"config.json\")\n                    with open(config_path, \"w\") as f:\n                        json.dump(config_dict, f, indent=4)\n\n                    logger.info(\"EMA weights saved to %s\", weight_path)\n\n                del ema_model\n\n        # Save transformer_2 EMA\n        if self.generator_ema_2 is not None:\n            ema_2_model = self.get_ema_2_model_copy()\n            if ema_2_model is None:\n                logger.warning(\"Failed to create EMA_2 model copy\")\n            else:\n                ema_2_save_dir = os.path.join(output_dir,\n                                              f\"ema_2_checkpoint-{step}\")\n                os.makedirs(ema_2_save_dir, exist_ok=True)\n\n                # save as diffusers format\n                from safetensors.torch import save_file\n\n                from fastvideo.training.training_utils import (\n                    custom_to_hf_state_dict, gather_state_dict_on_cpu_rank0)\n                cpu_state_2 = gather_state_dict_on_cpu_rank0(ema_2_model,\n                                                             device=None)\n\n                if self.global_rank == 0:\n                    weight_path_2 = os.path.join(\n                        ema_2_save_dir,\n                        \"diffusion_pytorch_model.safetensors\")\n                    diffusers_state_dict_2 = custom_to_hf_state_dict(\n                        cpu_state_2,\n                        ema_2_model.reverse_param_names_mapping)\n                    save_file(diffusers_state_dict_2, weight_path_2)\n\n                    config_dict_2 = ema_2_model.hf_config\n                    if \"dtype\" in config_dict_2:\n                        del config_dict_2[\"dtype\"]\n                    config_path_2 = os.path.join(ema_2_save_dir,\n                                                 \"config.json\")\n                    with open(config_path_2, \"w\") as f:\n                        json.dump(config_dict_2, f, indent=4)\n\n                    logger.info(\"EMA_2 weights saved to %s\", weight_path_2)\n\n                del ema_2_model\n\n    except Exception as e:\n        logger.error(\"Failed to save EMA weights: %s\", str(e))\n</code></pre> fastvideo.training.distillation_pipeline.DistillationPipeline.train \u00b6 <pre><code>train() -&gt; None\n</code></pre> <p>Main training loop with distillation-specific logging.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def train(self) -&gt; None:\n    \"\"\"Main training loop with distillation-specific logging.\"\"\"\n    assert self.training_args.seed is not None, \"seed must be set\"\n    seed = self.training_args.seed\n\n    # Set the same seed within each SP group to ensure reproducibility\n    if self.sp_world_size &gt; 1:\n        # Use the same seed for all processes within the same SP group\n        sp_group_seed = seed + (self.global_rank // self.sp_world_size)\n        set_random_seed(sp_group_seed)\n        logger.info(\"Rank %s: Using SP group seed %s\", self.global_rank,\n                    sp_group_seed)\n    else:\n        set_random_seed(seed + self.global_rank)\n\n    # Set random seeds for deterministic training\n    self.noise_random_generator = torch.Generator(device=\"cpu\").manual_seed(\n        self.seed)\n    self.noise_gen_cuda = torch.Generator(device=\"cuda\").manual_seed(\n        self.seed)\n    self.validation_random_generator = torch.Generator(\n        device=\"cpu\").manual_seed(self.seed)\n    logger.info(\"Initialized random seeds with seed: %s\", seed)\n\n    # Initialize current_trainstep for EMA ready checks\n    #TODO: check if needed\n    self.current_trainstep = self.init_steps\n\n    # Resume from checkpoint if specified (this will restore random states)\n    if self.training_args.resume_from_checkpoint:\n        self._resume_from_checkpoint()\n        logger.info(\"Resumed from checkpoint, random states restored\")\n    else:\n        logger.info(\"Starting training from scratch\")\n\n    self.train_loader_iter = iter(self.train_dataloader)\n\n    step_times: deque[float] = deque(maxlen=100)\n\n    self._log_training_info()\n    self._log_validation(self.transformer, self.training_args,\n                         self.init_steps)\n\n    progress_bar = tqdm(\n        range(0, self.training_args.max_train_steps),\n        initial=self.init_steps,\n        desc=\"Steps\",\n        disable=self.local_rank &gt; 0,\n    )\n\n    use_vsa = vsa_available and envs.FASTVIDEO_ATTENTION_BACKEND == \"VIDEO_SPARSE_ATTN\"\n    for step in range(self.init_steps + 1,\n                      self.training_args.max_train_steps + 1):\n        start_time = time.perf_counter()\n        if use_vsa:\n            vsa_sparsity = self.training_args.VSA_sparsity\n            vsa_decay_rate = self.training_args.VSA_decay_rate\n            vsa_decay_interval_steps = self.training_args.VSA_decay_interval_steps\n            if vsa_decay_interval_steps &gt; 1:\n                current_decay_times = min(step // vsa_decay_interval_steps,\n                                          vsa_sparsity // vsa_decay_rate)\n                current_vsa_sparsity = current_decay_times * vsa_decay_rate\n            else:\n                current_vsa_sparsity = vsa_sparsity\n        else:\n            current_vsa_sparsity = 0.0\n\n        training_batch = TrainingBatch()\n        self.current_trainstep = step\n        training_batch.current_vsa_sparsity = current_vsa_sparsity\n\n        if (step &gt;= self.training_args.ema_start_step) and \\\n                (self.generator_ema is None) and (self.training_args.ema_decay &gt; 0):\n            self.generator_ema = EMA_FSDP(\n                self.transformer, decay=self.training_args.ema_decay)\n            logger.info(\"Created generator EMA at step %s with decay=%s\",\n                        step, self.training_args.ema_decay)\n\n            # Create EMA for transformer_2 if it exists\n            if self.transformer_2 is not None and self.generator_ema_2 is None:\n                self.generator_ema_2 = EMA_FSDP(\n                    self.transformer_2, decay=self.training_args.ema_decay)\n                logger.info(\n                    \"Created generator EMA_2 at step %s with decay=%s\",\n                    step, self.training_args.ema_decay)\n\n        with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n            training_batch = self.train_one_step(training_batch)\n\n        total_loss = training_batch.total_loss\n        generator_loss = training_batch.generator_loss\n        fake_score_loss = training_batch.fake_score_loss\n        grad_norm = training_batch.grad_norm\n\n        step_time = time.perf_counter() - start_time\n        step_times.append(step_time)\n        avg_step_time = sum(step_times) / len(step_times)\n\n        progress_bar.set_postfix({\n            \"total_loss\":\n            f\"{total_loss:.4f}\",\n            \"generator_loss\":\n            f\"{generator_loss:.4f}\",\n            \"fake_score_loss\":\n            f\"{fake_score_loss:.4f}\",\n            \"step_time\":\n            f\"{step_time:.2f}s\",\n            \"grad_norm\":\n            grad_norm,\n            \"ema\":\n            \"\u2713\" if (self.generator_ema is not None and self.is_ema_ready())\n            else \"\u2717\",\n            \"ema2\":\n            \"\u2713\" if (self.generator_ema_2 is not None\n                    and self.is_ema_ready()) else \"\u2717\",\n        })\n        progress_bar.update(1)\n\n        if self.global_rank == 0:\n            # Prepare logging data\n            log_data = {\n                \"train_total_loss\":\n                total_loss,\n                \"train_fake_score_loss\":\n                fake_score_loss,\n                \"learning_rate\":\n                self.lr_scheduler.get_last_lr()[0],\n                \"fake_score_learning_rate\":\n                self.fake_score_lr_scheduler.get_last_lr()[0],\n                \"step_time\":\n                step_time,\n                \"avg_step_time\":\n                avg_step_time,\n                \"grad_norm\":\n                grad_norm,\n            }\n            # Only log generator loss when generator is actually trained\n            if (step % self.generator_update_interval == 0):\n                log_data[\"train_generator_loss\"] = generator_loss\n            if use_vsa:\n                log_data[\"VSA_train_sparsity\"] = current_vsa_sparsity\n\n            if self.generator_ema is not None or self.generator_ema_2 is not None:\n                log_data[\"ema_enabled\"] = self.generator_ema is not None\n                log_data[\"ema_2_enabled\"] = self.generator_ema_2 is not None\n                log_data[\"ema_decay\"] = self.training_args.ema_decay\n            else:\n                log_data[\"ema_enabled\"] = False\n                log_data[\"ema_2_enabled\"] = False\n\n            ema_stats = self.get_ema_stats()\n            log_data.update(ema_stats)\n\n            if training_batch.dmd_latent_vis_dict:\n                dmd_additional_logs = {\n                    \"generator_timestep\":\n                    training_batch.\n                    dmd_latent_vis_dict[\"generator_timestep\"].item(),\n                    \"dmd_timestep\":\n                    training_batch.dmd_latent_vis_dict[\"dmd_timestep\"].item(\n                    ),\n                }\n                log_data.update(dmd_additional_logs)\n\n            faker_score_additional_logs = {\n                \"fake_score_timestep\":\n                training_batch.\n                fake_score_latent_vis_dict[\"fake_score_timestep\"].item(),\n            }\n            log_data.update(faker_score_additional_logs)\n\n            self.tracker.log(log_data, step)\n\n        # Save training state checkpoint (for resuming training)\n        if (self.training_args.training_state_checkpointing_steps &gt; 0\n                and step %\n                self.training_args.training_state_checkpointing_steps == 0):\n            print(\"rank\", self.global_rank,\n                  \"save training state checkpoint at step\", step)\n            save_distillation_checkpoint(\n                self.transformer,\n                self.fake_score_transformer,\n                self.global_rank,\n                self.training_args.output_dir,\n                step,\n                self.optimizer,\n                self.fake_score_optimizer,\n                self.train_dataloader,\n                self.lr_scheduler,\n                self.fake_score_lr_scheduler,\n                self.noise_random_generator,\n                self.generator_ema,\n                # MoE support\n                generator_transformer_2=getattr(self, 'transformer_2',\n                                                None),\n                real_score_transformer_2=getattr(\n                    self, 'real_score_transformer_2', None),\n                fake_score_transformer_2=getattr(\n                    self, 'fake_score_transformer_2', None),\n                generator_optimizer_2=getattr(self, 'optimizer_2', None),\n                fake_score_optimizer_2=getattr(self,\n                                               'fake_score_optimizer_2',\n                                               None),\n                generator_scheduler_2=getattr(self, 'lr_scheduler_2', None),\n                fake_score_scheduler_2=getattr(self,\n                                               'fake_score_lr_scheduler_2',\n                                               None),\n                generator_ema_2=getattr(self, 'generator_ema_2', None))\n\n            if self.transformer:\n                self.transformer.train()\n            self.sp_group.barrier()\n\n        # Save weight-only checkpoint\n        if (self.training_args.weight_only_checkpointing_steps &gt; 0\n                and step %\n                self.training_args.weight_only_checkpointing_steps == 0):\n            print(\"rank\", self.global_rank,\n                  \"save weight-only checkpoint at step\", step)\n            save_distillation_checkpoint(\n                self.transformer,\n                self.fake_score_transformer,\n                self.global_rank,\n                self.training_args.output_dir,\n                f\"{step}_weight_only\",\n                only_save_generator_weight=True,\n                generator_ema=self.generator_ema,\n                # MoE support\n                generator_transformer_2=getattr(self, 'transformer_2',\n                                                None),\n                real_score_transformer_2=getattr(\n                    self, 'real_score_transformer_2', None),\n                fake_score_transformer_2=getattr(\n                    self, 'fake_score_transformer_2', None),\n                generator_optimizer_2=getattr(self, 'optimizer_2', None),\n                fake_score_optimizer_2=getattr(self,\n                                               'fake_score_optimizer_2',\n                                               None),\n                generator_scheduler_2=getattr(self, 'lr_scheduler_2', None),\n                fake_score_scheduler_2=getattr(self,\n                                               'fake_score_lr_scheduler_2',\n                                               None),\n                generator_ema_2=getattr(self, 'generator_ema_2', None))\n\n            if self.training_args.use_ema and self.is_ema_ready():\n                self.save_ema_weights(self.training_args.output_dir, step)\n\n        if self.training_args.log_validation and step % self.training_args.validation_steps == 0:\n            if self.training_args.log_visualization:\n                self.visualize_intermediate_latents(training_batch,\n                                                    self.training_args,\n                                                    step)\n            self._log_validation(self.transformer, self.training_args, step)\n\n    self.tracker.finish()\n\n    # Save final training state checkpoint\n    print(\"rank\", self.global_rank,\n          \"save final training state checkpoint at step\",\n          self.training_args.max_train_steps)\n    save_distillation_checkpoint(\n        self.transformer,\n        self.fake_score_transformer,\n        self.global_rank,\n        self.training_args.output_dir,\n        self.training_args.max_train_steps,\n        self.optimizer,\n        self.fake_score_optimizer,\n        self.train_dataloader,\n        self.lr_scheduler,\n        self.fake_score_lr_scheduler,\n        self.noise_random_generator,\n        self.generator_ema,\n        # MoE support\n        generator_transformer_2=getattr(self, 'transformer_2', None),\n        real_score_transformer_2=getattr(self, 'real_score_transformer_2',\n                                         None),\n        fake_score_transformer_2=getattr(self, 'fake_score_transformer_2',\n                                         None),\n        generator_optimizer_2=getattr(self, 'optimizer_2', None),\n        fake_score_optimizer_2=getattr(self, 'fake_score_optimizer_2',\n                                       None),\n        generator_scheduler_2=getattr(self, 'lr_scheduler_2', None),\n        fake_score_scheduler_2=getattr(self, 'fake_score_lr_scheduler_2',\n                                       None),\n        generator_ema_2=getattr(self, 'generator_ema_2', None))\n\n    if self.training_args.use_ema and self.is_ema_ready():\n        self.save_ema_weights(self.training_args.output_dir,\n                              self.training_args.max_train_steps)\n\n    if envs.FASTVIDEO_TORCH_PROFILER_DIR:\n        logger.info(\"Stopping profiler...\")\n        self.profiler_controller.stop()\n        logger.info(\"Profiler stopped.\")\n\n    if get_sp_group():\n        cleanup_dist_env_and_memory()\n</code></pre> fastvideo.training.distillation_pipeline.DistillationPipeline.visualize_intermediate_latents \u00b6 <pre><code>visualize_intermediate_latents(\n    training_batch: TrainingBatch,\n    training_args: TrainingArgs,\n    step: int,\n)\n</code></pre> <p>Add visualization data to tracker logging and save frames to disk.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def visualize_intermediate_latents(self, training_batch: TrainingBatch,\n                                   training_args: TrainingArgs, step: int):\n    \"\"\"Add visualization data to tracker logging and save frames to disk.\"\"\"\n    tracker_loss_dict: dict[str, Any] = {}\n    dmd_latents_vis_dict = training_batch.dmd_latent_vis_dict\n    fake_score_latents_vis_dict = training_batch.fake_score_latent_vis_dict\n    fake_score_log_keys = ['generator_pred_video']\n    dmd_log_keys = ['faker_score_pred_video', 'real_score_pred_video']\n\n    for latent_key in fake_score_log_keys:\n        latents = fake_score_latents_vis_dict[latent_key]\n        latents = latents.permute(0, 2, 1, 3, 4)\n\n        if isinstance(self.vae.scaling_factor, torch.Tensor):\n            latents = latents / self.vae.scaling_factor.to(\n                latents.device, latents.dtype)\n        else:\n            latents = latents / self.vae.scaling_factor\n\n        # Apply shifting if needed\n        if (hasattr(self.vae, \"shift_factor\")\n                and self.vae.shift_factor is not None):\n            if isinstance(self.vae.shift_factor, torch.Tensor):\n                latents += self.vae.shift_factor.to(latents.device,\n                                                    latents.dtype)\n            else:\n                latents += self.vae.shift_factor\n            with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n                video = self.vae.decode(latents)\n            video = (video / 2 + 0.5).clamp(0, 1)\n            video = video.cpu().float()\n            video = video.permute(0, 2, 1, 3, 4)\n            video = (video * 255).numpy().astype(np.uint8)\n            video_artifact = self.tracker.video(\n                video, fps=24, format=\"mp4\")  # change to 16 for Wan2.1\n            if video_artifact is not None:\n                tracker_loss_dict[latent_key] = video_artifact\n            # Clean up references\n            del video, latents\n\n    # Process DMD training data if available - use decode_stage instead of self.vae.decode\n    if 'generator_pred_video' in dmd_latents_vis_dict:\n        for latent_key in dmd_log_keys:\n            latents = dmd_latents_vis_dict[latent_key]\n            latents = latents.permute(0, 2, 1, 3, 4)\n            # decoded_latent = decode_stage(ForwardBatch(data_type=\"video\", latents=latents), training_args)\n            if isinstance(self.vae.scaling_factor, torch.Tensor):\n                latents = latents / self.vae.scaling_factor.to(\n                    latents.device, latents.dtype)\n            else:\n                latents = latents / self.vae.scaling_factor\n\n            # Apply shifting if needed\n            if (hasattr(self.vae, \"shift_factor\")\n                    and self.vae.shift_factor is not None):\n                if isinstance(self.vae.shift_factor, torch.Tensor):\n                    latents += self.vae.shift_factor.to(\n                        latents.device, latents.dtype)\n                else:\n                    latents += self.vae.shift_factor\n            with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n                video = self.vae.decode(latents)\n            video = (video / 2 + 0.5).clamp(0, 1)\n            video = video.cpu().float()\n            video = video.permute(0, 2, 1, 3, 4)\n            video = (video * 255).numpy().astype(np.uint8)\n            video_artifact = self.tracker.video(\n                video, fps=24, format=\"mp4\")  # change to 16 for Wan2.1\n            if video_artifact is not None:\n                tracker_loss_dict[latent_key] = video_artifact\n            # Clean up references\n            del video, latents\n\n    # Log to tracker\n    if self.global_rank == 0 and tracker_loss_dict:\n        self.tracker.log_artifacts(tracker_loss_dict, step)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.training.distillation_pipeline-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideo.training.ode_causal_pipeline","title":"fastvideo.training.ode_causal_pipeline","text":""},{"location":"api/fastvideo/#fastvideo.training.ode_causal_pipeline-classes","title":"Classes","text":"fastvideo.training.ode_causal_pipeline.ODEInitTrainingPipeline \u00b6 <pre><code>ODEInitTrainingPipeline(\n    model_path: str,\n    fastvideo_args: TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>TrainingPipeline</code></p> <p>Training pipeline for ODE-init using precomputed denoising trajectories.</p> <p>Supervision: predict the next latent in the stored trajectory by - feeding current latent at timestep t into the transformer to predict noise - stepping the scheduler with the predicted noise - minimizing MSE to the stored next latent at timestep t_next</p> Source code in <code>fastvideo/training/training_pipeline.py</code> <pre><code>def __init__(\n        self,\n        model_path: str,\n        fastvideo_args: TrainingArgs,\n        required_config_modules: list[str] | None = None,\n        loaded_modules: dict[str, torch.nn.Module] | None = None) -&gt; None:\n    fastvideo_args.inference_mode = False\n    self.lora_training = fastvideo_args.lora_training\n    if self.lora_training and fastvideo_args.lora_rank is None:\n        raise ValueError(\"lora rank must be set when using lora training\")\n\n    set_random_seed(fastvideo_args.seed)  # for lora param init\n    super().__init__(model_path, fastvideo_args, required_config_modules,\n                     loaded_modules)  # type: ignore\n    self.tracker = DummyTracker()\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.training.ode_causal_pipeline-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideo.training.self_forcing_distillation_pipeline","title":"fastvideo.training.self_forcing_distillation_pipeline","text":""},{"location":"api/fastvideo/#fastvideo.training.self_forcing_distillation_pipeline-classes","title":"Classes","text":"fastvideo.training.self_forcing_distillation_pipeline.SelfForcingDistillationPipeline \u00b6 <pre><code>SelfForcingDistillationPipeline(\n    model_path: str,\n    fastvideo_args: TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>DistillationPipeline</code></p> <p>A self-forcing distillation pipeline that alternates between training the generator and critic based on the self-forcing methodology.</p> <p>This implementation follows the self-forcing approach where: 1. Generator and critic are trained in alternating steps 2. Generator loss uses DMD-style loss with the critic as fake score 3. Critic loss trains the fake score model to distinguish real vs fake</p> Source code in <code>fastvideo/training/training_pipeline.py</code> <pre><code>def __init__(\n        self,\n        model_path: str,\n        fastvideo_args: TrainingArgs,\n        required_config_modules: list[str] | None = None,\n        loaded_modules: dict[str, torch.nn.Module] | None = None) -&gt; None:\n    fastvideo_args.inference_mode = False\n    self.lora_training = fastvideo_args.lora_training\n    if self.lora_training and fastvideo_args.lora_rank is None:\n        raise ValueError(\"lora rank must be set when using lora training\")\n\n    set_random_seed(fastvideo_args.seed)  # for lora param init\n    super().__init__(model_path, fastvideo_args, required_config_modules,\n                     loaded_modules)  # type: ignore\n    self.tracker = DummyTracker()\n</code></pre> Functions\u00b6 fastvideo.training.self_forcing_distillation_pipeline.SelfForcingDistillationPipeline.critic_loss \u00b6 <pre><code>critic_loss(\n    training_batch: TrainingBatch,\n) -&gt; tuple[torch.Tensor, dict[str, Any]]\n</code></pre> <p>Compute critic loss using flow matching between noise and generator output. The critic learns to predict the flow from noise to the generator's output.</p> Source code in <code>fastvideo/training/self_forcing_distillation_pipeline.py</code> <pre><code>def critic_loss(\n        self, training_batch: TrainingBatch\n) -&gt; tuple[torch.Tensor, dict[str, Any]]:\n    \"\"\"\n    Compute critic loss using flow matching between noise and generator output.\n    The critic learns to predict the flow from noise to the generator's output.\n    \"\"\"\n    updated_batch, flow_matching_loss = self.faker_score_forward(\n        training_batch)\n    training_batch.fake_score_latent_vis_dict = updated_batch.fake_score_latent_vis_dict\n    log_dict: dict[str, Any] = {}\n\n    return flow_matching_loss, log_dict\n</code></pre> fastvideo.training.self_forcing_distillation_pipeline.SelfForcingDistillationPipeline.generate_and_sync_list \u00b6 <pre><code>generate_and_sync_list(\n    num_blocks: int,\n    num_denoising_steps: int,\n    device: device,\n) -&gt; list[int]\n</code></pre> <p>Generate and synchronize random exit flags across distributed processes.</p> Source code in <code>fastvideo/training/self_forcing_distillation_pipeline.py</code> <pre><code>def generate_and_sync_list(self, num_blocks: int, num_denoising_steps: int,\n                           device: torch.device) -&gt; list[int]:\n    \"\"\"Generate and synchronize random exit flags across distributed processes.\"\"\"\n    logger.info(\n        \"RANK: %s, enter generate_and_sync_list blocks=%s steps=%s device=%s\",\n        self.global_rank,\n        num_blocks,\n        num_denoising_steps,\n        str(device),\n        local_main_process_only=False)\n    rank = dist.get_rank() if dist.is_initialized() else 0\n\n    if rank == 0:\n        # Generate random indices\n        indices = torch.randint(low=0,\n                                high=num_denoising_steps,\n                                size=(num_blocks, ),\n                                device=device)\n        if self.last_step_only:\n            indices = torch.ones_like(indices) * (num_denoising_steps - 1)\n    else:\n        indices = torch.empty(num_blocks, dtype=torch.long, device=device)\n\n    if dist.is_initialized():\n        dist.broadcast(indices,\n                       src=0)  # Broadcast the random indices to all ranks\n    flags = indices.tolist()\n    logger.info(\n        \"RANK: %s, exit generate_and_sync_list flags_len=%s first=%s\",\n        self.global_rank,\n        len(flags),\n        flags[0] if len(flags) &gt; 0 else None,\n        local_main_process_only=False)\n    return flags\n</code></pre> fastvideo.training.self_forcing_distillation_pipeline.SelfForcingDistillationPipeline.generator_loss \u00b6 <pre><code>generator_loss(\n    training_batch: TrainingBatch,\n) -&gt; tuple[torch.Tensor, dict[str, Any]]\n</code></pre> <p>Compute generator loss using DMD-style approach. The generator tries to fool the critic (fake_score_transformer).</p> Source code in <code>fastvideo/training/self_forcing_distillation_pipeline.py</code> <pre><code>def generator_loss(\n        self, training_batch: TrainingBatch\n) -&gt; tuple[torch.Tensor, dict[str, Any]]:\n    \"\"\"\n    Compute generator loss using DMD-style approach.\n    The generator tries to fool the critic (fake_score_transformer).\n    \"\"\"\n    with set_forward_context(\n            current_timestep=training_batch.timesteps,\n            attn_metadata=training_batch.attn_metadata_vsa):\n        generator_pred_video = self._generator_multi_step_simulation_forward(\n            training_batch)\n\n    with set_forward_context(current_timestep=training_batch.timesteps,\n                             attn_metadata=training_batch.attn_metadata):\n        dmd_loss = self._dmd_forward(\n            generator_pred_video=generator_pred_video,\n            training_batch=training_batch)\n\n    log_dict = {\n        \"dmdtrain_gradient_norm\": torch.tensor(0.0, device=self.device)\n    }\n\n    return dmd_loss, log_dict\n</code></pre> fastvideo.training.self_forcing_distillation_pipeline.SelfForcingDistillationPipeline.initialize_training_pipeline \u00b6 <pre><code>initialize_training_pipeline(training_args: TrainingArgs)\n</code></pre> <p>Initialize the self-forcing training pipeline.</p> Source code in <code>fastvideo/training/self_forcing_distillation_pipeline.py</code> <pre><code>def initialize_training_pipeline(self, training_args: TrainingArgs):\n    \"\"\"Initialize the self-forcing training pipeline.\"\"\"\n    logger.info(\"Initializing self-forcing distillation pipeline...\")\n\n    self.generator_ema: EMA_FSDP | None = None\n    self.generator_ema_2: EMA_FSDP | None = None\n\n    super().initialize_training_pipeline(training_args)\n    try:\n        logger.info(\"RANK: %s, entered initialize_training_pipeline\",\n                    self.global_rank,\n                    local_main_process_only=False)\n    except Exception:\n        logger.info(\"Entered initialize_training_pipeline (rank unknown)\")\n\n    self.noise_scheduler = SelfForcingFlowMatchScheduler(\n        num_inference_steps=1000,\n        shift=5.0,\n        sigma_min=0.0,\n        extra_one_step=True,\n        training=True)\n    self.dfake_gen_update_ratio = getattr(training_args,\n                                          'dfake_gen_update_ratio', 5)\n\n    self.num_frame_per_block = getattr(training_args, 'num_frame_per_block',\n                                       3)\n    self.independent_first_frame = getattr(training_args,\n                                           'independent_first_frame', False)\n    self.same_step_across_blocks = getattr(training_args,\n                                           'same_step_across_blocks', False)\n    self.last_step_only = getattr(training_args, 'last_step_only', False)\n    self.context_noise = getattr(training_args, 'context_noise', 0)\n\n    self.kv_cache1: list[dict[str, Any]] | None = None\n    self.crossattn_cache: list[dict[str, Any]] | None = None\n\n    logger.info(\"Self-forcing generator update ratio: %s\",\n                self.dfake_gen_update_ratio)\n    logger.info(\"RANK: %s, exiting initialize_training_pipeline\",\n                self.global_rank,\n                local_main_process_only=False)\n</code></pre> fastvideo.training.self_forcing_distillation_pipeline.SelfForcingDistillationPipeline.train \u00b6 <pre><code>train() -&gt; None\n</code></pre> <p>Main training loop with self-forcing specific logging.</p> Source code in <code>fastvideo/training/self_forcing_distillation_pipeline.py</code> <pre><code>@profile_region(\"profiler_region_training_train\")\ndef train(self) -&gt; None:\n    \"\"\"Main training loop with self-forcing specific logging.\"\"\"\n    assert self.training_args.seed is not None, \"seed must be set\"\n    seed = self.training_args.seed\n\n    # Set the same seed within each SP group to ensure reproducibility\n    if self.sp_world_size &gt; 1:\n        # Use the same seed for all processes within the same SP group\n        sp_group_seed = seed + (self.global_rank // self.sp_world_size)\n        set_random_seed(sp_group_seed)\n    else:\n        set_random_seed(seed + self.global_rank)\n\n    self.noise_random_generator = torch.Generator(device=\"cpu\").manual_seed(\n        self.seed)\n    self.noise_gen_cuda = torch.Generator(device=\"cuda\").manual_seed(\n        self.seed)\n    self.validation_random_generator = torch.Generator(\n        device=\"cpu\").manual_seed(self.seed)\n    logger.info(\"Initialized random seeds with seed: %s\", seed)\n\n    self.current_trainstep = self.init_steps\n\n    if self.training_args.resume_from_checkpoint:\n        self._resume_from_checkpoint()\n        logger.info(\"Resumed from checkpoint, random states restored\")\n    else:\n        logger.info(\"Starting training from scratch\")\n\n    self.train_loader_iter = iter(self.train_dataloader)\n\n    step_times: deque[float] = deque(maxlen=100)\n\n    self._log_training_info()\n    self._log_validation(self.transformer, self.training_args,\n                         self.init_steps)\n\n    progress_bar = tqdm(\n        range(0, self.training_args.max_train_steps),\n        initial=self.init_steps,\n        desc=\"Steps\",\n        disable=self.local_rank &gt; 0,\n    )\n\n    use_vsa = vsa_available and envs.FASTVIDEO_ATTENTION_BACKEND == \"VIDEO_SPARSE_ATTN\"\n    for step in range(self.init_steps + 1,\n                      self.training_args.max_train_steps + 1):\n        start_time = time.perf_counter()\n        if use_vsa:\n            vsa_sparsity = self.training_args.VSA_sparsity\n            vsa_decay_rate = self.training_args.VSA_decay_rate\n            vsa_decay_interval_steps = self.training_args.VSA_decay_interval_steps\n            if vsa_decay_interval_steps &gt; 1:\n                current_decay_times = min(step // vsa_decay_interval_steps,\n                                          vsa_sparsity // vsa_decay_rate)\n                current_vsa_sparsity = current_decay_times * vsa_decay_rate\n            else:\n                current_vsa_sparsity = vsa_sparsity\n        else:\n            current_vsa_sparsity = 0.0\n\n        training_batch = TrainingBatch()\n        self.current_trainstep = step\n        training_batch.current_vsa_sparsity = current_vsa_sparsity\n\n        if (step &gt;= self.training_args.ema_start_step) and \\\n                (self.generator_ema is None) and (self.training_args.ema_decay &gt; 0):\n            self.generator_ema = EMA_FSDP(\n                self.transformer, decay=self.training_args.ema_decay)\n            logger.info(\"Created generator EMA at step %s with decay=%s\",\n                        step, self.training_args.ema_decay)\n\n            # Create EMA for transformer_2 if it exists\n            if self.transformer_2 is not None and self.generator_ema_2 is None:\n                self.generator_ema_2 = EMA_FSDP(\n                    self.transformer_2, decay=self.training_args.ema_decay)\n                logger.info(\n                    \"Created generator EMA_2 at step %s with decay=%s\",\n                    step, self.training_args.ema_decay)\n\n        with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n            training_batch = self.train_one_step(training_batch)\n\n        total_loss = training_batch.total_loss\n        generator_loss = training_batch.generator_loss\n        fake_score_loss = training_batch.fake_score_loss\n        grad_norm = training_batch.grad_norm\n\n        step_time = time.perf_counter() - start_time\n        step_times.append(step_time)\n        avg_step_time = sum(step_times) / len(step_times)\n\n        progress_bar.set_postfix({\n            \"total_loss\":\n            f\"{total_loss:.4f}\",\n            \"generator_loss\":\n            f\"{generator_loss:.4f}\",\n            \"fake_score_loss\":\n            f\"{fake_score_loss:.4f}\",\n            \"step_time\":\n            f\"{step_time:.2f}s\",\n            \"grad_norm\":\n            grad_norm,\n            \"ema\":\n            \"\u2713\" if (self.generator_ema is not None and self.is_ema_ready())\n            else \"\u2717\",\n            \"ema2\":\n            \"\u2713\" if (self.generator_ema_2 is not None\n                    and self.is_ema_ready()) else \"\u2717\",\n        })\n        progress_bar.update(1)\n\n        if self.global_rank == 0:\n            log_data = {\n                \"train_total_loss\":\n                total_loss,\n                \"train_fake_score_loss\":\n                fake_score_loss,\n                \"learning_rate\":\n                self.lr_scheduler.get_last_lr()[0],\n                \"fake_score_learning_rate\":\n                self.fake_score_lr_scheduler.get_last_lr()[0],\n                \"step_time\":\n                step_time,\n                \"avg_step_time\":\n                avg_step_time,\n                \"grad_norm\":\n                grad_norm,\n            }\n            if (step % self.dfake_gen_update_ratio == 0):\n                log_data[\"train_generator_loss\"] = generator_loss\n            if use_vsa:\n                log_data[\"VSA_train_sparsity\"] = current_vsa_sparsity\n\n            if self.generator_ema is not None or self.generator_ema_2 is not None:\n                log_data[\"ema_enabled\"] = self.generator_ema is not None\n                log_data[\"ema_2_enabled\"] = self.generator_ema_2 is not None\n                log_data[\"ema_decay\"] = self.training_args.ema_decay\n            else:\n                log_data[\"ema_enabled\"] = False\n                log_data[\"ema_2_enabled\"] = False\n\n            ema_stats = self.get_ema_stats()\n            log_data.update(ema_stats)\n\n            if training_batch.dmd_latent_vis_dict:\n                dmd_additional_logs = {\n                    \"generator_timestep\":\n                    training_batch.\n                    dmd_latent_vis_dict[\"generator_timestep\"].item(),\n                    \"dmd_timestep\":\n                    training_batch.dmd_latent_vis_dict[\"dmd_timestep\"].item(\n                    ),\n                }\n                log_data.update(dmd_additional_logs)\n\n            faker_score_additional_logs = {\n                \"fake_score_timestep\":\n                training_batch.\n                fake_score_latent_vis_dict[\"fake_score_timestep\"].item(),\n            }\n            log_data.update(faker_score_additional_logs)\n\n            self.tracker.log(log_data, step)\n\n            if self.training_args.log_validation and step % self.training_args.validation_steps == 0 and self.training_args.log_visualization:\n                self.visualize_intermediate_latents(training_batch,\n                                                    self.training_args,\n                                                    step)\n\n        if (self.training_args.training_state_checkpointing_steps &gt; 0\n                and step %\n                self.training_args.training_state_checkpointing_steps == 0):\n            print(\"rank\", self.global_rank,\n                  \"save training state checkpoint at step\", step)\n            save_distillation_checkpoint(\n                self.transformer,\n                self.fake_score_transformer,\n                self.global_rank,\n                self.training_args.output_dir,\n                step,\n                self.optimizer,\n                self.fake_score_optimizer,\n                self.train_dataloader,\n                self.lr_scheduler,\n                self.fake_score_lr_scheduler,\n                self.noise_random_generator,\n                self.generator_ema,\n                # MoE support\n                generator_transformer_2=getattr(self, 'transformer_2',\n                                                None),\n                real_score_transformer_2=getattr(\n                    self, 'real_score_transformer_2', None),\n                fake_score_transformer_2=getattr(\n                    self, 'fake_score_transformer_2', None),\n                generator_optimizer_2=getattr(self, 'optimizer_2', None),\n                fake_score_optimizer_2=getattr(self,\n                                               'fake_score_optimizer_2',\n                                               None),\n                generator_scheduler_2=getattr(self, 'lr_scheduler_2', None),\n                fake_score_scheduler_2=getattr(self,\n                                               'fake_score_lr_scheduler_2',\n                                               None),\n                generator_ema_2=getattr(self, 'generator_ema_2', None))\n\n            if self.transformer:\n                self.transformer.train()\n            self.sp_group.barrier()\n\n        if (self.training_args.weight_only_checkpointing_steps &gt; 0\n                and step %\n                self.training_args.weight_only_checkpointing_steps == 0):\n            print(\"rank\", self.global_rank,\n                  \"save weight-only checkpoint at step\", step)\n            save_distillation_checkpoint(\n                self.transformer,\n                self.fake_score_transformer,\n                self.global_rank,\n                self.training_args.output_dir,\n                f\"{step}_weight_only\",\n                only_save_generator_weight=True,\n                generator_ema=self.generator_ema,\n                # MoE support\n                generator_transformer_2=getattr(self, 'transformer_2',\n                                                None),\n                real_score_transformer_2=getattr(\n                    self, 'real_score_transformer_2', None),\n                fake_score_transformer_2=getattr(\n                    self, 'fake_score_transformer_2', None),\n                generator_optimizer_2=getattr(self, 'optimizer_2', None),\n                fake_score_optimizer_2=getattr(self,\n                                               'fake_score_optimizer_2',\n                                               None),\n                generator_scheduler_2=getattr(self, 'lr_scheduler_2', None),\n                fake_score_scheduler_2=getattr(self,\n                                               'fake_score_lr_scheduler_2',\n                                               None),\n                generator_ema_2=getattr(self, 'generator_ema_2', None))\n\n            if self.training_args.use_ema and self.is_ema_ready():\n                self.save_ema_weights(self.training_args.output_dir, step)\n\n        if self.training_args.log_validation and step % self.training_args.validation_steps == 0:\n            self._log_validation(self.transformer, self.training_args, step)\n\n    self.tracker.finish()\n\n    print(\"rank\", self.global_rank,\n          \"save final training state checkpoint at step\",\n          self.training_args.max_train_steps)\n    save_distillation_checkpoint(\n        self.transformer,\n        self.fake_score_transformer,\n        self.global_rank,\n        self.training_args.output_dir,\n        self.training_args.max_train_steps,\n        self.optimizer,\n        self.fake_score_optimizer,\n        self.train_dataloader,\n        self.lr_scheduler,\n        self.fake_score_lr_scheduler,\n        self.noise_random_generator,\n        self.generator_ema,\n        # MoE support\n        generator_transformer_2=getattr(self, 'transformer_2', None),\n        real_score_transformer_2=getattr(self, 'real_score_transformer_2',\n                                         None),\n        fake_score_transformer_2=getattr(self, 'fake_score_transformer_2',\n                                         None),\n        generator_optimizer_2=getattr(self, 'optimizer_2', None),\n        fake_score_optimizer_2=getattr(self, 'fake_score_optimizer_2',\n                                       None),\n        generator_scheduler_2=getattr(self, 'lr_scheduler_2', None),\n        fake_score_scheduler_2=getattr(self, 'fake_score_lr_scheduler_2',\n                                       None),\n        generator_ema_2=getattr(self, 'generator_ema_2', None))\n\n    if self.training_args.use_ema and self.is_ema_ready():\n        self.save_ema_weights(self.training_args.output_dir,\n                              self.training_args.max_train_steps)\n\n    if envs.FASTVIDEO_TORCH_PROFILER_DIR:\n        logger.info(\"Stopping profiler...\")\n        self.profiler_controller.stop()\n        logger.info(\"Profiler stopped.\")\n\n    if get_sp_group():\n        cleanup_dist_env_and_memory()\n</code></pre> fastvideo.training.self_forcing_distillation_pipeline.SelfForcingDistillationPipeline.train_one_step \u00b6 <pre><code>train_one_step(\n    training_batch: TrainingBatch,\n) -&gt; TrainingBatch\n</code></pre> <p>Self-forcing training step that alternates between generator and critic training.</p> Source code in <code>fastvideo/training/self_forcing_distillation_pipeline.py</code> <pre><code>def train_one_step(self, training_batch: TrainingBatch) -&gt; TrainingBatch:\n    \"\"\"\n    Self-forcing training step that alternates between generator and critic training.\n    \"\"\"\n    gradient_accumulation_steps = getattr(self.training_args,\n                                          'gradient_accumulation_steps', 1)\n    train_generator = (self.current_trainstep %\n                       self.dfake_gen_update_ratio == 0)\n\n    batches = []\n    for _ in range(gradient_accumulation_steps):\n        batch = self._prepare_distillation(training_batch)\n        batch = self._get_next_batch(batch)\n        batch = self._normalize_dit_input(batch)\n        batch = self._prepare_dit_inputs(batch)\n        batch = self._build_attention_metadata(batch)\n        batch.attn_metadata_vsa = copy.deepcopy(batch.attn_metadata)\n        if batch.attn_metadata is not None:\n            batch.attn_metadata.VSA_sparsity = 0.0\n        batches.append(batch)\n\n    training_batch.dmd_latent_vis_dict = {}\n    training_batch.fake_score_latent_vis_dict = {}\n\n    if train_generator:\n        logger.debug(\"Training generator at step %s\",\n                     self.current_trainstep)\n        self.optimizer.zero_grad()\n        if self.transformer_2 is not None:\n            self.optimizer_2.zero_grad()\n        total_generator_loss = 0.0\n        generator_log_dict = {}\n\n        for batch in batches:\n            # Create a new batch with detached tensors\n            batch_gen = TrainingBatch()\n            for key, value in batch.__dict__.items():\n                if isinstance(value, torch.Tensor):\n                    setattr(batch_gen, key, value.detach().clone())\n                elif isinstance(value, dict):\n                    setattr(\n                        batch_gen, key, {\n                            k:\n                            v.detach().clone() if isinstance(\n                                v, torch.Tensor) else copy.deepcopy(v)\n                            for k, v in value.items()\n                        })\n                else:\n                    setattr(batch_gen, key, copy.deepcopy(value))\n\n            generator_loss, gen_log_dict = self.generator_loss(batch_gen)\n            with set_forward_context(current_timestep=batch_gen.timesteps,\n                                     attn_metadata=batch_gen.attn_metadata):\n                (generator_loss / gradient_accumulation_steps).backward()\n            total_generator_loss += generator_loss.detach().item()\n            generator_log_dict.update(gen_log_dict)\n            # Store visualization data from generator training\n            if hasattr(batch_gen, 'dmd_latent_vis_dict'):\n                training_batch.dmd_latent_vis_dict.update(\n                    batch_gen.dmd_latent_vis_dict)\n\n        # Only clip gradients and step optimizer for the model that is currently training\n        if hasattr(\n                self, 'train_transformer_2'\n        ) and self.train_transformer_2 and self.transformer_2 is not None:\n            self._clip_model_grad_norm_(batch_gen, self.transformer_2)\n            self.optimizer_2.step()\n            self.lr_scheduler_2.step()\n        else:\n            self._clip_model_grad_norm_(batch_gen, self.transformer)\n            self.optimizer.step()\n            self.lr_scheduler.step()\n\n        if self.generator_ema is not None:\n            if hasattr(\n                    self, 'train_transformer_2'\n            ) and self.train_transformer_2 and self.transformer_2 is not None:\n                # Update EMA for transformer_2 when training it\n                if self.generator_ema_2 is not None:\n                    self.generator_ema_2.update(self.transformer_2)\n            else:\n                self.generator_ema.update(self.transformer)\n\n        avg_generator_loss = torch.tensor(total_generator_loss /\n                                          gradient_accumulation_steps,\n                                          device=self.device)\n        world_group = get_world_group()\n        world_group.all_reduce(avg_generator_loss,\n                               op=torch.distributed.ReduceOp.AVG)\n        training_batch.generator_loss = avg_generator_loss.item()\n    else:\n        training_batch.generator_loss = 0.0\n\n    logger.debug(\"Training critic at step %s\", self.current_trainstep)\n    self.fake_score_optimizer.zero_grad()\n    total_critic_loss = 0.0\n    critic_log_dict = {}\n\n    for batch in batches:\n        # Create a new batch with detached tensors\n        batch_critic = TrainingBatch()\n        for key, value in batch.__dict__.items():\n            if isinstance(value, torch.Tensor):\n                setattr(batch_critic, key, value.detach().clone())\n            elif isinstance(value, dict):\n                setattr(\n                    batch_critic, key, {\n                        k:\n                        v.detach().clone()\n                        if isinstance(v, torch.Tensor) else copy.deepcopy(v)\n                        for k, v in value.items()\n                    })\n            else:\n                setattr(batch_critic, key, copy.deepcopy(value))\n\n        critic_loss, crit_log_dict = self.critic_loss(batch_critic)\n        with set_forward_context(current_timestep=batch_critic.timesteps,\n                                 attn_metadata=batch_critic.attn_metadata):\n            (critic_loss / gradient_accumulation_steps).backward()\n        total_critic_loss += critic_loss.detach().item()\n        critic_log_dict.update(crit_log_dict)\n        # Store visualization data from critic training\n        if hasattr(batch_critic, 'fake_score_latent_vis_dict'):\n            training_batch.fake_score_latent_vis_dict.update(\n                batch_critic.fake_score_latent_vis_dict)\n\n    if self.train_fake_score_transformer_2 and self.fake_score_transformer_2 is not None:\n        self._clip_model_grad_norm_(batch_critic,\n                                    self.fake_score_transformer_2)\n        self.fake_score_optimizer_2.step()\n        self.fake_score_lr_scheduler_2.step()\n    else:\n        self._clip_model_grad_norm_(batch_critic,\n                                    self.fake_score_transformer)\n        self.fake_score_optimizer.step()\n        self.fake_score_lr_scheduler.step()\n\n    avg_critic_loss = torch.tensor(total_critic_loss /\n                                   gradient_accumulation_steps,\n                                   device=self.device)\n    world_group = get_world_group()\n    world_group.all_reduce(avg_critic_loss,\n                           op=torch.distributed.ReduceOp.AVG)\n    training_batch.fake_score_loss = avg_critic_loss.item()\n\n    training_batch.total_loss = training_batch.generator_loss + training_batch.fake_score_loss\n    return training_batch\n</code></pre> fastvideo.training.self_forcing_distillation_pipeline.SelfForcingDistillationPipeline.visualize_intermediate_latents \u00b6 <pre><code>visualize_intermediate_latents(\n    training_batch: TrainingBatch,\n    training_args: TrainingArgs,\n    step: int,\n)\n</code></pre> <p>Add visualization data to tracker logging and save frames to disk.</p> Source code in <code>fastvideo/training/self_forcing_distillation_pipeline.py</code> <pre><code>def visualize_intermediate_latents(self, training_batch: TrainingBatch,\n                                   training_args: TrainingArgs, step: int):\n    \"\"\"Add visualization data to tracker logging and save frames to disk.\"\"\"\n    tracker_loss_dict: dict[str, Any] = {}\n\n    # Debug logging\n    if hasattr(training_batch, 'dmd_latent_vis_dict'):\n        logger.info(\"DMD latent keys: %s\",\n                    list(training_batch.dmd_latent_vis_dict.keys()))\n    if hasattr(training_batch, 'fake_score_latent_vis_dict'):\n        logger.info(\"Fake score latent keys: %s\",\n                    list(training_batch.fake_score_latent_vis_dict.keys()))\n\n    # Process generator predictions if available\n    if hasattr(\n            training_batch,\n            'dmd_latent_vis_dict') and training_batch.dmd_latent_vis_dict:\n        dmd_latents_vis_dict = training_batch.dmd_latent_vis_dict\n        dmd_log_keys = [\n            'generator_pred_video', 'real_score_pred_video',\n            'faker_score_pred_video'\n        ]\n\n        for latent_key in dmd_log_keys:\n            if latent_key in dmd_latents_vis_dict:\n                logger.info(\"Processing DMD latent: %s\", latent_key)\n                latents = dmd_latents_vis_dict[latent_key]\n                if not isinstance(latents, torch.Tensor):\n                    logger.warning(\"Expected tensor for %s, got %s\",\n                                   latent_key, type(latents))\n                    continue\n\n                latents = latents.detach()\n                latents = latents.permute(0, 2, 1, 3, 4)\n\n                if isinstance(self.vae.scaling_factor, torch.Tensor):\n                    latents = latents / self.vae.scaling_factor.to(\n                        latents.device, latents.dtype)\n                else:\n                    latents = latents / self.vae.scaling_factor\n\n                if (hasattr(self.vae, \"shift_factor\")\n                        and self.vae.shift_factor is not None):\n                    if isinstance(self.vae.shift_factor, torch.Tensor):\n                        latents += self.vae.shift_factor.to(\n                            latents.device, latents.dtype)\n                    else:\n                        latents += self.vae.shift_factor\n\n                with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n                    video = self.vae.decode(latents)\n                video = (video / 2 + 0.5).clamp(0, 1)\n                video = video.cpu().float()\n                video = video.permute(0, 2, 1, 3, 4)\n                video = (video * 255).numpy().astype(np.uint8)\n                video_artifact = self.tracker.video(video,\n                                                    fps=24,\n                                                    format=\"mp4\")\n                if video_artifact is not None:\n                    tracker_loss_dict[f\"dmd_{latent_key}\"] = video_artifact\n                del video, latents\n\n    # Process critic predictions\n    if hasattr(training_batch, 'fake_score_latent_vis_dict'\n               ) and training_batch.fake_score_latent_vis_dict:\n        fake_score_latents_vis_dict = training_batch.fake_score_latent_vis_dict\n        fake_score_log_keys = ['generator_pred_video']\n\n        for latent_key in fake_score_log_keys:\n            if latent_key in fake_score_latents_vis_dict:\n                logger.info(\"Processing critic latent: %s\", latent_key)\n                latents = fake_score_latents_vis_dict[latent_key]\n                if not isinstance(latents, torch.Tensor):\n                    logger.warning(\"Expected tensor for %s, got %s\",\n                                   latent_key, type(latents))\n                    continue\n\n                latents = latents.detach()\n                latents = latents.permute(0, 2, 1, 3, 4)\n\n                if isinstance(self.vae.scaling_factor, torch.Tensor):\n                    latents = latents / self.vae.scaling_factor.to(\n                        latents.device, latents.dtype)\n                else:\n                    latents = latents / self.vae.scaling_factor\n\n                if (hasattr(self.vae, \"shift_factor\")\n                        and self.vae.shift_factor is not None):\n                    if isinstance(self.vae.shift_factor, torch.Tensor):\n                        latents += self.vae.shift_factor.to(\n                            latents.device, latents.dtype)\n                    else:\n                        latents += self.vae.shift_factor\n\n                with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n                    video = self.vae.decode(latents)\n                video = (video / 2 + 0.5).clamp(0, 1)\n                video = video.cpu().float()\n                video = video.permute(0, 2, 1, 3, 4)\n                video = (video * 255).numpy().astype(np.uint8)\n                video_artifact = self.tracker.video(video,\n                                                    fps=24,\n                                                    format=\"mp4\")\n                if video_artifact is not None:\n                    tracker_loss_dict[\n                        f\"critic_{latent_key}\"] = video_artifact\n                del video, latents\n\n    # Log metadata\n    if hasattr(\n            training_batch,\n            'dmd_latent_vis_dict') and training_batch.dmd_latent_vis_dict:\n        if \"generator_timestep\" in training_batch.dmd_latent_vis_dict:\n            tracker_loss_dict[\n                \"generator_timestep\"] = training_batch.dmd_latent_vis_dict[\n                    \"generator_timestep\"].item()\n        if \"dmd_timestep\" in training_batch.dmd_latent_vis_dict:\n            tracker_loss_dict[\n                \"dmd_timestep\"] = training_batch.dmd_latent_vis_dict[\n                    \"dmd_timestep\"].item()\n\n    if hasattr(\n            training_batch, 'fake_score_latent_vis_dict'\n    ) and training_batch.fake_score_latent_vis_dict and \"fake_score_timestep\" in training_batch.fake_score_latent_vis_dict:\n        tracker_loss_dict[\n            \"fake_score_timestep\"] = training_batch.fake_score_latent_vis_dict[\n                \"fake_score_timestep\"].item()\n\n    # Log final dict contents\n    logger.info(\"Final tracker_loss_dict keys: %s\",\n                list(tracker_loss_dict.keys()))\n\n    if self.global_rank == 0 and tracker_loss_dict:\n        self.tracker.log_artifacts(tracker_loss_dict, step)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.training.self_forcing_distillation_pipeline-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideo.training.trackers","title":"fastvideo.training.trackers","text":"<p>Utilities for logging metrics and artifacts to external trackers.</p> <p>This module is inspired by the trackers implementation in https://github.com/huggingface/finetrainers and provides a minimal, shared interface that can be used across all FastVideo training pipelines.</p>"},{"location":"api/fastvideo/#fastvideo.training.trackers-classes","title":"Classes","text":"fastvideo.training.trackers.BaseTracker \u00b6 <pre><code>BaseTracker()\n</code></pre> <p>Base tracker implementation.</p> <p>The default tracker stores timing information but does not emit any logs.</p> Source code in <code>fastvideo/training/trackers.py</code> <pre><code>def __init__(self) -&gt; None:\n    self._timed_metrics: dict[str, float] = {}\n</code></pre> Functions\u00b6 fastvideo.training.trackers.BaseTracker.finish \u00b6 <pre><code>finish() -&gt; None\n</code></pre> <p>Finalize the tracker session.</p> Source code in <code>fastvideo/training/trackers.py</code> <pre><code>def finish(self) -&gt; None:  # pragma: no cover - interface\n    \"\"\"Finalize the tracker session.\"\"\"\n</code></pre> fastvideo.training.trackers.BaseTracker.log \u00b6 <pre><code>log(metrics: dict[str, Any], step: int) -&gt; None\n</code></pre> <p>Log metrics for the given step.</p> Source code in <code>fastvideo/training/trackers.py</code> <pre><code>def log(self, metrics: dict[str, Any],\n        step: int) -&gt; None:  # pragma: no cover - interface\n    \"\"\"Log metrics for the given step.\"\"\"\n    # Merge timing metrics with provided metrics\n    metrics = {**self._timed_metrics, **metrics}\n    self._timed_metrics = {}\n</code></pre> fastvideo.training.trackers.BaseTracker.log_artifacts \u00b6 <pre><code>log_artifacts(artifacts: dict[str, Any], step: int) -&gt; None\n</code></pre> <p>Log artifacts such as videos or images.</p> <p>By default this is treated the same as :meth:<code>log</code>.</p> Source code in <code>fastvideo/training/trackers.py</code> <pre><code>def log_artifacts(self, artifacts: dict[str, Any], step: int) -&gt; None:\n    \"\"\"Log artifacts such as videos or images.\n\n    By default this is treated the same as :meth:`log`.\n    \"\"\"\n\n    if artifacts:\n        self.log(artifacts, step)\n</code></pre> fastvideo.training.trackers.BaseTracker.video \u00b6 <pre><code>video(\n    data: Any,\n    *,\n    caption: str | None = None,\n    fps: int | None = None,\n    format: str | None = None\n) -&gt; Any | None\n</code></pre> <p>Create a tracker specific video artifact.</p> <p>Trackers that do not support video artifacts should return <code>None</code>.</p> Source code in <code>fastvideo/training/trackers.py</code> <pre><code>def video(\n    self,\n    data: Any,\n    *,\n    caption: str | None = None,\n    fps: int | None = None,\n    format: str | None = None,\n) -&gt; Any | None:\n    \"\"\"Create a tracker specific video artifact.\n\n    Trackers that do not support video artifacts should return ``None``.\n    \"\"\"\n\n    return None\n</code></pre> fastvideo.training.trackers.DummyTracker \u00b6 <pre><code>DummyTracker()\n</code></pre> <p>               Bases: <code>BaseTracker</code></p> <p>Tracker implementation used when logging is disabled.</p> Source code in <code>fastvideo/training/trackers.py</code> <pre><code>def __init__(self) -&gt; None:\n    self._timed_metrics: dict[str, float] = {}\n</code></pre> fastvideo.training.trackers.SequentialTracker \u00b6 <pre><code>SequentialTracker(trackers: Iterable[BaseTracker])\n</code></pre> <p>               Bases: <code>BaseTracker</code></p> <p>A tracker that forwards logging calls to a sequence of trackers.</p> Source code in <code>fastvideo/training/trackers.py</code> <pre><code>def __init__(self, trackers: Iterable[BaseTracker]) -&gt; None:\n    super().__init__()\n    self._trackers: list[BaseTracker] = list(trackers)\n</code></pre> fastvideo.training.trackers.Timer <code>dataclass</code> \u00b6 <pre><code>Timer(\n    name: str,\n    _start_time: float | None = None,\n    _end_time: float | None = None,\n)\n</code></pre> <p>Simple timer utility used by the trackers.</p> fastvideo.training.trackers.WandbTracker \u00b6 <pre><code>WandbTracker(\n    experiment_name: str,\n    log_dir: str,\n    *,\n    config: dict[str, Any] | None = None,\n    run_name: str | None = None\n)\n</code></pre> <p>               Bases: <code>BaseTracker</code></p> <p>Tracker implementation for Weights &amp; Biases.</p> Source code in <code>fastvideo/training/trackers.py</code> <pre><code>def __init__(\n    self,\n    experiment_name: str,\n    log_dir: str,\n    *,\n    config: dict[str, Any] | None = None,\n    run_name: str | None = None,\n) -&gt; None:\n    super().__init__()\n\n    import wandb\n\n    pathlib.Path(log_dir).mkdir(parents=True, exist_ok=True)\n\n    self._wandb = wandb\n    self._run = wandb.init(\n        project=experiment_name,\n        dir=log_dir,\n        config=config,\n        name=run_name,\n    )\n    logger.info(\"Initialized Weights &amp; Biases tracker\")\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.training.trackers-functions","title":"Functions","text":"fastvideo.training.trackers.initialize_trackers \u00b6 <pre><code>initialize_trackers(\n    trackers: Iterable[str],\n    *,\n    experiment_name: str,\n    config: dict[str, Any] | None,\n    log_dir: str,\n    run_name: str | None = None\n) -&gt; BaseTracker\n</code></pre> <p>Create tracker instances based on <code>trackers</code> configuration.</p> Source code in <code>fastvideo/training/trackers.py</code> <pre><code>def initialize_trackers(\n    trackers: Iterable[str],\n    *,\n    experiment_name: str,\n    config: dict[str, Any] | None,\n    log_dir: str,\n    run_name: str | None = None,\n) -&gt; BaseTracker:\n    \"\"\"Create tracker instances based on ``trackers`` configuration.\"\"\"\n\n    tracker_names = [tracker.lower() for tracker in trackers]\n    if not tracker_names:\n        return DummyTracker()\n\n    unsupported = [\n        name for name in tracker_names if name not in SUPPORTED_TRACKERS\n    ]\n    if unsupported:\n        raise ValueError(\n            f\"Unsupported tracker(s) provided: {unsupported}. Supported trackers: {sorted(SUPPORTED_TRACKERS)}\"\n        )\n\n    tracker_instances: list[BaseTracker] = []\n    for tracker_name in tracker_names:\n        if tracker_name == Trackers.NONE.value:\n            tracker_instances.append(DummyTracker())\n        elif tracker_name == Trackers.WANDB.value:\n            tracker_instances.append(\n                WandbTracker(\n                    experiment_name,\n                    os.path.abspath(log_dir),\n                    config=config,\n                    run_name=run_name,\n                ))\n\n    if not tracker_instances:\n        return DummyTracker()\n\n    if len(tracker_instances) == 1:\n        return tracker_instances[0]\n\n    return SequentialTracker(tracker_instances)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.training.training_pipeline","title":"fastvideo.training.training_pipeline","text":""},{"location":"api/fastvideo/#fastvideo.training.training_pipeline-classes","title":"Classes","text":"fastvideo.training.training_pipeline.TrainingPipeline \u00b6 <pre><code>TrainingPipeline(\n    model_path: str,\n    fastvideo_args: TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>LoRAPipeline</code>, <code>ABC</code></p> <p>A pipeline for training a model. All training pipelines should inherit from this class. All reusable components and code should be implemented in this class.</p> Source code in <code>fastvideo/training/training_pipeline.py</code> <pre><code>def __init__(\n        self,\n        model_path: str,\n        fastvideo_args: TrainingArgs,\n        required_config_modules: list[str] | None = None,\n        loaded_modules: dict[str, torch.nn.Module] | None = None) -&gt; None:\n    fastvideo_args.inference_mode = False\n    self.lora_training = fastvideo_args.lora_training\n    if self.lora_training and fastvideo_args.lora_rank is None:\n        raise ValueError(\"lora rank must be set when using lora training\")\n\n    set_random_seed(fastvideo_args.seed)  # for lora param init\n    super().__init__(model_path, fastvideo_args, required_config_modules,\n                     loaded_modules)  # type: ignore\n    self.tracker = DummyTracker()\n</code></pre> Functions\u00b6 fastvideo.training.training_pipeline.TrainingPipeline.visualize_intermediate_latents \u00b6 <pre><code>visualize_intermediate_latents(\n    training_batch: TrainingBatch,\n    training_args: TrainingArgs,\n    step: int,\n)\n</code></pre> <p>Add visualization data to tracker logging and save frames to disk.</p> Source code in <code>fastvideo/training/training_pipeline.py</code> <pre><code>def visualize_intermediate_latents(self, training_batch: TrainingBatch,\n                                   training_args: TrainingArgs, step: int):\n    \"\"\"Add visualization data to tracker logging and save frames to disk.\"\"\"\n    raise NotImplementedError(\n        \"Visualize intermediate latents is not implemented for training pipeline\"\n    )\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.training.training_pipeline-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideo.training.training_utils","title":"fastvideo.training.training_utils","text":""},{"location":"api/fastvideo/#fastvideo.training.training_utils-classes","title":"Classes","text":"fastvideo.training.training_utils.EMA_FSDP \u00b6 <pre><code>EMA_FSDP(\n    module, decay: float = 0.999, mode: str = \"local_shard\"\n)\n</code></pre> FSDP2-friendly EMA with two modes <ul> <li>mode=\"local_shard\" (default): maintain float32 CPU EMA of local parameter shards on every rank.   Provides a context manager to temporarily swap EMA weights into the live model for teacher forward.</li> <li>mode=\"rank0_full\": maintain a consolidated float32 CPU EMA of full parameters on rank 0 only   using gather_state_dict_on_cpu_rank0(). Useful for checkpoint export; not for teacher forward.</li> </ul> <p>Usage (local_shard for CM teacher):   ema = EMA_FSDP(model, decay=0.999, mode=\"local_shard\")   for step in ...:       ema.update(model)   with ema.apply_to_model(model):       with torch.no_grad():           y_teacher = model(...)</p> <p>Usage (rank0_full for export):   ema = EMA_FSDP(model, decay=0.999, mode=\"rank0_full\")   ema.update(model)   ema.state_dict()  # on rank 0</p> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def __init__(self, module, decay: float = 0.999, mode: str = \"local_shard\"):\n    self.decay = float(decay)\n    self.mode = mode\n    self.shadow: dict[str, torch.Tensor] = {}\n    self.rank = dist.get_rank() if dist.is_initialized() else 0\n    if self.mode not in {\"local_shard\", \"rank0_full\"}:\n        raise ValueError(f\"Unsupported EMA_FSDP mode: {self.mode}\")\n    self._init_shadow(module)\n</code></pre> Functions\u00b6 fastvideo.training.training_utils.EMA_FSDP.copy_to_unwrapped \u00b6 <pre><code>copy_to_unwrapped(module) -&gt; None\n</code></pre> <p>Copy EMA weights into a non-sharded (unwrapped) module. Intended for export/eval. For mode=\"rank0_full\", only rank 0 has the full EMA state.</p> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>@torch.no_grad()\ndef copy_to_unwrapped(self, module) -&gt; None:\n    \"\"\"\n    Copy EMA weights into a non-sharded (unwrapped) module. Intended for export/eval.\n    For mode=\"rank0_full\", only rank 0 has the full EMA state.\n    \"\"\"\n    if self.mode == \"rank0_full\" and self.rank != 0:\n        return\n    name_to_param = dict(module.named_parameters())\n    for n, w in self.shadow.items():\n        if n in name_to_param:\n            p = name_to_param[n]\n            p.data.copy_(w.to(dtype=p.dtype, device=p.device))\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.training.training_utils-functions","title":"Functions","text":"fastvideo.training.training_utils.clip_grad_norm_ \u00b6 <pre><code>clip_grad_norm_(\n    parameters: Tensor | list[Tensor],\n    max_norm: float,\n    norm_type: float = 2.0,\n    error_if_nonfinite: bool = False,\n    foreach: bool | None = None,\n    pp_mesh: DeviceMesh | None = None,\n) -&gt; torch.Tensor\n</code></pre> <p>Clip the gradient norm of parameters.</p> <p>Gradient norm clipping requires computing the gradient norm over the entire model. <code>torch.nn.utils.clip_grad_norm_</code> only computes gradient norm along DP/FSDP/TP dimensions. We need to manually reduce the gradient norm across PP stages. See https://github.com/pytorch/torchtitan/issues/596 for details.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>`torch.Tensor` or `List[torch.Tensor]`</code> <p>Tensors that will have gradients normalized.</p> required <code>max_norm</code> <code>`float`</code> <p>Maximum norm of the gradients after clipping.</p> required <code>norm_type</code> <code>`float`, defaults to `2.0`</code> <p>Type of p-norm to use. Can be <code>inf</code> for infinity norm.</p> <code>2.0</code> <code>error_if_nonfinite</code> <code>`bool`, defaults to `False`</code> <p>If <code>True</code>, an error is thrown if the total norm of the gradients from <code>parameters</code> is <code>nan</code>, <code>inf</code>, or <code>-inf</code>.</p> <code>False</code> <code>foreach</code> <code>`bool`, defaults to `None`</code> <p>Use the faster foreach-based implementation. If <code>None</code>, use the foreach implementation for CUDA and CPU native tensors and silently fall back to the slow implementation for other device types.</p> <code>None</code> <code>pp_mesh</code> <code>`torch.distributed.device_mesh.DeviceMesh`, defaults to `None`</code> <p>Pipeline parallel device mesh. If not <code>None</code>, will reduce gradient norm across PP stages.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p><code>torch.Tensor</code>: Total norm of the gradients</p> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>@torch.no_grad()\ndef clip_grad_norm_(\n    parameters: torch.Tensor | list[torch.Tensor],\n    max_norm: float,\n    norm_type: float = 2.0,\n    error_if_nonfinite: bool = False,\n    foreach: bool | None = None,\n    pp_mesh: torch.distributed.device_mesh.DeviceMesh | None = None,\n) -&gt; torch.Tensor:\n    r\"\"\"\n    Clip the gradient norm of parameters.\n\n    Gradient norm clipping requires computing the gradient norm over the entire model.\n    `torch.nn.utils.clip_grad_norm_` only computes gradient norm along DP/FSDP/TP dimensions.\n    We need to manually reduce the gradient norm across PP stages.\n    See https://github.com/pytorch/torchtitan/issues/596 for details.\n\n    Args:\n        parameters (`torch.Tensor` or `List[torch.Tensor]`):\n            Tensors that will have gradients normalized.\n        max_norm (`float`):\n            Maximum norm of the gradients after clipping.\n        norm_type (`float`, defaults to `2.0`):\n            Type of p-norm to use. Can be `inf` for infinity norm.\n        error_if_nonfinite (`bool`, defaults to `False`):\n            If `True`, an error is thrown if the total norm of the gradients from `parameters` is `nan`, `inf`, or `-inf`.\n        foreach (`bool`, defaults to `None`):\n            Use the faster foreach-based implementation. If `None`, use the foreach implementation for CUDA and CPU native tensors\n            and silently fall back to the slow implementation for other device types.\n        pp_mesh (`torch.distributed.device_mesh.DeviceMesh`, defaults to `None`):\n            Pipeline parallel device mesh. If not `None`, will reduce gradient norm across PP stages.\n\n    Returns:\n        `torch.Tensor`:\n            Total norm of the gradients\n    \"\"\"\n    grads = [p.grad for p in parameters if p.grad is not None]\n\n    # TODO(aryan): Wait for next Pytorch release to use `torch.nn.utils.get_total_norm`\n    # total_norm = torch.nn.utils.get_total_norm(grads, norm_type, error_if_nonfinite, foreach)\n    total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)\n\n    # If total_norm is a DTensor, the placements must be `torch.distributed._tensor.ops.math_ops._NormPartial`.\n    # We can simply reduce the DTensor to get the total norm in this tensor's process group\n    # and then convert it to a local tensor.\n    # It has two purposes:\n    #   1. to make sure the total norm is computed correctly when PP is used (see below)\n    #   2. to return a reduced total_norm tensor whose .item() would return the correct value\n    if isinstance(total_norm, torch.distributed.tensor.DTensor):\n        # Will reach here if any non-PP parallelism is used.\n        # If only using PP, total_norm will be a local tensor.\n        total_norm = total_norm.full_tensor()\n\n    if pp_mesh is not None:\n        raise NotImplementedError(\"Pipeline parallel is not supported\")\n        if math.isinf(norm_type):\n            dist.all_reduce(total_norm,\n                            op=dist.ReduceOp.MAX,\n                            group=pp_mesh.get_group())\n        else:\n            total_norm **= norm_type\n            dist.all_reduce(total_norm,\n                            op=dist.ReduceOp.SUM,\n                            group=pp_mesh.get_group())\n            total_norm **= 1.0 / norm_type\n\n    _clip_grads_with_norm_(parameters, max_norm, total_norm, foreach)\n    return total_norm\n</code></pre> fastvideo.training.training_utils.compute_density_for_timestep_sampling \u00b6 <pre><code>compute_density_for_timestep_sampling(\n    weighting_scheme: str,\n    batch_size: int,\n    generator,\n    logit_mean: float | None = None,\n    logit_std: float | None = None,\n    mode_scale: float | None = None,\n)\n</code></pre> <p>Compute the density for sampling the timesteps when doing SD3 training.</p> <p>Courtesy: This was contributed by Rafie Walker in https://github.com/huggingface/diffusers/pull/8528.</p> <p>SD3 paper reference: https://arxiv.org/abs/2403.03206v1.</p> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def compute_density_for_timestep_sampling(\n    weighting_scheme: str,\n    batch_size: int,\n    generator,\n    logit_mean: float | None = None,\n    logit_std: float | None = None,\n    mode_scale: float | None = None,\n):\n    \"\"\"\n    Compute the density for sampling the timesteps when doing SD3 training.\n\n    Courtesy: This was contributed by Rafie Walker in https://github.com/huggingface/diffusers/pull/8528.\n\n    SD3 paper reference: https://arxiv.org/abs/2403.03206v1.\n    \"\"\"\n    if weighting_scheme == \"logit_normal\":\n        # See 3.1 in the SD3 paper ($rf/lognorm(0.00,1.00)$).\n        u = torch.normal(\n            mean=logit_mean,\n            std=logit_std,\n            size=(batch_size, ),\n            device=\"cpu\",\n            generator=generator,\n        )\n        u = torch.nn.functional.sigmoid(u)\n    elif weighting_scheme == \"mode\":\n        u = torch.rand(size=(batch_size, ), device=\"cpu\", generator=generator)\n        u = 1 - u - mode_scale * (torch.cos(math.pi * u / 2)**2 - 1 + u)\n    else:\n        u = torch.rand(size=(batch_size, ), device=\"cpu\", generator=generator)\n    return u\n</code></pre> fastvideo.training.training_utils.custom_to_hf_state_dict \u00b6 <pre><code>custom_to_hf_state_dict(\n    state_dict: dict[str, Any]\n    | Iterator[tuple[str, Tensor]],\n    reverse_param_names_mapping: dict[\n        str, tuple[str, int, int]\n    ],\n) -&gt; dict[str, Any]\n</code></pre> <p>Convert fastvideo's custom model format to diffusers format using reverse_param_names_mapping.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>dict[str, Any] | Iterator[tuple[str, Tensor]]</code> <p>State dict in fastvideo's custom format</p> required <code>reverse_param_names_mapping</code> <code>dict[str, tuple[str, int, int]]</code> <p>Reverse mapping from fastvideo's custom format to diffusers format</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>State dict in diffusers format</p> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def custom_to_hf_state_dict(\n    state_dict: dict[str, Any] | Iterator[tuple[str, torch.Tensor]],\n    reverse_param_names_mapping: dict[str, tuple[str, int,\n                                                 int]]) -&gt; dict[str, Any]:\n    \"\"\"\n    Convert fastvideo's custom model format to diffusers format using reverse_param_names_mapping.\n\n    Args:\n        state_dict: State dict in fastvideo's custom format\n        reverse_param_names_mapping: Reverse mapping from fastvideo's custom format to diffusers format\n\n    Returns:\n        State dict in diffusers format\n    \"\"\"\n    assert len(\n        reverse_param_names_mapping) &gt; 0, \"reverse_param_names_mapping is empty\"\n    if isinstance(state_dict, Iterator):\n        state_dict = dict(state_dict)\n    new_state_dict = {}\n    # Group parameters that need to be split (merged parameters)\n    merge_groups: dict[str, list[tuple[str, int, int]]] = {}\n\n    # First pass: collect all merge groups\n    for training_key, (\n            diffusers_key, merge_index,\n            num_params_to_merge) in reverse_param_names_mapping.items():\n        if merge_index is not None:\n            # This is a merged parameter that needs to be split\n            if training_key not in merge_groups:\n                merge_groups[training_key] = []\n            merge_groups[training_key].append(\n                (diffusers_key, merge_index, num_params_to_merge))\n\n    # Second pass: handle merged parameters by splitting them\n    used_keys = set()\n    for training_key, splits in merge_groups.items():\n        if training_key in state_dict:\n            v = state_dict[training_key]\n            # Sort by merge_index to ensure correct order\n            splits.sort(key=lambda x: x[1])\n            total = splits[0][2]\n            split_size = v.shape[0] // total\n            split_tensors = torch.split(v, split_size, dim=0)\n\n            for diffusers_key, split_index, _ in splits:\n                new_state_dict[diffusers_key] = split_tensors[split_index]\n            used_keys.add(training_key)\n\n    # Third pass: handle regular parameters (direct mappings)\n    for training_key, v in state_dict.items():\n        if training_key in used_keys:\n            continue\n\n        if training_key in reverse_param_names_mapping:\n            diffusers_key, merge_index, _ = reverse_param_names_mapping[\n                training_key]\n            if merge_index is None:\n                # Direct mapping\n                new_state_dict[diffusers_key] = v\n        else:\n            # No mapping found, keep as is\n            new_state_dict[training_key] = v\n\n    return new_state_dict\n</code></pre> fastvideo.training.training_utils.get_constant_schedule \u00b6 <pre><code>get_constant_schedule(\n    optimizer: Optimizer, last_epoch: int = -1\n) -&gt; LambdaLR\n</code></pre> <p>Create a schedule with a constant learning rate, using the learning rate set in optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>[`~torch.optim.Optimizer`]</code> <p>The optimizer for which to schedule the learning rate.</p> required <code>last_epoch</code> <code>`int`, *optional*, defaults to -1</code> <p>The index of the last epoch when resuming training.</p> <code>-1</code> Return <p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def get_constant_schedule(optimizer: Optimizer,\n                          last_epoch: int = -1) -&gt; LambdaLR:\n    \"\"\"\n    Create a schedule with a constant learning rate, using the learning rate set in optimizer.\n\n    Args:\n        optimizer ([`~torch.optim.Optimizer`]):\n            The optimizer for which to schedule the learning rate.\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n\n    Return:\n        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"\n    return LambdaLR(optimizer, lambda _: 1, last_epoch=last_epoch)\n</code></pre> fastvideo.training.training_utils.get_constant_schedule_with_warmup \u00b6 <pre><code>get_constant_schedule_with_warmup(\n    optimizer: Optimizer,\n    num_warmup_steps: int,\n    last_epoch: int = -1,\n) -&gt; LambdaLR\n</code></pre> <p>Create a schedule with a constant learning rate preceded by a warmup period during which the learning rate increases linearly between 0 and the initial lr set in the optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>[`~torch.optim.Optimizer`]</code> <p>The optimizer for which to schedule the learning rate.</p> required <code>num_warmup_steps</code> <code>`int`</code> <p>The number of steps for the warmup phase.</p> required <code>last_epoch</code> <code>`int`, *optional*, defaults to -1</code> <p>The index of the last epoch when resuming training.</p> <code>-1</code> Return <p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def get_constant_schedule_with_warmup(optimizer: Optimizer,\n                                      num_warmup_steps: int,\n                                      last_epoch: int = -1) -&gt; LambdaLR:\n    \"\"\"\n    Create a schedule with a constant learning rate preceded by a warmup period during which the learning rate\n    increases linearly between 0 and the initial lr set in the optimizer.\n\n    Args:\n        optimizer ([`~torch.optim.Optimizer`]):\n            The optimizer for which to schedule the learning rate.\n        num_warmup_steps (`int`):\n            The number of steps for the warmup phase.\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n\n    Return:\n        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"\n\n    def lr_lambda(current_step: int):\n        if current_step &lt; num_warmup_steps:\n            return float(current_step) / float(max(1.0, num_warmup_steps))\n        return 1.0\n\n    return LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n</code></pre> fastvideo.training.training_utils.get_cosine_schedule_with_min_lr \u00b6 <pre><code>get_cosine_schedule_with_min_lr(\n    optimizer: Optimizer,\n    num_warmup_steps: int,\n    num_training_steps: int,\n    min_lr_ratio: float = 0.1,\n    num_cycles: float = 0.5,\n    last_epoch: int = -1,\n) -&gt; LambdaLR\n</code></pre> <p>Create a schedule with a learning rate that decreases following the values of the cosine function between the initial lr set in the optimizer to a minimum lr (min_lr_ratio * initial_lr), after a warmup period during which  it increases linearly between 0 and the initial lr set in the optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>[`~torch.optim.Optimizer`]</code> <p>The optimizer for which to schedule the learning rate.</p> required <code>num_warmup_steps</code> <code>`int`</code> <p>The number of steps for the warmup phase.</p> required <code>num_training_steps</code> <code>`int`</code> <p>The total number of training steps.</p> required <code>min_lr_ratio</code> <code>`float`, *optional*, defaults to 0.1</code> <p>The ratio of minimum learning rate to initial learning rate.</p> <code>0.1</code> <code>num_cycles</code> <code>`float`, *optional*, defaults to 0.5</code> <p>The number of periods of the cosine function in a schedule.</p> <code>0.5</code> <code>last_epoch</code> <code>`int`, *optional*, defaults to -1</code> <p>The index of the last epoch when resuming training.</p> <code>-1</code> Return <p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def get_cosine_schedule_with_min_lr(optimizer: Optimizer,\n                                    num_warmup_steps: int,\n                                    num_training_steps: int,\n                                    min_lr_ratio: float = 0.1,\n                                    num_cycles: float = 0.5,\n                                    last_epoch: int = -1) -&gt; LambdaLR:\n    \"\"\"\n    Create a schedule with a learning rate that decreases following the values of the cosine function between the\n    initial lr set in the optimizer to a minimum lr (min_lr_ratio * initial_lr), after a warmup period during which \n    it increases linearly between 0 and the initial lr set in the optimizer.\n\n    Args:\n        optimizer ([`~torch.optim.Optimizer`]):\n            The optimizer for which to schedule the learning rate.\n        num_warmup_steps (`int`):\n            The number of steps for the warmup phase.\n        num_training_steps (`int`):\n            The total number of training steps.\n        min_lr_ratio (`float`, *optional*, defaults to 0.1):\n            The ratio of minimum learning rate to initial learning rate.\n        num_cycles (`float`, *optional*, defaults to 0.5):\n            The number of periods of the cosine function in a schedule.\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n\n    Return:\n        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"\n\n    def lr_lambda(current_step):\n        if current_step &lt; num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        progress = float(current_step - num_warmup_steps) / float(\n            max(1, num_training_steps - num_warmup_steps))\n        # Cosine decay from 1.0 to min_lr_ratio over num_cycles periods\n        # Use the same formula as standard cosine but ensure minimum is min_lr_ratio instead of 0\n        cosine_value = 0.5 * (\n            1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))\n        # Ensure the value doesn't go below min_lr_ratio\n        return max(min_lr_ratio, cosine_value)\n\n    return LambdaLR(optimizer, lr_lambda, last_epoch)\n</code></pre> fastvideo.training.training_utils.get_cosine_schedule_with_warmup \u00b6 <pre><code>get_cosine_schedule_with_warmup(\n    optimizer: Optimizer,\n    num_warmup_steps: int,\n    num_training_steps: int,\n    num_cycles: float = 0.5,\n    last_epoch: int = -1,\n) -&gt; LambdaLR\n</code></pre> <p>Create a schedule with a learning rate that decreases following the values of the cosine function between the initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the initial lr set in the optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>[`~torch.optim.Optimizer`]</code> <p>The optimizer for which to schedule the learning rate.</p> required <code>num_warmup_steps</code> <code>`int`</code> <p>The number of steps for the warmup phase.</p> required <code>num_training_steps</code> <code>`int`</code> <p>The total number of training steps.</p> required <code>num_periods</code> <code>`float`, *optional*, defaults to 0.5</code> <p>The number of periods of the cosine function in a schedule (the default is to just decrease from the max value to 0 following a half-cosine).</p> required <code>last_epoch</code> <code>`int`, *optional*, defaults to -1</code> <p>The index of the last epoch when resuming training.</p> <code>-1</code> Return <p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def get_cosine_schedule_with_warmup(optimizer: Optimizer,\n                                    num_warmup_steps: int,\n                                    num_training_steps: int,\n                                    num_cycles: float = 0.5,\n                                    last_epoch: int = -1) -&gt; LambdaLR:\n    \"\"\"\n    Create a schedule with a learning rate that decreases following the values of the cosine function between the\n    initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the\n    initial lr set in the optimizer.\n\n    Args:\n        optimizer ([`~torch.optim.Optimizer`]):\n            The optimizer for which to schedule the learning rate.\n        num_warmup_steps (`int`):\n            The number of steps for the warmup phase.\n        num_training_steps (`int`):\n            The total number of training steps.\n        num_periods (`float`, *optional*, defaults to 0.5):\n            The number of periods of the cosine function in a schedule (the default is to just decrease from the max\n            value to 0 following a half-cosine).\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n\n    Return:\n        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"\n\n    def lr_lambda(current_step):\n        if current_step &lt; num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        progress = float(current_step - num_warmup_steps) / float(\n            max(1, num_training_steps - num_warmup_steps))\n        return max(\n            0.0, 0.5 *\n            (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))\n\n    return LambdaLR(optimizer, lr_lambda, last_epoch)\n</code></pre> fastvideo.training.training_utils.get_cosine_with_hard_restarts_schedule_with_warmup \u00b6 <pre><code>get_cosine_with_hard_restarts_schedule_with_warmup(\n    optimizer: Optimizer,\n    num_warmup_steps: int,\n    num_training_steps: int,\n    num_cycles: int = 1,\n    last_epoch: int = -1,\n) -&gt; LambdaLR\n</code></pre> <p>Create a schedule with a learning rate that decreases following the values of the cosine function between the initial lr set in the optimizer to 0, with several hard restarts, after a warmup period during which it increases linearly between 0 and the initial lr set in the optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>[`~torch.optim.Optimizer`]</code> <p>The optimizer for which to schedule the learning rate.</p> required <code>num_warmup_steps</code> <code>`int`</code> <p>The number of steps for the warmup phase.</p> required <code>num_training_steps</code> <code>`int`</code> <p>The total number of training steps.</p> required <code>num_cycles</code> <code>`int`, *optional*, defaults to 1</code> <p>The number of hard restarts to use.</p> <code>1</code> <code>last_epoch</code> <code>`int`, *optional*, defaults to -1</code> <p>The index of the last epoch when resuming training.</p> <code>-1</code> Return <p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def get_cosine_with_hard_restarts_schedule_with_warmup(\n        optimizer: Optimizer,\n        num_warmup_steps: int,\n        num_training_steps: int,\n        num_cycles: int = 1,\n        last_epoch: int = -1) -&gt; LambdaLR:\n    \"\"\"\n    Create a schedule with a learning rate that decreases following the values of the cosine function between the\n    initial lr set in the optimizer to 0, with several hard restarts, after a warmup period during which it increases\n    linearly between 0 and the initial lr set in the optimizer.\n\n    Args:\n        optimizer ([`~torch.optim.Optimizer`]):\n            The optimizer for which to schedule the learning rate.\n        num_warmup_steps (`int`):\n            The number of steps for the warmup phase.\n        num_training_steps (`int`):\n            The total number of training steps.\n        num_cycles (`int`, *optional*, defaults to 1):\n            The number of hard restarts to use.\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n\n    Return:\n        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"\n\n    def lr_lambda(current_step):\n        if current_step &lt; num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        progress = float(current_step - num_warmup_steps) / float(\n            max(1, num_training_steps - num_warmup_steps))\n        if progress &gt;= 1.0:\n            return 0.0\n        return max(\n            0.0, 0.5 * (1.0 + math.cos(math.pi *\n                                       ((float(num_cycles) * progress) % 1.0))))\n\n    return LambdaLR(optimizer, lr_lambda, last_epoch)\n</code></pre> fastvideo.training.training_utils.get_linear_schedule_with_warmup \u00b6 <pre><code>get_linear_schedule_with_warmup(\n    optimizer: Optimizer,\n    num_warmup_steps: int,\n    num_training_steps: int,\n    last_epoch: int = -1,\n) -&gt; LambdaLR\n</code></pre> <p>Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>[`~torch.optim.Optimizer`]</code> <p>The optimizer for which to schedule the learning rate.</p> required <code>num_warmup_steps</code> <code>`int`</code> <p>The number of steps for the warmup phase.</p> required <code>num_training_steps</code> <code>`int`</code> <p>The total number of training steps.</p> required <code>last_epoch</code> <code>`int`, *optional*, defaults to -1</code> <p>The index of the last epoch when resuming training.</p> <code>-1</code> Return <p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def get_linear_schedule_with_warmup(optimizer: Optimizer,\n                                    num_warmup_steps: int,\n                                    num_training_steps: int,\n                                    last_epoch: int = -1) -&gt; LambdaLR:\n    \"\"\"\n    Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after\n    a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.\n\n    Args:\n        optimizer ([`~torch.optim.Optimizer`]):\n            The optimizer for which to schedule the learning rate.\n        num_warmup_steps (`int`):\n            The number of steps for the warmup phase.\n        num_training_steps (`int`):\n            The total number of training steps.\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n\n    Return:\n        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"\n\n    def lr_lambda(current_step: int):\n        if current_step &lt; num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        return max(\n            0.0,\n            float(num_training_steps - current_step) /\n            float(max(1, num_training_steps - num_warmup_steps)))\n\n    return LambdaLR(optimizer, lr_lambda, last_epoch)\n</code></pre> fastvideo.training.training_utils.get_piecewise_constant_schedule \u00b6 <pre><code>get_piecewise_constant_schedule(\n    optimizer: Optimizer,\n    step_rules: str,\n    last_epoch: int = -1,\n) -&gt; LambdaLR\n</code></pre> <p>Create a schedule with a constant learning rate, using the learning rate set in optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>[`~torch.optim.Optimizer`]</code> <p>The optimizer for which to schedule the learning rate.</p> required <code>step_rules</code> <code>`string`</code> <p>The rules for the learning rate. ex: rule_steps=\"1:10,0.1:20,0.01:30,0.005\" it means that the learning rate if multiple 1 for the first 10 steps, multiple 0.1 for the next 20 steps, multiple 0.01 for the next 30 steps and multiple 0.005 for the other steps.</p> required <code>last_epoch</code> <code>`int`, *optional*, defaults to -1</code> <p>The index of the last epoch when resuming training.</p> <code>-1</code> Return <p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def get_piecewise_constant_schedule(optimizer: Optimizer,\n                                    step_rules: str,\n                                    last_epoch: int = -1) -&gt; LambdaLR:\n    \"\"\"\n    Create a schedule with a constant learning rate, using the learning rate set in optimizer.\n\n    Args:\n        optimizer ([`~torch.optim.Optimizer`]):\n            The optimizer for which to schedule the learning rate.\n        step_rules (`string`):\n            The rules for the learning rate. ex: rule_steps=\"1:10,0.1:20,0.01:30,0.005\" it means that the learning rate\n            if multiple 1 for the first 10 steps, multiple 0.1 for the next 20 steps, multiple 0.01 for the next 30\n            steps and multiple 0.005 for the other steps.\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n\n    Return:\n        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"\n\n    rules_dict = {}\n    rule_list = step_rules.split(\",\")\n    for rule_str in rule_list[:-1]:\n        value_str, steps_str = rule_str.split(\":\")\n        steps = int(steps_str)\n        value = float(value_str)\n        rules_dict[steps] = value\n    last_lr_multiple = float(rule_list[-1])\n\n    def create_rules_function(\n            rules_dict: dict,\n            last_lr_multiple: float) -&gt; Callable[[int], float]:\n\n        def rule_func(steps: int) -&gt; float:\n            for step_threshold, lr_multiple in sorted(rules_dict.items()):\n                if steps &lt; step_threshold:\n                    return lr_multiple\n            return last_lr_multiple\n\n        return rule_func\n\n    rules_func = create_rules_function(rules_dict, last_lr_multiple)\n\n    return LambdaLR(optimizer, rules_func, last_epoch=last_epoch)\n</code></pre> fastvideo.training.training_utils.get_polynomial_decay_schedule_with_warmup \u00b6 <pre><code>get_polynomial_decay_schedule_with_warmup(\n    optimizer: Optimizer,\n    num_warmup_steps: int,\n    num_training_steps: int,\n    lr_end: float = 1e-07,\n    power: float = 1.0,\n    last_epoch: int = -1,\n) -&gt; LambdaLR\n</code></pre> <p>Create a schedule with a learning rate that decreases as a polynomial decay from the initial lr set in the optimizer to end lr defined by lr_end, after a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>[`~torch.optim.Optimizer`]</code> <p>The optimizer for which to schedule the learning rate.</p> required <code>num_warmup_steps</code> <code>`int`</code> <p>The number of steps for the warmup phase.</p> required <code>num_training_steps</code> <code>`int`</code> <p>The total number of training steps.</p> required <code>lr_end</code> <code>`float`, *optional*, defaults to 1e-7</code> <p>The end LR.</p> <code>1e-07</code> <code>power</code> <code>`float`, *optional*, defaults to 1.0</code> <p>Power factor.</p> <code>1.0</code> <code>last_epoch</code> <code>`int`, *optional*, defaults to -1</code> <p>The index of the last epoch when resuming training.</p> <code>-1</code> <p>Note: power defaults to 1.0 as in the fairseq implementation, which in turn is based on the original BERT implementation at https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37</p> Return <p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def get_polynomial_decay_schedule_with_warmup(\n    optimizer: Optimizer,\n    num_warmup_steps: int,\n    num_training_steps: int,\n    lr_end: float = 1e-7,\n    power: float = 1.0,\n    last_epoch: int = -1,\n) -&gt; LambdaLR:\n    \"\"\"\n    Create a schedule with a learning rate that decreases as a polynomial decay from the initial lr set in the\n    optimizer to end lr defined by *lr_end*, after a warmup period during which it increases linearly from 0 to the\n    initial lr set in the optimizer.\n\n    Args:\n        optimizer ([`~torch.optim.Optimizer`]):\n            The optimizer for which to schedule the learning rate.\n        num_warmup_steps (`int`):\n            The number of steps for the warmup phase.\n        num_training_steps (`int`):\n            The total number of training steps.\n        lr_end (`float`, *optional*, defaults to 1e-7):\n            The end LR.\n        power (`float`, *optional*, defaults to 1.0):\n            Power factor.\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n\n    Note: *power* defaults to 1.0 as in the fairseq implementation, which in turn is based on the original BERT\n    implementation at\n    https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37\n\n    Return:\n        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n\n    \"\"\"\n\n    lr_init = optimizer.defaults[\"lr\"]\n    if not (lr_init &gt; lr_end):\n        raise ValueError(\n            f\"lr_end ({lr_end}) must be smaller than initial lr ({lr_init})\")\n\n    def lr_lambda(current_step: int):\n        if current_step &lt; num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        elif current_step &gt; num_training_steps:\n            return lr_end / lr_init  # as LambdaLR multiplies by lr_init\n        else:\n            lr_range = lr_init - lr_end\n            decay_steps = num_training_steps - num_warmup_steps\n            pct_remaining = 1 - (current_step - num_warmup_steps) / decay_steps\n            decay = lr_range * pct_remaining**power + lr_end\n            return decay / lr_init  # as LambdaLR multiplies by lr_init\n\n    return LambdaLR(optimizer, lr_lambda, last_epoch)\n</code></pre> fastvideo.training.training_utils.get_scheduler \u00b6 <pre><code>get_scheduler(\n    name: str | SchedulerType,\n    optimizer: Optimizer,\n    step_rules: str | None = None,\n    num_warmup_steps: int | None = None,\n    num_training_steps: int | None = None,\n    num_cycles: int = 1,\n    power: float = 1.0,\n    min_lr_ratio: float = 0.1,\n    last_epoch: int = -1,\n) -&gt; LambdaLR\n</code></pre> <p>Unified API to get any scheduler from its name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>`str` or `SchedulerType`</code> <p>The name of the scheduler to use.</p> required <code>optimizer</code> <code>`torch.optim.Optimizer`</code> <p>The optimizer that will be used during training.</p> required <code>step_rules</code> <code>`str`, *optional*</code> <p>A string representing the step rules to use. This is only used by the <code>PIECEWISE_CONSTANT</code> scheduler.</p> <code>None</code> <code>num_warmup_steps</code> <code>`int`, *optional*</code> <p>The number of warmup steps to do. This is not required by all schedulers (hence the argument being optional), the function will raise an error if it's unset and the scheduler type requires it.</p> <code>None</code> <code>num_training_steps</code> <code>`int``, *optional*</code> <p>The number of training steps to do. This is not required by all schedulers (hence the argument being optional), the function will raise an error if it's unset and the scheduler type requires it.</p> <code>None</code> <code>num_cycles</code> <code>`int`, *optional*</code> <p>The number of hard restarts used in <code>COSINE_WITH_RESTARTS</code> scheduler.</p> <code>1</code> <code>power</code> <code>`float`, *optional*, defaults to 1.0</code> <p>Power factor. See <code>POLYNOMIAL</code> scheduler</p> <code>1.0</code> <code>min_lr_ratio</code> <code>`float`, *optional*, defaults to 0.1</code> <p>The ratio of minimum learning rate to initial learning rate. Used in <code>COSINE_WITH_MIN_LR</code> scheduler.</p> <code>0.1</code> <code>last_epoch</code> <code>`int`, *optional*, defaults to -1</code> <p>The index of the last epoch when resuming training.</p> <code>-1</code> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def get_scheduler(\n    name: str | SchedulerType,\n    optimizer: Optimizer,\n    step_rules: str | None = None,\n    num_warmup_steps: int | None = None,\n    num_training_steps: int | None = None,\n    num_cycles: int = 1,\n    power: float = 1.0,\n    min_lr_ratio: float = 0.1,\n    last_epoch: int = -1,\n) -&gt; LambdaLR:\n    \"\"\"\n    Unified API to get any scheduler from its name.\n\n    Args:\n        name (`str` or `SchedulerType`):\n            The name of the scheduler to use.\n        optimizer (`torch.optim.Optimizer`):\n            The optimizer that will be used during training.\n        step_rules (`str`, *optional*):\n            A string representing the step rules to use. This is only used by the `PIECEWISE_CONSTANT` scheduler.\n        num_warmup_steps (`int`, *optional*):\n            The number of warmup steps to do. This is not required by all schedulers (hence the argument being\n            optional), the function will raise an error if it's unset and the scheduler type requires it.\n        num_training_steps (`int``, *optional*):\n            The number of training steps to do. This is not required by all schedulers (hence the argument being\n            optional), the function will raise an error if it's unset and the scheduler type requires it.\n        num_cycles (`int`, *optional*):\n            The number of hard restarts used in `COSINE_WITH_RESTARTS` scheduler.\n        power (`float`, *optional*, defaults to 1.0):\n            Power factor. See `POLYNOMIAL` scheduler\n        min_lr_ratio (`float`, *optional*, defaults to 0.1):\n            The ratio of minimum learning rate to initial learning rate. Used in `COSINE_WITH_MIN_LR` scheduler.\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n    \"\"\"\n    name = SchedulerType(name)\n    schedule_func = TYPE_TO_SCHEDULER_FUNCTION[name]\n    if name == SchedulerType.CONSTANT:\n        return schedule_func(optimizer, last_epoch=last_epoch)\n\n    if name == SchedulerType.PIECEWISE_CONSTANT:\n        return schedule_func(optimizer,\n                             step_rules=step_rules,\n                             last_epoch=last_epoch)\n\n    # All other schedulers require `num_warmup_steps`\n    if num_warmup_steps is None:\n        raise ValueError(\n            f\"{name} requires `num_warmup_steps`, please provide that argument.\"\n        )\n\n    if name == SchedulerType.CONSTANT_WITH_WARMUP:\n        return schedule_func(optimizer,\n                             num_warmup_steps=num_warmup_steps,\n                             last_epoch=last_epoch)\n\n    # All other schedulers require `num_training_steps`\n    if num_training_steps is None:\n        raise ValueError(\n            f\"{name} requires `num_training_steps`, please provide that argument.\"\n        )\n\n    if name == SchedulerType.COSINE_WITH_RESTARTS:\n        return schedule_func(\n            optimizer,\n            num_warmup_steps=num_warmup_steps,\n            num_training_steps=num_training_steps,\n            num_cycles=num_cycles,\n            last_epoch=last_epoch,\n        )\n\n    if name == SchedulerType.POLYNOMIAL:\n        return schedule_func(\n            optimizer,\n            num_warmup_steps=num_warmup_steps,\n            num_training_steps=num_training_steps,\n            power=power,\n            last_epoch=last_epoch,\n        )\n\n    if name == SchedulerType.COSINE_WITH_MIN_LR:\n        return schedule_func(\n            optimizer,\n            num_warmup_steps=num_warmup_steps,\n            num_training_steps=num_training_steps,\n            min_lr_ratio=min_lr_ratio,\n            last_epoch=last_epoch,\n        )\n\n    return schedule_func(optimizer,\n                         num_warmup_steps=num_warmup_steps,\n                         num_training_steps=num_training_steps,\n                         last_epoch=last_epoch)\n</code></pre> fastvideo.training.training_utils.load_checkpoint \u00b6 <pre><code>load_checkpoint(\n    transformer,\n    rank,\n    checkpoint_path,\n    optimizer=None,\n    dataloader=None,\n    scheduler=None,\n    noise_generator=None,\n) -&gt; int\n</code></pre> <p>Load checkpoint following finetrainer's distributed checkpoint approach. Returns the step number from which training should resume.</p> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def load_checkpoint(transformer,\n                    rank,\n                    checkpoint_path,\n                    optimizer=None,\n                    dataloader=None,\n                    scheduler=None,\n                    noise_generator=None) -&gt; int:\n    \"\"\"\n    Load checkpoint following finetrainer's distributed checkpoint approach.\n    Returns the step number from which training should resume.\n    \"\"\"\n    if not os.path.exists(checkpoint_path):\n        logger.warning(\"Checkpoint path %s does not exist\", checkpoint_path)\n        return 0\n\n    # Extract step number from checkpoint path\n    step = int(os.path.basename(checkpoint_path).split('-')[-1])\n\n    if rank == 0:\n        logger.info(\"Loading checkpoint from step %s\", step)\n\n    dcp_dir = os.path.join(checkpoint_path, \"distributed_checkpoint\")\n\n    if not os.path.exists(dcp_dir):\n        logger.warning(\"Distributed checkpoint directory %s does not exist\",\n                       dcp_dir)\n        return 0\n\n    states = {\n        \"model\": ModelWrapper(transformer),\n        \"random_state\": RandomStateWrapper(noise_generator),\n    }\n\n    if optimizer is not None:\n        states[\"optimizer\"] = OptimizerWrapper(transformer, optimizer)\n\n    if dataloader is not None:\n        states[\"dataloader\"] = dataloader\n\n    if scheduler is not None:\n        states[\"scheduler\"] = SchedulerWrapper(scheduler)\n\n    logger.info(\"rank: %s, loading distributed checkpoint from %s\",\n                rank,\n                dcp_dir,\n                local_main_process_only=False)\n\n    begin_time = time.perf_counter()\n    dcp.load(states, checkpoint_id=dcp_dir)\n    end_time = time.perf_counter()\n\n    logger.info(\"rank: %s, distributed checkpoint loaded in %.2f seconds\",\n                rank,\n                end_time - begin_time,\n                local_main_process_only=False)\n    logger.info(\"--&gt; checkpoint loaded from step %s\", step)\n\n    return step\n</code></pre> fastvideo.training.training_utils.load_distillation_checkpoint \u00b6 <pre><code>load_distillation_checkpoint(\n    generator_transformer,\n    fake_score_transformer,\n    rank,\n    checkpoint_path,\n    generator_optimizer=None,\n    fake_score_optimizer=None,\n    dataloader=None,\n    generator_scheduler=None,\n    fake_score_scheduler=None,\n    noise_generator=None,\n    generator_ema=None,\n    generator_transformer_2=None,\n    real_score_transformer_2=None,\n    fake_score_transformer_2=None,\n    generator_optimizer_2=None,\n    fake_score_optimizer_2=None,\n    generator_scheduler_2=None,\n    fake_score_scheduler_2=None,\n    generator_ema_2=None,\n) -&gt; int\n</code></pre> <p>Load distillation checkpoint with both generator and fake_score models. Supports MoE (Mixture of Experts) models with transformer_2 variants. Returns the step number from which training should resume.</p> <p>Parameters:</p> Name Type Description Default <code>generator_transformer</code> <p>Main generator transformer model</p> required <code>fake_score_transformer</code> <p>Main fake score transformer model</p> required <code>generator_transformer_2</code> <p>Secondary generator transformer for MoE (optional)</p> <code>None</code> <code>real_score_transformer_2</code> <p>Secondary real score transformer for MoE (optional)</p> <code>None</code> <code>fake_score_transformer_2</code> <p>Secondary fake score transformer for MoE (optional)</p> <code>None</code> <code>generator_optimizer_2</code> <p>Optimizer for generator_transformer_2 (optional)</p> <code>None</code> <code>fake_score_optimizer_2</code> <p>Optimizer for fake_score_transformer_2 (optional)</p> <code>None</code> <code>generator_scheduler_2</code> <p>Scheduler for generator_transformer_2 (optional)</p> <code>None</code> <code>fake_score_scheduler_2</code> <p>Scheduler for fake_score_transformer_2 (optional)</p> <code>None</code> <code>generator_ema_2</code> <p>EMA for generator_transformer_2 (optional)</p> <code>None</code> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def load_distillation_checkpoint(\n        generator_transformer,\n        fake_score_transformer,\n        rank,\n        checkpoint_path,\n        generator_optimizer=None,\n        fake_score_optimizer=None,\n        dataloader=None,\n        generator_scheduler=None,\n        fake_score_scheduler=None,\n        noise_generator=None,\n        generator_ema=None,\n        # MoE support\n        generator_transformer_2=None,\n        real_score_transformer_2=None,\n        fake_score_transformer_2=None,\n        generator_optimizer_2=None,\n        fake_score_optimizer_2=None,\n        generator_scheduler_2=None,\n        fake_score_scheduler_2=None,\n        generator_ema_2=None) -&gt; int:\n    \"\"\"\n    Load distillation checkpoint with both generator and fake_score models.\n    Supports MoE (Mixture of Experts) models with transformer_2 variants.\n    Returns the step number from which training should resume.\n\n    Args:\n        generator_transformer: Main generator transformer model\n        fake_score_transformer: Main fake score transformer model\n        generator_transformer_2: Secondary generator transformer for MoE (optional)\n        real_score_transformer_2: Secondary real score transformer for MoE (optional)\n        fake_score_transformer_2: Secondary fake score transformer for MoE (optional)\n        generator_optimizer_2: Optimizer for generator_transformer_2 (optional)\n        fake_score_optimizer_2: Optimizer for fake_score_transformer_2 (optional)\n        generator_scheduler_2: Scheduler for generator_transformer_2 (optional)\n        fake_score_scheduler_2: Scheduler for fake_score_transformer_2 (optional)\n        generator_ema_2: EMA for generator_transformer_2 (optional)\n    \"\"\"\n    if not os.path.exists(checkpoint_path):\n        logger.warning(\"Distillation checkpoint path %s does not exist\",\n                       checkpoint_path)\n        return 0\n\n    # Extract step number from checkpoint path\n    step = int(os.path.basename(checkpoint_path).split('-')[-1])\n\n    if rank == 0:\n        logger.info(\"Loading distillation checkpoint from step %s\", step)\n\n    # Load generator distributed checkpoint\n    generator_dcp_dir = os.path.join(checkpoint_path, \"distributed_checkpoint\",\n                                     \"generator\")\n    if not os.path.exists(generator_dcp_dir):\n        logger.warning(\n            \"Generator distributed checkpoint directory %s does not exist\",\n            generator_dcp_dir)\n        return 0\n\n    generator_states = {\n        \"model\": ModelWrapper(generator_transformer),\n    }\n\n    if generator_optimizer is not None:\n        generator_states[\"optimizer\"] = OptimizerWrapper(\n            generator_transformer, generator_optimizer)\n\n    if dataloader is not None:\n        generator_states[\"dataloader\"] = dataloader\n\n    if generator_scheduler is not None:\n        generator_states[\"scheduler\"] = SchedulerWrapper(generator_scheduler)\n\n    logger.info(\"rank: %s, loading generator distributed checkpoint from %s\",\n                rank,\n                generator_dcp_dir,\n                local_main_process_only=False)\n\n    begin_time = time.perf_counter()\n    dcp.load(generator_states, checkpoint_id=generator_dcp_dir)\n    end_time = time.perf_counter()\n\n    logger.info(\n        \"rank: %s, generator distributed checkpoint loaded in %.2f seconds\",\n        rank,\n        end_time - begin_time,\n        local_main_process_only=False)\n\n    # Load EMA state if available and generator_ema is provided\n    if generator_ema is not None:\n        try:\n            ema_state = generator_states.get(\"ema\")\n            if ema_state is not None:\n                generator_ema.load_state_dict(ema_state)\n                logger.info(\"rank: %s, generator EMA state loaded successfully\",\n                            rank)\n            else:\n                logger.info(\"rank: %s, no EMA state found in checkpoint\", rank)\n        except Exception as e:\n            logger.warning(\"rank: %s, failed to load EMA state: %s\", rank,\n                           str(e))\n\n    # Load generator_2 distributed checkpoint (MoE support)\n    if generator_transformer_2 is not None:\n        generator_2_dcp_dir = os.path.join(checkpoint_path,\n                                           \"distributed_checkpoint\",\n                                           \"generator_2\")\n        if os.path.exists(generator_2_dcp_dir):\n            generator_2_states = {\n                \"model\": ModelWrapper(generator_transformer_2),\n            }\n\n            if generator_optimizer_2 is not None:\n                generator_2_states[\"optimizer\"] = OptimizerWrapper(\n                    generator_transformer_2, generator_optimizer_2)\n\n            if dataloader is not None:\n                generator_2_states[\"dataloader\"] = dataloader\n\n            if generator_scheduler_2 is not None:\n                generator_2_states[\"scheduler\"] = SchedulerWrapper(\n                    generator_scheduler_2)\n\n            logger.info(\n                \"rank: %s, loading generator_2 distributed checkpoint from %s\",\n                rank,\n                generator_2_dcp_dir,\n                local_main_process_only=False)\n\n            begin_time = time.perf_counter()\n            dcp.load(generator_2_states, checkpoint_id=generator_2_dcp_dir)\n            end_time = time.perf_counter()\n\n            logger.info(\n                \"rank: %s, generator_2 distributed checkpoint loaded in %.2f seconds\",\n                rank,\n                end_time - begin_time,\n                local_main_process_only=False)\n\n            # Load EMA_2 state if available and generator_ema_2 is provided\n            if generator_ema_2 is not None:\n                try:\n                    ema_2_state = generator_2_states.get(\"ema\")\n                    if ema_2_state is not None:\n                        generator_ema_2.load_state_dict(ema_2_state)\n                        logger.info(\n                            \"rank: %s, generator_2 EMA state loaded successfully\",\n                            rank)\n                    else:\n                        logger.info(\n                            \"rank: %s, no EMA_2 state found in checkpoint\",\n                            rank)\n                except Exception as e:\n                    logger.warning(\"rank: %s, failed to load EMA_2 state: %s\",\n                                   rank, str(e))\n        else:\n            logger.info(\"rank: %s, generator_2 checkpoint not found, skipping\",\n                        rank)\n\n    # Load critic distributed checkpoint\n    critic_dcp_dir = os.path.join(checkpoint_path, \"distributed_checkpoint\",\n                                  \"critic\")\n    if not os.path.exists(critic_dcp_dir):\n        logger.warning(\n            \"Critic distributed checkpoint directory %s does not exist\",\n            critic_dcp_dir)\n        return 0\n\n    critic_states = {\n        \"model\": ModelWrapper(fake_score_transformer),\n    }\n\n    if fake_score_optimizer is not None:\n        critic_states[\"optimizer\"] = OptimizerWrapper(fake_score_transformer,\n                                                      fake_score_optimizer)\n\n    if dataloader is not None:\n        critic_states[\"dataloader\"] = dataloader\n\n    if fake_score_scheduler is not None:\n        critic_states[\"scheduler\"] = SchedulerWrapper(fake_score_scheduler)\n\n    logger.info(\"rank: %s, loading critic distributed checkpoint from %s\",\n                rank,\n                critic_dcp_dir,\n                local_main_process_only=False)\n\n    begin_time = time.perf_counter()\n    dcp.load(critic_states, checkpoint_id=critic_dcp_dir)\n    end_time = time.perf_counter()\n\n    logger.info(\n        \"rank: %s, critic distributed checkpoint loaded in %.2f seconds\",\n        rank,\n        end_time - begin_time,\n        local_main_process_only=False)\n\n    # Load critic_2 distributed checkpoint (MoE support)\n    if fake_score_transformer_2 is not None:\n        critic_2_dcp_dir = os.path.join(checkpoint_path,\n                                        \"distributed_checkpoint\", \"critic_2\")\n        if os.path.exists(critic_2_dcp_dir):\n            critic_2_states = {\n                \"model\": ModelWrapper(fake_score_transformer_2),\n            }\n\n            if fake_score_optimizer_2 is not None:\n                critic_2_states[\"optimizer\"] = OptimizerWrapper(\n                    fake_score_transformer_2, fake_score_optimizer_2)\n\n            if dataloader is not None:\n                critic_2_states[\"dataloader\"] = dataloader\n\n            if fake_score_scheduler_2 is not None:\n                critic_2_states[\"scheduler\"] = SchedulerWrapper(\n                    fake_score_scheduler_2)\n\n            logger.info(\n                \"rank: %s, loading critic_2 distributed checkpoint from %s\",\n                rank,\n                critic_2_dcp_dir,\n                local_main_process_only=False)\n\n            begin_time = time.perf_counter()\n            dcp.load(critic_2_states, checkpoint_id=critic_2_dcp_dir)\n            end_time = time.perf_counter()\n\n            logger.info(\n                \"rank: %s, critic_2 distributed checkpoint loaded in %.2f seconds\",\n                rank,\n                end_time - begin_time,\n                local_main_process_only=False)\n        else:\n            logger.info(\"rank: %s, critic_2 checkpoint not found, skipping\",\n                        rank)\n\n    # Load real_score_2 distributed checkpoint (MoE support)\n    if real_score_transformer_2 is not None:\n        real_score_2_dcp_dir = os.path.join(checkpoint_path,\n                                            \"distributed_checkpoint\",\n                                            \"real_score_2\")\n        if os.path.exists(real_score_2_dcp_dir):\n            real_score_2_states = {\n                \"model\": ModelWrapper(real_score_transformer_2),\n            }\n\n            if dataloader is not None:\n                real_score_2_states[\"dataloader\"] = dataloader\n\n            logger.info(\n                \"rank: %s, loading real_score_2 distributed checkpoint from %s\",\n                rank,\n                real_score_2_dcp_dir,\n                local_main_process_only=False)\n\n            begin_time = time.perf_counter()\n            dcp.load(real_score_2_states, checkpoint_id=real_score_2_dcp_dir)\n            end_time = time.perf_counter()\n\n            logger.info(\n                \"rank: %s, real_score_2 distributed checkpoint loaded in %.2f seconds\",\n                rank,\n                end_time - begin_time,\n                local_main_process_only=False)\n        else:\n            logger.info(\"rank: %s, real_score_2 checkpoint not found, skipping\",\n                        rank)\n\n    # Load shared random state\n    shared_dcp_dir = os.path.join(checkpoint_path, \"distributed_checkpoint\",\n                                  \"shared\")\n    if not os.path.exists(shared_dcp_dir):\n        logger.warning(\"Shared random state directory %s does not exist\",\n                       shared_dcp_dir)\n        return 0\n\n    shared_states = {\n        \"random_state\": RandomStateWrapper(noise_generator),\n    }\n\n    begin_time = time.perf_counter()\n    dcp.load(shared_states, checkpoint_id=shared_dcp_dir)\n    end_time = time.perf_counter()\n\n    logger.info(\"rank: %s, shared random state loaded in %.2f seconds\",\n                rank,\n                end_time - begin_time,\n                local_main_process_only=False)\n    logger.info(\"--&gt; distillation checkpoint loaded from step %s\", step)\n    return step\n</code></pre> fastvideo.training.training_utils.save_checkpoint \u00b6 <pre><code>save_checkpoint(\n    transformer,\n    rank,\n    output_dir,\n    step,\n    optimizer=None,\n    dataloader=None,\n    scheduler=None,\n    noise_generator=None,\n) -&gt; None\n</code></pre> <p>Save checkpoint following finetrainer's distributed checkpoint approach. Saves both distributed checkpoint and consolidated model weights.</p> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def save_checkpoint(transformer,\n                    rank,\n                    output_dir,\n                    step,\n                    optimizer=None,\n                    dataloader=None,\n                    scheduler=None,\n                    noise_generator=None) -&gt; None:\n    \"\"\"\n    Save checkpoint following finetrainer's distributed checkpoint approach.\n    Saves both distributed checkpoint and consolidated model weights.\n    \"\"\"\n    save_dir = os.path.join(output_dir, f\"checkpoint-{step}\")\n    os.makedirs(save_dir, exist_ok=True)\n\n    states = {\n        \"model\": ModelWrapper(transformer),\n        \"random_state\": RandomStateWrapper(noise_generator),\n    }\n\n    if optimizer is not None:\n        states[\"optimizer\"] = OptimizerWrapper(transformer, optimizer)\n\n    if dataloader is not None:\n        states[\"dataloader\"] = dataloader\n\n    if scheduler is not None:\n        states[\"scheduler\"] = SchedulerWrapper(scheduler)\n    dcp_dir = os.path.join(save_dir, \"distributed_checkpoint\")\n    logger.info(\"rank: %s, saving distributed checkpoint to %s\",\n                rank,\n                dcp_dir,\n                local_main_process_only=False)\n\n    begin_time = time.perf_counter()\n    dcp.save(states, checkpoint_id=dcp_dir)\n    end_time = time.perf_counter()\n\n    logger.info(\"rank: %s, distributed checkpoint saved in %.2f seconds\",\n                rank,\n                end_time - begin_time,\n                local_main_process_only=False)\n\n    cpu_state = gather_state_dict_on_cpu_rank0(transformer, device=None)\n    if rank == 0:\n        # Save model weights (consolidated)\n        transformer_save_dir = os.path.join(save_dir, \"transformer\")\n        os.makedirs(transformer_save_dir, exist_ok=True)\n        weight_path = os.path.join(transformer_save_dir,\n                                   \"diffusion_pytorch_model.safetensors\")\n        logger.info(\"rank: %s, saving consolidated checkpoint to %s\",\n                    rank,\n                    weight_path,\n                    local_main_process_only=False)\n\n        # Convert training format to diffusers format and save\n        diffusers_state_dict = custom_to_hf_state_dict(\n            cpu_state, transformer.reverse_param_names_mapping)\n        save_file(diffusers_state_dict, weight_path)\n\n        logger.info(\"rank: %s, consolidated checkpoint saved to %s\",\n                    rank,\n                    weight_path,\n                    local_main_process_only=False)\n\n        # Save model config\n        config_dict = transformer.hf_config\n        if \"dtype\" in config_dict:\n            del config_dict[\"dtype\"]  # TODO\n        config_path = os.path.join(transformer_save_dir, \"config.json\")\n        # save dict as json\n        with open(config_path, \"w\") as f:\n            json.dump(config_dict, f, indent=4)\n        logger.info(\"--&gt; checkpoint saved at step %s to %s\", step, weight_path)\n</code></pre> fastvideo.training.training_utils.save_distillation_checkpoint \u00b6 <pre><code>save_distillation_checkpoint(\n    generator_transformer,\n    fake_score_transformer,\n    rank,\n    output_dir,\n    step,\n    generator_optimizer=None,\n    fake_score_optimizer=None,\n    dataloader=None,\n    generator_scheduler=None,\n    fake_score_scheduler=None,\n    noise_generator=None,\n    generator_ema=None,\n    only_save_generator_weight=False,\n    generator_transformer_2=None,\n    real_score_transformer_2=None,\n    fake_score_transformer_2=None,\n    generator_optimizer_2=None,\n    fake_score_optimizer_2=None,\n    generator_scheduler_2=None,\n    fake_score_scheduler_2=None,\n    generator_ema_2=None,\n) -&gt; None\n</code></pre> <p>Save distillation checkpoint with both generator and fake_score models. Supports MoE (Mixture of Experts) models with transformer_2 variants. Saves both distributed checkpoint and consolidated model weights. Only saves the generator model for inference (consolidated weights).</p> <p>Parameters:</p> Name Type Description Default <code>generator_transformer</code> <p>Main generator transformer model</p> required <code>fake_score_transformer</code> <p>Main fake score transformer model</p> required <code>only_save_generator_weight</code> <p>If True, only save the generator model weights for inference                        without saving distributed checkpoint for training resume.</p> <code>False</code> <code>generator_transformer_2</code> <p>Secondary generator transformer for MoE (optional)</p> <code>None</code> <code>real_score_transformer_2</code> <p>Secondary real score transformer for MoE (optional) </p> <code>None</code> <code>fake_score_transformer_2</code> <p>Secondary fake score transformer for MoE (optional)</p> <code>None</code> <code>generator_optimizer_2</code> <p>Optimizer for generator_transformer_2 (optional)</p> <code>None</code> <code>fake_score_optimizer_2</code> <p>Optimizer for fake_score_transformer_2 (optional)</p> <code>None</code> <code>generator_scheduler_2</code> <p>Scheduler for generator_transformer_2 (optional)</p> <code>None</code> <code>fake_score_scheduler_2</code> <p>Scheduler for fake_score_transformer_2 (optional)</p> <code>None</code> <code>generator_ema_2</code> <p>EMA for generator_transformer_2 (optional)</p> <code>None</code> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def save_distillation_checkpoint(\n        generator_transformer,\n        fake_score_transformer,\n        rank,\n        output_dir,\n        step,\n        generator_optimizer=None,\n        fake_score_optimizer=None,\n        dataloader=None,\n        generator_scheduler=None,\n        fake_score_scheduler=None,\n        noise_generator=None,\n        generator_ema=None,\n        only_save_generator_weight=False,\n        # MoE support\n        generator_transformer_2=None,\n        real_score_transformer_2=None,\n        fake_score_transformer_2=None,\n        generator_optimizer_2=None,\n        fake_score_optimizer_2=None,\n        generator_scheduler_2=None,\n        fake_score_scheduler_2=None,\n        generator_ema_2=None) -&gt; None:\n    \"\"\"\n    Save distillation checkpoint with both generator and fake_score models.\n    Supports MoE (Mixture of Experts) models with transformer_2 variants.\n    Saves both distributed checkpoint and consolidated model weights.\n    Only saves the generator model for inference (consolidated weights).\n\n    Args:\n        generator_transformer: Main generator transformer model\n        fake_score_transformer: Main fake score transformer model\n        only_save_generator_weight: If True, only save the generator model weights for inference\n                                   without saving distributed checkpoint for training resume.\n        generator_transformer_2: Secondary generator transformer for MoE (optional)\n        real_score_transformer_2: Secondary real score transformer for MoE (optional) \n        fake_score_transformer_2: Secondary fake score transformer for MoE (optional)\n        generator_optimizer_2: Optimizer for generator_transformer_2 (optional)\n        fake_score_optimizer_2: Optimizer for fake_score_transformer_2 (optional)\n        generator_scheduler_2: Scheduler for generator_transformer_2 (optional)\n        fake_score_scheduler_2: Scheduler for fake_score_transformer_2 (optional)\n        generator_ema_2: EMA for generator_transformer_2 (optional)\n    \"\"\"\n    save_dir = os.path.join(output_dir, f\"checkpoint-{step}\")\n    os.makedirs(save_dir, exist_ok=True)\n\n    # Create directories for models\n    inference_save_dir = os.path.join(save_dir,\n                                      \"generator_inference_transformer\")\n\n    # Only save distributed checkpoint if not only saving generator weight\n    if not only_save_generator_weight:\n        # Save generator distributed checkpoint\n        generator_states = {\n            \"model\": ModelWrapper(generator_transformer),\n        }\n        if generator_optimizer is not None:\n            generator_states[\"optimizer\"] = OptimizerWrapper(\n                generator_transformer, generator_optimizer)\n        if dataloader is not None:\n            generator_states[\"dataloader\"] = dataloader\n        if generator_scheduler is not None:\n            generator_states[\"scheduler\"] = SchedulerWrapper(\n                generator_scheduler)\n        if generator_ema is not None:\n            generator_states[\"ema\"] = generator_ema.state_dict()\n\n        generator_dcp_dir = os.path.join(save_dir, \"distributed_checkpoint\",\n                                         \"generator\")\n        logger.info(\"rank: %s, saving generator distributed checkpoint to %s\",\n                    rank,\n                    generator_dcp_dir,\n                    local_main_process_only=False)\n\n        begin_time = time.perf_counter()\n        dcp.save(generator_states, checkpoint_id=generator_dcp_dir)\n        end_time = time.perf_counter()\n\n        logger.info(\n            \"rank: %s, generator distributed checkpoint saved in %.2f seconds\",\n            rank,\n            end_time - begin_time,\n            local_main_process_only=False)\n\n        # Save generator_2 distributed checkpoint (MoE support)\n        if generator_transformer_2 is not None:\n            generator_2_states = {\n                \"model\": ModelWrapper(generator_transformer_2),\n            }\n            if generator_optimizer_2 is not None:\n                generator_2_states[\"optimizer\"] = OptimizerWrapper(\n                    generator_transformer_2, generator_optimizer_2)\n            if dataloader is not None:\n                generator_2_states[\"dataloader\"] = dataloader\n            if generator_scheduler_2 is not None:\n                generator_2_states[\"scheduler\"] = SchedulerWrapper(\n                    generator_scheduler_2)\n            if generator_ema_2 is not None:\n                generator_2_states[\"ema\"] = generator_ema_2.state_dict()\n\n            generator_2_dcp_dir = os.path.join(save_dir,\n                                               \"distributed_checkpoint\",\n                                               \"generator_2\")\n            logger.info(\n                \"rank: %s, saving generator_2 distributed checkpoint to %s\",\n                rank,\n                generator_2_dcp_dir,\n                local_main_process_only=False)\n\n            begin_time = time.perf_counter()\n            dcp.save(generator_2_states, checkpoint_id=generator_2_dcp_dir)\n            end_time = time.perf_counter()\n\n            logger.info(\n                \"rank: %s, generator_2 distributed checkpoint saved in %.2f seconds\",\n                rank,\n                end_time - begin_time,\n                local_main_process_only=False)\n\n        # Save critic distributed checkpoint\n        critic_states = {\n            \"model\": ModelWrapper(fake_score_transformer),\n        }\n        if fake_score_optimizer is not None:\n            critic_states[\"optimizer\"] = OptimizerWrapper(\n                fake_score_transformer, fake_score_optimizer)\n        if dataloader is not None:\n            critic_states[\"dataloader\"] = dataloader\n        if fake_score_scheduler is not None:\n            critic_states[\"scheduler\"] = SchedulerWrapper(fake_score_scheduler)\n\n        critic_dcp_dir = os.path.join(save_dir, \"distributed_checkpoint\",\n                                      \"critic\")\n        logger.info(\"rank: %s, saving critic distributed checkpoint to %s\",\n                    rank,\n                    critic_dcp_dir,\n                    local_main_process_only=False)\n\n        begin_time = time.perf_counter()\n        dcp.save(critic_states, checkpoint_id=critic_dcp_dir)\n        end_time = time.perf_counter()\n\n        logger.info(\n            \"rank: %s, critic distributed checkpoint saved in %.2f seconds\",\n            rank,\n            end_time - begin_time,\n            local_main_process_only=False)\n\n        # Save critic_2 distributed checkpoint (MoE support)\n        if fake_score_transformer_2 is not None:\n            critic_2_states = {\n                \"model\": ModelWrapper(fake_score_transformer_2),\n            }\n            if fake_score_optimizer_2 is not None:\n                critic_2_states[\"optimizer\"] = OptimizerWrapper(\n                    fake_score_transformer_2, fake_score_optimizer_2)\n            if dataloader is not None:\n                critic_2_states[\"dataloader\"] = dataloader\n            if fake_score_scheduler_2 is not None:\n                critic_2_states[\"scheduler\"] = SchedulerWrapper(\n                    fake_score_scheduler_2)\n\n            critic_2_dcp_dir = os.path.join(save_dir, \"distributed_checkpoint\",\n                                            \"critic_2\")\n            logger.info(\n                \"rank: %s, saving critic_2 distributed checkpoint to %s\",\n                rank,\n                critic_2_dcp_dir,\n                local_main_process_only=False)\n\n            begin_time = time.perf_counter()\n            dcp.save(critic_2_states, checkpoint_id=critic_2_dcp_dir)\n            end_time = time.perf_counter()\n\n            logger.info(\n                \"rank: %s, critic_2 distributed checkpoint saved in %.2f seconds\",\n                rank,\n                end_time - begin_time,\n                local_main_process_only=False)\n\n        # Save real_score_transformer_2 distributed checkpoint (MoE support)\n        if real_score_transformer_2 is not None:\n            real_score_2_states = {\n                \"model\": ModelWrapper(real_score_transformer_2),\n            }\n            # Note: real_score_transformer_2 typically doesn't have optimizer/scheduler\n            # since it's used for inference only, but we include dataloader for consistency\n            if dataloader is not None:\n                real_score_2_states[\"dataloader\"] = dataloader\n\n            real_score_2_dcp_dir = os.path.join(save_dir,\n                                                \"distributed_checkpoint\",\n                                                \"real_score_2\")\n            logger.info(\n                \"rank: %s, saving real_score_2 distributed checkpoint to %s\",\n                rank,\n                real_score_2_dcp_dir,\n                local_main_process_only=False)\n\n            begin_time = time.perf_counter()\n            dcp.save(real_score_2_states, checkpoint_id=real_score_2_dcp_dir)\n            end_time = time.perf_counter()\n\n            logger.info(\n                \"rank: %s, real_score_2 distributed checkpoint saved in %.2f seconds\",\n                rank,\n                end_time - begin_time,\n                local_main_process_only=False)\n\n        # Save shared random state separately\n        shared_states = {\n            \"random_state\": RandomStateWrapper(noise_generator),\n        }\n        shared_dcp_dir = os.path.join(save_dir, \"distributed_checkpoint\",\n                                      \"shared\")\n\n        dcp.save(shared_states, checkpoint_id=shared_dcp_dir)\n\n    else:\n        logger.info(\n            \"rank: %s, skipping distributed checkpoint save (only_save_generator_weight=True)\",\n            rank,\n            local_main_process_only=False)\n\n    # Save generator model weights (consolidated) for inference\n    cpu_state = gather_state_dict_on_cpu_rank0(generator_transformer,\n                                               device=None)\n\n    if rank == 0:\n        # Save generator model weights (consolidated) for inference\n        os.makedirs(inference_save_dir, exist_ok=True)\n        weight_path = os.path.join(inference_save_dir,\n                                   \"diffusion_pytorch_model.safetensors\")\n        logger.info(\n            \"rank: %s, saving consolidated generator inference checkpoint to %s\",\n            rank,\n            weight_path,\n            local_main_process_only=False)\n\n        # Convert training format to diffusers format and save\n        diffusers_state_dict = custom_to_hf_state_dict(\n            cpu_state, generator_transformer.reverse_param_names_mapping)\n        save_file(diffusers_state_dict, weight_path)\n\n        logger.info(\n            \"rank: %s, consolidated generator inference checkpoint saved to %s\",\n            rank,\n            weight_path,\n            local_main_process_only=False)\n\n        # Save model config\n        config_dict = generator_transformer.hf_config\n        if \"dtype\" in config_dict:\n            del config_dict[\"dtype\"]  # TODO\n        config_path = os.path.join(inference_save_dir, \"config.json\")\n        # save dict as json\n        with open(config_path, \"w\") as f:\n            json.dump(config_dict, f, indent=4)\n        logger.info(\"--&gt; distillation checkpoint saved at step %s to %s\", step,\n                    weight_path)\n\n        # Save generator_2 model weights (consolidated) for inference (MoE support)\n        if generator_transformer_2 is not None:\n            inference_save_dir_2 = os.path.join(\n                save_dir, \"generator_2_inference_transformer\")\n            cpu_state_2 = gather_state_dict_on_cpu_rank0(\n                generator_transformer_2, device=None)\n\n            if rank == 0:\n                os.makedirs(inference_save_dir_2, exist_ok=True)\n                weight_path_2 = os.path.join(\n                    inference_save_dir_2, \"diffusion_pytorch_model.safetensors\")\n                logger.info(\n                    \"rank: %s, saving consolidated generator_2 inference checkpoint to %s\",\n                    rank,\n                    weight_path_2,\n                    local_main_process_only=False)\n\n                # Convert training format to diffusers format and save\n                diffusers_state_dict_2 = custom_to_hf_state_dict(\n                    cpu_state_2,\n                    generator_transformer_2.reverse_param_names_mapping)\n                save_file(diffusers_state_dict_2, weight_path_2)\n\n                logger.info(\n                    \"rank: %s, consolidated generator_2 inference checkpoint saved to %s\",\n                    rank,\n                    weight_path_2,\n                    local_main_process_only=False)\n\n                # Save model config\n                config_dict_2 = generator_transformer_2.hf_config\n                if \"dtype\" in config_dict_2:\n                    del config_dict_2[\"dtype\"]  # TODO\n                config_path_2 = os.path.join(inference_save_dir_2,\n                                             \"config.json\")\n                with open(config_path_2, \"w\") as f:\n                    json.dump(config_dict_2, f, indent=4)\n                logger.info(\n                    \"--&gt; generator_2 distillation checkpoint saved at step %s to %s\",\n                    step, weight_path_2)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.training.wan_distillation_pipeline","title":"fastvideo.training.wan_distillation_pipeline","text":""},{"location":"api/fastvideo/#fastvideo.training.wan_distillation_pipeline-classes","title":"Classes","text":"fastvideo.training.wan_distillation_pipeline.WanDistillationPipeline \u00b6 <pre><code>WanDistillationPipeline(\n    model_path: str,\n    fastvideo_args: TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>DistillationPipeline</code></p> <p>A distillation pipeline for Wan that uses a single transformer model. The main transformer serves as the student model, and copies are made for teacher and critic.</p> Source code in <code>fastvideo/training/training_pipeline.py</code> <pre><code>def __init__(\n        self,\n        model_path: str,\n        fastvideo_args: TrainingArgs,\n        required_config_modules: list[str] | None = None,\n        loaded_modules: dict[str, torch.nn.Module] | None = None) -&gt; None:\n    fastvideo_args.inference_mode = False\n    self.lora_training = fastvideo_args.lora_training\n    if self.lora_training and fastvideo_args.lora_rank is None:\n        raise ValueError(\"lora rank must be set when using lora training\")\n\n    set_random_seed(fastvideo_args.seed)  # for lora param init\n    super().__init__(model_path, fastvideo_args, required_config_modules,\n                     loaded_modules)  # type: ignore\n    self.tracker = DummyTracker()\n</code></pre> Functions\u00b6 fastvideo.training.wan_distillation_pipeline.WanDistillationPipeline.create_training_stages \u00b6 <pre><code>create_training_stages(training_args: TrainingArgs)\n</code></pre> <p>May be used in future refactors.</p> Source code in <code>fastvideo/training/wan_distillation_pipeline.py</code> <pre><code>def create_training_stages(self, training_args: TrainingArgs):\n    \"\"\"\n    May be used in future refactors.\n    \"\"\"\n    pass\n</code></pre> fastvideo.training.wan_distillation_pipeline.WanDistillationPipeline.initialize_pipeline \u00b6 <pre><code>initialize_pipeline(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Initialize Wan-specific scheduler.</p> Source code in <code>fastvideo/training/wan_distillation_pipeline.py</code> <pre><code>def initialize_pipeline(self, fastvideo_args: FastVideoArgs):\n    \"\"\"Initialize Wan-specific scheduler.\"\"\"\n    self.modules[\"scheduler\"] = FlowMatchEulerDiscreteScheduler(\n        shift=fastvideo_args.pipeline_config.flow_shift)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.training.wan_distillation_pipeline-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideo.training.wan_i2v_distillation_pipeline","title":"fastvideo.training.wan_i2v_distillation_pipeline","text":""},{"location":"api/fastvideo/#fastvideo.training.wan_i2v_distillation_pipeline-classes","title":"Classes","text":"fastvideo.training.wan_i2v_distillation_pipeline.WanI2VDistillationPipeline \u00b6 <pre><code>WanI2VDistillationPipeline(\n    model_path: str,\n    fastvideo_args: TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>DistillationPipeline</code></p> <p>A distillation pipeline for Wan that uses a single transformer model. The main transformer serves as the student model, and copies are made for teacher and critic.</p> Source code in <code>fastvideo/training/training_pipeline.py</code> <pre><code>def __init__(\n        self,\n        model_path: str,\n        fastvideo_args: TrainingArgs,\n        required_config_modules: list[str] | None = None,\n        loaded_modules: dict[str, torch.nn.Module] | None = None) -&gt; None:\n    fastvideo_args.inference_mode = False\n    self.lora_training = fastvideo_args.lora_training\n    if self.lora_training and fastvideo_args.lora_rank is None:\n        raise ValueError(\"lora rank must be set when using lora training\")\n\n    set_random_seed(fastvideo_args.seed)  # for lora param init\n    super().__init__(model_path, fastvideo_args, required_config_modules,\n                     loaded_modules)  # type: ignore\n    self.tracker = DummyTracker()\n</code></pre> Functions\u00b6 fastvideo.training.wan_i2v_distillation_pipeline.WanI2VDistillationPipeline.create_training_stages \u00b6 <pre><code>create_training_stages(training_args: TrainingArgs)\n</code></pre> <p>May be used in future refactors.</p> Source code in <code>fastvideo/training/wan_i2v_distillation_pipeline.py</code> <pre><code>def create_training_stages(self, training_args: TrainingArgs):\n    \"\"\"\n    May be used in future refactors.\n    \"\"\"\n    pass\n</code></pre> fastvideo.training.wan_i2v_distillation_pipeline.WanI2VDistillationPipeline.initialize_pipeline \u00b6 <pre><code>initialize_pipeline(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Initialize Wan-specific scheduler.</p> Source code in <code>fastvideo/training/wan_i2v_distillation_pipeline.py</code> <pre><code>def initialize_pipeline(self, fastvideo_args: FastVideoArgs):\n    \"\"\"Initialize Wan-specific scheduler.\"\"\"\n    self.modules[\"scheduler\"] = FlowMatchEulerDiscreteScheduler(\n        shift=fastvideo_args.pipeline_config.flow_shift)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.training.wan_i2v_distillation_pipeline-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideo.training.wan_i2v_training_pipeline","title":"fastvideo.training.wan_i2v_training_pipeline","text":""},{"location":"api/fastvideo/#fastvideo.training.wan_i2v_training_pipeline-classes","title":"Classes","text":"fastvideo.training.wan_i2v_training_pipeline.WanI2VTrainingPipeline \u00b6 <pre><code>WanI2VTrainingPipeline(\n    model_path: str,\n    fastvideo_args: TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>TrainingPipeline</code></p> <p>A training pipeline for Wan.</p> Source code in <code>fastvideo/training/training_pipeline.py</code> <pre><code>def __init__(\n        self,\n        model_path: str,\n        fastvideo_args: TrainingArgs,\n        required_config_modules: list[str] | None = None,\n        loaded_modules: dict[str, torch.nn.Module] | None = None) -&gt; None:\n    fastvideo_args.inference_mode = False\n    self.lora_training = fastvideo_args.lora_training\n    if self.lora_training and fastvideo_args.lora_rank is None:\n        raise ValueError(\"lora rank must be set when using lora training\")\n\n    set_random_seed(fastvideo_args.seed)  # for lora param init\n    super().__init__(model_path, fastvideo_args, required_config_modules,\n                     loaded_modules)  # type: ignore\n    self.tracker = DummyTracker()\n</code></pre> Functions\u00b6 fastvideo.training.wan_i2v_training_pipeline.WanI2VTrainingPipeline.create_training_stages \u00b6 <pre><code>create_training_stages(training_args: TrainingArgs)\n</code></pre> <p>May be used in future refactors.</p> Source code in <code>fastvideo/training/wan_i2v_training_pipeline.py</code> <pre><code>def create_training_stages(self, training_args: TrainingArgs):\n    \"\"\"\n    May be used in future refactors.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.training.wan_i2v_training_pipeline-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideo.training.wan_self_forcing_distillation_pipeline","title":"fastvideo.training.wan_self_forcing_distillation_pipeline","text":""},{"location":"api/fastvideo/#fastvideo.training.wan_self_forcing_distillation_pipeline-classes","title":"Classes","text":"fastvideo.training.wan_self_forcing_distillation_pipeline.WanSelfForcingDistillationPipeline \u00b6 <pre><code>WanSelfForcingDistillationPipeline(\n    model_path: str,\n    fastvideo_args: TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>SelfForcingDistillationPipeline</code></p> <p>A self-forcing distillation pipeline for Wan that uses the self-forcing methodology with DMD for video generation.</p> Source code in <code>fastvideo/training/training_pipeline.py</code> <pre><code>def __init__(\n        self,\n        model_path: str,\n        fastvideo_args: TrainingArgs,\n        required_config_modules: list[str] | None = None,\n        loaded_modules: dict[str, torch.nn.Module] | None = None) -&gt; None:\n    fastvideo_args.inference_mode = False\n    self.lora_training = fastvideo_args.lora_training\n    if self.lora_training and fastvideo_args.lora_rank is None:\n        raise ValueError(\"lora rank must be set when using lora training\")\n\n    set_random_seed(fastvideo_args.seed)  # for lora param init\n    super().__init__(model_path, fastvideo_args, required_config_modules,\n                     loaded_modules)  # type: ignore\n    self.tracker = DummyTracker()\n</code></pre> Functions\u00b6 fastvideo.training.wan_self_forcing_distillation_pipeline.WanSelfForcingDistillationPipeline.create_training_stages \u00b6 <pre><code>create_training_stages(training_args: TrainingArgs)\n</code></pre> <p>May be used in future refactors.</p> Source code in <code>fastvideo/training/wan_self_forcing_distillation_pipeline.py</code> <pre><code>def create_training_stages(self, training_args: TrainingArgs):\n    \"\"\"\n    May be used in future refactors.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.training.wan_self_forcing_distillation_pipeline-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideo.training.wan_training_pipeline","title":"fastvideo.training.wan_training_pipeline","text":""},{"location":"api/fastvideo/#fastvideo.training.wan_training_pipeline-classes","title":"Classes","text":"fastvideo.training.wan_training_pipeline.WanTrainingPipeline \u00b6 <pre><code>WanTrainingPipeline(\n    model_path: str,\n    fastvideo_args: TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>TrainingPipeline</code></p> <p>A training pipeline for Wan.</p> Source code in <code>fastvideo/training/training_pipeline.py</code> <pre><code>def __init__(\n        self,\n        model_path: str,\n        fastvideo_args: TrainingArgs,\n        required_config_modules: list[str] | None = None,\n        loaded_modules: dict[str, torch.nn.Module] | None = None) -&gt; None:\n    fastvideo_args.inference_mode = False\n    self.lora_training = fastvideo_args.lora_training\n    if self.lora_training and fastvideo_args.lora_rank is None:\n        raise ValueError(\"lora rank must be set when using lora training\")\n\n    set_random_seed(fastvideo_args.seed)  # for lora param init\n    super().__init__(model_path, fastvideo_args, required_config_modules,\n                     loaded_modules)  # type: ignore\n    self.tracker = DummyTracker()\n</code></pre> Functions\u00b6 fastvideo.training.wan_training_pipeline.WanTrainingPipeline.create_training_stages \u00b6 <pre><code>create_training_stages(training_args: TrainingArgs)\n</code></pre> <p>May be used in future refactors.</p> Source code in <code>fastvideo/training/wan_training_pipeline.py</code> <pre><code>def create_training_stages(self, training_args: TrainingArgs):\n    \"\"\"\n    May be used in future refactors.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.training.wan_training_pipeline-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideoworkflow","title":"fastvideo.workflow","text":""},{"location":"api/fastvideo/#fastvideo.workflow","title":"workflow","text":""},{"location":"api/fastvideo/#fastvideo.workflow-modules","title":"Modules","text":""},{"location":"api/fastvideo/#fastvideo.workflow.preprocess","title":"fastvideo.workflow.preprocess","text":""},{"location":"api/fastvideo/#fastvideo.workflow.preprocess-modules","title":"Modules","text":"fastvideo.workflow.preprocess.components \u00b6 Classes\u00b6 fastvideo.workflow.preprocess.components.ParquetDatasetSaver \u00b6 <pre><code>ParquetDatasetSaver(\n    flush_frequency: int,\n    samples_per_file: int,\n    schema: Schema,\n    record_creator: Callable[..., list[dict[str, Any]]],\n)\n</code></pre> <p>Component for saving and writing Parquet datasets using shared parquet_io.</p> Source code in <code>fastvideo/workflow/preprocess/components.py</code> <pre><code>def __init__(self, flush_frequency: int, samples_per_file: int,\n             schema: pa.Schema,\n             record_creator: Callable[..., list[dict[str, Any]]]):\n    self.flush_frequency = flush_frequency\n    self.samples_per_file = samples_per_file\n    self.schema = schema\n    self.create_records_from_batch = record_creator\n    self.num_processed_samples: int = 0\n    self._writer: ParquetDatasetWriter | None = None\n</code></pre> Functions\u00b6 fastvideo.workflow.preprocess.components.ParquetDatasetSaver.clean_up \u00b6 <pre><code>clean_up() -&gt; None\n</code></pre> <p>Clean up all tables</p> Source code in <code>fastvideo/workflow/preprocess/components.py</code> <pre><code>def clean_up(self) -&gt; None:\n    \"\"\"Clean up all tables\"\"\"\n    self.flush_tables(write_remainder=True)\n    self._writer = None\n    self.num_processed_samples = 0\n    gc.collect()\n</code></pre> fastvideo.workflow.preprocess.components.ParquetDatasetSaver.flush_tables \u00b6 <pre><code>flush_tables(write_remainder: bool = False)\n</code></pre> <p>Flush buffered records to disk.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <p>Directory where parquet files are written. Kept for API symmetry (writer already configured with this path).</p> required <code>write_remainder</code> <code>bool</code> <p>If True, also write any leftover rows smaller than <code>samples_per_file</code> as a final small file. Useful for the last flush.</p> <code>False</code> Source code in <code>fastvideo/workflow/preprocess/components.py</code> <pre><code>def flush_tables(self, write_remainder: bool = False):\n    \"\"\"Flush buffered records to disk.\n\n    Args:\n        output_dir: Directory where parquet files are written. Kept for API\n            symmetry (writer already configured with this path).\n        write_remainder: If True, also write any leftover rows smaller than\n            ``samples_per_file`` as a final small file. Useful for the last flush.\n    \"\"\"\n    if self._writer is None:\n        return\n    _ = self._writer.flush(write_remainder=write_remainder)\n    # Reset processed sample count modulo samples_per_file\n    remainder = self.num_processed_samples % self.samples_per_file\n    self.num_processed_samples = 0 if write_remainder else remainder\n</code></pre> fastvideo.workflow.preprocess.components.ParquetDatasetSaver.save_and_write_parquet_batch \u00b6 <pre><code>save_and_write_parquet_batch(\n    batch: PreprocessBatch,\n    output_dir: str,\n    extra_features: dict[str, Any] | None = None,\n) -&gt; None\n</code></pre> <p>Save and write Parquet dataset batch</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>PreprocessBatch</code> <p>PreprocessBatch containing video and metadata information</p> required <code>output_dir</code> <code>str</code> <p>Output directory</p> required <code>extra_features</code> <code>dict[str, Any] | None</code> <p>Extra features</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>Number of processed samples</p> Source code in <code>fastvideo/workflow/preprocess/components.py</code> <pre><code>def save_and_write_parquet_batch(\n        self,\n        batch: PreprocessBatch,\n        output_dir: str,\n        extra_features: dict[str, Any] | None = None) -&gt; None:\n    \"\"\"\n    Save and write Parquet dataset batch\n\n    Args:\n        batch: PreprocessBatch containing video and metadata information\n        output_dir: Output directory\n        extra_features: Extra features\n\n    Returns:\n        Number of processed samples\n    \"\"\"\n    assert isinstance(batch.latents, torch.Tensor)\n    assert isinstance(batch.prompt_embeds, list)\n    assert isinstance(batch.prompt_attention_mask, list)\n\n    # Process non-padded embeddings (if needed)\n    if batch.prompt_attention_mask is not None:\n        batch.prompt_embeds = self._process_non_padded_embeddings(\n            batch.prompt_embeds[0], batch.prompt_attention_mask[0])\n    else:\n        raise ValueError(\"prompt_attention_mask is None\")\n\n    # Prepare batch data for Parquet dataset\n    batch_data: list[dict[str, Any]] = []\n\n    for key in dataclasses.fields(batch):\n        value = getattr(batch, key.name)\n        if isinstance(value, list):\n            for idx in range(len(value)):\n                if isinstance(value[idx], torch.Tensor):\n                    value[idx] = value[idx].cpu().numpy()\n        elif isinstance(value, torch.Tensor):\n            value = value.cpu().numpy()\n            setattr(batch, key.name, value)\n\n    # Create record for Parquet dataset\n    records = self.create_records_from_batch(batch)\n    batch_data.extend(records)\n\n    if batch_data:\n        self.num_processed_samples += len(batch_data)\n        table = records_to_table(batch_data, self.schema)\n        if self._writer is None:\n            os.makedirs(output_dir, exist_ok=True)\n            self._writer = ParquetDatasetWriter(\n                out_dir=output_dir, samples_per_file=self.samples_per_file)\n        self._writer.append_table(table)\n        logger.debug(\"Collected batch with %s samples\", len(table))\n\n    # If flush is needed\n    if self.num_processed_samples &gt;= self.flush_frequency:\n        self.flush_tables()\n</code></pre> fastvideo.workflow.preprocess.components.PreprocessingDataValidator \u00b6 <pre><code>PreprocessingDataValidator(\n    max_height: int = 1024,\n    max_width: int = 1024,\n    max_h_div_w_ratio: float = 17 / 16,\n    min_h_div_w_ratio: float = 8 / 16,\n    num_frames: int = 16,\n    train_fps: int = 24,\n    speed_factor: float = 1.0,\n    video_length_tolerance_range: float = 5.0,\n    drop_short_ratio: float = 0.0,\n    hw_aspect_threshold: float = 1.5,\n)\n</code></pre> Source code in <code>fastvideo/workflow/preprocess/components.py</code> <pre><code>def __init__(self,\n             max_height: int = 1024,\n             max_width: int = 1024,\n             max_h_div_w_ratio: float = 17 / 16,\n             min_h_div_w_ratio: float = 8 / 16,\n             num_frames: int = 16,\n             train_fps: int = 24,\n             speed_factor: float = 1.0,\n             video_length_tolerance_range: float = 5.0,\n             drop_short_ratio: float = 0.0,\n             hw_aspect_threshold: float = 1.5):\n    self.max_height = max_height\n    self.max_width = max_width\n    self.max_h_div_w_ratio = max_h_div_w_ratio\n    self.min_h_div_w_ratio = min_h_div_w_ratio\n    self.num_frames = num_frames\n    self.train_fps = train_fps\n    self.speed_factor = speed_factor\n    self.video_length_tolerance_range = video_length_tolerance_range\n    self.drop_short_ratio = drop_short_ratio\n    self.hw_aspect_threshold = hw_aspect_threshold\n    self.validators: dict[str, Callable[[dict[str, Any]], bool]] = {}\n    self.filter_counts: dict[str, int] = {}\n\n    self.num_items_before_filtering = 0\n    self.num_items_after_filtering = 0\n\n    self.register_validators()\n</code></pre> Functions\u00b6 fastvideo.workflow.preprocess.components.PreprocessingDataValidator.__call__ \u00b6 <pre><code>__call__(batch: dict[str, Any]) -&gt; bool\n</code></pre> <p>Validate whether the preprocessing data batch is valid.</p> Source code in <code>fastvideo/workflow/preprocess/components.py</code> <pre><code>def __call__(self, batch: dict[str, Any]) -&gt; bool:\n    \"\"\"\n    Validate whether the preprocessing data batch is valid.\n    \"\"\"\n    self.num_items_before_filtering += 1\n\n    for name, validator in self.validators.items():\n        if not validator(batch):\n            self.filter_counts[name] += 1\n            return False\n\n    self.num_items_after_filtering += 1\n    return True\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/#fastvideo.workflow.workflow_base","title":"fastvideo.workflow.workflow_base","text":""},{"location":"api/fastvideo/#fastvideo.workflow.workflow_base-classes","title":"Classes","text":"fastvideo.workflow.workflow_base.WorkflowBase \u00b6 <pre><code>WorkflowBase(fastvideo_args: FastVideoArgs)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for defining video processing workflows.</p> <p>A workflow serves as the top-level orchestrator that coordinates multiple pipelines and components to accomplish a specific video processing task. The workflow pattern provides several key benefits:</p> <ol> <li> <p>Separation of Concerns: Workflows separate high-level orchestration logic    from low-level processing implementations in pipelines.</p> </li> <li> <p>Modularity: Different workflows can be created for different execution modes    (preprocess, inference, etc.) while sharing common pipeline components.</p> </li> <li> <p>Configuration Management: Workflows manage the configuration and initialization    of multiple related pipelines and components in a centralized manner.</p> </li> <li> <p>Environment Setup: Workflows handle system-level setup and resource    allocation before pipeline execution begins.</p> </li> <li> <p>Lifecycle Management: Workflows control the complete lifecycle from    initialization through execution to cleanup.</p> </li> </ol> <p>The workflow acts as a factory and coordinator, creating the appropriate pipelines based on configuration, setting up the execution environment, and orchestrating the overall processing flow.</p> <p>Initialize the workflow with configuration arguments.</p> <p>Parameters:</p> Name Type Description Default <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>Configuration object containing all parameters           needed for workflow and pipeline setup.</p> required Source code in <code>fastvideo/workflow/workflow_base.py</code> <pre><code>def __init__(self, fastvideo_args: FastVideoArgs):\n    \"\"\"\n    Initialize the workflow with configuration arguments.\n\n    Args:\n        fastvideo_args: Configuration object containing all parameters\n                      needed for workflow and pipeline setup.\n    \"\"\"\n    self.fastvideo_args = fastvideo_args\n\n    # TODO: pipeline_config should be: dict[str, PipelineConfig]\n    # pipeline_type should be included in the PipelineConfig\n    # pipeline_config[pipeline_name] = (pipeline_type, fastvideo_args)\n    self._pipeline_configs: dict[str, tuple[PipelineType,\n                                            FastVideoArgs]] = {}\n    self._pipelines: dict[str, ComposedPipelineBase] = {}\n    self._components: dict[str, Any] = {}\n    self.register_pipelines()\n    self.register_components()\n\n    self.prepare_system_environment()\n    self.load_pipelines()\n</code></pre> Functions\u00b6 fastvideo.workflow.workflow_base.WorkflowBase.add_component \u00b6 <pre><code>add_component(component_name: str, component: Any) -&gt; None\n</code></pre> <p>Register a component instance with the workflow.</p> <p>Components are auxiliary objects that may be shared across pipelines or used for workflow-level functionality (e.g., databases, caches, external services).</p> <p>Parameters:</p> Name Type Description Default <code>component_name</code> <code>str</code> <p>Unique identifier for the component.</p> required <code>component</code> <code>Any</code> <p>The component instance to register.</p> required Source code in <code>fastvideo/workflow/workflow_base.py</code> <pre><code>def add_component(self, component_name: str, component: Any) -&gt; None:\n    \"\"\"\n    Register a component instance with the workflow.\n\n    Components are auxiliary objects that may be shared across pipelines\n    or used for workflow-level functionality (e.g., databases, caches,\n    external services).\n\n    Args:\n        component_name: Unique identifier for the component.\n        component: The component instance to register.\n    \"\"\"\n    self._components[component_name] = component\n    setattr(self, component_name, component)\n</code></pre> fastvideo.workflow.workflow_base.WorkflowBase.add_pipeline_config \u00b6 <pre><code>add_pipeline_config(\n    pipeline_name: str,\n    pipeline_config: tuple[PipelineType, FastVideoArgs],\n) -&gt; None\n</code></pre> <p>Register a pipeline configuration for later instantiation.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>Unique identifier for the pipeline.</p> required <code>pipeline_config</code> <code>tuple[PipelineType, FastVideoArgs]</code> <p>Tuple containing the pipeline type and            configuration arguments.</p> required Source code in <code>fastvideo/workflow/workflow_base.py</code> <pre><code>def add_pipeline_config(\n        self, pipeline_name: str,\n        pipeline_config: tuple[PipelineType, FastVideoArgs]) -&gt; None:\n    \"\"\"\n    Register a pipeline configuration for later instantiation.\n\n    Args:\n        pipeline_name: Unique identifier for the pipeline.\n        pipeline_config: Tuple containing the pipeline type and\n                       configuration arguments.\n    \"\"\"\n    self._pipeline_configs[pipeline_name] = pipeline_config\n</code></pre> fastvideo.workflow.workflow_base.WorkflowBase.get_component \u00b6 <pre><code>get_component(component_name: str) -&gt; Any\n</code></pre> <p>Retrieve a registered component by name.</p> <p>Parameters:</p> Name Type Description Default <code>component_name</code> <code>str</code> <p>The name of the component to retrieve.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The component instance.</p> Source code in <code>fastvideo/workflow/workflow_base.py</code> <pre><code>def get_component(self, component_name: str) -&gt; Any:\n    \"\"\"\n    Retrieve a registered component by name.\n\n    Args:\n        component_name: The name of the component to retrieve.\n\n    Returns:\n        The component instance.\n    \"\"\"\n    return self._components[component_name]\n</code></pre> fastvideo.workflow.workflow_base.WorkflowBase.get_workflow_cls <code>classmethod</code> \u00b6 <pre><code>get_workflow_cls(\n    fastvideo_args: FastVideoArgs,\n) -&gt; Optional[WorkflowBase]\n</code></pre> <p>Factory method to get the appropriate workflow class based on execution mode.</p> <p>This method acts as a workflow factory, returning the appropriate workflow class implementation based on the specified execution mode in the configuration arguments.</p> <p>Parameters:</p> Name Type Description Default <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>Configuration object containing the execution mode           and other parameters.</p> required <p>Returns:</p> Type Description <code>Optional[WorkflowBase]</code> <p>The appropriate workflow class for the specified execution mode,</p> <code>Optional[WorkflowBase]</code> <p>or None if no workflow is available for the given mode.</p> Source code in <code>fastvideo/workflow/workflow_base.py</code> <pre><code>@classmethod\ndef get_workflow_cls(\n        cls, fastvideo_args: FastVideoArgs) -&gt; Optional[\"WorkflowBase\"]:\n    \"\"\"\n    Factory method to get the appropriate workflow class based on execution mode.\n\n    This method acts as a workflow factory, returning the appropriate\n    workflow class implementation based on the specified execution mode\n    in the configuration arguments.\n\n    Args:\n        fastvideo_args: Configuration object containing the execution mode\n                      and other parameters.\n\n    Returns:\n        The appropriate workflow class for the specified execution mode,\n        or None if no workflow is available for the given mode.\n    \"\"\"\n    if fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        from fastvideo.workflow.preprocess.preprocess_workflow import (\n            PreprocessWorkflow)\n        return PreprocessWorkflow.get_workflow_cls(fastvideo_args)\n    else:\n        raise ValueError(\n            f\"Execution mode: {fastvideo_args.mode} is not supported in workflow.\"\n        )\n</code></pre> fastvideo.workflow.workflow_base.WorkflowBase.load_pipelines \u00b6 <pre><code>load_pipelines() -&gt; None\n</code></pre> <p>Create and initialize all registered pipelines.</p> <p>This method instantiates pipeline objects from their configurations and makes them available as both dictionary entries and instance attributes for convenient access.</p> Source code in <code>fastvideo/workflow/workflow_base.py</code> <pre><code>def load_pipelines(self) -&gt; None:\n    \"\"\"\n    Create and initialize all registered pipelines.\n\n    This method instantiates pipeline objects from their configurations\n    and makes them available as both dictionary entries and instance\n    attributes for convenient access.\n    \"\"\"\n    for pipeline_name, pipeline_config in self._pipeline_configs.items():\n        pipeline_type, fastvideo_args = pipeline_config\n        pipeline = build_pipeline(fastvideo_args, pipeline_type)\n        self._pipelines[pipeline_name] = pipeline\n        setattr(self, pipeline_name, pipeline)\n</code></pre> fastvideo.workflow.workflow_base.WorkflowBase.prepare_system_environment <code>abstractmethod</code> \u00b6 <pre><code>prepare_system_environment() -&gt; None\n</code></pre> <p>Prepare the system environment for workflow execution.</p> <p>Subclasses must implement this method to handle any system-level setup required before pipeline execution (e.g., GPU initialization, temporary directories, resource allocation).</p> Source code in <code>fastvideo/workflow/workflow_base.py</code> <pre><code>@abstractmethod\ndef prepare_system_environment(self) -&gt; None:\n    \"\"\"\n    Prepare the system environment for workflow execution.\n\n    Subclasses must implement this method to handle any system-level\n    setup required before pipeline execution (e.g., GPU initialization,\n    temporary directories, resource allocation).\n    \"\"\"\n    pass\n</code></pre> fastvideo.workflow.workflow_base.WorkflowBase.register_components <code>abstractmethod</code> \u00b6 <pre><code>register_components() -&gt; None\n</code></pre> <p>Register workflow-specific components.</p> <p>Subclasses must implement this method to register any components needed for their specific workflow (e.g., databases, external APIs, shared resources).</p> Source code in <code>fastvideo/workflow/workflow_base.py</code> <pre><code>@abstractmethod\ndef register_components(self) -&gt; None:\n    \"\"\"\n    Register workflow-specific components.\n\n    Subclasses must implement this method to register any components\n    needed for their specific workflow (e.g., databases, external APIs,\n    shared resources).\n    \"\"\"\n    pass\n</code></pre> fastvideo.workflow.workflow_base.WorkflowBase.register_pipelines <code>abstractmethod</code> \u00b6 <pre><code>register_pipelines() -&gt; None\n</code></pre> <p>Register workflow-specific pipelines.</p> <p>Subclasses must implement this method to define which pipelines are needed for their specific workflow and how they should be configured.</p> Source code in <code>fastvideo/workflow/workflow_base.py</code> <pre><code>@abstractmethod\ndef register_pipelines(self) -&gt; None:\n    \"\"\"\n    Register workflow-specific pipelines.\n\n    Subclasses must implement this method to define which pipelines\n    are needed for their specific workflow and how they should be\n    configured.\n    \"\"\"\n    pass\n</code></pre> fastvideo.workflow.workflow_base.WorkflowBase.run <code>abstractmethod</code> \u00b6 <pre><code>run()\n</code></pre> <p>Execute the main workflow logic.</p> <p>Subclasses must implement this method to define the specific execution flow for their workflow, coordinating the registered pipelines and components to accomplish the desired task.</p> Source code in <code>fastvideo/workflow/workflow_base.py</code> <pre><code>@abstractmethod\ndef run(self):\n    \"\"\"\n    Execute the main workflow logic.\n\n    Subclasses must implement this method to define the specific\n    execution flow for their workflow, coordinating the registered\n    pipelines and components to accomplish the desired task.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.workflow.workflow_base-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideodataset","title":"fastvideo.dataset","text":""},{"location":"api/fastvideo/#fastvideo.dataset","title":"dataset","text":""},{"location":"api/fastvideo/#fastvideo.dataset-classes","title":"Classes","text":""},{"location":"api/fastvideo/#fastvideo.dataset.TextDataset","title":"fastvideo.dataset.TextDataset","text":"<pre><code>TextDataset(\n    data_merge_path: str,\n    args,\n    start_idx: int = 0,\n    seed: int = 42,\n)\n</code></pre> <p>               Bases: <code>IterableDataset</code>, <code>Stateful</code></p> <p>Text-only dataset for processing prompts from a simple text file.</p> <p>Assumes that data_merge_path is a text file with one prompt per line: A cat playing with a ball A dog running in the park A person cooking dinner ...</p> <p>This dataset processes text data through text encoding stages only.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def __init__(self,\n             data_merge_path: str,\n             args,\n             start_idx: int = 0,\n             seed: int = 42):\n    self.data_merge_path = data_merge_path\n    self.start_idx = start_idx\n    self.args = args\n    self.seed = seed\n\n    # Initialize tokenizer\n    tokenizer_path = os.path.join(args.model_path, \"tokenizer\")\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path,\n                                              cache_dir=args.cache_dir)\n\n    # Initialize text encoding stage\n    self.text_encoding_stage = TextEncodingStage(\n        tokenizer=tokenizer,\n        text_max_length=args.text_max_length,\n        cfg_rate=getattr(args, 'training_cfg_rate', 0.0),\n        seed=self.seed)\n\n    # Process text data\n    self.processed_batches = self._process_text_data()\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.dataset.TextDataset-functions","title":"Functions","text":"fastvideo.dataset.TextDataset.__iter__ \u00b6 <pre><code>__iter__()\n</code></pre> <p>Iterator for the dataset.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def __iter__(self):\n    \"\"\"Iterator for the dataset.\"\"\"\n    # Set up distributed sampling if needed\n    if torch.distributed.is_available() and torch.distributed.is_initialized():\n        rank = torch.distributed.get_rank()\n        world_size = torch.distributed.get_world_size()\n    else:\n        rank = 0\n        world_size = 1\n\n    # Calculate chunk for this rank\n    total_items = len(self.processed_batches)\n    items_per_rank = math.ceil(total_items / world_size)\n    start_idx = rank * items_per_rank + self.start_idx\n    end_idx = min(start_idx + items_per_rank, total_items)\n\n    # Yield items for this rank\n    for idx in range(start_idx, end_idx):\n        if idx &lt; len(self.processed_batches):\n            yield self._get_item(idx)\n</code></pre> fastvideo.dataset.TextDataset.load_state_dict \u00b6 <pre><code>load_state_dict(state_dict: dict[str, Any]) -&gt; None\n</code></pre> <p>Load state dict from checkpoint.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def load_state_dict(self, state_dict: dict[str, Any]) -&gt; None:\n    \"\"\"Load state dict from checkpoint.\"\"\"\n    self.processed_batches = state_dict[\"processed_batches\"]\n</code></pre> fastvideo.dataset.TextDataset.state_dict \u00b6 <pre><code>state_dict() -&gt; dict[str, Any]\n</code></pre> <p>Return state dict for checkpointing.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def state_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return state dict for checkpointing.\"\"\"\n    return {\"processed_batches\": self.processed_batches}\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.dataset.ValidationDataset","title":"fastvideo.dataset.ValidationDataset","text":"<pre><code>ValidationDataset(filename: str)\n</code></pre> <p>               Bases: <code>IterableDataset</code></p> Source code in <code>fastvideo/dataset/validation_dataset.py</code> <pre><code>def __init__(self, filename: str):\n    super().__init__()\n\n    self.filename = pathlib.Path(filename)\n    # get directory of filename\n    self.dir = os.path.abspath(self.filename.parent)\n\n    if not self.filename.exists():\n        raise FileNotFoundError(\n            f\"File {self.filename.as_posix()} does not exist\")\n\n    if self.filename.suffix == \".csv\":\n        data = datasets.load_dataset(\"csv\",\n                                     data_files=self.filename.as_posix(),\n                                     split=\"train\")\n    elif self.filename.suffix == \".json\":\n        data = datasets.load_dataset(\"json\",\n                                     data_files=self.filename.as_posix(),\n                                     split=\"train\",\n                                     field=\"data\")\n    elif self.filename.suffix == \".parquet\":\n        data = datasets.load_dataset(\"parquet\",\n                                     data_files=self.filename.as_posix(),\n                                     split=\"train\")\n    elif self.filename.suffix == \".arrow\":\n        data = datasets.load_dataset(\"arrow\",\n                                     data_files=self.filename.as_posix(),\n                                     split=\"train\")\n    else:\n        _SUPPORTED_FILE_FORMATS = [\".csv\", \".json\", \".parquet\", \".arrow\"]\n        raise ValueError(\n            f\"Unsupported file format {self.filename.suffix} for validation dataset. Supported formats are: {_SUPPORTED_FILE_FORMATS}\"\n        )\n\n    # Get distributed training info\n    self.global_rank = get_world_rank()\n    self.world_size = get_world_size()\n    self.sp_world_size = get_sp_world_size()\n    self.num_sp_groups = self.world_size // self.sp_world_size\n\n    # Convert to list to get total samples\n    self.all_samples = list(data)\n    self.original_total_samples = len(self.all_samples)\n\n    # Extend samples to be a multiple of DP degree (num_sp_groups)\n    remainder = self.original_total_samples % self.num_sp_groups\n    if remainder != 0:\n        samples_to_add = self.num_sp_groups - remainder\n\n        # Duplicate samples cyclically to reach the target\n        additional_samples = []\n        for i in range(samples_to_add):\n            additional_samples.append(\n                self.all_samples[i % self.original_total_samples])\n\n        self.all_samples.extend(additional_samples)\n\n    self.total_samples = len(self.all_samples)\n\n    # Calculate which SP group this rank belongs to\n    self.sp_group_id = self.global_rank // self.sp_world_size\n\n    # Now all SP groups will have equal number of samples\n    self.samples_per_sp_group = self.total_samples // self.num_sp_groups\n\n    # Calculate start and end indices for this SP group\n    self.start_idx = self.sp_group_id * self.samples_per_sp_group\n    self.end_idx = self.start_idx + self.samples_per_sp_group\n\n    # Get samples for this SP group\n    self.sp_group_samples = self.all_samples[self.start_idx:self.end_idx]\n\n    logger.info(\n        \"Rank %s (SP group %s): \"\n        \"Original samples: %s, \"\n        \"Extended samples: %s, \"\n        \"SP group samples: %s, \"\n        \"Range: [%s:%s]\",\n        self.global_rank,\n        self.sp_group_id,\n        self.original_total_samples,\n        self.total_samples,\n        len(self.sp_group_samples),\n        self.start_idx,\n        self.end_idx,\n        local_main_process_only=False)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.dataset.ValidationDataset-functions","title":"Functions","text":"fastvideo.dataset.ValidationDataset.__len__ \u00b6 <pre><code>__len__()\n</code></pre> <p>Return the number of samples for this SP group.</p> Source code in <code>fastvideo/dataset/validation_dataset.py</code> <pre><code>def __len__(self):\n    \"\"\"Return the number of samples for this SP group.\"\"\"\n    return len(self.sp_group_samples)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.dataset.VideoCaptionMergedDataset","title":"fastvideo.dataset.VideoCaptionMergedDataset","text":"<pre><code>VideoCaptionMergedDataset(\n    data_merge_path: str,\n    args,\n    transform,\n    temporal_sample,\n    transform_topcrop,\n    start_idx: int = 0,\n    seed: int = 42,\n)\n</code></pre> <p>               Bases: <code>IterableDataset</code>, <code>Stateful</code></p> <p>Merged dataset for video and caption data with stage-based processing. Assumes that data_merge_path is a txt file with the following format: , <pre><code>The folder should contain videos.\n\nThe json file should be a list of dictionaries with the following format:\n[\n{\n    \"path\": \"1gGQy4nxyUo-Scene-016.mp4\",\n    \"resolution\": {\n    \"width\": 1920,\n    \"height\": 1080\n    },\n    \"size\": 2439112,\n    \"fps\": 25.0,\n    \"duration\": 6.88,\n    \"num_frames\": 172,\n    \"cap\": [\n    \"A watermelon wearing a helmet is crushed by a hydraulic press, causing it to flatten and burst open.\"\n    ]\n},\n...\n]\n</code></pre> <p>This dataset processes video and image data through a series of stages: - Data validation - Resolution filtering - Frame sampling - Transformation - Text encoding</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def __init__(self,\n             data_merge_path: str,\n             args,\n             transform,\n             temporal_sample,\n             transform_topcrop,\n             start_idx: int = 0,\n             seed: int = 42):\n    self.data_merge_path = data_merge_path\n    self.start_idx = start_idx\n    self.args = args\n    self.temporal_sample = temporal_sample\n    self.seed = seed\n\n    # Initialize tokenizer\n    tokenizer_path = os.path.join(args.model_path, \"tokenizer\")\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path,\n                                              cache_dir=args.cache_dir)\n\n    # Initialize processing stages\n    self._init_stages(args, transform, transform_topcrop, tokenizer)\n\n    # Process metadata\n    self.processed_batches = self._process_metadata()\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.dataset.VideoCaptionMergedDataset-functions","title":"Functions","text":"fastvideo.dataset.VideoCaptionMergedDataset.__iter__ \u00b6 <pre><code>__iter__()\n</code></pre> <p>Iterate through processed data items.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def __iter__(self):\n    \"\"\"Iterate through processed data items.\"\"\"\n    for idx in range(len(self.processed_batches)):\n        yield self._get_item(idx)\n</code></pre> fastvideo.dataset.VideoCaptionMergedDataset.load_state_dict \u00b6 <pre><code>load_state_dict(state_dict: dict[str, Any]) -&gt; None\n</code></pre> <p>Load state dict from checkpoint.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def load_state_dict(self, state_dict: dict[str, Any]) -&gt; None:\n    \"\"\"Load state dict from checkpoint.\"\"\"\n    self.processed_batches = state_dict[\"processed_batches\"]\n</code></pre> fastvideo.dataset.VideoCaptionMergedDataset.state_dict \u00b6 <pre><code>state_dict() -&gt; dict[str, Any]\n</code></pre> <p>Return state dict for checkpointing.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def state_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return state dict for checkpointing.\"\"\"\n    return {\"processed_batches\": self.processed_batches}\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.dataset-modules","title":"Modules","text":""},{"location":"api/fastvideo/#fastvideo.dataset.parquet_dataset_iterable_style","title":"fastvideo.dataset.parquet_dataset_iterable_style","text":""},{"location":"api/fastvideo/#fastvideo.dataset.parquet_dataset_iterable_style-classes","title":"Classes","text":"fastvideo.dataset.parquet_dataset_iterable_style.LatentsParquetIterStyleDataset \u00b6 <pre><code>LatentsParquetIterStyleDataset(\n    path: str,\n    batch_size: int = 1024,\n    cfg_rate: float = 0.1,\n    num_workers: int = 1,\n    drop_last: bool = True,\n    text_padding_length: int = 512,\n    seed: int = 42,\n    read_batch_size: int = 32,\n    parquet_schema: Schema = None,\n)\n</code></pre> <p>               Bases: <code>IterableDataset</code></p> <p>Efficient loader for video-text data from a directory of Parquet files.</p> Source code in <code>fastvideo/dataset/parquet_dataset_iterable_style.py</code> <pre><code>def __init__(self,\n             path: str,\n             batch_size: int = 1024,\n             cfg_rate: float = 0.1,\n             num_workers: int = 1,\n             drop_last: bool = True,\n             text_padding_length: int = 512,\n             seed: int = 42,\n             read_batch_size: int = 32,\n             parquet_schema: pa.Schema = None):\n    super().__init__()\n    self.path = str(path)\n    self.batch_size = batch_size\n    self.parquet_schema = parquet_schema\n    self.cfg_rate = cfg_rate\n    self.text_padding_length = text_padding_length\n    self.seed = seed\n    self.read_batch_size = read_batch_size\n    # Get distributed training info\n    self.global_rank = get_world_rank()\n    self.world_size = get_world_size()\n    self.sp_world_size = get_sp_world_size()\n    self.num_sp_groups = self.world_size // self.sp_world_size\n    num_workers = 1 if num_workers == 0 else num_workers\n    # Get sharding info\n    shard_parquet_files, shard_total_samples, shard_parquet_lengths = shard_parquet_files_across_sp_groups_and_workers(\n        self.path, self.num_sp_groups, num_workers, seed)\n\n    if drop_last:\n        self.worker_num_samples = min(\n            shard_total_samples) // batch_size * batch_size\n        # Assign files to current rank's SP group\n        ith_sp_group = self.global_rank // self.sp_world_size\n        self.sp_group_parquet_files = shard_parquet_files[ith_sp_group::self\n                                                          .num_sp_groups]\n        self.sp_group_parquet_lengths = shard_parquet_lengths[\n            ith_sp_group::self.num_sp_groups]\n        self.sp_group_num_samples = shard_total_samples[ith_sp_group::self.\n                                                        num_sp_groups]\n        logger.info(\n            \"In total %d parquet files, %d samples, after sharding we retain %d samples due to drop_last\",\n            sum([len(shard) for shard in shard_parquet_files]),\n            sum(shard_total_samples),\n            self.worker_num_samples * self.num_sp_groups * num_workers)\n    else:\n        raise ValueError(\"drop_last must be True\")\n    logger.info(\"Each dataloader worker will load %d samples\",\n                self.worker_num_samples)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.dataset.parquet_dataset_iterable_style-functions","title":"Functions","text":"fastvideo.dataset.parquet_dataset_iterable_style.build_parquet_iterable_style_dataloader \u00b6 <pre><code>build_parquet_iterable_style_dataloader(\n    path: str,\n    batch_size: int,\n    num_data_workers: int,\n    cfg_rate: float = 0.0,\n    drop_last: bool = True,\n    text_padding_length: int = 512,\n    seed: int = 42,\n    read_batch_size: int = 32,\n) -&gt; tuple[\n    LatentsParquetIterStyleDataset, StatefulDataLoader\n]\n</code></pre> <p>Build a dataloader for the LatentsParquetIterStyleDataset.</p> Source code in <code>fastvideo/dataset/parquet_dataset_iterable_style.py</code> <pre><code>def build_parquet_iterable_style_dataloader(\n    path: str,\n    batch_size: int,\n    num_data_workers: int,\n    cfg_rate: float = 0.0,\n    drop_last: bool = True,\n    text_padding_length: int = 512,\n    seed: int = 42,\n    read_batch_size: int = 32\n) -&gt; tuple[LatentsParquetIterStyleDataset, StatefulDataLoader]:\n    \"\"\"Build a dataloader for the LatentsParquetIterStyleDataset.\"\"\"\n    dataset = LatentsParquetIterStyleDataset(\n        path=path,\n        batch_size=batch_size,\n        cfg_rate=cfg_rate,\n        num_workers=num_data_workers,\n        drop_last=drop_last,\n        text_padding_length=text_padding_length,\n        seed=seed,\n        read_batch_size=read_batch_size)\n\n    loader = StatefulDataLoader(\n        dataset,\n        batch_size=1,\n        num_workers=num_data_workers,\n        pin_memory=True,\n    )\n    return dataset, loader\n</code></pre> fastvideo.dataset.parquet_dataset_iterable_style.shard_parquet_files_across_sp_groups_and_workers \u00b6 <pre><code>shard_parquet_files_across_sp_groups_and_workers(\n    path: str,\n    num_sp_groups: int,\n    num_workers: int,\n    seed: int = 42,\n) -&gt; tuple[\n    list[list[str]], list[int], list[dict[str, int]]\n]\n</code></pre> <p>Shard parquet files across SP groups and workers in a balanced way.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Directory containing parquet files</p> required <code>num_sp_groups</code> <code>int</code> <p>Number of SP groups to shard across</p> required <code>num_workers</code> <code>int</code> <p>Number of workers per SP group</p> required <code>seed</code> <code>int</code> <p>Random seed for shuffling</p> <code>42</code> <p>Returns:</p> Type Description <code>list[list[str]]</code> <p>Tuple containing:</p> <code>list[int]</code> <ul> <li>List of lists of parquet files for each shard</li> </ul> <code>list[dict[str, int]]</code> <ul> <li>List of total samples per shard</li> </ul> <code>tuple[list[list[str]], list[int], list[dict[str, int]]]</code> <ul> <li>List of dictionaries mapping file paths to their lengths</li> </ul> Source code in <code>fastvideo/dataset/parquet_dataset_iterable_style.py</code> <pre><code>def shard_parquet_files_across_sp_groups_and_workers(\n    path: str,\n    num_sp_groups: int,\n    num_workers: int,\n    seed: int = 42,\n) -&gt; tuple[list[list[str]], list[int], list[dict[str, int]]]:\n    \"\"\"\n    Shard parquet files across SP groups and workers in a balanced way.\n\n    Args:\n        path: Directory containing parquet files\n        num_sp_groups: Number of SP groups to shard across\n        num_workers: Number of workers per SP group\n        seed: Random seed for shuffling\n\n    Returns:\n        Tuple containing:\n        - List of lists of parquet files for each shard\n        - List of total samples per shard\n        - List of dictionaries mapping file paths to their lengths\n    \"\"\"\n    # Check if sharding plan already exists\n    sharding_info_dir = os.path.join(\n        path, f\"sharding_info_{num_sp_groups}_sp_groups_{num_workers}_workers\")\n\n    # Only rank 0 handles cache checking and file scanning\n    if get_world_rank() == 0:\n        cache_loaded = False\n        shard_parquet_files = None\n        shard_total_samples = None\n        shard_parquet_lengths = None\n\n        # First try to load existing sharding plan\n        if os.path.exists(sharding_info_dir):\n            logger.info(\"Loading sharding plan from %s\", sharding_info_dir)\n            try:\n                with open(\n                        os.path.join(sharding_info_dir,\n                                     \"shard_parquet_files.pkl\"), \"rb\") as f:\n                    shard_parquet_files = pickle.load(f)\n                with open(\n                        os.path.join(sharding_info_dir,\n                                     \"shard_total_samples.pkl\"), \"rb\") as f:\n                    shard_total_samples = pickle.load(f)\n                with open(\n                        os.path.join(sharding_info_dir,\n                                     \"shard_parquet_lengths.pkl\"), \"rb\") as f:\n                    shard_parquet_lengths = pickle.load(f)\n                cache_loaded = True\n                logger.info(\"Successfully loaded sharding plan\")\n            except Exception as e:\n                logger.error(\"Error loading sharding plan: %s\", str(e))\n                logger.info(\"Falling back to creating new sharding plan\")\n                cache_loaded = False\n\n        # If cache not loaded (either doesn't exist or failed to load), create sharding plan\n        if not cache_loaded:\n            logger.info(\"Creating new sharding plan\")\n            logger.info(\"Scanning for parquet files in %s\", path)\n\n            # Find all parquet files\n            parquet_files = []\n\n            for root, _, files in os.walk(path):\n                for file in files:\n                    if file.endswith('.parquet'):\n                        parquet_files.append(os.path.join(root, file))\n\n            if not parquet_files:\n                raise ValueError(\"No parquet files found in %s\", path)\n\n            # Calculate file lengths efficiently using a single pass\n            logger.info(\"Calculating file lengths...\")\n            lengths = []\n            for file in tqdm.tqdm(parquet_files, desc=\"Reading parquet files\"):\n                lengths.append(pq.ParquetFile(file).metadata.num_rows)\n\n            total_samples = sum(lengths)\n            logger.info(\"Found %d files with %d total samples\",\n                        len(parquet_files), total_samples)\n\n            # Sort files by length for better balancing\n            sorted_indices = np.argsort(lengths)\n            sorted_files = [parquet_files[i] for i in sorted_indices]\n            sorted_lengths = [lengths[i] for i in sorted_indices]\n\n            # Create shards\n            num_shards = num_sp_groups * num_workers\n            shard_parquet_files = [[] for _ in range(num_shards)]\n            shard_total_samples = [0] * num_shards\n            shard_parquet_lengths = [{} for _ in range(num_shards)]\n\n            # Distribute files to shards using a greedy approach\n            logger.info(\"Distributing files to shards...\")\n            for file, length in zip(reversed(sorted_files),\n                                    reversed(sorted_lengths),\n                                    strict=True):\n                # Find shard with minimum current length\n                target_shard = np.argmin(shard_total_samples)\n                shard_parquet_files[target_shard].append(file)\n                shard_total_samples[target_shard] += length\n                shard_parquet_lengths[target_shard][file] = length\n            #randomize each shard\n            for shard in shard_parquet_files:\n                rng = random.Random(seed)\n                rng.shuffle(shard)\n\n            # Save the sharding plan\n            os.makedirs(sharding_info_dir, exist_ok=True)\n            with open(\n                    os.path.join(sharding_info_dir, \"shard_parquet_files.pkl\"),\n                    \"wb\") as f:\n                pickle.dump(shard_parquet_files, f)\n            with open(\n                    os.path.join(sharding_info_dir, \"shard_total_samples.pkl\"),\n                    \"wb\") as f:\n                pickle.dump(shard_total_samples, f)\n            with open(\n                    os.path.join(sharding_info_dir,\n                                 \"shard_parquet_lengths.pkl\"), \"wb\") as f:\n                pickle.dump(shard_parquet_lengths, f)\n            logger.info(\"Saved sharding info to %s\", sharding_info_dir)\n\n    # Wait for rank 0 to finish creating/loading sharding plan\n    world_group = get_world_group()\n    world_group.barrier()\n\n    # Now all ranks load the sharding plan (it should exist and be valid now)\n    logger.info(\"Loading sharding plan from %s after barrier\",\n                sharding_info_dir)\n    with open(os.path.join(sharding_info_dir, \"shard_parquet_files.pkl\"),\n              \"rb\") as f:\n        shard_parquet_files = pickle.load(f)\n    with open(os.path.join(sharding_info_dir, \"shard_total_samples.pkl\"),\n              \"rb\") as f:\n        shard_total_samples = pickle.load(f)\n    with open(os.path.join(sharding_info_dir, \"shard_parquet_lengths.pkl\"),\n              \"rb\") as f:\n        shard_parquet_lengths = pickle.load(f)\n\n    return shard_parquet_files, shard_total_samples, shard_parquet_lengths\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.dataset.parquet_dataset_map_style","title":"fastvideo.dataset.parquet_dataset_map_style","text":""},{"location":"api/fastvideo/#fastvideo.dataset.parquet_dataset_map_style-classes","title":"Classes","text":"fastvideo.dataset.parquet_dataset_map_style.DP_SP_BatchSampler \u00b6 <pre><code>DP_SP_BatchSampler(\n    batch_size: int,\n    dataset_size: int,\n    num_sp_groups: int,\n    sp_world_size: int,\n    global_rank: int,\n    drop_last: bool = True,\n    drop_first_row: bool = False,\n    seed: int = 0,\n)\n</code></pre> <p>               Bases: <code>Sampler[list[int]]</code></p> <p>A simple sequential batch sampler that yields batches of indices.</p> Source code in <code>fastvideo/dataset/parquet_dataset_map_style.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int,\n    dataset_size: int,\n    num_sp_groups: int,\n    sp_world_size: int,\n    global_rank: int,\n    drop_last: bool = True,\n    drop_first_row: bool = False,\n    seed: int = 0,\n):\n    self.batch_size = batch_size\n    self.dataset_size = dataset_size\n    self.drop_last = drop_last\n    self.seed = seed\n    self.num_sp_groups = num_sp_groups\n    self.global_rank = global_rank\n    self.sp_world_size = sp_world_size\n\n    # \u2500\u2500 epoch-level RNG \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    rng = torch.Generator().manual_seed(self.seed)\n    # Create a random permutation of all indices\n    global_indices = torch.randperm(self.dataset_size, generator=rng)\n\n    if drop_first_row:\n        # drop 0 in global_indices\n        global_indices = global_indices[global_indices != 0]\n        self.dataset_size = self.dataset_size - 1\n\n    if self.drop_last:\n        # For drop_last=True, we:\n        # 1. Ensure total samples is divisible by (batch_size * num_sp_groups)\n        # 2. This guarantees each SP group gets same number of complete batches\n        # 3. Prevents uneven batch sizes across SP groups at end of epoch\n        num_batches = self.dataset_size // self.batch_size\n        num_global_batches = num_batches // self.num_sp_groups\n        global_indices = global_indices[:num_global_batches *\n                                        self.num_sp_groups *\n                                        self.batch_size]\n    else:\n        if self.dataset_size % (self.num_sp_groups * self.batch_size) != 0:\n            # add more indices to make it divisible by (batch_size * num_sp_groups)\n            padding_size = self.num_sp_groups * self.batch_size - (\n                self.dataset_size % (self.num_sp_groups * self.batch_size))\n            logger.info(\"Padding the dataset from %d to %d\",\n                        self.dataset_size, self.dataset_size + padding_size)\n            global_indices = torch.cat(\n                [global_indices, global_indices[:padding_size]])\n\n    # shard the indices to each sp group\n    ith_sp_group = self.global_rank // self.sp_world_size\n    sp_group_local_indices = global_indices[ith_sp_group::self.\n                                            num_sp_groups]\n    self.sp_group_local_indices = sp_group_local_indices\n    logger.info(\"Dataset size for each sp group: %d\",\n                len(sp_group_local_indices))\n</code></pre> fastvideo.dataset.parquet_dataset_map_style.LatentsParquetMapStyleDataset \u00b6 <pre><code>LatentsParquetMapStyleDataset(\n    path: str,\n    batch_size: int,\n    parquet_schema: Schema,\n    cfg_rate: float = 0.0,\n    seed: int = 42,\n    drop_last: bool = True,\n    drop_first_row: bool = False,\n    text_padding_length: int = 512,\n)\n</code></pre> <p>               Bases: <code>Dataset</code></p> <p>Return latents[B,C,T,H,W] and embeddings[B,L,D] in pinned CPU memory. Note:  Using parquet for map style dataset is not efficient, we mainly keep it for backward compatibility and debugging.</p> Source code in <code>fastvideo/dataset/parquet_dataset_map_style.py</code> <pre><code>def __init__(\n    self,\n    path: str,\n    batch_size: int,\n    parquet_schema: pa.Schema,\n    cfg_rate: float = 0.0,\n    seed: int = 42,\n    drop_last: bool = True,\n    drop_first_row: bool = False,\n    text_padding_length: int = 512,\n):\n    super().__init__()\n    self.path = path\n    self.cfg_rate = cfg_rate\n    self.parquet_schema = parquet_schema\n    self.seed = seed\n    # Create a seeded random generator for deterministic CFG\n    self.rng = random.Random(seed)\n    logger.info(\"Initializing LatentsParquetMapStyleDataset with path: %s\",\n                path)\n    self.parquet_files, self.lengths = get_parquet_files_and_length(path)\n    self.batch = batch_size\n    self.text_padding_length = text_padding_length\n    self.sampler = DP_SP_BatchSampler(\n        batch_size=batch_size,\n        dataset_size=sum(self.lengths),\n        num_sp_groups=get_world_size() // get_sp_world_size(),\n        sp_world_size=get_sp_world_size(),\n        global_rank=get_world_rank(),\n        drop_last=drop_last,\n        drop_first_row=drop_first_row,\n        seed=seed,\n    )\n    logger.info(\"Dataset initialized with %d parquet files and %d rows\",\n                len(self.parquet_files), sum(self.lengths))\n</code></pre> Functions\u00b6 fastvideo.dataset.parquet_dataset_map_style.LatentsParquetMapStyleDataset.__getitems__ \u00b6 <pre><code>__getitems__(indices: list[int]) -&gt; dict[str, Any]\n</code></pre> <p>Batch fetch using read_row_from_parquet_file for each index.</p> Source code in <code>fastvideo/dataset/parquet_dataset_map_style.py</code> <pre><code>def __getitems__(self, indices: list[int]) -&gt; dict[str, Any]:\n    \"\"\"\n    Batch fetch using read_row_from_parquet_file for each index.\n    \"\"\"\n    rows = [\n        read_row_from_parquet_file(self.parquet_files, idx, self.lengths)\n        for idx in indices\n    ]\n\n    batch = collate_rows_from_parquet_schema(rows,\n                                             self.parquet_schema,\n                                             self.text_padding_length,\n                                             cfg_rate=self.cfg_rate,\n                                             rng=self.rng)\n    return batch\n</code></pre> fastvideo.dataset.parquet_dataset_map_style.LatentsParquetMapStyleDataset.get_validation_negative_prompt \u00b6 <pre><code>get_validation_negative_prompt() -&gt; tuple[\n    torch.Tensor, torch.Tensor, str\n]\n</code></pre> <p>Get the negative prompt for validation.  This method ensures the negative prompt is loaded and cached properly. Returns the processed negative prompt data (latents, embeddings, masks, info).</p> Source code in <code>fastvideo/dataset/parquet_dataset_map_style.py</code> <pre><code>def get_validation_negative_prompt(\n        self) -&gt; tuple[torch.Tensor, torch.Tensor, str]:\n    \"\"\"\n    Get the negative prompt for validation. \n    This method ensures the negative prompt is loaded and cached properly.\n    Returns the processed negative prompt data (latents, embeddings, masks, info).\n    \"\"\"\n\n    # Read first row from first parquet file\n    file_path = self.parquet_files[0]\n    row_idx = 0\n    # Read the negative prompt data\n    row_dict = read_row_from_parquet_file([file_path], row_idx,\n                                          [self.lengths[0]])\n\n    batch = collate_rows_from_parquet_schema([row_dict],\n                                             self.parquet_schema,\n                                             self.text_padding_length,\n                                             cfg_rate=0.0,\n                                             rng=self.rng)\n    negative_prompt = batch['info_list'][0]['prompt']\n    negative_prompt_embedding = batch['text_embedding']\n    negative_prompt_attention_mask = batch['text_attention_mask']\n    if len(negative_prompt_embedding.shape) == 2:\n        negative_prompt_embedding = negative_prompt_embedding.unsqueeze(0)\n    if len(negative_prompt_attention_mask.shape) == 1:\n        negative_prompt_attention_mask = negative_prompt_attention_mask.unsqueeze(\n            0).unsqueeze(0)\n\n    return negative_prompt_embedding, negative_prompt_attention_mask, negative_prompt\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.dataset.parquet_dataset_map_style-functions","title":"Functions","text":"fastvideo.dataset.parquet_dataset_map_style.read_row_from_parquet_file \u00b6 <pre><code>read_row_from_parquet_file(\n    parquet_files: list[str],\n    global_row_idx: int,\n    lengths: list[int],\n) -&gt; dict[str, Any]\n</code></pre> <p>Read a row from a parquet file. Args:     parquet_files: List[str]     global_row_idx: int     lengths: List[int] Returns:</p> Source code in <code>fastvideo/dataset/parquet_dataset_map_style.py</code> <pre><code>def read_row_from_parquet_file(parquet_files: list[str], global_row_idx: int,\n                               lengths: list[int]) -&gt; dict[str, Any]:\n    '''\n    Read a row from a parquet file.\n    Args:\n        parquet_files: List[str]\n        global_row_idx: int\n        lengths: List[int]\n    Returns:\n    '''\n    # find the parquet file and local row index\n    cumulative = 0\n    file_index = 0\n    local_row_idx = 0\n\n    for file_index in range(len(lengths)):\n        if cumulative + lengths[file_index] &gt; global_row_idx:\n            local_row_idx = global_row_idx - cumulative\n            break\n        cumulative += lengths[file_index]\n    else:\n        # If we reach here, global_row_idx is out of bounds\n        raise IndexError(\n            f\"global_row_idx {global_row_idx} is out of bounds for dataset\")\n\n    parquet_file = pq.ParquetFile(parquet_files[file_index])\n\n    # Calculate the row group to read into memory and the local idx\n    # This way we can avoid reading in the entire parquet file\n    cumulative = 0\n    row_group_index = 0\n    local_index = 0\n\n    for i in range(parquet_file.num_row_groups):\n        num_rows = parquet_file.metadata.row_group(i).num_rows\n        if cumulative + num_rows &gt; local_row_idx:\n            row_group_index = i\n            local_index = local_row_idx - cumulative\n            break\n        cumulative += num_rows\n    else:\n        # If we reach here, local_row_idx is out of bounds for this parquet file\n        raise IndexError(\n            f\"local_row_idx {local_row_idx} is out of bounds for parquet file {parquet_files[file_index]}\"\n        )\n\n    row_group = parquet_file.read_row_group(row_group_index).to_pydict()\n    row_dict = {k: v[local_index] for k, v in row_group.items()}\n    del row_group\n\n    return row_dict\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.dataset.preprocessing_datasets","title":"fastvideo.dataset.preprocessing_datasets","text":""},{"location":"api/fastvideo/#fastvideo.dataset.preprocessing_datasets-classes","title":"Classes","text":"fastvideo.dataset.preprocessing_datasets.DataValidationStage \u00b6 <p>               Bases: <code>DatasetFilterStage</code></p> <p>Stage for validating data items.</p> Functions\u00b6 fastvideo.dataset.preprocessing_datasets.DataValidationStage.process \u00b6 <pre><code>process(\n    batch: PreprocessBatch, **kwargs\n) -&gt; PreprocessBatch\n</code></pre> <p>Process does nothing for validation - filtering is handled by should_keep.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def process(self, batch: PreprocessBatch, **kwargs) -&gt; PreprocessBatch:\n    \"\"\"Process does nothing for validation - filtering is handled by should_keep.\"\"\"\n    return batch\n</code></pre> fastvideo.dataset.preprocessing_datasets.DataValidationStage.should_keep \u00b6 <pre><code>should_keep(batch: PreprocessBatch, **kwargs) -&gt; bool\n</code></pre> <p>Validate data item.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>PreprocessBatch</code> <p>Dataset batch to validate</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if valid, False if invalid</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def should_keep(self, batch: PreprocessBatch, **kwargs) -&gt; bool:\n    \"\"\"\n    Validate data item.\n\n    Args:\n        batch: Dataset batch to validate\n\n    Returns:\n        True if valid, False if invalid\n    \"\"\"\n    # Check for caption\n    if batch.cap is None:\n        return False\n\n    if batch.is_video:\n        # Validate video-specific fields\n        if batch.duration is None or batch.fps is None:\n            return False\n    elif not batch.is_image:\n        return False\n\n    return True\n</code></pre> fastvideo.dataset.preprocessing_datasets.DatasetFilterStage \u00b6 <p>               Bases: <code>ABC</code></p> <p>Abstract base class for dataset filtering stages.</p> <p>These stages can filter out items during metadata processing.</p> Functions\u00b6 fastvideo.dataset.preprocessing_datasets.DatasetFilterStage.process <code>abstractmethod</code> \u00b6 <pre><code>process(\n    batch: PreprocessBatch, **kwargs\n) -&gt; PreprocessBatch\n</code></pre> <p>Process the dataset batch (for non-filtering operations).</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>PreprocessBatch</code> <p>Dataset batch to process</p> required <code>**kwargs</code> <p>Additional processing parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>PreprocessBatch</code> <p>Processed batch</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>@abstractmethod\ndef process(self, batch: PreprocessBatch, **kwargs) -&gt; PreprocessBatch:\n    \"\"\"\n    Process the dataset batch (for non-filtering operations).\n\n    Args:\n        batch: Dataset batch to process\n        **kwargs: Additional processing parameters\n\n    Returns:\n        Processed batch\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.dataset.preprocessing_datasets.DatasetFilterStage.should_keep <code>abstractmethod</code> \u00b6 <pre><code>should_keep(batch: PreprocessBatch, **kwargs) -&gt; bool\n</code></pre> <p>Check if batch should be kept.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>PreprocessBatch</code> <p>Dataset batch to check</p> required <code>**kwargs</code> <p>Additional parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if batch should be kept, False otherwise</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>@abstractmethod\ndef should_keep(self, batch: PreprocessBatch, **kwargs) -&gt; bool:\n    \"\"\"\n    Check if batch should be kept.\n\n    Args:\n        batch: Dataset batch to check\n        **kwargs: Additional parameters\n\n    Returns:\n        True if batch should be kept, False otherwise\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.dataset.preprocessing_datasets.DatasetStage \u00b6 <p>               Bases: <code>ABC</code></p> <p>Abstract base class for dataset processing stages.</p> <p>Similar to PipelineStage but designed for dataset preprocessing operations.</p> Functions\u00b6 fastvideo.dataset.preprocessing_datasets.DatasetStage.process <code>abstractmethod</code> \u00b6 <pre><code>process(\n    batch: PreprocessBatch, **kwargs\n) -&gt; PreprocessBatch\n</code></pre> <p>Process the dataset batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>PreprocessBatch</code> <p>Dataset batch to process</p> required <code>**kwargs</code> <p>Additional processing parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>PreprocessBatch</code> <p>Processed batch</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>@abstractmethod\ndef process(self, batch: PreprocessBatch, **kwargs) -&gt; PreprocessBatch:\n    \"\"\"\n    Process the dataset batch.\n\n    Args:\n        batch: Dataset batch to process\n        **kwargs: Additional processing parameters\n\n    Returns:\n        Processed batch\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.dataset.preprocessing_datasets.FrameSamplingStage \u00b6 <pre><code>FrameSamplingStage(\n    num_frames: int,\n    train_fps: int,\n    speed_factor: int = 1,\n    video_length_tolerance_range: float = 5.0,\n    drop_short_ratio: float = 0.0,\n    seed: int = 42,\n)\n</code></pre> <p>               Bases: <code>DatasetFilterStage</code></p> <p>Stage for temporal frame sampling and indexing.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def __init__(self,\n             num_frames: int,\n             train_fps: int,\n             speed_factor: int = 1,\n             video_length_tolerance_range: float = 5.0,\n             drop_short_ratio: float = 0.0,\n             seed: int = 42):\n    self.num_frames = num_frames\n    self.train_fps = train_fps\n    self.speed_factor = speed_factor\n    self.video_length_tolerance_range = video_length_tolerance_range\n    self.drop_short_ratio = drop_short_ratio\n    # Create a seeded random generator for deterministic sampling\n    self.rng = random.Random(seed)\n</code></pre> Functions\u00b6 fastvideo.dataset.preprocessing_datasets.FrameSamplingStage.process \u00b6 <pre><code>process(\n    batch: PreprocessBatch,\n    temporal_sample_fn=None,\n    **kwargs\n) -&gt; PreprocessBatch\n</code></pre> <p>Process frame sampling for video data items.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>PreprocessBatch</code> <p>Dataset batch</p> required <code>temporal_sample_fn</code> <p>Function for temporal sampling</p> <code>None</code> <p>Returns:</p> Type Description <code>PreprocessBatch</code> <p>Updated batch with frame sampling info</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def process(self,\n            batch: PreprocessBatch,\n            temporal_sample_fn=None,\n            **kwargs) -&gt; PreprocessBatch:\n    \"\"\"\n    Process frame sampling for video data items.\n\n    Args:\n        batch: Dataset batch\n        temporal_sample_fn: Function for temporal sampling\n\n    Returns:\n        Updated batch with frame sampling info\n    \"\"\"\n    if batch.is_image:\n        # For images, just add sample info\n        batch.sample_frame_index = [0]\n        batch.sample_num_frames = 1\n        return batch\n\n    assert batch.duration is not None and batch.fps is not None\n    batch.num_frames = math.ceil(batch.fps * batch.duration)\n\n    # Resample frame indices\n    frame_interval = batch.fps / self.train_fps\n    start_frame_idx = 0\n    frame_indices = np.arange(start_frame_idx, batch.num_frames,\n                              frame_interval).astype(int)\n\n    # Temporal crop if too long\n    if len(frame_indices) &gt; self.num_frames:\n        if temporal_sample_fn is not None:\n            begin_index, end_index = temporal_sample_fn(len(frame_indices))\n            frame_indices = frame_indices[begin_index:end_index]\n        else:\n            frame_indices = frame_indices[:self.num_frames]\n\n    batch.sample_frame_index = frame_indices.tolist()\n    batch.sample_num_frames = len(frame_indices)\n\n    return batch\n</code></pre> fastvideo.dataset.preprocessing_datasets.FrameSamplingStage.should_keep \u00b6 <pre><code>should_keep(batch: PreprocessBatch, **kwargs) -&gt; bool\n</code></pre> <p>Check if video should be kept based on length constraints.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>PreprocessBatch</code> <p>Dataset batch</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if should be kept, False otherwise</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def should_keep(self, batch: PreprocessBatch, **kwargs) -&gt; bool:\n    \"\"\"\n    Check if video should be kept based on length constraints.\n\n    Args:\n        batch: Dataset batch\n\n    Returns:\n        True if should be kept, False otherwise\n    \"\"\"\n    if batch.is_image:\n        return True\n\n    if batch.duration is None or batch.fps is None:\n        return False\n\n    num_frames = math.ceil(batch.fps * batch.duration)\n\n    # Check if video is too long\n    if (num_frames / batch.fps &gt; self.video_length_tolerance_range *\n        (self.num_frames / self.train_fps * self.speed_factor)):\n        return False\n\n    # Resample frame indices to check length\n    frame_interval = batch.fps / self.train_fps\n    start_frame_idx = 0\n    frame_indices = np.arange(start_frame_idx, num_frames,\n                              frame_interval).astype(int)\n\n    # Filter short videos\n    return not (len(frame_indices) &lt; self.num_frames\n                and self.rng.random() &lt; self.drop_short_ratio)\n</code></pre> fastvideo.dataset.preprocessing_datasets.ImageTransformStage \u00b6 <pre><code>ImageTransformStage(transform, transform_topcrop)\n</code></pre> <p>               Bases: <code>DatasetStage</code></p> <p>Stage for image data transformation.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def __init__(self, transform, transform_topcrop) -&gt; None:\n    self.transform = transform\n    self.transform_topcrop = transform_topcrop\n</code></pre> Functions\u00b6 fastvideo.dataset.preprocessing_datasets.ImageTransformStage.process \u00b6 <pre><code>process(\n    batch: PreprocessBatch, **kwargs\n) -&gt; PreprocessBatch\n</code></pre> <p>Transform image data.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>PreprocessBatch</code> <p>Dataset batch with image information</p> required <p>Returns:</p> Type Description <code>PreprocessBatch</code> <p>Batch with transformed image tensor</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def process(self, batch: PreprocessBatch, **kwargs) -&gt; PreprocessBatch:\n    \"\"\"\n    Transform image data.\n\n    Args:\n        batch: Dataset batch with image information\n\n    Returns:\n        Batch with transformed image tensor\n    \"\"\"\n    if not batch.is_image:\n        return batch\n\n    image = Image.open(batch.path).convert(\"RGB\")\n    image = torch.from_numpy(np.array(image))\n    image = rearrange(image, \"h w c -&gt; c h w\").unsqueeze(0)\n\n    if self.transform_topcrop is not None:\n        image = self.transform_topcrop(image)\n    elif self.transform is not None:\n        image = self.transform(image)\n\n    image = image.transpose(0, 1)  # [1 C H W] -&gt; [C 1 H W]\n    image = image.float() / 127.5 - 1.0\n    batch.pixel_values = image\n    return batch\n</code></pre> fastvideo.dataset.preprocessing_datasets.PreprocessBatch <code>dataclass</code> \u00b6 <pre><code>PreprocessBatch(\n    path: str,\n    cap: str | list[str],\n    resolution: dict | None = None,\n    fps: float | None = None,\n    duration: float | None = None,\n    num_frames: int | None = None,\n    sample_frame_index: list[int] | None = None,\n    sample_num_frames: int | None = None,\n    pixel_values: Tensor | None = None,\n    text: str | None = None,\n    input_ids: Tensor | None = None,\n    cond_mask: Tensor | None = None,\n)\n</code></pre> <p>Batch information for dataset processing stages.</p> <p>This class holds all the information about a video-caption or image-caption pair as it moves through the processing pipeline. Fields are populated by different stages.</p> Attributes\u00b6 fastvideo.dataset.preprocessing_datasets.PreprocessBatch.is_image <code>property</code> \u00b6 <pre><code>is_image: bool\n</code></pre> <p>Check if this is an image item.</p> fastvideo.dataset.preprocessing_datasets.PreprocessBatch.is_video <code>property</code> \u00b6 <pre><code>is_video: bool\n</code></pre> <p>Check if this is a video item.</p> fastvideo.dataset.preprocessing_datasets.ResolutionFilterStage \u00b6 <pre><code>ResolutionFilterStage(\n    max_h_div_w_ratio: float = 17 / 16,\n    min_h_div_w_ratio: float = 8 / 16,\n    max_height: int = 1024,\n    max_width: int = 1024,\n)\n</code></pre> <p>               Bases: <code>DatasetFilterStage</code></p> <p>Stage for filtering data items based on resolution constraints.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def __init__(self,\n             max_h_div_w_ratio: float = 17 / 16,\n             min_h_div_w_ratio: float = 8 / 16,\n             max_height: int = 1024,\n             max_width: int = 1024):\n    self.max_h_div_w_ratio = max_h_div_w_ratio\n    self.min_h_div_w_ratio = min_h_div_w_ratio\n    self.max_height = max_height\n    self.max_width = max_width\n</code></pre> Functions\u00b6 fastvideo.dataset.preprocessing_datasets.ResolutionFilterStage.filter_resolution \u00b6 <pre><code>filter_resolution(\n    h: int,\n    w: int,\n    max_h_div_w_ratio: float,\n    min_h_div_w_ratio: float,\n) -&gt; bool\n</code></pre> <p>Filter based on height/width ratio.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def filter_resolution(self, h: int, w: int, max_h_div_w_ratio: float,\n                      min_h_div_w_ratio: float) -&gt; bool:\n    \"\"\"Filter based on height/width ratio.\"\"\"\n    return h / w &lt;= max_h_div_w_ratio and h / w &gt;= min_h_div_w_ratio\n</code></pre> fastvideo.dataset.preprocessing_datasets.ResolutionFilterStage.process \u00b6 <pre><code>process(\n    batch: PreprocessBatch, **kwargs\n) -&gt; PreprocessBatch\n</code></pre> <p>Process does nothing for resolution filtering - filtering is handled by should_keep.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def process(self, batch: PreprocessBatch, **kwargs) -&gt; PreprocessBatch:\n    \"\"\"Process does nothing for resolution filtering - filtering is handled by should_keep.\"\"\"\n    return batch\n</code></pre> fastvideo.dataset.preprocessing_datasets.ResolutionFilterStage.should_keep \u00b6 <pre><code>should_keep(batch: PreprocessBatch, **kwargs) -&gt; bool\n</code></pre> <p>Check if data item passes resolution filtering.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>PreprocessBatch</code> <p>Dataset batch with resolution information</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if passes filter, False otherwise</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def should_keep(self, batch: PreprocessBatch, **kwargs) -&gt; bool:\n    \"\"\"\n    Check if data item passes resolution filtering.\n\n    Args:\n        batch: Dataset batch with resolution information\n\n    Returns:\n        True if passes filter, False otherwise\n    \"\"\"\n    # Only apply to videos\n    if not batch.is_video:\n        return True\n\n    if batch.resolution is None:\n        return False\n\n    height = batch.resolution.get(\"height\", None)\n    width = batch.resolution.get(\"width\", None)\n    if height is None or width is None:\n        return False\n\n    # Check aspect ratio\n    aspect = self.max_height / self.max_width\n    hw_aspect_thr = 1.5\n\n    return self.filter_resolution(\n        height,\n        width,\n        max_h_div_w_ratio=hw_aspect_thr * aspect,\n        min_h_div_w_ratio=1 / hw_aspect_thr * aspect,\n    )\n</code></pre> fastvideo.dataset.preprocessing_datasets.TextDataset \u00b6 <pre><code>TextDataset(\n    data_merge_path: str,\n    args,\n    start_idx: int = 0,\n    seed: int = 42,\n)\n</code></pre> <p>               Bases: <code>IterableDataset</code>, <code>Stateful</code></p> <p>Text-only dataset for processing prompts from a simple text file.</p> <p>Assumes that data_merge_path is a text file with one prompt per line: A cat playing with a ball A dog running in the park A person cooking dinner ...</p> <p>This dataset processes text data through text encoding stages only.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def __init__(self,\n             data_merge_path: str,\n             args,\n             start_idx: int = 0,\n             seed: int = 42):\n    self.data_merge_path = data_merge_path\n    self.start_idx = start_idx\n    self.args = args\n    self.seed = seed\n\n    # Initialize tokenizer\n    tokenizer_path = os.path.join(args.model_path, \"tokenizer\")\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path,\n                                              cache_dir=args.cache_dir)\n\n    # Initialize text encoding stage\n    self.text_encoding_stage = TextEncodingStage(\n        tokenizer=tokenizer,\n        text_max_length=args.text_max_length,\n        cfg_rate=getattr(args, 'training_cfg_rate', 0.0),\n        seed=self.seed)\n\n    # Process text data\n    self.processed_batches = self._process_text_data()\n</code></pre> Functions\u00b6 fastvideo.dataset.preprocessing_datasets.TextDataset.__iter__ \u00b6 <pre><code>__iter__()\n</code></pre> <p>Iterator for the dataset.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def __iter__(self):\n    \"\"\"Iterator for the dataset.\"\"\"\n    # Set up distributed sampling if needed\n    if torch.distributed.is_available() and torch.distributed.is_initialized():\n        rank = torch.distributed.get_rank()\n        world_size = torch.distributed.get_world_size()\n    else:\n        rank = 0\n        world_size = 1\n\n    # Calculate chunk for this rank\n    total_items = len(self.processed_batches)\n    items_per_rank = math.ceil(total_items / world_size)\n    start_idx = rank * items_per_rank + self.start_idx\n    end_idx = min(start_idx + items_per_rank, total_items)\n\n    # Yield items for this rank\n    for idx in range(start_idx, end_idx):\n        if idx &lt; len(self.processed_batches):\n            yield self._get_item(idx)\n</code></pre> fastvideo.dataset.preprocessing_datasets.TextDataset.load_state_dict \u00b6 <pre><code>load_state_dict(state_dict: dict[str, Any]) -&gt; None\n</code></pre> <p>Load state dict from checkpoint.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def load_state_dict(self, state_dict: dict[str, Any]) -&gt; None:\n    \"\"\"Load state dict from checkpoint.\"\"\"\n    self.processed_batches = state_dict[\"processed_batches\"]\n</code></pre> fastvideo.dataset.preprocessing_datasets.TextDataset.state_dict \u00b6 <pre><code>state_dict() -&gt; dict[str, Any]\n</code></pre> <p>Return state dict for checkpointing.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def state_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return state dict for checkpointing.\"\"\"\n    return {\"processed_batches\": self.processed_batches}\n</code></pre> fastvideo.dataset.preprocessing_datasets.TextEncodingStage \u00b6 <pre><code>TextEncodingStage(\n    tokenizer,\n    text_max_length: int,\n    cfg_rate: float = 0.0,\n    seed: int = 42,\n)\n</code></pre> <p>               Bases: <code>DatasetStage</code></p> <p>Stage for text tokenization and encoding.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def __init__(self,\n             tokenizer,\n             text_max_length: int,\n             cfg_rate: float = 0.0,\n             seed: int = 42):\n    self.tokenizer = tokenizer\n    self.text_max_length = text_max_length\n    self.cfg_rate = cfg_rate\n    # Create a seeded random generator for deterministic CFG\n    self.rng = random.Random(seed)\n</code></pre> Functions\u00b6 fastvideo.dataset.preprocessing_datasets.TextEncodingStage.process \u00b6 <pre><code>process(\n    batch: PreprocessBatch, **kwargs\n) -&gt; PreprocessBatch\n</code></pre> <p>Process text data.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>PreprocessBatch</code> <p>Dataset batch with caption information</p> required <p>Returns:</p> Type Description <code>PreprocessBatch</code> <p>Batch with encoded text information</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def process(self, batch: PreprocessBatch, **kwargs) -&gt; PreprocessBatch:\n    \"\"\"\n    Process text data.\n\n    Args:\n        batch: Dataset batch with caption information\n\n    Returns:\n        Batch with encoded text information\n    \"\"\"\n    text = batch.cap\n    if not isinstance(text, list):\n        text = [text]\n    text = [self.rng.choice(text)]\n\n    text = text[0] if self.rng.random() &gt; self.cfg_rate else \"\"\n    text_tokens_and_mask = self.tokenizer(\n        text,\n        max_length=self.text_max_length,\n        padding=\"max_length\",\n        truncation=True,\n        return_attention_mask=True,\n        add_special_tokens=True,\n        return_tensors=\"pt\",\n    )\n\n    batch.text = text\n    batch.input_ids = text_tokens_and_mask[\"input_ids\"]\n    batch.cond_mask = text_tokens_and_mask[\"attention_mask\"]\n    return batch\n</code></pre> fastvideo.dataset.preprocessing_datasets.VideoCaptionMergedDataset \u00b6 <pre><code>VideoCaptionMergedDataset(\n    data_merge_path: str,\n    args,\n    transform,\n    temporal_sample,\n    transform_topcrop,\n    start_idx: int = 0,\n    seed: int = 42,\n)\n</code></pre> <p>               Bases: <code>IterableDataset</code>, <code>Stateful</code></p> <p>Merged dataset for video and caption data with stage-based processing. Assumes that data_merge_path is a txt file with the following format: , <pre><code>The folder should contain videos.\n\nThe json file should be a list of dictionaries with the following format:\n[\n{\n    \"path\": \"1gGQy4nxyUo-Scene-016.mp4\",\n    \"resolution\": {\n    \"width\": 1920,\n    \"height\": 1080\n    },\n    \"size\": 2439112,\n    \"fps\": 25.0,\n    \"duration\": 6.88,\n    \"num_frames\": 172,\n    \"cap\": [\n    \"A watermelon wearing a helmet is crushed by a hydraulic press, causing it to flatten and burst open.\"\n    ]\n},\n...\n]\n</code></pre> <p>This dataset processes video and image data through a series of stages: - Data validation - Resolution filtering - Frame sampling - Transformation - Text encoding</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def __init__(self,\n             data_merge_path: str,\n             args,\n             transform,\n             temporal_sample,\n             transform_topcrop,\n             start_idx: int = 0,\n             seed: int = 42):\n    self.data_merge_path = data_merge_path\n    self.start_idx = start_idx\n    self.args = args\n    self.temporal_sample = temporal_sample\n    self.seed = seed\n\n    # Initialize tokenizer\n    tokenizer_path = os.path.join(args.model_path, \"tokenizer\")\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path,\n                                              cache_dir=args.cache_dir)\n\n    # Initialize processing stages\n    self._init_stages(args, transform, transform_topcrop, tokenizer)\n\n    # Process metadata\n    self.processed_batches = self._process_metadata()\n</code></pre> Functions\u00b6 fastvideo.dataset.preprocessing_datasets.VideoCaptionMergedDataset.__iter__ \u00b6 <pre><code>__iter__()\n</code></pre> <p>Iterate through processed data items.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def __iter__(self):\n    \"\"\"Iterate through processed data items.\"\"\"\n    for idx in range(len(self.processed_batches)):\n        yield self._get_item(idx)\n</code></pre> fastvideo.dataset.preprocessing_datasets.VideoCaptionMergedDataset.load_state_dict \u00b6 <pre><code>load_state_dict(state_dict: dict[str, Any]) -&gt; None\n</code></pre> <p>Load state dict from checkpoint.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def load_state_dict(self, state_dict: dict[str, Any]) -&gt; None:\n    \"\"\"Load state dict from checkpoint.\"\"\"\n    self.processed_batches = state_dict[\"processed_batches\"]\n</code></pre> fastvideo.dataset.preprocessing_datasets.VideoCaptionMergedDataset.state_dict \u00b6 <pre><code>state_dict() -&gt; dict[str, Any]\n</code></pre> <p>Return state dict for checkpointing.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def state_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return state dict for checkpointing.\"\"\"\n    return {\"processed_batches\": self.processed_batches}\n</code></pre> fastvideo.dataset.preprocessing_datasets.VideoTransformStage \u00b6 <pre><code>VideoTransformStage(transform)\n</code></pre> <p>               Bases: <code>DatasetStage</code></p> <p>Stage for video data transformation.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def __init__(self, transform) -&gt; None:\n    self.transform = transform\n</code></pre> Functions\u00b6 fastvideo.dataset.preprocessing_datasets.VideoTransformStage.process \u00b6 <pre><code>process(\n    batch: PreprocessBatch, **kwargs\n) -&gt; PreprocessBatch\n</code></pre> <p>Transform video data.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>PreprocessBatch</code> <p>Dataset batch with video information</p> required <p>Returns:</p> Type Description <code>PreprocessBatch</code> <p>Batch with transformed video tensor</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def process(self, batch: PreprocessBatch, **kwargs) -&gt; PreprocessBatch:\n    \"\"\"\n    Transform video data.\n\n    Args:\n        batch: Dataset batch with video information\n\n    Returns:\n        Batch with transformed video tensor\n    \"\"\"\n    if not batch.is_video:\n        return batch\n\n    assert os.path.exists(batch.path), f\"file {batch.path} do not exist!\"\n    assert batch.sample_frame_index is not None, \"Frame indices must be set before transformation\"\n\n    torchvision_video, _, metadata = torchvision.io.read_video(\n        batch.path, output_format=\"TCHW\")\n    video = torchvision_video[batch.sample_frame_index]\n    if self.transform is not None:\n        video = self.transform(video)\n    video = rearrange(video, \"t c h w -&gt; c t h w\")\n    video = video.to(torch.uint8)\n\n    h, w = video.shape[-2:]\n    assert (\n        h / w &lt;= 17 / 16 and h / w &gt;= 8 / 16\n    ), f\"Only videos with a ratio (h/w) less than 17/16 and more than 8/16 are supported. But video ({batch.path}) found ratio is {round(h / w, 2)} with the shape of {video.shape}\"\n\n    video = video.float() / 127.5 - 1.0\n    batch.pixel_values = video\n    return batch\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.dataset.preprocessing_datasets-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideo.dataset.transform","title":"fastvideo.dataset.transform","text":""},{"location":"api/fastvideo/#fastvideo.dataset.transform-classes","title":"Classes","text":"fastvideo.dataset.transform.CenterCropResizeVideo \u00b6 <pre><code>CenterCropResizeVideo(\n    size, top_crop=False, interpolation_mode=\"bilinear\"\n)\n</code></pre> <p>First use the short side for cropping length, center crop video, then resize to the specified size</p> Source code in <code>fastvideo/dataset/transform.py</code> <pre><code>def __init__(\n    self,\n    size,\n    top_crop=False,\n    interpolation_mode=\"bilinear\",\n) -&gt; None:\n    if len(size) != 2:\n        raise ValueError(\n            f\"size should be tuple (height, width), instead got {size}\")\n    self.size = size\n    self.top_crop = top_crop\n    self.interpolation_mode = interpolation_mode\n</code></pre> Functions\u00b6 fastvideo.dataset.transform.CenterCropResizeVideo.__call__ \u00b6 <pre><code>__call__(clip) -&gt; torch.Tensor\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>clip</code> <code>tensor</code> <p>Video clip to be cropped. Size is (T, C, H, W)</p> required <p>Returns:     torch.tensor: scale resized / center cropped video clip.         size is (T, C, crop_size, crop_size)</p> Source code in <code>fastvideo/dataset/transform.py</code> <pre><code>def __call__(self, clip) -&gt; torch.Tensor:\n    \"\"\"\n    Args:\n        clip (torch.tensor): Video clip to be cropped. Size is (T, C, H, W)\n    Returns:\n        torch.tensor: scale resized / center cropped video clip.\n            size is (T, C, crop_size, crop_size)\n    \"\"\"\n    clip_center_crop = center_crop_th_tw(clip,\n                                         self.size[0],\n                                         self.size[1],\n                                         top_crop=self.top_crop)\n    clip_center_crop_resize = resize(\n        clip_center_crop,\n        target_size=self.size,\n        interpolation_mode=self.interpolation_mode,\n    )\n    return clip_center_crop_resize\n</code></pre> fastvideo.dataset.transform.Normalize255 \u00b6 <pre><code>Normalize255()\n</code></pre> <p>Convert tensor data type from uint8 to float, divide value by 255.0 and</p> Source code in <code>fastvideo/dataset/transform.py</code> <pre><code>def __init__(self) -&gt; None:\n    pass\n</code></pre> Functions\u00b6 fastvideo.dataset.transform.Normalize255.__call__ \u00b6 <pre><code>__call__(clip) -&gt; torch.Tensor\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>clip</code> <code>torch.tensor, dtype=torch.uint8</code> <p>Size is (T, C, H, W)</p> required <p>Return:     clip (torch.tensor, dtype=torch.float): Size is (T, C, H, W)</p> Source code in <code>fastvideo/dataset/transform.py</code> <pre><code>def __call__(self, clip) -&gt; torch.Tensor:\n    \"\"\"\n    Args:\n        clip (torch.tensor, dtype=torch.uint8): Size is (T, C, H, W)\n    Return:\n        clip (torch.tensor, dtype=torch.float): Size is (T, C, H, W)\n    \"\"\"\n    return normalize_video(clip)\n</code></pre> fastvideo.dataset.transform.TemporalRandomCrop \u00b6 <pre><code>TemporalRandomCrop(size)\n</code></pre> <p>Temporally crop the given frame indices at a random location.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>Desired length of frames will be seen in the model.</p> required Source code in <code>fastvideo/dataset/transform.py</code> <pre><code>def __init__(self, size) -&gt; None:\n    self.size = size\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.dataset.transform-functions","title":"Functions","text":"fastvideo.dataset.transform.crop \u00b6 <pre><code>crop(clip, i, j, h, w) -&gt; torch.Tensor\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>clip</code> <code>tensor</code> <p>Video clip to be cropped. Size is (T, C, H, W)</p> required Source code in <code>fastvideo/dataset/transform.py</code> <pre><code>def crop(clip, i, j, h, w) -&gt; torch.Tensor:\n    \"\"\"\n    Args:\n        clip (torch.tensor): Video clip to be cropped. Size is (T, C, H, W)\n    \"\"\"\n    if len(clip.size()) != 4:\n        raise ValueError(\"clip should be a 4D tensor\")\n    return clip[..., i:i + h, j:j + w]\n</code></pre> fastvideo.dataset.transform.normalize_video \u00b6 <pre><code>normalize_video(clip) -&gt; torch.Tensor\n</code></pre> <p>Convert tensor data type from uint8 to float, divide value by 255.0 and permute the dimensions of clip tensor Args:     clip (torch.tensor, dtype=torch.uint8): Size is (T, C, H, W) Return:     clip (torch.tensor, dtype=torch.float): Size is (T, C, H, W)</p> Source code in <code>fastvideo/dataset/transform.py</code> <pre><code>def normalize_video(clip) -&gt; torch.Tensor:\n    \"\"\"\n    Convert tensor data type from uint8 to float, divide value by 255.0 and\n    permute the dimensions of clip tensor\n    Args:\n        clip (torch.tensor, dtype=torch.uint8): Size is (T, C, H, W)\n    Return:\n        clip (torch.tensor, dtype=torch.float): Size is (T, C, H, W)\n    \"\"\"\n    _is_tensor_video_clip(clip)\n    if not clip.dtype == torch.uint8:\n        raise TypeError(\n            f\"clip tensor should have data type uint8. Got {clip.dtype}\")\n    # return clip.float().permute(3, 0, 1, 2) / 255.0\n    return clip.float() / 255.0\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.dataset.utils","title":"fastvideo.dataset.utils","text":""},{"location":"api/fastvideo/#fastvideo.dataset.utils-functions","title":"Functions","text":"fastvideo.dataset.utils.collate_rows_from_parquet_schema \u00b6 <pre><code>collate_rows_from_parquet_schema(\n    rows,\n    parquet_schema,\n    text_padding_length,\n    cfg_rate=0.0,\n    rng=None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Collate rows from parquet files based on the provided schema. Dynamically processes tensor fields based on schema and returns batched data.</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <p>List of row dictionaries from parquet files</p> required <code>parquet_schema</code> <p>PyArrow schema defining the structure of the data</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict containing batched tensors and metadata</p> Source code in <code>fastvideo/dataset/utils.py</code> <pre><code>def collate_rows_from_parquet_schema(rows,\n                                     parquet_schema,\n                                     text_padding_length,\n                                     cfg_rate=0.0,\n                                     rng=None) -&gt; dict[str, Any]:\n    \"\"\"\n    Collate rows from parquet files based on the provided schema.\n    Dynamically processes tensor fields based on schema and returns batched data.\n\n    Args:\n        rows: List of row dictionaries from parquet files\n        parquet_schema: PyArrow schema defining the structure of the data\n\n    Returns:\n        Dict containing batched tensors and metadata\n    \"\"\"\n    if not rows:\n        return cast(dict[str, Any], {})\n\n    # Initialize containers for different data types\n    batch_data: dict[str, Any] = {}\n\n    # Get tensor and metadata field names from schema (fields ending with '_bytes')\n    tensor_fields = []\n    metadata_fields = []\n    for field in parquet_schema.names:\n        if field.endswith('_bytes'):\n            shape_field = field.replace('_bytes', '_shape')\n            dtype_field = field.replace('_bytes', '_dtype')\n            tensor_name = field.replace('_bytes', '')\n            tensor_fields.append(tensor_name)\n            assert shape_field in parquet_schema.names, f\"Shape field {shape_field} not found in schema for field {field}. Currently we only support *_bytes fields for tensors.\"\n            assert dtype_field in parquet_schema.names, f\"Dtype field {dtype_field} not found in schema for field {field}. Currently we only support *_bytes fields for tensors.\"\n        elif not field.endswith('_shape') and not field.endswith('_dtype'):\n            # Only add actual metadata fields, not the shape/dtype helper fields\n            metadata_fields.append(field)\n\n    # Process each tensor field\n    for tensor_name in tensor_fields:\n        tensor_list = []\n\n        for row in rows:\n            # Get tensor data from row using the existing helper function pattern\n            shape_key = f\"{tensor_name}_shape\"\n            bytes_key = f\"{tensor_name}_bytes\"\n\n            if shape_key in row and bytes_key in row:\n                shape = row[shape_key]\n                bytes_data = row[bytes_key]\n\n                if len(bytes_data) == 0:\n                    tensor = torch.zeros(0, dtype=torch.bfloat16)\n                else:\n                    # Convert bytes to tensor using float32 as default\n                    if tensor_name == 'text_embedding' and (rng.random(\n                    ) if rng else random.random()) &lt; cfg_rate:\n                        data = np.zeros((512, 4096), dtype=np.float32)\n                    else:\n                        data = np.frombuffer(\n                            bytes_data, dtype=np.float32).reshape(shape).copy()\n                    tensor = torch.from_numpy(data)\n                    # if len(data.shape) == 3:\n                    #     B, L, D = tensor.shape\n                    #     assert B == 1, \"Batch size must be 1\"\n                    #     tensor = tensor.squeeze(0)\n\n                tensor_list.append(tensor)\n            else:\n                # Handle missing tensor data\n                tensor_list.append(torch.zeros(0, dtype=torch.bfloat16))\n\n        # Stack tensors with special handling for text embeddings\n        if tensor_name == 'text_embedding':\n            # Handle text embeddings with padding\n            padded_tensors = []\n            attention_masks = []\n\n            for tensor in tensor_list:\n                if tensor.numel() &gt; 0:\n                    padded_tensor, mask = pad(tensor, text_padding_length)\n                    padded_tensors.append(padded_tensor)\n                    attention_masks.append(mask)\n                else:\n                    # Handle empty embeddings - assume default embedding dimension\n                    padded_tensors.append(\n                        torch.zeros(text_padding_length,\n                                    768,\n                                    dtype=torch.bfloat16))\n                    attention_masks.append(torch.zeros(text_padding_length))\n\n            batch_data[tensor_name] = torch.stack(padded_tensors)\n            batch_data['text_attention_mask'] = torch.stack(attention_masks)\n        else:\n            # Stack all tensors to preserve batch consistency\n            # Don't filter out None or empty tensors as this breaks batch sizing\n            try:\n                batch_data[tensor_name] = torch.stack(tensor_list)\n            except ValueError as e:\n                shapes = [\n                    t.shape\n                    if t is not None and hasattr(t, 'shape') else 'None/Invalid'\n                    for t in tensor_list\n                ]\n                raise ValueError(\n                    f\"Failed to stack tensors for field '{tensor_name}'. \"\n                    f\"Tensor shapes: {shapes}. \"\n                    f\"All tensors in a batch must have compatible shapes. \"\n                    f\"Original error: {e}\") from e\n\n    # Process metadata fields into info_list\n    info_list = []\n    for row in rows:\n        info = {}\n        for field in metadata_fields:\n            info[field] = row.get(field, \"\")\n\n        # Add prompt field for backward compatibility\n        info[\"prompt\"] = info.get(\"caption\", \"\")\n        info_list.append(info)\n\n    batch_data['info_list'] = info_list\n\n    # Add caption_text for backward compatibility\n    if info_list and 'caption' in info_list[0]:\n        batch_data['caption_text'] = [info['caption'] for info in info_list]\n\n    return batch_data\n</code></pre> fastvideo.dataset.utils.get_torch_tensors_from_row_dict \u00b6 <pre><code>get_torch_tensors_from_row_dict(\n    row_dict, keys, cfg_rate, rng=None\n) -&gt; dict[str, Any]\n</code></pre> <p>Get the latents and prompts from a row dictionary.</p> Source code in <code>fastvideo/dataset/utils.py</code> <pre><code>def get_torch_tensors_from_row_dict(row_dict,\n                                    keys,\n                                    cfg_rate,\n                                    rng=None) -&gt; dict[str, Any]:\n    \"\"\"\n    Get the latents and prompts from a row dictionary.\n    \"\"\"\n    return_dict = {}\n    for key in keys:\n        shape, bytes = None, None\n        if isinstance(key, tuple):\n            for k in key:\n                try:\n                    shape = row_dict[f\"{k}_shape\"]\n                    bytes = row_dict[f\"{k}_bytes\"]\n                except KeyError:\n                    continue\n            key = key[0]\n            if shape is None or bytes is None:\n                raise ValueError(f\"Key {key} not found in row_dict\")\n        else:\n            shape = row_dict[f\"{key}_shape\"]\n            bytes = row_dict[f\"{key}_bytes\"]\n\n        # TODO (peiyuan): read precision\n        if key == 'text_embedding' and (rng.random()\n                                        if rng else random.random()) &lt; cfg_rate:\n            data = np.zeros((512, 4096), dtype=np.float32)\n        else:\n            data = np.frombuffer(bytes, dtype=np.float32).reshape(shape).copy()\n        data = torch.from_numpy(data)\n        if len(data.shape) == 3:\n            B, L, D = data.shape\n            assert B == 1, \"Batch size must be 1\"\n            data = data.squeeze(0)\n        return_dict[key] = data\n    return return_dict\n</code></pre> fastvideo.dataset.utils.pad \u00b6 <pre><code>pad(\n    t: Tensor, padding_length: int\n) -&gt; tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Pad or crop an embedding [L, D] to exactly padding_length tokens. Return: - [L, D] tensor in pinned CPU memory - [L] attention mask in pinned CPU memory</p> Source code in <code>fastvideo/dataset/utils.py</code> <pre><code>def pad(t: torch.Tensor, padding_length: int) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pad or crop an embedding [L, D] to exactly padding_length tokens.\n    Return:\n    - [L, D] tensor in pinned CPU memory\n    - [L] attention mask in pinned CPU memory\n    \"\"\"\n    L, D = t.shape\n    if padding_length &gt; L:  # pad\n        pad = torch.zeros(padding_length - L, D, dtype=t.dtype, device=t.device)\n        return torch.cat([t, pad], 0), torch.cat(\n            [torch.ones(L), torch.zeros(padding_length - L)], 0)\n    else:  # crop\n        return t[:padding_length], torch.ones(padding_length)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.dataset.validation_dataset","title":"fastvideo.dataset.validation_dataset","text":""},{"location":"api/fastvideo/#fastvideo.dataset.validation_dataset-classes","title":"Classes","text":"fastvideo.dataset.validation_dataset.ValidationDataset \u00b6 <pre><code>ValidationDataset(filename: str)\n</code></pre> <p>               Bases: <code>IterableDataset</code></p> Source code in <code>fastvideo/dataset/validation_dataset.py</code> <pre><code>def __init__(self, filename: str):\n    super().__init__()\n\n    self.filename = pathlib.Path(filename)\n    # get directory of filename\n    self.dir = os.path.abspath(self.filename.parent)\n\n    if not self.filename.exists():\n        raise FileNotFoundError(\n            f\"File {self.filename.as_posix()} does not exist\")\n\n    if self.filename.suffix == \".csv\":\n        data = datasets.load_dataset(\"csv\",\n                                     data_files=self.filename.as_posix(),\n                                     split=\"train\")\n    elif self.filename.suffix == \".json\":\n        data = datasets.load_dataset(\"json\",\n                                     data_files=self.filename.as_posix(),\n                                     split=\"train\",\n                                     field=\"data\")\n    elif self.filename.suffix == \".parquet\":\n        data = datasets.load_dataset(\"parquet\",\n                                     data_files=self.filename.as_posix(),\n                                     split=\"train\")\n    elif self.filename.suffix == \".arrow\":\n        data = datasets.load_dataset(\"arrow\",\n                                     data_files=self.filename.as_posix(),\n                                     split=\"train\")\n    else:\n        _SUPPORTED_FILE_FORMATS = [\".csv\", \".json\", \".parquet\", \".arrow\"]\n        raise ValueError(\n            f\"Unsupported file format {self.filename.suffix} for validation dataset. Supported formats are: {_SUPPORTED_FILE_FORMATS}\"\n        )\n\n    # Get distributed training info\n    self.global_rank = get_world_rank()\n    self.world_size = get_world_size()\n    self.sp_world_size = get_sp_world_size()\n    self.num_sp_groups = self.world_size // self.sp_world_size\n\n    # Convert to list to get total samples\n    self.all_samples = list(data)\n    self.original_total_samples = len(self.all_samples)\n\n    # Extend samples to be a multiple of DP degree (num_sp_groups)\n    remainder = self.original_total_samples % self.num_sp_groups\n    if remainder != 0:\n        samples_to_add = self.num_sp_groups - remainder\n\n        # Duplicate samples cyclically to reach the target\n        additional_samples = []\n        for i in range(samples_to_add):\n            additional_samples.append(\n                self.all_samples[i % self.original_total_samples])\n\n        self.all_samples.extend(additional_samples)\n\n    self.total_samples = len(self.all_samples)\n\n    # Calculate which SP group this rank belongs to\n    self.sp_group_id = self.global_rank // self.sp_world_size\n\n    # Now all SP groups will have equal number of samples\n    self.samples_per_sp_group = self.total_samples // self.num_sp_groups\n\n    # Calculate start and end indices for this SP group\n    self.start_idx = self.sp_group_id * self.samples_per_sp_group\n    self.end_idx = self.start_idx + self.samples_per_sp_group\n\n    # Get samples for this SP group\n    self.sp_group_samples = self.all_samples[self.start_idx:self.end_idx]\n\n    logger.info(\n        \"Rank %s (SP group %s): \"\n        \"Original samples: %s, \"\n        \"Extended samples: %s, \"\n        \"SP group samples: %s, \"\n        \"Range: [%s:%s]\",\n        self.global_rank,\n        self.sp_group_id,\n        self.original_total_samples,\n        self.total_samples,\n        len(self.sp_group_samples),\n        self.start_idx,\n        self.end_idx,\n        local_main_process_only=False)\n</code></pre> Functions\u00b6 fastvideo.dataset.validation_dataset.ValidationDataset.__len__ \u00b6 <pre><code>__len__()\n</code></pre> <p>Return the number of samples for this SP group.</p> Source code in <code>fastvideo/dataset/validation_dataset.py</code> <pre><code>def __len__(self):\n    \"\"\"Return the number of samples for this SP group.\"\"\"\n    return len(self.sp_group_samples)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.dataset.validation_dataset-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideolayers","title":"fastvideo.layers","text":""},{"location":"api/fastvideo/#fastvideo.layers","title":"layers","text":""},{"location":"api/fastvideo/#fastvideo.layers-modules","title":"Modules","text":""},{"location":"api/fastvideo/#fastvideo.layers.activation","title":"fastvideo.layers.activation","text":"<p>Custom activation functions.</p>"},{"location":"api/fastvideo/#fastvideo.layers.activation-classes","title":"Classes","text":"fastvideo.layers.activation.GeluAndMul \u00b6 <pre><code>GeluAndMul(approximate: str = 'none')\n</code></pre> <p>               Bases: <code>CustomOp</code></p> <p>An activation function for GeGLU.</p> <p>The function computes x -&gt; GELU(x[:d]) * x[d:] where d = x.shape[-1] // 2.</p> Shapes <p>x: (batch_size, seq_len, 2 * d) or (num_tokens, 2 * d) return: (batch_size, seq_len, d) or (num_tokens, d)</p> Source code in <code>fastvideo/layers/activation.py</code> <pre><code>def __init__(self, approximate: str = \"none\"):\n    super().__init__()\n    self.approximate = approximate\n    if approximate not in (\"none\", \"tanh\"):\n        raise ValueError(f\"Unknown approximate mode: {approximate}\")\n</code></pre> Functions\u00b6 fastvideo.layers.activation.GeluAndMul.forward_native \u00b6 <pre><code>forward_native(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>PyTorch-native implementation equivalent to forward().</p> Source code in <code>fastvideo/layers/activation.py</code> <pre><code>def forward_native(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"PyTorch-native implementation equivalent to forward().\"\"\"\n    d = x.shape[-1] // 2\n    return F.gelu(x[..., :d], approximate=self.approximate) * x[..., d:]\n</code></pre> fastvideo.layers.activation.NewGELU \u00b6 <pre><code>NewGELU()\n</code></pre> <p>               Bases: <code>CustomOp</code></p> Source code in <code>fastvideo/layers/activation.py</code> <pre><code>def __init__(self):\n    super().__init__()\n</code></pre> Functions\u00b6 fastvideo.layers.activation.NewGELU.forward_native \u00b6 <pre><code>forward_native(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>PyTorch-native implementation equivalent to forward().</p> Source code in <code>fastvideo/layers/activation.py</code> <pre><code>def forward_native(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"PyTorch-native implementation equivalent to forward().\"\"\"\n    c = math.sqrt(2.0 / math.pi)\n    return 0.5 * x * (1.0 + torch.tanh(c *\n                                       (x + 0.044715 * torch.pow(x, 3.0))))\n</code></pre> fastvideo.layers.activation.QuickGELU \u00b6 <pre><code>QuickGELU()\n</code></pre> <p>               Bases: <code>CustomOp</code></p> Source code in <code>fastvideo/layers/activation.py</code> <pre><code>def __init__(self):\n    super().__init__()\n</code></pre> Functions\u00b6 fastvideo.layers.activation.QuickGELU.forward_native \u00b6 <pre><code>forward_native(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>PyTorch-native implementation equivalent to forward().</p> Source code in <code>fastvideo/layers/activation.py</code> <pre><code>def forward_native(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"PyTorch-native implementation equivalent to forward().\"\"\"\n    return x * torch.sigmoid(1.702 * x)\n</code></pre> fastvideo.layers.activation.SiluAndMul \u00b6 <pre><code>SiluAndMul()\n</code></pre> <p>               Bases: <code>CustomOp</code></p> <p>An activation function for SwiGLU.</p> <p>The function computes x -&gt; silu(x[:d]) * x[d:] where d = x.shape[-1] // 2.</p> Shapes <p>x: (num_tokens, 2 * d) or (batch_size, seq_len, 2 * d) return: (num_tokens, d) or (batch_size, seq_len, d)</p> Source code in <code>fastvideo/layers/activation.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__()\n</code></pre> Functions\u00b6 fastvideo.layers.activation.SiluAndMul.forward_native \u00b6 <pre><code>forward_native(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>PyTorch-native implementation equivalent to forward().</p> Source code in <code>fastvideo/layers/activation.py</code> <pre><code>def forward_native(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"PyTorch-native implementation equivalent to forward().\"\"\"\n    d = x.shape[-1] // 2\n    return F.silu(x[..., :d]) * x[..., d:]\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.layers.activation-functions","title":"Functions","text":"fastvideo.layers.activation.get_act_and_mul_fn \u00b6 <pre><code>get_act_and_mul_fn(act_fn_name: str) -&gt; nn.Module\n</code></pre> <p>Get an activation-and-mul (i.e. SiluAndMul) function by name.</p> Source code in <code>fastvideo/layers/activation.py</code> <pre><code>def get_act_and_mul_fn(act_fn_name: str) -&gt; nn.Module:\n    \"\"\"Get an activation-and-mul (i.e. SiluAndMul) function by name.\"\"\"\n    act_fn_name = act_fn_name.lower()\n    if act_fn_name not in _ACTIVATION_AND_MUL_REGISTRY:\n        raise ValueError(\n            f\"Activation function {act_fn_name!r} is not supported.\")\n\n    return _ACTIVATION_AND_MUL_REGISTRY[act_fn_name]()\n</code></pre> fastvideo.layers.activation.get_act_fn \u00b6 <pre><code>get_act_fn(act_fn_name: str) -&gt; nn.Module\n</code></pre> <p>Get an activation function by name.</p> Source code in <code>fastvideo/layers/activation.py</code> <pre><code>def get_act_fn(act_fn_name: str) -&gt; nn.Module:\n    \"\"\"Get an activation function by name.\"\"\"\n    act_fn_name = act_fn_name.lower()\n    if act_fn_name not in _ACTIVATION_REGISTRY:\n        raise ValueError(\n            f\"Activation function {act_fn_name!r} is not supported.\")\n\n    return _ACTIVATION_REGISTRY[act_fn_name]()\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.layers.custom_op","title":"fastvideo.layers.custom_op","text":""},{"location":"api/fastvideo/#fastvideo.layers.custom_op-classes","title":"Classes","text":"fastvideo.layers.custom_op.CustomOp \u00b6 <pre><code>CustomOp()\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Base class for custom ops. Dispatches the forward method to the appropriate backend.</p> Source code in <code>fastvideo/layers/custom_op.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__()\n    self._forward_method = self.dispatch_forward()\n</code></pre> Functions\u00b6 fastvideo.layers.custom_op.CustomOp.default_on <code>staticmethod</code> \u00b6 <pre><code>default_on() -&gt; bool\n</code></pre> <p>On by default if level &lt; CompilationLevel.PIECEWISE Specifying 'all' or 'none' in custom_op takes precedence.</p> Source code in <code>fastvideo/layers/custom_op.py</code> <pre><code>@staticmethod\ndef default_on() -&gt; bool:\n    \"\"\"\n    On by default if level &lt; CompilationLevel.PIECEWISE\n    Specifying 'all' or 'none' in custom_op takes precedence.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.layers.custom_op.CustomOp.forward_native \u00b6 <pre><code>forward_native(*args, **kwargs) -&gt; Any\n</code></pre> <p>PyTorch-native implementation of the forward method. This method is optional. If implemented, it can be used with compilers such as torch.compile or PyTorch XLA. Also, it can be used for testing purposes.</p> Source code in <code>fastvideo/layers/custom_op.py</code> <pre><code>def forward_native(self, *args, **kwargs) -&gt; Any:\n    \"\"\"PyTorch-native implementation of the forward method.\n    This method is optional. If implemented, it can be used with compilers\n    such as torch.compile or PyTorch XLA. Also, it can be used for testing\n    purposes.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.layers.custom_op-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideo.layers.layernorm","title":"fastvideo.layers.layernorm","text":"<p>Custom normalization layers.</p>"},{"location":"api/fastvideo/#fastvideo.layers.layernorm-classes","title":"Classes","text":"fastvideo.layers.layernorm.LayerNormScaleShift \u00b6 <pre><code>LayerNormScaleShift(\n    hidden_size: int,\n    norm_type: str = \"rms\",\n    eps: float = 1e-06,\n    elementwise_affine: bool = False,\n    dtype: dtype = torch.float32,\n    compute_dtype: dtype | None = None,\n    prefix: str = \"\",\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Fused operation that combines LayerNorm with scale and shift operations. This reduces memory bandwidth by combining memory-bound operations.</p> Source code in <code>fastvideo/layers/layernorm.py</code> <pre><code>def __init__(\n    self,\n    hidden_size: int,\n    norm_type: str = \"rms\",\n    eps: float = 1e-6,\n    elementwise_affine: bool = False,\n    dtype: torch.dtype = torch.float32,\n    compute_dtype: torch.dtype | None = None,\n    prefix: str = \"\",\n):\n    super().__init__()\n    self.compute_dtype = compute_dtype\n    if norm_type == \"rms\":\n        self.norm = RMSNorm(hidden_size,\n                            has_weight=elementwise_affine,\n                            eps=eps)\n    elif norm_type == \"layer\":\n        if self.compute_dtype == torch.float32:\n            self.norm = FP32LayerNorm(hidden_size,\n                                      elementwise_affine=elementwise_affine,\n                                      eps=eps)\n        else:\n            self.norm = nn.LayerNorm(hidden_size,\n                                     elementwise_affine=elementwise_affine,\n                                     eps=eps,\n                                     dtype=dtype)\n    else:\n        raise NotImplementedError(f\"Norm type {norm_type} not implemented\")\n</code></pre> Functions\u00b6 fastvideo.layers.layernorm.LayerNormScaleShift.forward \u00b6 <pre><code>forward(\n    x: Tensor, shift: Tensor, scale: Tensor\n) -&gt; torch.Tensor\n</code></pre> <p>Apply ln followed by scale and shift in a single fused operation.</p> Source code in <code>fastvideo/layers/layernorm.py</code> <pre><code>def forward(self, x: torch.Tensor, shift: torch.Tensor,\n            scale: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Apply ln followed by scale and shift in a single fused operation.\"\"\"\n    # x.shape: [batch_size, seq_len, inner_dim]\n    normalized = self.norm(x)\n    if self.compute_dtype == torch.float32:\n        normalized = normalized.float()\n\n    if scale.dim() == 4:\n        # scale.shape: [batch_size, num_frames, 1, inner_dim]\n        num_frames = scale.shape[1]\n        frame_seqlen = normalized.shape[1] // num_frames\n        output = (\n            normalized.unflatten(dim=1, sizes=(num_frames, frame_seqlen)) *\n            (1.0 + scale) + shift).flatten(1, 2)\n    else:\n        # scale.shape: [batch_size, 1, inner_dim]\n        # shift.shape: [batch_size, 1, inner_dim]\n        output = normalized * (1.0 + scale) + shift\n\n    if self.compute_dtype == torch.float32:\n        output = output.to(x.dtype)\n\n    return output\n</code></pre> fastvideo.layers.layernorm.RMSNorm \u00b6 <pre><code>RMSNorm(\n    hidden_size: int,\n    eps: float = 1e-06,\n    dtype: dtype = torch.float32,\n    var_hidden_size: int | None = None,\n    has_weight: bool = True,\n)\n</code></pre> <p>               Bases: <code>CustomOp</code></p> <p>Root mean square normalization.</p> <p>Computes x -&gt; w * x / sqrt(E[x^2] + eps) where w is the learned weight. Refer to https://arxiv.org/abs/1910.07467</p> Source code in <code>fastvideo/layers/layernorm.py</code> <pre><code>def __init__(\n    self,\n    hidden_size: int,\n    eps: float = 1e-6,\n    dtype: torch.dtype = torch.float32,\n    var_hidden_size: int | None = None,\n    has_weight: bool = True,\n) -&gt; None:\n    super().__init__()\n\n    self.hidden_size = hidden_size\n    self.variance_epsilon = eps\n    self.variance_size_override = (None if var_hidden_size == hidden_size\n                                   else var_hidden_size)\n    self.has_weight = has_weight\n\n    from fastvideo.platforms import current_platform\n\n    self.weight = torch.ones(hidden_size) if current_platform.is_cuda_alike(\n    ) else torch.ones(hidden_size, dtype=dtype)\n    if self.has_weight:\n        self.weight = nn.Parameter(self.weight)\n</code></pre> Functions\u00b6 fastvideo.layers.layernorm.RMSNorm.forward_native \u00b6 <pre><code>forward_native(\n    x: Tensor, residual: Tensor | None = None\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>PyTorch-native implementation equivalent to forward().</p> Source code in <code>fastvideo/layers/layernorm.py</code> <pre><code>def forward_native(\n    self,\n    x: torch.Tensor,\n    residual: torch.Tensor | None = None,\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"PyTorch-native implementation equivalent to forward().\"\"\"\n    orig_dtype = x.dtype\n    x = x.to(torch.float32)\n    if residual is not None:\n        x = x + residual.to(torch.float32)\n        residual = x.to(orig_dtype)\n\n    hidden_size = x.shape[-1]\n    if hidden_size != self.hidden_size:\n        raise ValueError(\"Expected hidden_size to be \"\n                         f\"{self.hidden_size}, but found: {hidden_size}\")\n\n    if self.variance_size_override is None:\n        x_var = x\n    else:\n        if hidden_size &lt; self.variance_size_override:\n            raise ValueError(\n                \"Expected hidden_size to be at least \"\n                f\"{self.variance_size_override}, but found: {hidden_size}\")\n\n        x_var = x[:, :, :self.variance_size_override]\n\n    variance = x_var.pow(2).mean(dim=-1, keepdim=True)\n\n    x = x * torch.rsqrt(variance + self.variance_epsilon)\n    x = x.to(orig_dtype)\n    if self.has_weight:\n        x = x * self.weight\n    if residual is None:\n        return x\n    else:\n        return x, residual\n</code></pre> fastvideo.layers.layernorm.ScaleResidual \u00b6 <pre><code>ScaleResidual(prefix: str = '')\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Applies gated residual connection.</p> Source code in <code>fastvideo/layers/layernorm.py</code> <pre><code>def __init__(self, prefix: str = \"\"):\n    super().__init__()\n</code></pre> Functions\u00b6 fastvideo.layers.layernorm.ScaleResidual.forward \u00b6 <pre><code>forward(\n    residual: Tensor, x: Tensor, gate: Tensor\n) -&gt; torch.Tensor\n</code></pre> <p>Apply gated residual connection.</p> Source code in <code>fastvideo/layers/layernorm.py</code> <pre><code>def forward(self, residual: torch.Tensor, x: torch.Tensor,\n            gate: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Apply gated residual connection.\"\"\"\n    # x.shape: [batch_size, seq_len, inner_dim]\n    if gate.dim() == 4:\n        # gate.shape: [batch_size, num_frames, 1, inner_dim]\n        num_frames = gate.shape[1]\n        frame_seqlen = x.shape[1] // num_frames\n        return residual + (x.unflatten(\n            dim=1, sizes=(num_frames, frame_seqlen)) * gate).flatten(1, 2)\n    else:\n        # gate.shape: [batch_size, 1, inner_dim]\n        return residual + x * gate\n</code></pre> fastvideo.layers.layernorm.ScaleResidualLayerNormScaleShift \u00b6 <pre><code>ScaleResidualLayerNormScaleShift(\n    hidden_size: int,\n    norm_type: str = \"rms\",\n    eps: float = 1e-06,\n    elementwise_affine: bool = False,\n    dtype: dtype = torch.float32,\n    compute_dtype: dtype | None = None,\n    prefix: str = \"\",\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Fused operation that combines: 1. Gated residual connection 2. LayerNorm 3. Scale and shift operations</p> <p>This reduces memory bandwidth by combining memory-bound operations.</p> Source code in <code>fastvideo/layers/layernorm.py</code> <pre><code>def __init__(\n    self,\n    hidden_size: int,\n    norm_type: str = \"rms\",\n    eps: float = 1e-6,\n    elementwise_affine: bool = False,\n    dtype: torch.dtype = torch.float32,\n    compute_dtype: torch.dtype | None = None,\n    prefix: str = \"\",\n):\n    super().__init__()\n    if norm_type == \"rms\":\n        self.norm = RMSNorm(hidden_size,\n                            has_weight=elementwise_affine,\n                            eps=eps,\n                            dtype=dtype)\n    elif norm_type == \"layer\":\n        if compute_dtype == torch.float32:\n            self.norm = FP32LayerNorm(hidden_size,\n                                      elementwise_affine=elementwise_affine,\n                                      eps=eps)\n        else:\n            self.norm = nn.LayerNorm(hidden_size,\n                                     elementwise_affine=elementwise_affine,\n                                     eps=eps,\n                                     dtype=dtype)\n    else:\n        raise NotImplementedError(f\"Norm type {norm_type} not implemented\")\n</code></pre> Functions\u00b6 fastvideo.layers.layernorm.ScaleResidualLayerNormScaleShift.forward \u00b6 <pre><code>forward(\n    residual: Tensor,\n    x: Tensor,\n    gate: Tensor | int,\n    shift: Tensor,\n    scale: Tensor,\n) -&gt; tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Apply gated residual connection, followed by layernorm and  scale/shift in a single fused operation.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tuple containing:</p> <code>Tensor</code> <ul> <li>normalized and modulated output</li> </ul> <code>tuple[Tensor, Tensor]</code> <ul> <li>residual value (value after residual connection  but before normalization)</li> </ul> Source code in <code>fastvideo/layers/layernorm.py</code> <pre><code>def forward(self, residual: torch.Tensor, x: torch.Tensor,\n            gate: torch.Tensor | int, shift: torch.Tensor,\n            scale: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Apply gated residual connection, followed by layernorm and \n    scale/shift in a single fused operation.\n\n    Returns:\n        Tuple containing:\n        - normalized and modulated output\n        - residual value (value after residual connection \n          but before normalization)\n    \"\"\"\n    # x.shape: [batch_size, seq_len, inner_dim]\n    # Apply residual connection with gating\n    if isinstance(gate, int):\n        # used by cross-attention, should be 1\n        assert gate == 1\n        residual_output = residual + x\n    elif isinstance(gate, torch.Tensor):\n        if gate.dim() == 4:\n            # gate.shape: [batch_size, num_frames, 1, inner_dim]\n            num_frames = gate.shape[1]\n            frame_seqlen = x.shape[1] // num_frames\n            residual_output = residual + (\n                x.unflatten(dim=1, sizes=(num_frames, frame_seqlen)) *\n                gate).flatten(1, 2)\n        else:\n            # used by bidirectional self attention\n            # gate.shape: [batch_size, 1, inner_dim]\n            residual_output = residual + x * gate\n    else:\n        raise ValueError(f\"Gate type {type(gate)} not supported\")\n    # residual_output.shape: [batch_size, seq_len, inner_dim]\n\n    # Apply normalization\n    normalized = self.norm(residual_output)\n    # Apply scale and shift\n    if isinstance(scale, torch.Tensor) and scale.dim() == 4:\n        # scale.shape: [batch_size, num_frames, 1, inner_dim]\n        # shift.shape: [batch_size, num_frames, 1, inner_dim]\n        num_frames = scale.shape[1]\n        frame_seqlen = normalized.shape[1] // num_frames\n        modulated = (\n            normalized.unflatten(dim=1, sizes=(num_frames, frame_seqlen)) *\n            (1.0 + scale) + shift).flatten(1, 2)\n    else:\n        modulated = normalized * (1.0 + scale) + shift\n    return modulated, residual_output\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.layers.linear","title":"fastvideo.layers.linear","text":""},{"location":"api/fastvideo/#fastvideo.layers.linear-classes","title":"Classes","text":"fastvideo.layers.linear.ColumnParallelLinear \u00b6 <pre><code>ColumnParallelLinear(\n    input_size: int,\n    output_size: int,\n    bias: bool = True,\n    gather_output: bool = False,\n    skip_bias_add: bool = False,\n    params_dtype: dtype | None = None,\n    quant_config: QuantizationConfig | None = None,\n    output_sizes: list[int] | None = None,\n    prefix: str = \"\",\n)\n</code></pre> <p>               Bases: <code>LinearBase</code></p> <p>Linear layer with column parallelism.</p> <p>The linear layer is defined as Y = XA + b. A is parallelized along its second dimension as A = [A_1, ..., A_p].</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>first dimension of matrix A.</p> required <code>output_size</code> <code>int</code> <p>second dimension of matrix A.</p> required <code>bias</code> <code>bool</code> <p>If true, add bias.</p> <code>True</code> <code>gather_output</code> <code>bool</code> <p>If true, call all-gather on output and make Y available            to all GPUs, otherwise, every GPU will have its output            which is Y_i = XA_i</p> <code>False</code> <code>skip_bias_add</code> <code>bool</code> <p>This was added to enable performance optimizations where            bias can be fused with other element-wise operations. we            skip adding bias but instead return it.</p> <code>False</code> <code>params_dtype</code> <code>dtype | None</code> <p>Data type for the parameters.</p> <code>None</code> <code>quant_config</code> <code>QuantizationConfig | None</code> <p>Quantization configure.</p> <code>None</code> <code>output_sizes</code> <code>list[int] | None</code> <p>list of output sizes packed into one output, like for QKV            the list would be size 3.</p> <code>None</code> <code>prefix</code> <code>str</code> <p>The name of the layer in the state dict, including all parents             (e.g. model.layers.0.qkv_proj)</p> <code>''</code> Source code in <code>fastvideo/layers/linear.py</code> <pre><code>def __init__(self,\n             input_size: int,\n             output_size: int,\n             bias: bool = True,\n             gather_output: bool = False,\n             skip_bias_add: bool = False,\n             params_dtype: torch.dtype | None = None,\n             quant_config: QuantizationConfig | None = None,\n             output_sizes: list[int] | None = None,\n             prefix: str = \"\"):\n    # Divide the weight matrix along the last dimension.\n    self.tp_size = get_tp_world_size()\n    self.input_size_per_partition = input_size\n    self.output_size_per_partition = divide(output_size, self.tp_size)\n    self.output_partition_sizes = [self.output_size_per_partition]\n    # If QKV or MergedColumn, use output size of each partition.\n    if hasattr(self, \"output_sizes\"):\n        self.output_partition_sizes = [\n            divide(output_size, self.tp_size)\n            for output_size in self.output_sizes\n        ]\n\n    super().__init__(input_size, output_size, skip_bias_add, params_dtype,\n                     quant_config, prefix)\n\n    self.gather_output = gather_output\n\n    if output_sizes is None:\n        output_sizes = [output_size]\n\n    assert self.quant_method is not None\n    self.quant_method.create_weights(\n        layer=self,\n        input_size_per_partition=self.input_size_per_partition,\n        output_partition_sizes=self.output_partition_sizes,\n        input_size=self.input_size,\n        output_size=self.output_size,\n        params_dtype=self.params_dtype,\n        weight_loader=(\n            self.weight_loader_v2 if self.quant_method.__class__.__name__\n            in WEIGHT_LOADER_V2_SUPPORTED else self.weight_loader))\n    if bias:\n        self.bias = Parameter(\n            torch.empty(\n                self.output_size_per_partition,\n                dtype=params_dtype,\n            ))\n        set_weight_attrs(self.bias, {\n            \"output_dim\": 0,\n            \"weight_loader\": self.weight_loader,\n        })\n    else:\n        self.register_parameter(\"bias\", None)\n</code></pre> fastvideo.layers.linear.LinearBase \u00b6 <pre><code>LinearBase(\n    input_size: int,\n    output_size: int,\n    skip_bias_add: bool = False,\n    params_dtype: dtype | None = None,\n    quant_config: QuantizationConfig | None = None,\n    prefix: str = \"\",\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Base linear layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>input dimension of the linear layer.</p> required <code>output_size</code> <code>int</code> <p>output dimension of the linear layer.</p> required <code>bias</code> <p>If true, add bias.</p> required <code>skip_bias_add</code> <code>bool</code> <p>If true, skip adding bias but instead return it.</p> <code>False</code> <code>params_dtype</code> <code>dtype | None</code> <p>Data type for the parameters.</p> <code>None</code> <code>quant_config</code> <code>QuantizationConfig | None</code> <p>Quantization configure.</p> <code>None</code> Source code in <code>fastvideo/layers/linear.py</code> <pre><code>def __init__(\n    self,\n    input_size: int,\n    output_size: int,\n    skip_bias_add: bool = False,\n    params_dtype: torch.dtype | None = None,\n    quant_config: QuantizationConfig | None = None,\n    prefix: str = \"\",\n):\n    super().__init__()\n\n    # Keep input parameters\n    self.input_size = input_size\n    self.output_size = output_size\n    self.skip_bias_add = skip_bias_add\n    if params_dtype is None:\n        params_dtype = torch.get_default_dtype()\n    self.params_dtype = params_dtype\n    self.quant_config = quant_config\n    self.prefix = prefix\n    if quant_config is None:\n        self.quant_method: QuantizeMethodBase | None = UnquantizedLinearMethod(\n        )\n    else:\n        self.quant_method = quant_config.get_quant_method(self,\n                                                          prefix=prefix)\n</code></pre> fastvideo.layers.linear.LinearMethodBase \u00b6 <p>               Bases: <code>QuantizeMethodBase</code></p> <p>Base class for different (maybe quantized) linear methods.</p> Functions\u00b6 fastvideo.layers.linear.LinearMethodBase.apply <code>abstractmethod</code> \u00b6 <pre><code>apply(\n    layer: Module, x: Tensor, bias: Tensor | None = None\n) -&gt; torch.Tensor\n</code></pre> <p>Apply the weights in layer to the input tensor. Expects create_weights to have been called before on the layer.</p> Source code in <code>fastvideo/layers/linear.py</code> <pre><code>@abstractmethod\ndef apply(self,\n          layer: torch.nn.Module,\n          x: torch.Tensor,\n          bias: torch.Tensor | None = None) -&gt; torch.Tensor:\n    \"\"\"Apply the weights in layer to the input tensor.\n    Expects create_weights to have been called before on the layer.\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.layers.linear.LinearMethodBase.create_weights <code>abstractmethod</code> \u00b6 <pre><code>create_weights(\n    layer: Module,\n    input_size_per_partition: int,\n    output_partition_sizes: list[int],\n    input_size: int,\n    output_size: int,\n    params_dtype: dtype,\n    **extra_weight_attrs\n) -&gt; None\n</code></pre> <p>Create weights for a linear layer.     The weights will be set as attributes of the layer.</p> <p>Parameters:</p> Name Type Description Default <code>layer</code> <code>Module</code> <p>The layer that is using the LinearMethodBase factory.</p> required <code>input_size_per_partition</code> <code>int</code> <p>Size of the weight input dim on rank X.</p> required <code>output_partition_sizes</code> <code>list[int]</code> <p>Sizes of the output dim of each logical  weight on rank X. E.g., output_partition_sizes for QKVLinear is a list contains the width of Wq, Wk, Wv on rank X.</p> required <code>input_size</code> <code>int</code> <p>Size of the input dim of the weight across all ranks.</p> required <code>output_size</code> <code>int</code> <p>Size of the output dim of the weight across all ranks.</p> required <code>params_dtype</code> <code>dtype</code> <p>Datatype of the parameters.</p> required Source code in <code>fastvideo/layers/linear.py</code> <pre><code>@abstractmethod\ndef create_weights(self, layer: torch.nn.Module,\n                   input_size_per_partition: int,\n                   output_partition_sizes: list[int], input_size: int,\n                   output_size: int, params_dtype: torch.dtype,\n                   **extra_weight_attrs) -&gt; None:\n    \"\"\"Create weights for a linear layer. \n       The weights will be set as attributes of the layer.\n\n    Args:\n        layer: The layer that is using the LinearMethodBase factory.\n        input_size_per_partition: Size of the weight input dim on rank X.\n        output_partition_sizes: Sizes of the output dim of each logical \n            weight on rank X. E.g., output_partition_sizes for QKVLinear\n            is a list contains the width of Wq, Wk, Wv on rank X.\n        input_size: Size of the input dim of the weight across all ranks.\n        output_size: Size of the output dim of the weight across all ranks.\n        params_dtype: Datatype of the parameters.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.layers.linear.MergedColumnParallelLinear \u00b6 <pre><code>MergedColumnParallelLinear(\n    input_size: int,\n    output_sizes: list[int],\n    bias: bool = True,\n    gather_output: bool = False,\n    skip_bias_add: bool = False,\n    params_dtype: dtype | None = None,\n    quant_config: QuantizationConfig | None = None,\n    prefix: str = \"\",\n)\n</code></pre> <p>               Bases: <code>ColumnParallelLinear</code></p> <p>Packed linear layers with column parallelism.</p> <p>Similar to ColumnParallelLinear, but the weight matrix is concatenated along the output dimension. When the weight matrix is loaded, the different partitions are sharded separately.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>input dimension of the linear layer.</p> required <code>output_sizes</code> <code>list[int]</code> <p>list of output dimensions of the linear layer.</p> required <code>bias</code> <code>bool</code> <p>If true, add bias.</p> <code>True</code> <code>gather_output</code> <code>bool</code> <p>If true, call all-gather on output and make the output            available to all GPUs, otherwise, every GPU will have            its own output.</p> <code>False</code> <code>skip_bias_add</code> <code>bool</code> <p>This was added to enable performance optimizations where            bias can be fused with other element-wise operations. we            skip adding bias but instead return it.</p> <code>False</code> <code>params_dtype</code> <code>dtype | None</code> <p>Data type for the parameters.</p> <code>None</code> <code>quant_config</code> <code>QuantizationConfig | None</code> <p>Quantization configure.</p> <code>None</code> <code>prefix</code> <code>str</code> <p>The name of the layer in the state dict, including all parents             (e.g. model.layers.0.qkv_proj)</p> <code>''</code> Source code in <code>fastvideo/layers/linear.py</code> <pre><code>def __init__(self,\n             input_size: int,\n             output_sizes: list[int],\n             bias: bool = True,\n             gather_output: bool = False,\n             skip_bias_add: bool = False,\n             params_dtype: torch.dtype | None = None,\n             quant_config: QuantizationConfig | None = None,\n             prefix: str = \"\"):\n    self.output_sizes = output_sizes\n    tp_size = get_tp_world_size()\n    assert all(output_size % tp_size == 0 for output_size in output_sizes)\n    super().__init__(input_size=input_size,\n                     output_size=sum(output_sizes),\n                     bias=bias,\n                     gather_output=gather_output,\n                     skip_bias_add=skip_bias_add,\n                     params_dtype=params_dtype,\n                     quant_config=quant_config,\n                     prefix=prefix)\n</code></pre> fastvideo.layers.linear.QKVParallelLinear \u00b6 <pre><code>QKVParallelLinear(\n    hidden_size: int,\n    head_size: int,\n    total_num_heads: int,\n    total_num_kv_heads: int | None = None,\n    bias: bool = True,\n    skip_bias_add: bool = False,\n    params_dtype: dtype | None = None,\n    quant_config: QuantizationConfig | None = None,\n    prefix: str = \"\",\n)\n</code></pre> <p>               Bases: <code>ColumnParallelLinear</code></p> <p>Linear layers for the attention's QKV transformation.</p> <p>Linear layers for the linear transformation of the query, key, and value vectors in the attention layer. The weight matrix is concatenated along the output dimension. The layer is parallelized along the head dimension. When the number of key/value heads is smaller than the number of query heads (e.g., multi-query/grouped-query attention), the key/value head may be replicated while the query heads are partitioned.</p> <p>Parameters:</p> Name Type Description Default <code>hidden_size</code> <code>int</code> <p>input hidden state size of the transformer.</p> required <code>head_size</code> <code>int</code> <p>size of each attention head.</p> required <code>total_num_heads</code> <code>int</code> <p>total number of attention query heads.</p> required <code>total_num_kv_heads</code> <code>int | None</code> <p>total number of attention key/value heads. If                 None, assume total_num_kv_heads = total_num_heads.</p> <code>None</code> <code>bias</code> <code>bool</code> <p>If true, add bias.</p> <code>True</code> <code>skip_bias_add</code> <code>bool</code> <p>This was added to enable performance optimizations where            bias can be fused with other element-wise operations. we            skip adding bias but instead return it.</p> <code>False</code> <code>params_dtype</code> <code>dtype | None</code> <p>Data type for the parameters.</p> <code>None</code> <code>quant_config</code> <code>QuantizationConfig | None</code> <p>Quantization configure.</p> <code>None</code> <code>prefix</code> <code>str</code> <p>The name of the layer in the state dict, including all parents             (e.g. model.layers.0.qkv_proj)</p> <code>''</code> Source code in <code>fastvideo/layers/linear.py</code> <pre><code>def __init__(self,\n             hidden_size: int,\n             head_size: int,\n             total_num_heads: int,\n             total_num_kv_heads: int | None = None,\n             bias: bool = True,\n             skip_bias_add: bool = False,\n             params_dtype: torch.dtype | None = None,\n             quant_config: QuantizationConfig | None = None,\n             prefix: str = \"\"):\n    self.hidden_size = hidden_size\n    self.head_size = head_size\n    self.total_num_heads = total_num_heads\n    if total_num_kv_heads is None:\n        total_num_kv_heads = total_num_heads\n    self.total_num_kv_heads = total_num_kv_heads\n    # Divide the weight matrix along the last dimension.\n    tp_size = get_tp_world_size()\n    self.num_heads = divide(self.total_num_heads, tp_size)\n    if tp_size &gt;= self.total_num_kv_heads:\n        self.num_kv_heads = 1\n        self.num_kv_head_replicas = divide(tp_size, self.total_num_kv_heads)\n    else:\n        self.num_kv_heads = divide(self.total_num_kv_heads, tp_size)\n        self.num_kv_head_replicas = 1\n    input_size = self.hidden_size\n    output_size = (self.num_heads +\n                   2 * self.num_kv_heads) * tp_size * self.head_size\n    self.output_sizes = [\n        self.num_heads * self.head_size * tp_size,  # q_proj\n        self.num_kv_heads * self.head_size * tp_size,  # k_proj\n        self.num_kv_heads * self.head_size * tp_size,  # v_proj \n    ]\n\n    super().__init__(input_size=input_size,\n                     output_size=output_size,\n                     bias=bias,\n                     gather_output=False,\n                     skip_bias_add=skip_bias_add,\n                     params_dtype=params_dtype,\n                     quant_config=quant_config,\n                     prefix=prefix)\n</code></pre> fastvideo.layers.linear.ReplicatedLinear \u00b6 <pre><code>ReplicatedLinear(\n    input_size: int,\n    output_size: int,\n    bias: bool = True,\n    skip_bias_add: bool = False,\n    params_dtype: dtype | None = None,\n    quant_config: QuantizationConfig | None = None,\n    prefix: str = \"\",\n)\n</code></pre> <p>               Bases: <code>LinearBase</code></p> <p>Replicated linear layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>input dimension of the linear layer.</p> required <code>output_size</code> <code>int</code> <p>output dimension of the linear layer.</p> required <code>bias</code> <code>bool</code> <p>If true, add bias.</p> <code>True</code> <code>skip_bias_add</code> <code>bool</code> <p>If true, skip adding bias but instead return it.</p> <code>False</code> <code>params_dtype</code> <code>dtype | None</code> <p>Data type for the parameters.</p> <code>None</code> <code>quant_config</code> <code>QuantizationConfig | None</code> <p>Quantization configure.</p> <code>None</code> <code>prefix</code> <code>str</code> <p>The name of the layer in the state dict, including all parents             (e.g. model.layers.0.qkv_proj)</p> <code>''</code> Source code in <code>fastvideo/layers/linear.py</code> <pre><code>def __init__(self,\n             input_size: int,\n             output_size: int,\n             bias: bool = True,\n             skip_bias_add: bool = False,\n             params_dtype: torch.dtype | None = None,\n             quant_config: QuantizationConfig | None = None,\n             prefix: str = \"\"):\n    super().__init__(input_size,\n                     output_size,\n                     skip_bias_add,\n                     params_dtype,\n                     quant_config,\n                     prefix=prefix)\n\n    # All the linear layer supports quant method.\n    assert self.quant_method is not None\n    self.quant_method.create_weights(self,\n                                     self.input_size, [self.output_size],\n                                     self.input_size,\n                                     self.output_size,\n                                     self.params_dtype,\n                                     weight_loader=self.weight_loader)\n\n    if bias:\n        self.bias = Parameter(\n            torch.empty(\n                self.output_size,\n                dtype=self.params_dtype,\n            ))\n        set_weight_attrs(self.bias, {\n            \"output_dim\": 0,\n            \"weight_loader\": self.weight_loader,\n        })\n    else:\n        self.register_parameter(\"bias\", None)\n</code></pre> fastvideo.layers.linear.RowParallelLinear \u00b6 <pre><code>RowParallelLinear(\n    input_size: int,\n    output_size: int,\n    bias: bool = True,\n    input_is_parallel: bool = True,\n    skip_bias_add: bool = False,\n    params_dtype: dtype | None = None,\n    reduce_results: bool = True,\n    quant_config: QuantizationConfig | None = None,\n    prefix: str = \"\",\n)\n</code></pre> <p>               Bases: <code>LinearBase</code></p> <p>Linear layer with row parallelism.</p> <p>The linear layer is defined as Y = XA + b. A is parallelized along its first dimension and X along its second dimension as:            -   -           | A_1 |           | .   |       A = | .   |        X = [X_1, ..., X_p]           | .   |           | A_p |            -   - Arguments:     input_size: first dimension of matrix A.     output_size: second dimension of matrix A.     bias: If true, add bias. Note that bias is not parallelized.     input_is_parallel: If true, we assume that the input is already                        split across the GPUs and we do not split                        again.     skip_bias_add: This was added to enable performance optimization where                    bias can be fused with other element-wise operations.                    We skip adding bias but instead return it.     params_dtype: Data type for the parameters.     quant_config: Quantization configure.</p> Source code in <code>fastvideo/layers/linear.py</code> <pre><code>def __init__(self,\n             input_size: int,\n             output_size: int,\n             bias: bool = True,\n             input_is_parallel: bool = True,\n             skip_bias_add: bool = False,\n             params_dtype: torch.dtype | None = None,\n             reduce_results: bool = True,\n             quant_config: QuantizationConfig | None = None,\n             prefix: str = \"\"):\n    # Divide the weight matrix along the first dimension.\n    self.tp_rank = get_tp_rank()\n    self.tp_size = get_tp_world_size()\n    self.input_size_per_partition = divide(input_size, self.tp_size)\n    self.output_size_per_partition = output_size\n    self.output_partition_sizes = [output_size]\n\n    super().__init__(input_size, output_size, skip_bias_add, params_dtype,\n                     quant_config, prefix)\n\n    self.input_is_parallel = input_is_parallel\n    self.reduce_results = reduce_results\n\n    assert self.quant_method is not None\n    self.quant_method.create_weights(\n        layer=self,\n        input_size_per_partition=self.input_size_per_partition,\n        output_partition_sizes=self.output_partition_sizes,\n        input_size=self.input_size,\n        output_size=self.output_size,\n        params_dtype=self.params_dtype,\n        weight_loader=(\n            self.weight_loader_v2 if self.quant_method.__class__.__name__\n            in WEIGHT_LOADER_V2_SUPPORTED else self.weight_loader))\n    if not reduce_results and (bias and not skip_bias_add):\n        raise ValueError(\"When not reduce the results, adding bias to the \"\n                         \"results can lead to incorrect results\")\n\n    if bias:\n        self.bias = Parameter(\n            torch.empty(self.output_size, dtype=params_dtype))\n        set_weight_attrs(self.bias, {\n            \"output_dim\": 0,\n            \"weight_loader\": self.weight_loader,\n        })\n    else:\n        self.register_parameter(\"bias\", None)\n</code></pre> fastvideo.layers.linear.UnquantizedLinearMethod \u00b6 <p>               Bases: <code>LinearMethodBase</code></p> <p>Linear method without quantization.</p>"},{"location":"api/fastvideo/#fastvideo.layers.linear-functions","title":"Functions","text":"fastvideo.layers.linear.adjust_scalar_to_fused_array \u00b6 <pre><code>adjust_scalar_to_fused_array(\n    param: Tensor,\n    loaded_weight: Tensor,\n    shard_id: str | int,\n) -&gt; tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>For fused modules (QKV and MLP) we have an array of length N that holds 1 scale for each \"logical\" matrix. So the param is an array of length N. The loaded_weight corresponds to  one of the shards on disk. Here, we slice the param based on  the shard_id for loading.</p> Source code in <code>fastvideo/layers/linear.py</code> <pre><code>def adjust_scalar_to_fused_array(\n        param: torch.Tensor, loaded_weight: torch.Tensor,\n        shard_id: str | int) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"For fused modules (QKV and MLP) we have an array of length\n    N that holds 1 scale for each \"logical\" matrix. So the param\n    is an array of length N. The loaded_weight corresponds to \n    one of the shards on disk. Here, we slice the param based on \n    the shard_id for loading.\n    \"\"\"\n    qkv_idxs = {\"q\": 0, \"k\": 1, \"v\": 2}\n\n    if isinstance(shard_id, str):\n        shard_id = qkv_idxs[shard_id]\n    elif not isinstance(shard_id, int):\n        raise ValueError(f\"Unknown Shard Id {shard_id}\")\n\n    # AutoFP8 scales do not have a shape\n    # compressed-tensors scales do have a shape\n    if len(loaded_weight.shape) != 0:\n        assert loaded_weight.shape[0] == 1\n        loaded_weight = loaded_weight[0]\n\n    return param[shard_id], loaded_weight\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.layers.mlp","title":"fastvideo.layers.mlp","text":""},{"location":"api/fastvideo/#fastvideo.layers.mlp-classes","title":"Classes","text":"fastvideo.layers.mlp.MLP \u00b6 <pre><code>MLP(\n    input_dim: int,\n    mlp_hidden_dim: int,\n    output_dim: int | None = None,\n    bias: bool = True,\n    act_type: str = \"gelu_pytorch_tanh\",\n    dtype: dtype | None = None,\n    prefix: str = \"\",\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>MLP for DiT blocks, NO gated linear units</p> Source code in <code>fastvideo/layers/mlp.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    mlp_hidden_dim: int,\n    output_dim: int | None = None,\n    bias: bool = True,\n    act_type: str = \"gelu_pytorch_tanh\",\n    dtype: torch.dtype | None = None,\n    prefix: str = \"\",\n):\n    super().__init__()\n    self.fc_in = ReplicatedLinear(\n        input_dim,\n        mlp_hidden_dim,  # For activation func like SiLU that need 2x width\n        bias=bias,\n        params_dtype=dtype)\n\n    self.act = get_act_fn(act_type)\n    if output_dim is None:\n        output_dim = input_dim\n    self.fc_out = ReplicatedLinear(mlp_hidden_dim,\n                                   output_dim,\n                                   bias=bias,\n                                   params_dtype=dtype)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.layers.mlp-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideo.layers.quantization","title":"fastvideo.layers.quantization","text":""},{"location":"api/fastvideo/#fastvideo.layers.quantization-classes","title":"Classes","text":""},{"location":"api/fastvideo/#fastvideo.layers.quantization-functions","title":"Functions","text":"fastvideo.layers.quantization.register_quantization_config \u00b6 <pre><code>register_quantization_config(quantization: str)\n</code></pre> <p>Register a customized vllm quantization config.</p> <p>When a quantization method is not supported by vllm, you can register a customized quantization config to support it.</p> <p>Parameters:</p> Name Type Description Default <code>quantization</code> <code>str</code> <p>The quantization method name.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from fastvideo.layers.quantization import register_quantization_config\n&gt;&gt;&gt; from fastvideo.layers.quantization import get_quantization_config\n&gt;&gt;&gt; from fastvideo.layers.quantization.base_config import QuantizationConfig\n&gt;&gt;&gt;\n&gt;&gt;&gt; @register_quantization_config(\"my_quant\")\n... class MyQuantConfig(QuantizationConfig):\n...     pass\n&gt;&gt;&gt;\n&gt;&gt;&gt; get_quantization_config(\"my_quant\")\n&lt;class 'MyQuantConfig'&gt;\n</code></pre> Source code in <code>fastvideo/layers/quantization/__init__.py</code> <pre><code>def register_quantization_config(quantization: str):\n    \"\"\"Register a customized vllm quantization config.\n\n    When a quantization method is not supported by vllm, you can register a customized\n    quantization config to support it.\n\n    Args:\n        quantization (str): The quantization method name.\n\n    Examples:\n        &gt;&gt;&gt; from fastvideo.layers.quantization import register_quantization_config\n        &gt;&gt;&gt; from fastvideo.layers.quantization import get_quantization_config\n        &gt;&gt;&gt; from fastvideo.layers.quantization.base_config import QuantizationConfig\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; @register_quantization_config(\"my_quant\")\n        ... class MyQuantConfig(QuantizationConfig):\n        ...     pass\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; get_quantization_config(\"my_quant\")\n        &lt;class 'MyQuantConfig'&gt;\n    \"\"\"  # noqa: E501\n\n    def _wrapper(quant_config_cls):\n        if quantization in QUANTIZATION_METHODS:\n            raise ValueError(\n                f\"The quantization method `{quantization}` is already exists.\")\n        if not issubclass(quant_config_cls, QuantizationConfig):\n            raise ValueError(\"The quantization config must be a subclass of \"\n                             \"`QuantizationConfig`.\")\n        _CUSTOMIZED_METHOD_TO_QUANT_CONFIG[quantization] = quant_config_cls\n        QUANTIZATION_METHODS.append(quantization)\n        return quant_config_cls\n\n    return _wrapper\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.layers.quantization-modules","title":"Modules","text":"fastvideo.layers.quantization.base_config \u00b6 Classes\u00b6 fastvideo.layers.quantization.base_config.QuantizationConfig \u00b6 <pre><code>QuantizationConfig()\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for quantization configs.</p> Source code in <code>fastvideo/layers/quantization/base_config.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    # mapping is updated by models as they initialize\n    self.packed_modules_mapping: dict[str, list[str]] = dict()\n</code></pre> Functions\u00b6 fastvideo.layers.quantization.base_config.QuantizationConfig.from_config <code>abstractmethod</code> <code>classmethod</code> \u00b6 <pre><code>from_config(config: dict[str, Any]) -&gt; QuantizationConfig\n</code></pre> <p>Create a config class from the model's quantization config.</p> Source code in <code>fastvideo/layers/quantization/base_config.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_config(cls, config: dict[str, Any]) -&gt; \"QuantizationConfig\":\n    \"\"\"Create a config class from the model's quantization config.\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.layers.quantization.base_config.QuantizationConfig.get_config_filenames <code>abstractmethod</code> <code>staticmethod</code> \u00b6 <pre><code>get_config_filenames() -&gt; list[str]\n</code></pre> <p>List of filenames to search for in the model directory.</p> Source code in <code>fastvideo/layers/quantization/base_config.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef get_config_filenames() -&gt; list[str]:\n    \"\"\"List of filenames to search for in the model directory.\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.layers.quantization.base_config.QuantizationConfig.get_from_keys <code>staticmethod</code> \u00b6 <pre><code>get_from_keys(\n    config: dict[str, Any], keys: list[str]\n) -&gt; Any\n</code></pre> <p>Get a value from the model's quantization config.</p> Source code in <code>fastvideo/layers/quantization/base_config.py</code> <pre><code>@staticmethod\ndef get_from_keys(config: dict[str, Any], keys: list[str]) -&gt; Any:\n    \"\"\"Get a value from the model's quantization config.\"\"\"\n    for key in keys:\n        if key in config:\n            return config[key]\n    raise ValueError(f\"Cannot find any of {keys} in the model's \"\n                     \"quantization config.\")\n</code></pre> fastvideo.layers.quantization.base_config.QuantizationConfig.get_from_keys_or <code>staticmethod</code> \u00b6 <pre><code>get_from_keys_or(\n    config: dict[str, Any], keys: list[str], default: Any\n) -&gt; Any\n</code></pre> <p>Get a optional value from the model's quantization config.</p> Source code in <code>fastvideo/layers/quantization/base_config.py</code> <pre><code>@staticmethod\ndef get_from_keys_or(config: dict[str, Any], keys: list[str],\n                     default: Any) -&gt; Any:\n    \"\"\"Get a optional value from the model's quantization config.\"\"\"\n    try:\n        return QuantizationConfig.get_from_keys(config, keys)\n    except ValueError:\n        return default\n</code></pre> fastvideo.layers.quantization.base_config.QuantizationConfig.get_min_capability <code>abstractmethod</code> <code>classmethod</code> \u00b6 <pre><code>get_min_capability() -&gt; int\n</code></pre> <p>Minimum GPU capability to support the quantization method.</p> <p>E.g., 70 for Volta, 75 for Turing, 80 for Ampere. This requirement is due to the custom CUDA kernels used by the quantization method.</p> Source code in <code>fastvideo/layers/quantization/base_config.py</code> <pre><code>@classmethod\n@abstractmethod\ndef get_min_capability(cls) -&gt; int:\n    \"\"\"Minimum GPU capability to support the quantization method.\n\n    E.g., 70 for Volta, 75 for Turing, 80 for Ampere.\n    This requirement is due to the custom CUDA kernels used by the\n    quantization method.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.layers.quantization.base_config.QuantizationConfig.get_name <code>abstractmethod</code> \u00b6 <pre><code>get_name() -&gt; QuantizationMethods\n</code></pre> <p>Name of the quantization method.</p> Source code in <code>fastvideo/layers/quantization/base_config.py</code> <pre><code>@abstractmethod\ndef get_name(self) -&gt; QuantizationMethods:\n    \"\"\"Name of the quantization method.\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.layers.quantization.base_config.QuantizationConfig.get_quant_method <code>abstractmethod</code> \u00b6 <pre><code>get_quant_method(\n    layer: Module, prefix: str\n) -&gt; QuantizeMethodBase | None\n</code></pre> <p>Get the quantize method to use for the quantized layer.</p> <p>Parameters:</p> Name Type Description Default <code>layer</code> <code>Module</code> <p>The layer for the quant method.</p> required <code>prefix</code> <code>str</code> <p>The full name of the layer in the state dict</p> required <p>Returns:     The quantize method. None if the given layer doesn't support quant     method.</p> Source code in <code>fastvideo/layers/quantization/base_config.py</code> <pre><code>@abstractmethod\ndef get_quant_method(self, layer: torch.nn.Module,\n                     prefix: str) -&gt; QuantizeMethodBase | None:\n    \"\"\"Get the quantize method to use for the quantized layer.\n\n    Args:\n        layer: The layer for the quant method.\n        prefix: The full name of the layer in the state dict\n    Returns:\n        The quantize method. None if the given layer doesn't support quant\n        method.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.layers.quantization.base_config.QuantizationConfig.get_supported_act_dtypes <code>abstractmethod</code> \u00b6 <pre><code>get_supported_act_dtypes() -&gt; list[torch.dtype]\n</code></pre> <p>List of supported activation dtypes.</p> Source code in <code>fastvideo/layers/quantization/base_config.py</code> <pre><code>@abstractmethod\ndef get_supported_act_dtypes(self) -&gt; list[torch.dtype]:\n    \"\"\"List of supported activation dtypes.\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.layers.quantization.base_config.QuantizationConfig.override_quantization_method <code>classmethod</code> \u00b6 <pre><code>override_quantization_method(\n    hf_quant_cfg, user_quant\n) -&gt; QuantizationMethods | None\n</code></pre> <p>Detects if this quantization method can support a given checkpoint format by overriding the user specified quantization method --  this method should only be overwritten by subclasses in exceptional  circumstances</p> Source code in <code>fastvideo/layers/quantization/base_config.py</code> <pre><code>@classmethod\ndef override_quantization_method(cls, hf_quant_cfg,\n                                 user_quant) -&gt; QuantizationMethods | None:\n    \"\"\"\n       Detects if this quantization method can support a given checkpoint\n       format by overriding the user specified quantization method -- \n       this method should only be overwritten by subclasses in exceptional \n       circumstances\n    \"\"\"\n    return None\n</code></pre> fastvideo.layers.quantization.base_config.QuantizeMethodBase \u00b6 <p>               Bases: <code>ABC</code></p> <p>Base class for different quantized methods.</p> Functions\u00b6 fastvideo.layers.quantization.base_config.QuantizeMethodBase.apply <code>abstractmethod</code> \u00b6 <pre><code>apply(layer: Module, *args, **kwargs) -&gt; torch.Tensor\n</code></pre> <p>Apply the weights in layer to the input tensor.</p> <p>Expects create_weights to have been called before on the layer.</p> Source code in <code>fastvideo/layers/quantization/base_config.py</code> <pre><code>@abstractmethod\ndef apply(self, layer: torch.nn.Module, *args, **kwargs) -&gt; torch.Tensor:\n    \"\"\"Apply the weights in layer to the input tensor.\n\n    Expects create_weights to have been called before on the layer.\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.layers.quantization.base_config.QuantizeMethodBase.create_weights <code>abstractmethod</code> \u00b6 <pre><code>create_weights(\n    layer: Module, *weight_args, **extra_weight_attrs\n)\n</code></pre> <p>Create weights for a layer.</p> <p>The weights will be set as attributes of the layer.</p> Source code in <code>fastvideo/layers/quantization/base_config.py</code> <pre><code>@abstractmethod\ndef create_weights(self, layer: torch.nn.Module, *weight_args,\n                   **extra_weight_attrs):\n    \"\"\"Create weights for a layer.\n\n    The weights will be set as attributes of the layer.\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.layers.quantization.base_config.QuantizeMethodBase.embedding \u00b6 <pre><code>embedding(layer: Module, *args, **kwargs) -&gt; torch.Tensor\n</code></pre> <p>Gather embeddings in the layer based on indices in the input tensor.</p> <p>Expects create_weights to have been called before on the layer.</p> Source code in <code>fastvideo/layers/quantization/base_config.py</code> <pre><code>def embedding(self, layer: torch.nn.Module, *args,\n              **kwargs) -&gt; torch.Tensor:\n    \"\"\"Gather embeddings in the layer based on indices in the input tensor.\n\n    Expects create_weights to have been called before on the layer.\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.layers.quantization.base_config.QuantizeMethodBase.process_weights_after_loading \u00b6 <pre><code>process_weights_after_loading(layer: Module) -&gt; None\n</code></pre> <p>Process the weight after loading.</p> <p>This can be used for example, to transpose weights for computation.</p> Source code in <code>fastvideo/layers/quantization/base_config.py</code> <pre><code>def process_weights_after_loading(self, layer: nn.Module) -&gt; None:\n    \"\"\"Process the weight after loading.\n\n    This can be used for example, to transpose weights for computation.\n    \"\"\"\n    return\n</code></pre> Functions\u00b6 fastvideo.layers.quantization.base_config.method_has_implemented_embedding \u00b6 <pre><code>method_has_implemented_embedding(\n    method_class: type[QuantizeMethodBase],\n) -&gt; bool\n</code></pre> <p>Not all quant methods have embedding implemented, so we need to check that it exists for our given method. We check this by making sure the function has been changed from the base implementation.</p> Source code in <code>fastvideo/layers/quantization/base_config.py</code> <pre><code>def method_has_implemented_embedding(\n        method_class: type[QuantizeMethodBase]) -&gt; bool:\n    \"\"\"\n    Not all quant methods have embedding implemented, so we need to check that\n    it exists for our given method. We check this by making sure the function\n    has been changed from the base implementation.\n    \"\"\"\n    base_embedding = inspect.getattr_static(QuantizeMethodBase, \"embedding\",\n                                            None)\n    class_embedding = inspect.getattr_static(method_class, \"embedding\", None)\n\n    return (class_embedding is not None\n            and class_embedding is not base_embedding)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.layers.rotary_embedding","title":"fastvideo.layers.rotary_embedding","text":"<p>Rotary Positional Embeddings.</p>"},{"location":"api/fastvideo/#fastvideo.layers.rotary_embedding-classes","title":"Classes","text":"fastvideo.layers.rotary_embedding.RotaryEmbedding \u00b6 <pre><code>RotaryEmbedding(\n    head_size: int,\n    rotary_dim: int,\n    max_position_embeddings: int,\n    base: int | float,\n    is_neox_style: bool,\n    dtype: dtype,\n)\n</code></pre> <p>               Bases: <code>CustomOp</code></p> <p>Original rotary positional embedding.</p> Source code in <code>fastvideo/layers/rotary_embedding.py</code> <pre><code>def __init__(\n    self,\n    head_size: int,\n    rotary_dim: int,\n    max_position_embeddings: int,\n    base: int | float,\n    is_neox_style: bool,\n    dtype: torch.dtype,\n) -&gt; None:\n    super().__init__()\n    self.head_size = head_size\n    self.rotary_dim = rotary_dim\n    self.max_position_embeddings = max_position_embeddings\n    self.base = base\n    self.is_neox_style = is_neox_style\n    self.dtype = dtype\n\n    cache = self._compute_cos_sin_cache()\n    cache = cache.to(dtype)\n    self.cos_sin_cache: torch.Tensor\n    self.register_buffer(\"cos_sin_cache\", cache, persistent=False)\n</code></pre> Functions\u00b6 fastvideo.layers.rotary_embedding.RotaryEmbedding.forward_native \u00b6 <pre><code>forward_native(\n    positions: Tensor,\n    query: Tensor,\n    key: Tensor,\n    offsets: Tensor | None = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>A PyTorch-native implementation of forward().</p> Source code in <code>fastvideo/layers/rotary_embedding.py</code> <pre><code>def forward_native(\n    self,\n    positions: torch.Tensor,\n    query: torch.Tensor,\n    key: torch.Tensor,\n    offsets: torch.Tensor | None = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"A PyTorch-native implementation of forward().\"\"\"\n    if offsets is not None:\n        positions = positions + offsets\n    positions = positions.flatten()\n    num_tokens = positions.shape[0]\n    cos_sin = self.cos_sin_cache.index_select(0, positions)\n    cos, sin = cos_sin.chunk(2, dim=-1)\n\n    query_shape = query.shape\n    query = query.view(num_tokens, -1, self.head_size)\n    query_rot = query[..., :self.rotary_dim]\n    query_pass = query[..., self.rotary_dim:]\n    query_rot = _apply_rotary_emb(query_rot, cos, sin, self.is_neox_style)\n    query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)\n\n    key_shape = key.shape\n    key = key.view(num_tokens, -1, self.head_size)\n    key_rot = key[..., :self.rotary_dim]\n    key_pass = key[..., self.rotary_dim:]\n    key_rot = _apply_rotary_emb(key_rot, cos, sin, self.is_neox_style)\n    key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)\n    return query, key\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.layers.rotary_embedding-functions","title":"Functions","text":"fastvideo.layers.rotary_embedding.get_1d_rotary_pos_embed \u00b6 <pre><code>get_1d_rotary_pos_embed(\n    dim: int,\n    pos: FloatTensor | int,\n    theta: float = 10000.0,\n    theta_rescale_factor: float = 1.0,\n    interpolation_factor: float = 1.0,\n    dtype: dtype = torch.float32,\n) -&gt; tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Precompute the frequency tensor for complex exponential (cis) with given dimensions. (Note: <code>cis</code> means <code>cos + i * sin</code>, where i is the imaginary unit.)</p> <p>This function calculates a frequency tensor with complex exponential using the given dimension 'dim' and the end index 'end'. The 'theta' parameter scales the frequencies.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Dimension of the frequency tensor.</p> required <code>pos</code> <code>int or FloatTensor</code> <p>Position indices for the frequency tensor. [S] or scalar</p> required <code>theta</code> <code>float</code> <p>Scaling factor for frequency computation. Defaults to 10000.0.</p> <code>10000.0</code> <code>theta_rescale_factor</code> <code>float</code> <p>Rescale factor for theta. Defaults to 1.0.</p> <code>1.0</code> <code>interpolation_factor</code> <code>float</code> <p>Factor to scale positions. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>freqs_cos, freqs_sin: Precomputed frequency tensor with real and imaginary parts separately. [S, D]</p> Source code in <code>fastvideo/layers/rotary_embedding.py</code> <pre><code>def get_1d_rotary_pos_embed(\n    dim: int,\n    pos: torch.FloatTensor | int,\n    theta: float = 10000.0,\n    theta_rescale_factor: float = 1.0,\n    interpolation_factor: float = 1.0,\n    dtype: torch.dtype = torch.float32,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Precompute the frequency tensor for complex exponential (cis) with given dimensions.\n    (Note: `cis` means `cos + i * sin`, where i is the imaginary unit.)\n\n    This function calculates a frequency tensor with complex exponential using the given dimension 'dim'\n    and the end index 'end'. The 'theta' parameter scales the frequencies.\n\n    Args:\n        dim (int): Dimension of the frequency tensor.\n        pos (int or torch.FloatTensor): Position indices for the frequency tensor. [S] or scalar\n        theta (float, optional): Scaling factor for frequency computation. Defaults to 10000.0.\n        theta_rescale_factor (float, optional): Rescale factor for theta. Defaults to 1.0.\n        interpolation_factor (float, optional): Factor to scale positions. Defaults to 1.0.\n\n    Returns:\n        freqs_cos, freqs_sin: Precomputed frequency tensor with real and imaginary parts separately. [S, D]\n    \"\"\"\n    if isinstance(pos, int):\n        pos = torch.arange(pos).float()\n\n    # proposed by reddit user bloc97, to rescale rotary embeddings to longer sequence length without fine-tuning\n    # has some connection to NTK literature\n    if theta_rescale_factor != 1.0:\n        theta *= theta_rescale_factor**(dim / (dim - 2))\n\n    freqs = 1.0 / (theta**(torch.arange(0, dim, 2)[:(dim // 2)].to(dtype) / dim)\n                   )  # [D/2]\n    freqs = torch.outer(pos * interpolation_factor, freqs)  # [S, D/2]\n    freqs_cos = freqs.cos()  # [S, D/2]\n    freqs_sin = freqs.sin()  # [S, D/2]\n    return freqs_cos, freqs_sin\n</code></pre> fastvideo.layers.rotary_embedding.get_meshgrid_nd \u00b6 <pre><code>get_meshgrid_nd(\n    start: int | tuple[int, ...],\n    *args: int | tuple[int, ...],\n    dim: int = 2\n) -&gt; torch.Tensor\n</code></pre> <p>Get n-D meshgrid with start, stop and num.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int or tuple</code> <p>If len(args) == 0, start is num; If len(args) == 1, start is start, args[0] is stop, step is 1; If len(args) == 2, start is start, args[0] is stop, args[1] is num. For n-dim, start/stop/num should be int or n-tuple. If n-tuple is provided, the meshgrid will be stacked following the dim order in n-tuples.</p> required <code>*args</code> <code>int | tuple[int, ...]</code> <p>See above.</p> <code>()</code> <code>dim</code> <code>int</code> <p>Dimension of the meshgrid. Defaults to 2.</p> <code>2</code> <p>Returns:</p> Name Type Description <code>grid</code> <code>ndarray</code> <p>[dim, ...]</p> Source code in <code>fastvideo/layers/rotary_embedding.py</code> <pre><code>def get_meshgrid_nd(start: int | tuple[int, ...],\n                    *args: int | tuple[int, ...],\n                    dim: int = 2) -&gt; torch.Tensor:\n    \"\"\"\n    Get n-D meshgrid with start, stop and num.\n\n    Args:\n        start (int or tuple): If len(args) == 0, start is num; If len(args) == 1, start is start, args[0] is stop,\n            step is 1; If len(args) == 2, start is start, args[0] is stop, args[1] is num. For n-dim, start/stop/num\n            should be int or n-tuple. If n-tuple is provided, the meshgrid will be stacked following the dim order in\n            n-tuples.\n        *args: See above.\n        dim (int): Dimension of the meshgrid. Defaults to 2.\n\n    Returns:\n        grid (np.ndarray): [dim, ...]\n    \"\"\"\n    if len(args) == 0:\n        # start is grid_size\n        num = _to_tuple(start, dim=dim)\n        start = (0, ) * dim\n        stop = num\n    elif len(args) == 1:\n        # start is start, args[0] is stop, step is 1\n        start = _to_tuple(start, dim=dim)\n        stop = _to_tuple(args[0], dim=dim)\n        num = tuple(stop[i] - start[i] for i in range(dim))\n    elif len(args) == 2:\n        # start is start, args[0] is stop, args[1] is num\n        start = _to_tuple(start, dim=dim)  # Left-Top       eg: 12,0\n        stop = _to_tuple(args[0], dim=dim)  # Right-Bottom   eg: 20,32\n        num = _to_tuple(args[1], dim=dim)  # Target Size    eg: 32,124\n    else:\n        raise ValueError(f\"len(args) should be 0, 1 or 2, but got {len(args)}\")\n\n    # PyTorch implement of np.linspace(start[i], stop[i], num[i], endpoint=False)\n    axis_grid = []\n    for i in range(dim):\n        a, b, n = start[i], stop[i], num[i]\n        g = torch.linspace(a, b, n + 1, dtype=torch.float32)[:n]\n        axis_grid.append(g)\n    grid = torch.meshgrid(*axis_grid, indexing=\"ij\")  # dim x [W, H, D]\n    grid = torch.stack(grid, dim=0)  # [dim, W, H, D]\n\n    return grid\n</code></pre> fastvideo.layers.rotary_embedding.get_nd_rotary_pos_embed \u00b6 <pre><code>get_nd_rotary_pos_embed(\n    rope_dim_list,\n    start,\n    *args,\n    theta=10000.0,\n    theta_rescale_factor: float | list[float] = 1.0,\n    interpolation_factor: float | list[float] = 1.0,\n    shard_dim: int = 0,\n    sp_rank: int = 0,\n    sp_world_size: int = 1,\n    dtype: dtype = torch.float32,\n    start_frame: int = 0\n) -&gt; tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>This is a n-d version of precompute_freqs_cis, which is a RoPE for tokens with n-d structure. Supports sequence parallelism by allowing sharding of a specific dimension.</p> <p>Parameters:</p> Name Type Description Default <code>rope_dim_list</code> <code>list of int</code> <p>Dimension of each rope. len(rope_dim_list) should equal to n. sum(rope_dim_list) should equal to head_dim of attention layer.</p> required <code>start</code> <code>int | tuple of int | list of int</code> <p>If len(args) == 0, start is num; If len(args) == 1, start is start, args[0] is stop, step is 1; If len(args) == 2, start is start, args[0] is stop, args[1] is num.</p> required <code>*args</code> <p>See above.</p> <code>()</code> <code>theta</code> <code>float</code> <p>Scaling factor for frequency computation. Defaults to 10000.0.</p> <code>10000.0</code> <code>theta_rescale_factor</code> <code>float</code> <p>Rescale factor for theta. Defaults to 1.0.</p> <code>1.0</code> <code>interpolation_factor</code> <code>float</code> <p>Factor to scale positions. Defaults to 1.0.</p> <code>1.0</code> <code>shard_dim</code> <code>int</code> <p>Which dimension to shard for sequence parallelism. Defaults to 0.</p> <code>0</code> <code>sp_rank</code> <code>int</code> <p>Rank in the sequence parallel group. Defaults to 0.</p> <code>0</code> <code>sp_world_size</code> <code>int</code> <p>World size of the sequence parallel group. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>Tuple[torch.Tensor, torch.Tensor]: (cos, sin) tensors of shape [HW, D/2]</p> Source code in <code>fastvideo/layers/rotary_embedding.py</code> <pre><code>def get_nd_rotary_pos_embed(\n    rope_dim_list,\n    start,\n    *args,\n    theta=10000.0,\n    theta_rescale_factor: float | list[float] = 1.0,\n    interpolation_factor: float | list[float] = 1.0,\n    shard_dim: int = 0,\n    sp_rank: int = 0,\n    sp_world_size: int = 1,\n    dtype: torch.dtype = torch.float32,\n    start_frame: int = 0,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    This is a n-d version of precompute_freqs_cis, which is a RoPE for tokens with n-d structure.\n    Supports sequence parallelism by allowing sharding of a specific dimension.\n\n    Args:\n        rope_dim_list (list of int): Dimension of each rope. len(rope_dim_list) should equal to n.\n            sum(rope_dim_list) should equal to head_dim of attention layer.\n        start (int | tuple of int | list of int): If len(args) == 0, start is num; If len(args) == 1, start is start,\n            args[0] is stop, step is 1; If len(args) == 2, start is start, args[0] is stop, args[1] is num.\n        *args: See above.\n        theta (float): Scaling factor for frequency computation. Defaults to 10000.0.\n        theta_rescale_factor (float): Rescale factor for theta. Defaults to 1.0.\n        interpolation_factor (float): Factor to scale positions. Defaults to 1.0.\n        shard_dim (int): Which dimension to shard for sequence parallelism. Defaults to 0.\n        sp_rank (int): Rank in the sequence parallel group. Defaults to 0.\n        sp_world_size (int): World size of the sequence parallel group. Defaults to 1.\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor]: (cos, sin) tensors of shape [HW, D/2]\n    \"\"\"\n    # Get the full grid\n    full_grid = get_meshgrid_nd(\n        start, *args, dim=len(rope_dim_list))  # [3, W, H, D] / [2, W, H]\n\n    if start_frame &gt; 0:\n        full_grid[0] += start_frame\n\n    # Shard the grid if using sequence parallelism (sp_world_size &gt; 1)\n    assert shard_dim &lt; len(\n        rope_dim_list\n    ), f\"shard_dim {shard_dim} must be less than number of dimensions {len(rope_dim_list)}\"\n    if sp_world_size &gt; 1:\n        # Get the shape of the full grid\n        grid_shape = list(full_grid.shape[1:])\n\n        # Ensure the dimension to shard is divisible by sp_world_size\n        assert grid_shape[shard_dim] % sp_world_size == 0, (\n            f\"Dimension {shard_dim} with size {grid_shape[shard_dim]} is not divisible \"\n            f\"by sequence parallel world size {sp_world_size}\")\n\n        # Compute the start and end indices for this rank's shard\n        shard_size = grid_shape[shard_dim] // sp_world_size\n        start_idx = sp_rank * shard_size\n        end_idx = (sp_rank + 1) * shard_size\n\n        # Create slicing indices for each dimension\n        slice_indices = [slice(None) for _ in range(len(grid_shape))]\n        slice_indices[shard_dim] = slice(start_idx, end_idx)\n\n        # Shard the grid\n        # Update grid shape for the sharded dimension\n        grid_shape[shard_dim] = grid_shape[shard_dim] // sp_world_size\n        grid = torch.empty((len(rope_dim_list), ) + tuple(grid_shape),\n                           dtype=full_grid.dtype)\n        for i in range(len(rope_dim_list)):\n            grid[i] = full_grid[i][tuple(slice_indices)]\n    else:\n        grid = full_grid\n\n    if isinstance(theta_rescale_factor, int | float):\n        theta_rescale_factor = [theta_rescale_factor] * len(rope_dim_list)\n    elif isinstance(theta_rescale_factor,\n                    list) and len(theta_rescale_factor) == 1:\n        theta_rescale_factor = [theta_rescale_factor[0]] * len(rope_dim_list)\n    assert len(theta_rescale_factor) == len(\n        rope_dim_list\n    ), \"len(theta_rescale_factor) should equal to len(rope_dim_list)\"\n\n    if isinstance(interpolation_factor, int | float):\n        interpolation_factor = [interpolation_factor] * len(rope_dim_list)\n    elif isinstance(interpolation_factor,\n                    list) and len(interpolation_factor) == 1:\n        interpolation_factor = [interpolation_factor[0]] * len(rope_dim_list)\n    assert len(interpolation_factor) == len(\n        rope_dim_list\n    ), \"len(interpolation_factor) should equal to len(rope_dim_list)\"\n\n    # use 1/ndim of dimensions to encode grid_axis\n    embs = []\n    for i in range(len(rope_dim_list)):\n        emb = get_1d_rotary_pos_embed(\n            rope_dim_list[i],\n            grid[i].reshape(-1),\n            theta,\n            theta_rescale_factor=theta_rescale_factor[i],\n            interpolation_factor=interpolation_factor[i],\n            dtype=dtype,\n        )  # 2 x [WHD, rope_dim_list[i]]\n        embs.append(emb)\n\n    cos = torch.cat([emb[0] for emb in embs], dim=1)  # (WHD, D/2)\n    sin = torch.cat([emb[1] for emb in embs], dim=1)  # (WHD, D/2)\n    return cos, sin\n</code></pre> fastvideo.layers.rotary_embedding.get_rotary_pos_embed \u00b6 <pre><code>get_rotary_pos_embed(\n    rope_sizes,\n    hidden_size,\n    heads_num,\n    rope_dim_list,\n    rope_theta,\n    theta_rescale_factor=1.0,\n    interpolation_factor=1.0,\n    shard_dim: int = 0,\n    dtype: dtype = torch.float32,\n    start_frame: int = 0,\n) -&gt; tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Generate rotary positional embeddings for the given sizes.</p> <p>Parameters:</p> Name Type Description Default <code>rope_sizes</code> <p>Tuple of dimensions (t, h, w)</p> required <code>hidden_size</code> <p>Hidden dimension size</p> required <code>heads_num</code> <p>Number of attention heads</p> required <code>rope_dim_list</code> <p>List of dimensions for each axis, or None</p> required <code>rope_theta</code> <p>Base for frequency calculations</p> required <code>theta_rescale_factor</code> <p>Rescale factor for theta. Defaults to 1.0</p> <code>1.0</code> <code>interpolation_factor</code> <p>Factor to scale positions. Defaults to 1.0</p> <code>1.0</code> <code>shard_dim</code> <code>int</code> <p>Which dimension to shard for sequence parallelism. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>Tuple of (cos, sin) tensors for rotary embeddings</p> Source code in <code>fastvideo/layers/rotary_embedding.py</code> <pre><code>def get_rotary_pos_embed(\n    rope_sizes,\n    hidden_size,\n    heads_num,\n    rope_dim_list,\n    rope_theta,\n    theta_rescale_factor=1.0,\n    interpolation_factor=1.0,\n    shard_dim: int = 0,\n    dtype: torch.dtype = torch.float32,\n    start_frame: int = 0,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Generate rotary positional embeddings for the given sizes.\n\n    Args:\n        rope_sizes: Tuple of dimensions (t, h, w)\n        hidden_size: Hidden dimension size\n        heads_num: Number of attention heads\n        rope_dim_list: List of dimensions for each axis, or None\n        rope_theta: Base for frequency calculations\n        theta_rescale_factor: Rescale factor for theta. Defaults to 1.0\n        interpolation_factor: Factor to scale positions. Defaults to 1.0\n        shard_dim: Which dimension to shard for sequence parallelism. Defaults to 0.\n\n    Returns:\n        Tuple of (cos, sin) tensors for rotary embeddings\n    \"\"\"\n\n    target_ndim = 3\n    head_dim = hidden_size // heads_num\n\n    if rope_dim_list is None:\n        rope_dim_list = [head_dim // target_ndim for _ in range(target_ndim)]\n\n    assert sum(\n        rope_dim_list\n    ) == head_dim, \"sum(rope_dim_list) should equal to head_dim of attention layer\"\n\n    # Get SP info\n    sp_group = get_sp_group()\n    sp_rank = sp_group.rank_in_group\n    sp_world_size = sp_group.world_size\n\n    freqs_cos, freqs_sin = get_nd_rotary_pos_embed(\n        rope_dim_list,\n        rope_sizes,\n        theta=rope_theta,\n        theta_rescale_factor=theta_rescale_factor,\n        interpolation_factor=interpolation_factor,\n        shard_dim=shard_dim,\n        sp_rank=sp_rank,\n        sp_world_size=sp_world_size,\n        dtype=dtype,\n        start_frame=start_frame,\n    )\n    return freqs_cos, freqs_sin\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.layers.utils","title":"fastvideo.layers.utils","text":"<p>Utility methods for model layers.</p>"},{"location":"api/fastvideo/#fastvideo.layers.visual_embedding","title":"fastvideo.layers.visual_embedding","text":""},{"location":"api/fastvideo/#fastvideo.layers.visual_embedding-classes","title":"Classes","text":"fastvideo.layers.visual_embedding.ModulateProjection \u00b6 <pre><code>ModulateProjection(\n    hidden_size: int,\n    factor: int = 2,\n    act_layer: str = \"silu\",\n    dtype: dtype | None = None,\n    prefix: str = \"\",\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Modulation layer for DiT blocks.</p> Source code in <code>fastvideo/layers/visual_embedding.py</code> <pre><code>def __init__(\n    self,\n    hidden_size: int,\n    factor: int = 2,\n    act_layer: str = \"silu\",\n    dtype: torch.dtype | None = None,\n    prefix: str = \"\",\n):\n    super().__init__()\n    self.factor = factor\n    self.hidden_size = hidden_size\n    self.linear = ReplicatedLinear(hidden_size,\n                                   hidden_size * factor,\n                                   bias=True,\n                                   params_dtype=dtype)\n    self.act = get_act_fn(act_layer)\n</code></pre> fastvideo.layers.visual_embedding.PatchEmbed \u00b6 <pre><code>PatchEmbed(\n    patch_size=16,\n    in_chans=3,\n    embed_dim=768,\n    norm_layer=None,\n    flatten=True,\n    bias=True,\n    dtype=None,\n    prefix: str = \"\",\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>2D Image to Patch Embedding</p> <p>Image to Patch Embedding using Conv2d</p> <p>A convolution based approach to patchifying a 2D image w/ embedding projection.</p> <p>Based on the impl in https://github.com/google-research/vision_transformer</p> <p>Hacked together by / Copyright 2020 Ross Wightman</p> <p>Remove the _assert function in forward function to be compatible with multi-resolution images.</p> Source code in <code>fastvideo/layers/visual_embedding.py</code> <pre><code>def __init__(self,\n             patch_size=16,\n             in_chans=3,\n             embed_dim=768,\n             norm_layer=None,\n             flatten=True,\n             bias=True,\n             dtype=None,\n             prefix: str = \"\"):\n    super().__init__()\n    # Convert patch_size to 2-tuple\n    if isinstance(patch_size, list | tuple):\n        if len(patch_size) == 1:\n            patch_size = (patch_size[0], patch_size[0])\n    else:\n        patch_size = (patch_size, patch_size)\n\n    self.patch_size = patch_size\n    self.flatten = flatten\n\n    self.proj = nn.Conv3d(in_chans,\n                          embed_dim,\n                          kernel_size=patch_size,\n                          stride=patch_size,\n                          bias=bias,\n                          dtype=dtype)\n    self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n</code></pre> fastvideo.layers.visual_embedding.TimestepEmbedder \u00b6 <pre><code>TimestepEmbedder(\n    hidden_size,\n    act_layer=\"silu\",\n    frequency_embedding_size=256,\n    max_period=10000,\n    dtype=None,\n    freq_dtype=torch.float32,\n    prefix: str = \"\",\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Embeds scalar timesteps into vector representations.</p> Source code in <code>fastvideo/layers/visual_embedding.py</code> <pre><code>def __init__(\n    self,\n    hidden_size,\n    act_layer=\"silu\",\n    frequency_embedding_size=256,\n    max_period=10000,\n    dtype=None,\n    freq_dtype=torch.float32,\n    prefix: str = \"\",\n):\n    super().__init__()\n    self.frequency_embedding_size = frequency_embedding_size\n    self.max_period = max_period\n\n    self.mlp = MLP(frequency_embedding_size,\n                   hidden_size,\n                   hidden_size,\n                   act_type=act_layer,\n                   dtype=dtype)\n    self.freq_dtype = freq_dtype\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.layers.visual_embedding-functions","title":"Functions","text":"fastvideo.layers.visual_embedding.timestep_embedding \u00b6 <pre><code>timestep_embedding(\n    t: Tensor,\n    dim: int,\n    max_period: int = 10000,\n    dtype: dtype = torch.float32,\n) -&gt; torch.Tensor\n</code></pre> <p>Create sinusoidal timestep embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>Tensor of shape [B] with timesteps</p> required <code>dim</code> <code>int</code> <p>Embedding dimension</p> required <code>max_period</code> <code>int</code> <p>Controls the minimum frequency of the embeddings</p> <code>10000</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape [B, dim] with embeddings</p> Source code in <code>fastvideo/layers/visual_embedding.py</code> <pre><code>def timestep_embedding(t: torch.Tensor,\n                       dim: int,\n                       max_period: int = 10000,\n                       dtype: torch.dtype = torch.float32) -&gt; torch.Tensor:\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n\n    Args:\n        t: Tensor of shape [B] with timesteps\n        dim: Embedding dimension\n        max_period: Controls the minimum frequency of the embeddings\n\n    Returns:\n        Tensor of shape [B, dim] with embeddings\n    \"\"\"\n    half = dim // 2\n    freqs = torch.exp(-math.log(max_period) *\n                      torch.arange(start=0, end=half, dtype=dtype) /\n                      half).to(device=t.device)\n    args = t[:, None].float() * freqs[None]\n    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    if dim % 2:\n        embedding = torch.cat(\n            [embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n    return embedding\n</code></pre> fastvideo.layers.visual_embedding.unpatchify \u00b6 <pre><code>unpatchify(\n    x, t, h, w, patch_size, channels\n) -&gt; torch.Tensor\n</code></pre> <p>Convert patched representation back to image space.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>Tensor of shape [B, THW, CP_tP_h*P_w]</p> required <code>t, h, w</code> <p>Temporal and spatial dimensions</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Unpatchified tensor of shape [B, C, TP_t, HP_h, W*P_w]</p> Source code in <code>fastvideo/layers/visual_embedding.py</code> <pre><code>def unpatchify(x, t, h, w, patch_size, channels) -&gt; torch.Tensor:\n    \"\"\"\n    Convert patched representation back to image space.\n\n    Args:\n        x: Tensor of shape [B, T*H*W, C*P_t*P_h*P_w]\n        t, h, w: Temporal and spatial dimensions\n\n    Returns:\n        Unpatchified tensor of shape [B, C, T*P_t, H*P_h, W*P_w]\n    \"\"\"\n    assert x.ndim == 3, f\"x.ndim: {x.ndim}\"\n    assert len(patch_size) == 3, f\"patch_size: {patch_size}\"\n    assert t * h * w == x.shape[\n        1], f\"t * h * w: {t * h * w}, x.shape[1]: {x.shape[1]}\"\n    c = channels\n    pt, ph, pw = patch_size\n\n    x = x.reshape(shape=(x.shape[0], t, h, w, c, pt, ph, pw))\n    x = torch.einsum(\"nthwcopq-&gt;nctohpwq\", x)\n    imgs = x.reshape(shape=(x.shape[0], c, t * pt, h * ph, w * pw))\n\n    return imgs\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.layers.vocab_parallel_embedding","title":"fastvideo.layers.vocab_parallel_embedding","text":""},{"location":"api/fastvideo/#fastvideo.layers.vocab_parallel_embedding-classes","title":"Classes","text":"fastvideo.layers.vocab_parallel_embedding.UnquantizedEmbeddingMethod \u00b6 <p>               Bases: <code>QuantizeMethodBase</code></p> <p>Unquantized method for embeddings.</p> Functions\u00b6 fastvideo.layers.vocab_parallel_embedding.UnquantizedEmbeddingMethod.create_weights \u00b6 <pre><code>create_weights(\n    layer: Module,\n    input_size_per_partition: int,\n    output_partition_sizes: list[int],\n    input_size: int,\n    output_size: int,\n    params_dtype: dtype,\n    **extra_weight_attrs\n)\n</code></pre> <p>Create weights for embedding layer.</p> Source code in <code>fastvideo/layers/vocab_parallel_embedding.py</code> <pre><code>def create_weights(self, layer: torch.nn.Module,\n                   input_size_per_partition: int,\n                   output_partition_sizes: list[int], input_size: int,\n                   output_size: int, params_dtype: torch.dtype,\n                   **extra_weight_attrs):\n    \"\"\"Create weights for embedding layer.\"\"\"\n\n    weight = Parameter(torch.empty(\n        sum(output_partition_sizes),\n        input_size_per_partition,\n        dtype=params_dtype,\n    ),\n                       requires_grad=False)\n    set_weight_attrs(weight, {\"input_dim\": 1, \"output_dim\": 0})\n    layer.register_parameter(\"weight\", weight)\n    set_weight_attrs(weight, extra_weight_attrs)\n</code></pre> fastvideo.layers.vocab_parallel_embedding.VocabParallelEmbedding \u00b6 <pre><code>VocabParallelEmbedding(\n    num_embeddings: int,\n    embedding_dim: int,\n    params_dtype: dtype | None = None,\n    org_num_embeddings: int | None = None,\n    padding_size: int = DEFAULT_VOCAB_PADDING_SIZE,\n    quant_config: QuantizationConfig | None = None,\n    prefix: str = \"\",\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Embedding parallelized in the vocabulary dimension.</p> <p>Adapted from torch.nn.Embedding, note that we pad the vocabulary size to make sure it is divisible by the number of model parallel GPUs.</p> <p>In order to support various loading methods, we ensure that LoRA-added embeddings are always at the end of TP-sharded tensors. In other words, we shard base embeddings and LoRA embeddings separately (both padded), and place them in the same tensor. In this example, we will have the original vocab size = 1010, added vocab size = 16 and padding to 64. Therefore, the total vocab size with padding will be 1088 (because we first pad 1010 to 1024, add 16, and then pad to 1088). Therefore, the tensor format looks like the following: TP1, rank 0 (no sharding):                         |&lt; --------BASE-------- &gt;|&lt; -BASE PADDING-- &gt;|&lt; -----LORA------ &gt;|&lt; -LORA PADDING-- &gt;| corresponding token_id: |  0  |  1  | ... | 1009 |  -1  | ... |  -1  | 1010 | ... | 1015 |  -1  | ... |  -1  |                  index: |  0  |  1  | ... | 1009 | 1010 | ... | 1023 | 1024 | ... | 1039 | 1040 | ... | 1087 |</p> <p>TP2, rank 0:                         |&lt; --------------------BASE--------------------- &gt;|&lt; -----LORA------ &gt;|&lt; -LORA PADDING- &gt;| corresponding token_id: |  0  |  1  |  2  | ... | 497  | 498 | ...  | 511 | 1000 | ... | 1015 |  -1  | ... |  -1 |                  index: |  0  |  1  |  2  | ... | 497  | 498 | ...  | 511 | 512  | ... | 527  |  520 | ... | 543 | TP2, rank 1:                         |&lt; -----------BASE----------- &gt;|&lt; -BASE PADDING- &gt;|&lt; -----------LORA PADDING----------- &gt;| corresponding token_id: | 512 | 513 | 514 | ... | 1009 | -1  | ...  | -1  |  -1  | ... |  -1  | -1  | ... |   -1 |                  index: |  0  |  1  |  2  | ... | 497  | 498 | ...  | 511 | 512  | ... | 519  | 520 | ... |  543 |</p> <p>Parameters:</p> Name Type Description Default <code>num_embeddings</code> <code>int</code> <p>vocabulary size.</p> required <code>embedding_dim</code> <code>int</code> <p>size of hidden state.</p> required <code>params_dtype</code> <code>dtype | None</code> <p>type of the parameters.</p> <code>None</code> <code>org_num_embeddings</code> <code>int | None</code> <p>original vocabulary size (without LoRA).</p> <code>None</code> <code>padding_size</code> <code>int</code> <p>padding size for the vocabulary.</p> <code>DEFAULT_VOCAB_PADDING_SIZE</code> <code>quant_config</code> <code>QuantizationConfig | None</code> <p>quant config for the layer</p> <code>None</code> <code>prefix</code> <code>str</code> <p>full name of the layer in the state dict</p> <code>''</code> Source code in <code>fastvideo/layers/vocab_parallel_embedding.py</code> <pre><code>def __init__(self,\n             num_embeddings: int,\n             embedding_dim: int,\n             params_dtype: torch.dtype | None = None,\n             org_num_embeddings: int | None = None,\n             padding_size: int = DEFAULT_VOCAB_PADDING_SIZE,\n             quant_config: QuantizationConfig | None = None,\n             prefix: str = \"\"):\n    super().__init__()\n\n    # Keep the input dimensions.\n    tp_rank = get_tp_rank()\n    self.tp_size = get_tp_world_size()\n    self.num_embeddings = num_embeddings\n    self.padding_size = padding_size\n    self.org_vocab_size = org_num_embeddings or num_embeddings\n    num_added_embeddings = num_embeddings - self.org_vocab_size\n    self.org_vocab_size_padded = pad_vocab_size(self.org_vocab_size,\n                                                self.padding_size)\n    self.num_embeddings_padded = pad_vocab_size(\n        self.org_vocab_size_padded + num_added_embeddings,\n        self.padding_size)\n    assert self.org_vocab_size_padded &lt;= self.num_embeddings_padded\n\n    self.shard_indices = self._get_indices(self.num_embeddings_padded,\n                                           self.org_vocab_size_padded,\n                                           self.num_embeddings,\n                                           self.org_vocab_size, tp_rank,\n                                           self.tp_size)\n    self.embedding_dim = embedding_dim\n\n    quant_method = None\n    if quant_config is not None:\n        quant_method = quant_config.get_quant_method(self, prefix=prefix)\n    if quant_method is None:\n        quant_method = UnquantizedEmbeddingMethod()\n\n    # If we are making an embedding layer, then our quantization linear\n    # method must implement the embedding operation. If we are another\n    # layer type like ParallelLMHead, this is not important.\n    is_embedding_layer = type(self.__class__) is VocabParallelEmbedding\n    quant_method_implements_embedding = method_has_implemented_embedding(\n        type(quant_method))\n    if is_embedding_layer and not quant_method_implements_embedding:\n        raise NotImplementedError(\n            f\"The class {type(quant_method).__name__} must implement \"\n            \"the 'embedding' method, see UnquantizedEmbeddingMethod.\")\n\n    self.quant_method: QuantizeMethodBase = quant_method\n\n    if params_dtype is None:\n        params_dtype = torch.get_default_dtype()\n    # Divide the weight matrix along the vocaburaly dimension.\n    self.num_added_embeddings = self.num_embeddings - self.org_vocab_size\n    self.num_embeddings_per_partition = divide(self.num_embeddings_padded,\n                                               self.tp_size)\n    assert (self.shard_indices.num_elements_padded ==\n            self.num_embeddings_per_partition)\n    self.num_org_embeddings_per_partition = (\n        self.shard_indices.org_vocab_end_index -\n        self.shard_indices.org_vocab_start_index)\n    self.num_added_embeddings_per_partition = (\n        self.shard_indices.added_vocab_end_index -\n        self.shard_indices.added_vocab_start_index)\n\n    self.quant_method.create_weights(self,\n                                     self.embedding_dim,\n                                     [self.num_embeddings_per_partition],\n                                     self.embedding_dim,\n                                     self.num_embeddings_padded,\n                                     params_dtype=params_dtype,\n                                     weight_loader=self.weight_loader)\n</code></pre> Functions\u00b6 fastvideo.layers.vocab_parallel_embedding.VocabParallelEmbedding.get_sharded_to_full_mapping \u00b6 <pre><code>get_sharded_to_full_mapping() -&gt; list[int] | None\n</code></pre> <p>Get a mapping that can be used to reindex the gathered logits for sampling.</p> <p>During sampling, we gather logits from all ranks. The relationship of index-&gt;token_id will follow the same format as outlined in the class docstring. However, after the gather, we want to reindex the final logits tensor to map index-&gt;token_id one-to-one (the index is always equal the token_id it corresponds to). The indices returned by this method allow us to do that.</p> Source code in <code>fastvideo/layers/vocab_parallel_embedding.py</code> <pre><code>def get_sharded_to_full_mapping(self) -&gt; list[int] | None:\n    \"\"\"Get a mapping that can be used to reindex the gathered\n    logits for sampling.\n\n    During sampling, we gather logits from all ranks. The relationship\n    of index-&gt;token_id will follow the same format as outlined in the class\n    docstring. However, after the gather, we want to reindex the final\n    logits tensor to map index-&gt;token_id one-to-one (the index is always\n    equal the token_id it corresponds to). The indices returned by this\n    method allow us to do that.\n    \"\"\"\n    if self.tp_size &lt; 2:\n        return None\n\n    base_embeddings: list[int] = []\n    added_embeddings: list[int] = []\n    padding: list[int] = []\n    for tp_rank in range(self.tp_size):\n        shard_indices = self._get_indices(self.num_embeddings_padded,\n                                          self.org_vocab_size_padded,\n                                          self.num_embeddings,\n                                          self.org_vocab_size, tp_rank,\n                                          self.tp_size)\n        range_start = self.num_embeddings_per_partition * tp_rank\n        range_end = self.num_embeddings_per_partition * (tp_rank + 1)\n        base_embeddings.extend(\n            range(range_start,\n                  range_start + shard_indices.num_org_elements))\n        padding.extend(\n            range(range_start + shard_indices.num_org_elements,\n                  range_start + shard_indices.num_org_elements_padded))\n        added_embeddings.extend(\n            range(\n                range_start + shard_indices.num_org_elements_padded,\n                range_start + shard_indices.num_org_elements_padded +\n                shard_indices.num_added_elements))\n        padding.extend(\n            range(\n                range_start + shard_indices.num_org_elements_padded +\n                shard_indices.num_added_elements,\n                range_start + shard_indices.num_org_elements_padded +\n                shard_indices.num_added_elements_padded))\n        assert (range_start + shard_indices.num_org_elements_padded +\n                shard_indices.num_added_elements_padded == range_end)\n    ret = base_embeddings + added_embeddings + padding\n    assert len(ret) == self.num_embeddings_padded\n    return ret\n</code></pre> fastvideo.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices <code>dataclass</code> \u00b6 <pre><code>VocabParallelEmbeddingShardIndices(\n    padded_org_vocab_start_index: int,\n    padded_org_vocab_end_index: int,\n    padded_added_vocab_start_index: int,\n    padded_added_vocab_end_index: int,\n    org_vocab_start_index: int,\n    org_vocab_end_index: int,\n    added_vocab_start_index: int,\n    added_vocab_end_index: int,\n)\n</code></pre> <p>Indices for a shard of a vocab parallel embedding.</p>"},{"location":"api/fastvideo/#fastvideo.layers.vocab_parallel_embedding-functions","title":"Functions","text":"fastvideo.layers.vocab_parallel_embedding.pad_vocab_size \u00b6 <pre><code>pad_vocab_size(\n    vocab_size: int,\n    pad_to: int = DEFAULT_VOCAB_PADDING_SIZE,\n) -&gt; int\n</code></pre> <p>Pad the vocab size to the given value.</p> Source code in <code>fastvideo/layers/vocab_parallel_embedding.py</code> <pre><code>def pad_vocab_size(vocab_size: int,\n                   pad_to: int = DEFAULT_VOCAB_PADDING_SIZE) -&gt; int:\n    \"\"\"Pad the vocab size to the given value.\"\"\"\n    return ((vocab_size + pad_to - 1) // pad_to) * pad_to\n</code></pre>"},{"location":"api/fastvideo/#fastvideoplatforms","title":"fastvideo.platforms","text":""},{"location":"api/fastvideo/#fastvideo.platforms","title":"platforms","text":""},{"location":"api/fastvideo/#fastvideo.platforms-classes","title":"Classes","text":""},{"location":"api/fastvideo/#fastvideo.platforms.Platform","title":"fastvideo.platforms.Platform","text":""},{"location":"api/fastvideo/#fastvideo.platforms.Platform-functions","title":"Functions","text":"fastvideo.platforms.Platform.get_attn_backend_cls <code>classmethod</code> \u00b6 <pre><code>get_attn_backend_cls(\n    selected_backend: AttentionBackendEnum | None,\n    head_size: int,\n    dtype: dtype,\n) -&gt; str\n</code></pre> <p>Get the attention backend class of a device.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_attn_backend_cls(cls, selected_backend: AttentionBackendEnum | None,\n                         head_size: int, dtype: torch.dtype) -&gt; str:\n    \"\"\"Get the attention backend class of a device.\"\"\"\n    return \"\"\n</code></pre> fastvideo.platforms.Platform.get_cpu_architecture <code>classmethod</code> \u00b6 <pre><code>get_cpu_architecture() -&gt; CpuArchEnum\n</code></pre> <p>Get the CPU architecture of the current platform.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_cpu_architecture(cls) -&gt; CpuArchEnum:\n    \"\"\"Get the CPU architecture of the current platform.\"\"\"\n    return CpuArchEnum.UNSPECIFIED\n</code></pre> fastvideo.platforms.Platform.get_current_memory_usage <code>classmethod</code> \u00b6 <pre><code>get_current_memory_usage(\n    device: Device | None = None,\n) -&gt; float\n</code></pre> <p>Return the memory usage in bytes.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_current_memory_usage(cls,\n                             device: torch.types.Device | None = None\n                             ) -&gt; float:\n    \"\"\"\n    Return the memory usage in bytes.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.platforms.Platform.get_device_capability <code>classmethod</code> \u00b6 <pre><code>get_device_capability(\n    device_id: int = 0,\n) -&gt; DeviceCapability | None\n</code></pre> <p>Stateless version of :func:<code>torch.cuda.get_device_capability</code>.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_device_capability(\n    cls,\n    device_id: int = 0,\n) -&gt; DeviceCapability | None:\n    \"\"\"Stateless version of :func:`torch.cuda.get_device_capability`.\"\"\"\n    return None\n</code></pre> fastvideo.platforms.Platform.get_device_communicator_cls <code>classmethod</code> \u00b6 <pre><code>get_device_communicator_cls() -&gt; str\n</code></pre> <p>Get device specific communicator class for distributed communication.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_device_communicator_cls(cls) -&gt; str:\n    \"\"\"\n    Get device specific communicator class for distributed communication.\n    \"\"\"\n    return \"fastvideo.distributed.device_communicators.base_device_communicator.DeviceCommunicatorBase\"  # noqa\n</code></pre> fastvideo.platforms.Platform.get_device_name <code>classmethod</code> \u00b6 <pre><code>get_device_name(device_id: int = 0) -&gt; str\n</code></pre> <p>Get the name of a device.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_device_name(cls, device_id: int = 0) -&gt; str:\n    \"\"\"Get the name of a device.\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.platforms.Platform.get_device_total_memory <code>classmethod</code> \u00b6 <pre><code>get_device_total_memory(device_id: int = 0) -&gt; int\n</code></pre> <p>Get the total memory of a device in bytes.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_device_total_memory(cls, device_id: int = 0) -&gt; int:\n    \"\"\"Get the total memory of a device in bytes.\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.platforms.Platform.get_device_uuid <code>classmethod</code> \u00b6 <pre><code>get_device_uuid(device_id: int = 0) -&gt; str\n</code></pre> <p>Get the uuid of a device, e.g. the PCI bus ID.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_device_uuid(cls, device_id: int = 0) -&gt; str:\n    \"\"\"Get the uuid of a device, e.g. the PCI bus ID.\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.platforms.Platform.get_torch_device <code>classmethod</code> \u00b6 <pre><code>get_torch_device()\n</code></pre> <p>Check if the current platform supports torch device.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_torch_device(cls):\n    \"\"\"\n    Check if the current platform supports torch device.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.platforms.Platform.has_device_capability <code>classmethod</code> \u00b6 <pre><code>has_device_capability(\n    capability: tuple[int, int] | int, device_id: int = 0\n) -&gt; bool\n</code></pre> <p>Test whether this platform is compatible with a device capability.</p> <p>The <code>capability</code> argument can either be:</p> <ul> <li>A tuple <code>(major, minor)</code>.</li> <li>An integer <code>&lt;major&gt;&lt;minor&gt;</code>. (See :meth:<code>DeviceCapability.to_int</code>)</li> </ul> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef has_device_capability(\n    cls,\n    capability: tuple[int, int] | int,\n    device_id: int = 0,\n) -&gt; bool:\n    \"\"\"\n    Test whether this platform is compatible with a device capability.\n\n    The ``capability`` argument can either be:\n\n    - A tuple ``(major, minor)``.\n    - An integer ``&lt;major&gt;&lt;minor&gt;``. (See :meth:`DeviceCapability.to_int`)\n    \"\"\"\n    current_capability = cls.get_device_capability(device_id=device_id)\n    if current_capability is None:\n        return False\n\n    if isinstance(capability, tuple):\n        return current_capability &gt;= capability\n\n    return current_capability.to_int() &gt;= capability\n</code></pre> fastvideo.platforms.Platform.inference_mode <code>classmethod</code> \u00b6 <pre><code>inference_mode()\n</code></pre> <p>A device-specific wrapper of <code>torch.inference_mode</code>.</p> <p>This wrapper is recommended because some hardware backends such as TPU do not support <code>torch.inference_mode</code>. In such a case, they will fall back to <code>torch.no_grad</code> by overriding this method.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef inference_mode(cls):\n    \"\"\"A device-specific wrapper of `torch.inference_mode`.\n\n    This wrapper is recommended because some hardware backends such as TPU\n    do not support `torch.inference_mode`. In such a case, they will fall\n    back to `torch.no_grad` by overriding this method.\n    \"\"\"\n    return torch.inference_mode(mode=True)\n</code></pre> fastvideo.platforms.Platform.is_async_output_supported <code>classmethod</code> \u00b6 <pre><code>is_async_output_supported(\n    enforce_eager: bool | None,\n) -&gt; bool\n</code></pre> <p>Check if the current platform supports async output.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef is_async_output_supported(cls, enforce_eager: bool | None) -&gt; bool:\n    \"\"\"\n    Check if the current platform supports async output.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.platforms.Platform.is_cuda_alike \u00b6 <pre><code>is_cuda_alike() -&gt; bool\n</code></pre> <p>Stateless version of :func:<code>torch.cuda.is_available</code>.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>def is_cuda_alike(self) -&gt; bool:\n    \"\"\"Stateless version of :func:`torch.cuda.is_available`.\"\"\"\n    return self._enum in (PlatformEnum.CUDA, PlatformEnum.ROCM)\n</code></pre> fastvideo.platforms.Platform.seed_everything <code>classmethod</code> \u00b6 <pre><code>seed_everything(seed: int | None = None) -&gt; None\n</code></pre> <p>Set the seed of each random module. <code>torch.manual_seed</code> will set seed on all devices.</p> <p>Loosely based on: https://github.com/Lightning-AI/pytorch-lightning/blob/2.4.0/src/lightning/fabric/utilities/seed.py#L20</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef seed_everything(cls, seed: int | None = None) -&gt; None:\n    \"\"\"\n    Set the seed of each random module.\n    `torch.manual_seed` will set seed on all devices.\n\n    Loosely based on: https://github.com/Lightning-AI/pytorch-lightning/blob/2.4.0/src/lightning/fabric/utilities/seed.py#L20\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n</code></pre> fastvideo.platforms.Platform.verify_model_arch <code>classmethod</code> \u00b6 <pre><code>verify_model_arch(model_arch: str) -&gt; None\n</code></pre> <p>Verify whether the current platform supports the specified model architecture.</p> <ul> <li>This will raise an Error or Warning based on the model support on the current platform.</li> <li>By default all models are considered supported.</li> </ul> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef verify_model_arch(cls, model_arch: str) -&gt; None:\n    \"\"\"\n    Verify whether the current platform supports the specified model\n    architecture.\n\n    - This will raise an Error or Warning based on the model support on\n    the current platform.\n    - By default all models are considered supported.\n    \"\"\"\n    pass\n</code></pre> fastvideo.platforms.Platform.verify_quantization <code>classmethod</code> \u00b6 <pre><code>verify_quantization(quant: str) -&gt; None\n</code></pre> <p>Verify whether the quantization is supported by the current platform.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef verify_quantization(cls, quant: str) -&gt; None:\n    \"\"\"\n    Verify whether the quantization is supported by the current platform.\n    \"\"\"\n    if cls.supported_quantization and \\\n        quant not in cls.supported_quantization:\n        raise ValueError(\n            f\"{quant} quantization is currently not supported in \"\n            f\"{cls.device_name}.\")\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.platforms-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideo.platforms.cpu_platform_plugin","title":"fastvideo.platforms.cpu_platform_plugin","text":"<pre><code>cpu_platform_plugin() -&gt; str | None\n</code></pre> <p>Detect if CPU platform should be used.</p> Source code in <code>fastvideo/platforms/__init__.py</code> <pre><code>def cpu_platform_plugin() -&gt; str | None:\n    \"\"\"Detect if CPU platform should be used.\"\"\"\n    # CPU is always available as a fallback\n    return \"fastvideo.platforms.cpu.CpuPlatform\"\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.platforms.mps_platform_plugin","title":"fastvideo.platforms.mps_platform_plugin","text":"<pre><code>mps_platform_plugin() -&gt; str | None\n</code></pre> <p>Detect if MPS (Metal Performance Shaders) is available on macOS.</p> Source code in <code>fastvideo/platforms/__init__.py</code> <pre><code>def mps_platform_plugin() -&gt; str | None:\n    \"\"\"Detect if MPS (Metal Performance Shaders) is available on macOS.\"\"\"\n    is_mps = False\n\n    try:\n        import torch\n        if torch.backends.mps.is_available():\n            is_mps = True\n            logger.info(\"MPS (Metal Performance Shaders) is available\")\n    except Exception as e:\n        logger.info(\"MPS detection failed: %s\", e)\n\n    return \"fastvideo.platforms.mps.MpsPlatform\" if is_mps else None\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.platforms-modules","title":"Modules","text":""},{"location":"api/fastvideo/#fastvideo.platforms.cpu","title":"fastvideo.platforms.cpu","text":""},{"location":"api/fastvideo/#fastvideo.platforms.cpu-classes","title":"Classes","text":"fastvideo.platforms.cpu.CpuPlatform \u00b6 <p>               Bases: <code>Platform</code></p> Functions\u00b6 fastvideo.platforms.cpu.CpuPlatform.get_cpu_architecture <code>classmethod</code> \u00b6 <pre><code>get_cpu_architecture() -&gt; CpuArchEnum\n</code></pre> <p>Get the CPU architecture.</p> Source code in <code>fastvideo/platforms/cpu.py</code> <pre><code>@classmethod\ndef get_cpu_architecture(cls) -&gt; CpuArchEnum:\n    \"\"\"Get the CPU architecture.\"\"\"\n    machine = platform.machine().lower()\n    if machine in (\"x86_64\", \"amd64\", \"i386\", \"i686\"):\n        return CpuArchEnum.X86\n    elif machine in (\"arm64\", \"aarch64\"):\n        return CpuArchEnum.ARM\n    else:\n        return CpuArchEnum.UNSPECIFIED\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.platforms.cuda","title":"fastvideo.platforms.cuda","text":"<p>Code inside this file can safely assume cuda platform, e.g. importing pynvml. However, it should not initialize cuda context.</p>"},{"location":"api/fastvideo/#fastvideo.platforms.cuda-classes","title":"Classes","text":"fastvideo.platforms.cuda.CudaPlatformBase \u00b6 <p>               Bases: <code>Platform</code></p> Functions\u00b6 fastvideo.platforms.cuda.CudaPlatformBase.get_torch_device <code>classmethod</code> \u00b6 <pre><code>get_torch_device()\n</code></pre> <p>Return torch.cuda</p> Source code in <code>fastvideo/platforms/cuda.py</code> <pre><code>@classmethod\ndef get_torch_device(cls):\n    \"\"\"\n    Return torch.cuda\n    \"\"\"\n    return torch.cuda\n</code></pre> fastvideo.platforms.cuda.NvmlCudaPlatform \u00b6 <p>               Bases: <code>CudaPlatformBase</code></p> Functions\u00b6 fastvideo.platforms.cuda.NvmlCudaPlatform.is_full_nvlink <code>classmethod</code> \u00b6 <pre><code>is_full_nvlink(physical_device_ids: list[int]) -&gt; bool\n</code></pre> <p>query if the set of gpus are fully connected by nvlink (1 hop)</p> Source code in <code>fastvideo/platforms/cuda.py</code> <pre><code>@classmethod\n@with_nvml_context\ndef is_full_nvlink(cls, physical_device_ids: list[int]) -&gt; bool:\n    \"\"\"\n    query if the set of gpus are fully connected by nvlink (1 hop)\n    \"\"\"\n    handles = [\n        pynvml.nvmlDeviceGetHandleByIndex(i) for i in physical_device_ids\n    ]\n    for i, handle in enumerate(handles):\n        for j, peer_handle in enumerate(handles):\n            if i &lt; j:\n                try:\n                    p2p_status = pynvml.nvmlDeviceGetP2PStatus(\n                        handle,\n                        peer_handle,\n                        pynvml.NVML_P2P_CAPS_INDEX_NVLINK,\n                    )\n                    if p2p_status != pynvml.NVML_P2P_STATUS_OK:\n                        return False\n                except pynvml.NVMLError:\n                    logger.exception(\n                        \"NVLink detection failed. This is normal if\"\n                        \" your machine has no NVLink equipped.\")\n                    return False\n    return True\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.platforms.cuda-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideo.platforms.interface","title":"fastvideo.platforms.interface","text":""},{"location":"api/fastvideo/#fastvideo.platforms.interface-classes","title":"Classes","text":"fastvideo.platforms.interface.DeviceCapability \u00b6 <p>               Bases: <code>NamedTuple</code></p> Functions\u00b6 fastvideo.platforms.interface.DeviceCapability.to_int \u00b6 <pre><code>to_int() -&gt; int\n</code></pre> <p>Express device capability as an integer <code>&lt;major&gt;&lt;minor&gt;</code>.</p> <p>It is assumed that the minor version is always a single digit.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>def to_int(self) -&gt; int:\n    \"\"\"\n    Express device capability as an integer ``&lt;major&gt;&lt;minor&gt;``.\n\n    It is assumed that the minor version is always a single digit.\n    \"\"\"\n    assert 0 &lt;= self.minor &lt; 10\n    return self.major * 10 + self.minor\n</code></pre> fastvideo.platforms.interface.Platform \u00b6 Functions\u00b6 fastvideo.platforms.interface.Platform.get_attn_backend_cls <code>classmethod</code> \u00b6 <pre><code>get_attn_backend_cls(\n    selected_backend: AttentionBackendEnum | None,\n    head_size: int,\n    dtype: dtype,\n) -&gt; str\n</code></pre> <p>Get the attention backend class of a device.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_attn_backend_cls(cls, selected_backend: AttentionBackendEnum | None,\n                         head_size: int, dtype: torch.dtype) -&gt; str:\n    \"\"\"Get the attention backend class of a device.\"\"\"\n    return \"\"\n</code></pre> fastvideo.platforms.interface.Platform.get_cpu_architecture <code>classmethod</code> \u00b6 <pre><code>get_cpu_architecture() -&gt; CpuArchEnum\n</code></pre> <p>Get the CPU architecture of the current platform.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_cpu_architecture(cls) -&gt; CpuArchEnum:\n    \"\"\"Get the CPU architecture of the current platform.\"\"\"\n    return CpuArchEnum.UNSPECIFIED\n</code></pre> fastvideo.platforms.interface.Platform.get_current_memory_usage <code>classmethod</code> \u00b6 <pre><code>get_current_memory_usage(\n    device: Device | None = None,\n) -&gt; float\n</code></pre> <p>Return the memory usage in bytes.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_current_memory_usage(cls,\n                             device: torch.types.Device | None = None\n                             ) -&gt; float:\n    \"\"\"\n    Return the memory usage in bytes.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.platforms.interface.Platform.get_device_capability <code>classmethod</code> \u00b6 <pre><code>get_device_capability(\n    device_id: int = 0,\n) -&gt; DeviceCapability | None\n</code></pre> <p>Stateless version of :func:<code>torch.cuda.get_device_capability</code>.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_device_capability(\n    cls,\n    device_id: int = 0,\n) -&gt; DeviceCapability | None:\n    \"\"\"Stateless version of :func:`torch.cuda.get_device_capability`.\"\"\"\n    return None\n</code></pre> fastvideo.platforms.interface.Platform.get_device_communicator_cls <code>classmethod</code> \u00b6 <pre><code>get_device_communicator_cls() -&gt; str\n</code></pre> <p>Get device specific communicator class for distributed communication.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_device_communicator_cls(cls) -&gt; str:\n    \"\"\"\n    Get device specific communicator class for distributed communication.\n    \"\"\"\n    return \"fastvideo.distributed.device_communicators.base_device_communicator.DeviceCommunicatorBase\"  # noqa\n</code></pre> fastvideo.platforms.interface.Platform.get_device_name <code>classmethod</code> \u00b6 <pre><code>get_device_name(device_id: int = 0) -&gt; str\n</code></pre> <p>Get the name of a device.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_device_name(cls, device_id: int = 0) -&gt; str:\n    \"\"\"Get the name of a device.\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.platforms.interface.Platform.get_device_total_memory <code>classmethod</code> \u00b6 <pre><code>get_device_total_memory(device_id: int = 0) -&gt; int\n</code></pre> <p>Get the total memory of a device in bytes.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_device_total_memory(cls, device_id: int = 0) -&gt; int:\n    \"\"\"Get the total memory of a device in bytes.\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.platforms.interface.Platform.get_device_uuid <code>classmethod</code> \u00b6 <pre><code>get_device_uuid(device_id: int = 0) -&gt; str\n</code></pre> <p>Get the uuid of a device, e.g. the PCI bus ID.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_device_uuid(cls, device_id: int = 0) -&gt; str:\n    \"\"\"Get the uuid of a device, e.g. the PCI bus ID.\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.platforms.interface.Platform.get_torch_device <code>classmethod</code> \u00b6 <pre><code>get_torch_device()\n</code></pre> <p>Check if the current platform supports torch device.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_torch_device(cls):\n    \"\"\"\n    Check if the current platform supports torch device.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.platforms.interface.Platform.has_device_capability <code>classmethod</code> \u00b6 <pre><code>has_device_capability(\n    capability: tuple[int, int] | int, device_id: int = 0\n) -&gt; bool\n</code></pre> <p>Test whether this platform is compatible with a device capability.</p> <p>The <code>capability</code> argument can either be:</p> <ul> <li>A tuple <code>(major, minor)</code>.</li> <li>An integer <code>&lt;major&gt;&lt;minor&gt;</code>. (See :meth:<code>DeviceCapability.to_int</code>)</li> </ul> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef has_device_capability(\n    cls,\n    capability: tuple[int, int] | int,\n    device_id: int = 0,\n) -&gt; bool:\n    \"\"\"\n    Test whether this platform is compatible with a device capability.\n\n    The ``capability`` argument can either be:\n\n    - A tuple ``(major, minor)``.\n    - An integer ``&lt;major&gt;&lt;minor&gt;``. (See :meth:`DeviceCapability.to_int`)\n    \"\"\"\n    current_capability = cls.get_device_capability(device_id=device_id)\n    if current_capability is None:\n        return False\n\n    if isinstance(capability, tuple):\n        return current_capability &gt;= capability\n\n    return current_capability.to_int() &gt;= capability\n</code></pre> fastvideo.platforms.interface.Platform.inference_mode <code>classmethod</code> \u00b6 <pre><code>inference_mode()\n</code></pre> <p>A device-specific wrapper of <code>torch.inference_mode</code>.</p> <p>This wrapper is recommended because some hardware backends such as TPU do not support <code>torch.inference_mode</code>. In such a case, they will fall back to <code>torch.no_grad</code> by overriding this method.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef inference_mode(cls):\n    \"\"\"A device-specific wrapper of `torch.inference_mode`.\n\n    This wrapper is recommended because some hardware backends such as TPU\n    do not support `torch.inference_mode`. In such a case, they will fall\n    back to `torch.no_grad` by overriding this method.\n    \"\"\"\n    return torch.inference_mode(mode=True)\n</code></pre> fastvideo.platforms.interface.Platform.is_async_output_supported <code>classmethod</code> \u00b6 <pre><code>is_async_output_supported(\n    enforce_eager: bool | None,\n) -&gt; bool\n</code></pre> <p>Check if the current platform supports async output.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef is_async_output_supported(cls, enforce_eager: bool | None) -&gt; bool:\n    \"\"\"\n    Check if the current platform supports async output.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.platforms.interface.Platform.is_cuda_alike \u00b6 <pre><code>is_cuda_alike() -&gt; bool\n</code></pre> <p>Stateless version of :func:<code>torch.cuda.is_available</code>.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>def is_cuda_alike(self) -&gt; bool:\n    \"\"\"Stateless version of :func:`torch.cuda.is_available`.\"\"\"\n    return self._enum in (PlatformEnum.CUDA, PlatformEnum.ROCM)\n</code></pre> fastvideo.platforms.interface.Platform.seed_everything <code>classmethod</code> \u00b6 <pre><code>seed_everything(seed: int | None = None) -&gt; None\n</code></pre> <p>Set the seed of each random module. <code>torch.manual_seed</code> will set seed on all devices.</p> <p>Loosely based on: https://github.com/Lightning-AI/pytorch-lightning/blob/2.4.0/src/lightning/fabric/utilities/seed.py#L20</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef seed_everything(cls, seed: int | None = None) -&gt; None:\n    \"\"\"\n    Set the seed of each random module.\n    `torch.manual_seed` will set seed on all devices.\n\n    Loosely based on: https://github.com/Lightning-AI/pytorch-lightning/blob/2.4.0/src/lightning/fabric/utilities/seed.py#L20\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n</code></pre> fastvideo.platforms.interface.Platform.verify_model_arch <code>classmethod</code> \u00b6 <pre><code>verify_model_arch(model_arch: str) -&gt; None\n</code></pre> <p>Verify whether the current platform supports the specified model architecture.</p> <ul> <li>This will raise an Error or Warning based on the model support on the current platform.</li> <li>By default all models are considered supported.</li> </ul> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef verify_model_arch(cls, model_arch: str) -&gt; None:\n    \"\"\"\n    Verify whether the current platform supports the specified model\n    architecture.\n\n    - This will raise an Error or Warning based on the model support on\n    the current platform.\n    - By default all models are considered supported.\n    \"\"\"\n    pass\n</code></pre> fastvideo.platforms.interface.Platform.verify_quantization <code>classmethod</code> \u00b6 <pre><code>verify_quantization(quant: str) -&gt; None\n</code></pre> <p>Verify whether the quantization is supported by the current platform.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef verify_quantization(cls, quant: str) -&gt; None:\n    \"\"\"\n    Verify whether the quantization is supported by the current platform.\n    \"\"\"\n    if cls.supported_quantization and \\\n        quant not in cls.supported_quantization:\n        raise ValueError(\n            f\"{quant} quantization is currently not supported in \"\n            f\"{cls.device_name}.\")\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.platforms.interface-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideo.platforms.mps","title":"fastvideo.platforms.mps","text":""},{"location":"api/fastvideo/#fastvideo.platforms.mps-classes","title":"Classes","text":"fastvideo.platforms.mps.MpsPlatform \u00b6 <p>               Bases: <code>Platform</code></p> Functions\u00b6 fastvideo.platforms.mps.MpsPlatform.seed_everything <code>classmethod</code> \u00b6 <pre><code>seed_everything(seed: int | None = None) -&gt; None\n</code></pre> <p>Set the seed for MPS device.</p> Source code in <code>fastvideo/platforms/mps.py</code> <pre><code>@classmethod\ndef seed_everything(cls, seed: int | None = None) -&gt; None:\n    \"\"\"Set the seed for MPS device.\"\"\"\n    if seed is not None:\n        import random\n\n        import numpy as np\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.platforms.mps-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideo.platforms.npu","title":"fastvideo.platforms.npu","text":""},{"location":"api/fastvideo/#fastvideo.platforms.npu-classes","title":"Classes","text":"fastvideo.platforms.npu.NPUPlatform \u00b6 <p>               Bases: <code>Platform</code></p> Functions\u00b6 fastvideo.platforms.npu.NPUPlatform.get_torch_device <code>classmethod</code> \u00b6 <pre><code>get_torch_device()\n</code></pre> <p>Return torch.npu</p> Source code in <code>fastvideo/platforms/npu.py</code> <pre><code>@classmethod\ndef get_torch_device(cls):\n    \"\"\"\n    Return torch.npu\n    \"\"\"\n    return torch.npu\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.platforms.npu-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideo.platforms.rocm","title":"fastvideo.platforms.rocm","text":"<p>This file is a platform abstraction for ROCm GPUs, adjusted to match the structure and interface of <code>cuda.py</code>.</p>"},{"location":"api/fastvideo/#fastvideo.platforms.rocm-classes","title":"Classes","text":""},{"location":"api/fastvideo/#fastvideo.platforms.rocm-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideoutils","title":"fastvideo.utils","text":""},{"location":"api/fastvideo/#fastvideo.utils","title":"utils","text":""},{"location":"api/fastvideo/#fastvideo.utils-classes","title":"Classes","text":""},{"location":"api/fastvideo/#fastvideo.utils.FlexibleArgumentParser","title":"fastvideo.utils.FlexibleArgumentParser","text":"<pre><code>FlexibleArgumentParser(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ArgumentParser</code></p> <p>ArgumentParser that allows both underscore and dash in names.</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    # Set the default 'formatter_class' to SortedHelpFormatter\n    if 'formatter_class' not in kwargs:\n        kwargs['formatter_class'] = SortedHelpFormatter\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.utils.SortedHelpFormatter","title":"fastvideo.utils.SortedHelpFormatter","text":"<p>               Bases: <code>HelpFormatter</code></p> <p>SortedHelpFormatter that sorts arguments by their option strings.</p>"},{"location":"api/fastvideo/#fastvideo.utils-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideo.utils.align_to","title":"fastvideo.utils.align_to","text":"<pre><code>align_to(value: int, alignment: int) -&gt; int\n</code></pre> <p>align height, width according to alignment</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int</code> <p>height or width</p> required <code>alignment</code> <code>int</code> <p>target alignment factor</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>the aligned value</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def align_to(value: int, alignment: int) -&gt; int:\n    \"\"\"align height, width according to alignment\n\n    Args:\n        value (int): height or width\n        alignment (int): target alignment factor\n\n    Returns:\n        int: the aligned value\n    \"\"\"\n    return int(math.ceil(value / alignment) * alignment)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.utils.cuda_is_initialized","title":"fastvideo.utils.cuda_is_initialized","text":"<pre><code>cuda_is_initialized() -&gt; bool\n</code></pre> <p>Check if CUDA is initialized.</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def cuda_is_initialized() -&gt; bool:\n    \"\"\"Check if CUDA is initialized.\"\"\"\n    if not torch.cuda._is_compiled():\n        return False\n    return torch.cuda.is_initialized()\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.utils.current_stream","title":"fastvideo.utils.current_stream","text":"<pre><code>current_stream() -&gt; torch.cuda.Stream | None\n</code></pre> <p>replace <code>torch.cuda.current_stream()</code> with <code>fastvideo.utils.current_stream()</code>. it turns out that <code>torch.cuda.current_stream()</code> is quite expensive, as it will construct a new stream object at each call. here we patch <code>torch.cuda.set_stream</code> to keep track of the current stream directly, so that we can avoid calling <code>torch.cuda.current_stream()</code>.</p> <p>the underlying hypothesis is that we do not call <code>torch._C._cuda_setStream</code> from C/C++ code.</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def current_stream() -&gt; torch.cuda.Stream | None:\n    \"\"\"\n    replace `torch.cuda.current_stream()` with `fastvideo.utils.current_stream()`.\n    it turns out that `torch.cuda.current_stream()` is quite expensive,\n    as it will construct a new stream object at each call.\n    here we patch `torch.cuda.set_stream` to keep track of the current stream\n    directly, so that we can avoid calling `torch.cuda.current_stream()`.\n\n    the underlying hypothesis is that we do not call `torch._C._cuda_setStream`\n    from C/C++ code.\n    \"\"\"\n    from fastvideo.platforms import current_platform\n\n    # For non-CUDA platforms, return None\n    if not current_platform.is_cuda_alike():\n        return None\n\n    global _current_stream\n    if _current_stream is None:\n        # when this function is called before any stream is set,\n        # we return the default stream.\n        # On ROCm using the default 0 stream in combination with RCCL\n        # is hurting performance. Therefore creating a dedicated stream\n        # per process\n        _current_stream = torch.cuda.Stream() if current_platform.is_rocm(\n        ) else torch.cuda.current_stream()\n    return _current_stream\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.utils.decorate_logs","title":"fastvideo.utils.decorate_logs","text":"<pre><code>decorate_logs(process_name: str | None = None) -&gt; None\n</code></pre> <p>Adds a process-specific prefix to each line of output written to stdout and stderr.</p> <p>Parameters:</p> Name Type Description Default <code>process_name</code> <code>str | None</code> <p>Optional; the name of the process to use in the prefix. If not provided, the current process name from the multiprocessing context is used.</p> <code>None</code> Source code in <code>fastvideo/utils.py</code> <pre><code>def decorate_logs(process_name: str | None = None) -&gt; None:\n    \"\"\"\n    Adds a process-specific prefix to each line of output written to stdout and\n    stderr.\n\n    Args:\n        process_name: Optional; the name of the process to use in the prefix.\n            If not provided, the current process name from the multiprocessing\n            context is used.\n    \"\"\"\n    if process_name is None:\n        process_name = get_mp_context().current_process().name\n    pid = os.getpid()\n    _add_prefix(sys.stdout, process_name, pid)\n    _add_prefix(sys.stderr, process_name, pid)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.utils.dict_to_3d_list","title":"fastvideo.utils.dict_to_3d_list","text":"<pre><code>dict_to_3d_list(\n    mask_strategy: dict[str, Any] | None = None,\n    t_max: int | None = None,\n    l_max: int | None = None,\n    h_max: int | None = None,\n) -&gt; list[list[list[torch.Tensor | None]]]\n</code></pre> <p>Convert a dictionary of mask indices to a 3D list of tensors. Args:     mask_strategy: keys are \"t_l_h\", values are torch.Tensor masks.     t_max, l_max, h_max: if provided (all three), force the output shape to (t_max, l_max, h_max).                         If all three are None, infer shape from the data.</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def dict_to_3d_list(\n    mask_strategy: dict[str, Any] | None = None,\n    t_max: int | None = None,\n    l_max: int | None = None,\n    h_max: int | None = None,\n) -&gt; list[list[list[torch.Tensor | None]]]:\n    \"\"\"\n    Convert a dictionary of mask indices to a 3D list of tensors.\n    Args:\n        mask_strategy: keys are \"t_l_h\", values are torch.Tensor masks.\n        t_max, l_max, h_max: if provided (all three), force the output shape to (t_max, l_max, h_max).\n                            If all three are None, infer shape from the data.\n    \"\"\"\n    # Case 1: no data, but fixed shape requested\n    if mask_strategy is None:\n        assert t_max is not None and l_max is not None and h_max is not None, (\n            \"If mask_strategy is None, you must provide t_max, l_max, and h_max\"\n        )\n        return [[[None for _ in range(h_max)] for _ in range(l_max)]\n                for _ in range(t_max)]\n\n    # Parse all keys into integer tuples\n    indices = [tuple(map(int, key.split(\"_\"))) for key in mask_strategy]\n\n    # Decide on dimensions\n    if t_max is None and l_max is None and h_max is None:\n        # fully dynamic: infer from data\n        max_timesteps_idx = max(t for t, _, _ in indices) + 1\n        max_layer_idx = max(l for _, l, _ in indices) + 1  # noqa: E741\n        max_head_idx = max(h for _, _, h in indices) + 1\n    else:\n        # require all three to be provided\n        assert t_max is not None and l_max is not None and h_max is not None, (\n            \"Either supply none of (t_max, l_max, h_max) to infer dimensions, \"\n            \"or supply all three to fix the shape.\")\n        max_timesteps_idx = t_max\n        max_layer_idx = l_max\n        max_head_idx = h_max\n\n    # Preallocate\n    result = [[[None for _ in range(max_head_idx)]\n               for _ in range(max_layer_idx)] for _ in range(max_timesteps_idx)]\n\n    # Fill in, skipping any out-of-bounds entries\n    for key, value in mask_strategy.items():\n        t, l, h = map(int, key.split(\"_\"))  # noqa: E741\n        if 0 &lt;= t &lt; max_timesteps_idx and 0 &lt;= l &lt; max_layer_idx and 0 &lt;= h &lt; max_head_idx:\n            result[t][l][h] = value\n        # else: silently ignore any key that doesn't fit\n\n    return result\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.utils.find_hccl_library","title":"fastvideo.utils.find_hccl_library","text":"<pre><code>find_hccl_library() -&gt; str\n</code></pre> <p>We either use the library file specified by the <code>HCCL_SO_PATH</code> environment variable, or we find the library file brought by PyTorch. After importing <code>torch</code>, <code>libhccl.so</code> can be found by <code>ctypes</code> automatically.</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def find_hccl_library() -&gt; str:\n    \"\"\"\n    We either use the library file specified by the `HCCL_SO_PATH`\n    environment variable, or we find the library file brought by PyTorch.\n    After importing `torch`, `libhccl.so` can be\n    found by `ctypes` automatically.\n    \"\"\"\n    so_file = envs.HCCL_SO_PATH\n\n    # manually load the nccl library\n    if so_file:\n        logger.info(\"Found hccl from environment variable HCCL_SO_PATH=%s\",\n                    so_file)\n    else:\n        if torch.version.cann is not None:  # codespell:ignore cann\n            so_file = \"libhccl.so\"\n        else:\n            raise ValueError(\"HCCL only supports Ascend NPU backends.\")\n        logger.info(\"Found hccl from library %s\", so_file)\n    return so_file\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.utils.find_nccl_library","title":"fastvideo.utils.find_nccl_library","text":"<pre><code>find_nccl_library() -&gt; str\n</code></pre> <p>We either use the library file specified by the <code>FASTVIDEO_NCCL_SO_PATH</code> environment variable, or we find the library file brought by PyTorch. After importing <code>torch</code>, <code>libnccl.so.2</code> or <code>librccl.so.1</code> can be found by <code>ctypes</code> automatically.</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def find_nccl_library() -&gt; str:\n    \"\"\"\n    We either use the library file specified by the `FASTVIDEO_NCCL_SO_PATH`\n    environment variable, or we find the library file brought by PyTorch.\n    After importing `torch`, `libnccl.so.2` or `librccl.so.1` can be\n    found by `ctypes` automatically.\n    \"\"\"\n    so_file = envs.FASTVIDEO_NCCL_SO_PATH\n\n    # manually load the nccl library\n    if so_file:\n        logger.info(\n            \"Found nccl from environment variable FASTVIDEO_NCCL_SO_PATH=%s\",\n            so_file)\n    else:\n        if torch.version.cuda is not None:\n            so_file = \"libnccl.so.2\"\n        elif torch.version.hip is not None:\n            so_file = \"librccl.so.1\"\n        else:\n            raise ValueError(\"NCCL only supports CUDA and ROCm backends.\")\n        logger.info(\"Found nccl from library %s\", so_file)\n    return str(so_file)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.utils.get_compute_dtype","title":"fastvideo.utils.get_compute_dtype","text":"<pre><code>get_compute_dtype() -&gt; torch.dtype\n</code></pre> <p>Get the current compute dtype from mixed precision policy.</p> <p>Returns:</p> Type Description <code>dtype</code> <p>torch.dtype: The compute dtype to use, defaults to get_default_dtype() if no policy set</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def get_compute_dtype() -&gt; torch.dtype:\n    \"\"\"Get the current compute dtype from mixed precision policy.\n\n    Returns:\n        torch.dtype: The compute dtype to use, defaults to get_default_dtype() if no policy set\n    \"\"\"\n    if not hasattr(_mixed_precision_state, 'state'):\n        return torch.get_default_dtype()\n    else:\n        state = get_mixed_precision_state()\n        return state.param_dtype\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.utils.get_mixed_precision_state","title":"fastvideo.utils.get_mixed_precision_state","text":"<pre><code>get_mixed_precision_state() -&gt; MixedPrecisionState\n</code></pre> <p>Get the current mixed precision state.</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def get_mixed_precision_state() -&gt; MixedPrecisionState:\n    \"\"\"Get the current mixed precision state.\"\"\"\n    if not hasattr(_mixed_precision_state, 'state'):\n        raise ValueError(\"Mixed precision state not set\")\n    return cast(MixedPrecisionState, _mixed_precision_state.state)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.utils.get_mp_context","title":"fastvideo.utils.get_mp_context","text":"<pre><code>get_mp_context() -&gt; BaseContext\n</code></pre> <p>Get a multiprocessing context with a particular method (spawn or fork). By default we follow the value of the FASTVIDEO_WORKER_MULTIPROC_METHOD to determine the multiprocessing method (default is fork). However, under certain conditions, we may enforce spawn and override the value of FASTVIDEO_WORKER_MULTIPROC_METHOD.</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def get_mp_context() -&gt; BaseContext:\n    \"\"\"Get a multiprocessing context with a particular method (spawn or fork).\n    By default we follow the value of the FASTVIDEO_WORKER_MULTIPROC_METHOD to\n    determine the multiprocessing method (default is fork). However, under\n    certain conditions, we may enforce spawn and override the value of\n    FASTVIDEO_WORKER_MULTIPROC_METHOD.\n    \"\"\"\n    force_spawn()\n    mp_method = envs.FASTVIDEO_WORKER_MULTIPROC_METHOD\n    return multiprocessing.get_context(mp_method)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.utils.import_pynvml","title":"fastvideo.utils.import_pynvml","text":"<pre><code>import_pynvml()\n</code></pre> <p>Historical comments:</p> <p>libnvml.so is the library behind nvidia-smi, and pynvml is a Python wrapper around it. We use it to get GPU status without initializing CUDA context in the current process. Historically, there are two packages that provide pynvml: - <code>nvidia-ml-py</code> (https://pypi.org/project/nvidia-ml-py/): The official     wrapper. It is a dependency of FastVideo, and is installed when users     install FastVideo. It provides a Python module named <code>pynvml</code>. - <code>pynvml</code> (https://pypi.org/project/pynvml/): An unofficial wrapper.     Prior to version 12.0, it also provides a Python module <code>pynvml</code>,     and therefore conflicts with the official one which is a standalone Python file.     This causes errors when both of them are installed.     Starting from version 12.0, it migrates to a new module     named <code>pynvml_utils</code> to avoid the conflict. It is so confusing that many packages in the community use the unofficial one by mistake, and we have to handle this case. For example, <code>nvcr.io/nvidia/pytorch:24.12-py3</code> uses the unofficial one, and it will cause errors, see the issue https://github.com/vllm-project/vllm/issues/12847 for example. After all the troubles, we decide to copy the official <code>pynvml</code> module to our codebase, and use it directly.</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def import_pynvml():\n    \"\"\"\n    Historical comments:\n\n    libnvml.so is the library behind nvidia-smi, and\n    pynvml is a Python wrapper around it. We use it to get GPU\n    status without initializing CUDA context in the current process.\n    Historically, there are two packages that provide pynvml:\n    - `nvidia-ml-py` (https://pypi.org/project/nvidia-ml-py/): The official\n        wrapper. It is a dependency of FastVideo, and is installed when users\n        install FastVideo. It provides a Python module named `pynvml`.\n    - `pynvml` (https://pypi.org/project/pynvml/): An unofficial wrapper.\n        Prior to version 12.0, it also provides a Python module `pynvml`,\n        and therefore conflicts with the official one which is a standalone Python file.\n        This causes errors when both of them are installed.\n        Starting from version 12.0, it migrates to a new module\n        named `pynvml_utils` to avoid the conflict.\n    It is so confusing that many packages in the community use the\n    unofficial one by mistake, and we have to handle this case.\n    For example, `nvcr.io/nvidia/pytorch:24.12-py3` uses the unofficial\n    one, and it will cause errors, see the issue\n    https://github.com/vllm-project/vllm/issues/12847 for example.\n    After all the troubles, we decide to copy the official `pynvml`\n    module to our codebase, and use it directly.\n    \"\"\"\n    import fastvideo.third_party.pynvml as pynvml\n    return pynvml\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.utils.log_torch_cuda_memory","title":"fastvideo.utils.log_torch_cuda_memory","text":"<pre><code>log_torch_cuda_memory(\n    tag: str | None = None,\n    *,\n    log_fn: Callable[[str], None] | None = None,\n    log_file_path: str\n    | PathLike[str]\n    | None = \"memory_trace.txt\"\n) -&gt; None\n</code></pre> <p>Log CUDA memory statistics via logger and append to a trace file.</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def log_torch_cuda_memory(\n        tag: str | None = None,\n        *,\n        log_fn: Callable[[str], None] | None = None,\n        log_file_path: str | os.PathLike[str] | None = \"memory_trace.txt\"\n) -&gt; None:\n    \"\"\"Log CUDA memory statistics via logger and append to a trace file.\"\"\"\n\n    log_fn = log_fn or logger.info\n    prefix = f\"[{tag}] \" if tag else \"\"\n\n    if not torch.cuda.is_available():\n        message = f\"{prefix}CUDA not available on this host.\"\n        log_fn(message)\n        _append_to_memory_trace(message, log_file_path)\n        return\n\n    try:\n        device_index = torch.cuda.current_device()\n        device_name = torch.cuda.get_device_name(device_index)\n        allocated = torch.cuda.memory_allocated(device_index)\n        reserved = torch.cuda.memory_reserved(device_index)\n        max_allocated = torch.cuda.max_memory_allocated(device_index)\n        max_reserved = torch.cuda.max_memory_reserved(device_index)\n        free_mem, total_mem = torch.cuda.mem_get_info(device_index)\n    except Exception as exc:  # noqa: BLE001\n        message = f\"{prefix}Unable to query CUDA memory stats: {exc}\"\n        log_fn(message)\n        _append_to_memory_trace(message, log_file_path)\n        return\n\n    used_mem = total_mem - free_mem\n\n    stats = [\n        f\"device={device_name} (index={device_index})\",\n        f\"allocated={_format_bytes(allocated)}\",\n        f\"reserved={_format_bytes(reserved)}\",\n        f\"max_allocated={_format_bytes(max_allocated)}\",\n        f\"max_reserved={_format_bytes(max_reserved)}\",\n        f\"used={_format_bytes(used_mem)}\",\n        f\"free={_format_bytes(free_mem)}\",\n        f\"total={_format_bytes(total_mem)}\",\n    ]\n\n    message = f\"{prefix}CUDA memory stats: {' | '.join(stats)}\"\n    log_fn(message)\n    _append_to_memory_trace(message, log_file_path)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.utils.maybe_download_lora","title":"fastvideo.utils.maybe_download_lora","text":"<pre><code>maybe_download_lora(\n    model_name_or_path: str,\n    local_dir: str | None = None,\n    download: bool = True,\n) -&gt; str\n</code></pre> <p>Check if the model path is a Hugging Face Hub model ID and download it if needed. Args:     model_name_or_path: Local path or Hugging Face Hub model ID     local_dir: Local directory to save the model     download: Whether to download the model from Hugging Face Hub</p> <p>Returns:</p> Type Description <code>str</code> <p>Local path to the model</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def maybe_download_lora(model_name_or_path: str,\n                        local_dir: str | None = None,\n                        download: bool = True) -&gt; str:\n    \"\"\"\n    Check if the model path is a Hugging Face Hub model ID and download it if needed.\n    Args:\n        model_name_or_path: Local path or Hugging Face Hub model ID\n        local_dir: Local directory to save the model\n        download: Whether to download the model from Hugging Face Hub\n\n    Returns:\n        Local path to the model\n    \"\"\"\n\n    local_path = maybe_download_model(model_name_or_path, local_dir, download)\n    weight_name = _best_guess_weight_name(model_name_or_path,\n                                          file_extension=\".safetensors\")\n    return os.path.join(local_path, weight_name)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.utils.maybe_download_model","title":"fastvideo.utils.maybe_download_model","text":"<pre><code>maybe_download_model(\n    model_name_or_path: str,\n    local_dir: str | None = None,\n    download: bool = True,\n) -&gt; str\n</code></pre> <p>Check if the model path is a Hugging Face Hub model ID and download it if needed.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>Local path or Hugging Face Hub model ID</p> required <code>local_dir</code> <code>str | None</code> <p>Local directory to save the model</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download the model from Hugging Face Hub</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Local path to the model</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def maybe_download_model(model_name_or_path: str,\n                         local_dir: str | None = None,\n                         download: bool = True) -&gt; str:\n    \"\"\"\n    Check if the model path is a Hugging Face Hub model ID and download it if needed.\n\n    Args:\n        model_name_or_path: Local path or Hugging Face Hub model ID\n        local_dir: Local directory to save the model\n        download: Whether to download the model from Hugging Face Hub\n\n    Returns:\n        Local path to the model\n    \"\"\"\n\n    # If the path exists locally, return it\n    if os.path.exists(model_name_or_path):\n        logger.info(\"Model already exists locally at %s\", model_name_or_path)\n        return model_name_or_path\n\n    # Otherwise, assume it's a HF Hub model ID and try to download it\n    try:\n        logger.info(\"Downloading model snapshot from HF Hub for %s...\",\n                    model_name_or_path)\n        with get_lock(model_name_or_path):\n            local_path = snapshot_download(\n                repo_id=model_name_or_path,\n                ignore_patterns=[\"*.onnx\", \"*.msgpack\"],\n                local_dir=local_dir)\n        logger.info(\"Downloaded model to %s\", local_path)\n        return str(local_path)\n    except Exception as e:\n        raise ValueError(\n            f\"Could not find model at {model_name_or_path} and failed to download from HF Hub: {e}\"\n        ) from e\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.utils.maybe_download_model_index","title":"fastvideo.utils.maybe_download_model_index","text":"<pre><code>maybe_download_model_index(\n    model_name_or_path: str,\n) -&gt; dict[str, Any]\n</code></pre> <p>Download and extract just the model_index.json for a Hugging Face model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>Path or HF Hub model ID</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The parsed model_index.json as a dictionary</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def maybe_download_model_index(model_name_or_path: str) -&gt; dict[str, Any]:\n    \"\"\"\n    Download and extract just the model_index.json for a Hugging Face model.\n\n    Args:\n        model_name_or_path: Path or HF Hub model ID\n\n    Returns:\n        The parsed model_index.json as a dictionary\n    \"\"\"\n    import tempfile\n\n    from huggingface_hub import hf_hub_download\n\n    # If it's a local path, verify it directly\n    if os.path.exists(model_name_or_path):\n        return verify_model_config_and_directory(model_name_or_path)\n\n    # For remote models, download just the model_index.json\n    try:\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            # Download just the model_index.json file\n            model_index_path = hf_hub_download(repo_id=model_name_or_path,\n                                               filename=\"model_index.json\",\n                                               local_dir=tmp_dir)\n\n            # Load the model_index.json\n            with open(model_index_path) as f:\n                config: dict[str, Any] = json.load(f)\n\n            # Verify it has the required fields\n            if \"_class_name\" not in config:\n                raise ValueError(\n                    f\"model_index.json for {model_name_or_path} does not contain _class_name field\"\n                )\n\n            if \"_diffusers_version\" not in config:\n                raise ValueError(\n                    f\"model_index.json for {model_name_or_path} does not contain _diffusers_version field\"\n                )\n\n            # Add the pipeline name for downstream use\n            config[\"pipeline_name\"] = config[\"_class_name\"]\n\n            logger.info(\"Downloaded model_index.json for %s, pipeline: %s\",\n                        model_name_or_path, config[\"_class_name\"])\n            return config\n\n    except Exception as e:\n        raise ValueError(\n            f\"Failed to download or parse model_index.json for {model_name_or_path}: {e}\"\n        ) from e\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.utils.resolve_obj_by_qualname","title":"fastvideo.utils.resolve_obj_by_qualname","text":"<pre><code>resolve_obj_by_qualname(qualname: str) -&gt; Any\n</code></pre> <p>Resolve an object by its fully qualified name.</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def resolve_obj_by_qualname(qualname: str) -&gt; Any:\n    \"\"\"\n    Resolve an object by its fully qualified name.\n    \"\"\"\n    module_name, obj_name = qualname.rsplit(\".\", 1)\n    module = importlib.import_module(module_name)\n    return getattr(module, obj_name)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.utils.run_method","title":"fastvideo.utils.run_method","text":"<pre><code>run_method(\n    obj: Any,\n    method: str | bytes | Callable,\n    args: tuple[Any],\n    kwargs: dict[str, Any],\n) -&gt; Any\n</code></pre> <p>Run a method of an object with the given arguments and keyword arguments. If the method is string, it will be converted to a method using getattr. If the method is serialized bytes and will be deserialized using cloudpickle. If the method is a callable, it will be called directly.</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def run_method(obj: Any, method: str | bytes | Callable, args: tuple[Any],\n               kwargs: dict[str, Any]) -&gt; Any:\n    \"\"\"\n    Run a method of an object with the given arguments and keyword arguments.\n    If the method is string, it will be converted to a method using getattr.\n    If the method is serialized bytes and will be deserialized using\n    cloudpickle.\n    If the method is a callable, it will be called directly.\n    \"\"\"\n    if isinstance(method, bytes):\n        func = partial(cloudpickle.loads(method), obj)\n    elif isinstance(method, str):\n        try:\n            func = getattr(obj, method)\n        except AttributeError:\n            raise NotImplementedError(f\"Method {method!r} is not\"\n                                      \" implemented.\") from None\n    else:\n        func = partial(method, obj)  # type: ignore\n    return func(*args, **kwargs)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.utils.set_mixed_precision_policy","title":"fastvideo.utils.set_mixed_precision_policy","text":"<pre><code>set_mixed_precision_policy(\n    param_dtype: dtype,\n    reduce_dtype: dtype,\n    output_dtype: dtype | None = None,\n    mp_policy: MixedPrecisionPolicy | None = None,\n)\n</code></pre> <p>Set mixed precision policy globally.</p> <p>Parameters:</p> Name Type Description Default <code>param_dtype</code> <code>dtype</code> <p>Parameter dtype used for training</p> required <code>reduce_dtype</code> <code>dtype</code> <p>Reduction dtype used for gradients</p> required <code>output_dtype</code> <code>dtype | None</code> <p>Optional output dtype</p> <code>None</code> Source code in <code>fastvideo/utils.py</code> <pre><code>def set_mixed_precision_policy(\n    param_dtype: torch.dtype,\n    reduce_dtype: torch.dtype,\n    output_dtype: torch.dtype | None = None,\n    mp_policy: MixedPrecisionPolicy | None = None,\n):\n    \"\"\"Set mixed precision policy globally.\n\n    Args:\n        param_dtype: Parameter dtype used for training\n        reduce_dtype: Reduction dtype used for gradients\n        output_dtype: Optional output dtype\n    \"\"\"\n    state = MixedPrecisionState(\n        param_dtype=param_dtype,\n        reduce_dtype=reduce_dtype,\n        output_dtype=output_dtype,\n        mp_policy=mp_policy,\n    )\n    _mixed_precision_state.state = state\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.utils.verify_model_config_and_directory","title":"fastvideo.utils.verify_model_config_and_directory","text":"<pre><code>verify_model_config_and_directory(\n    model_path: str,\n) -&gt; dict[str, Any]\n</code></pre> <p>Verify that the model directory contains a valid diffusers configuration.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the model directory</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The loaded model configuration as a dictionary</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def verify_model_config_and_directory(model_path: str) -&gt; dict[str, Any]:\n    \"\"\"\n    Verify that the model directory contains a valid diffusers configuration.\n\n    Args:\n        model_path: Path to the model directory\n\n    Returns:\n        The loaded model configuration as a dictionary\n    \"\"\"\n\n    # Check for model_index.json which is required for diffusers models\n    config_path = os.path.join(model_path, \"model_index.json\")\n    if not os.path.exists(config_path):\n        raise ValueError(\n            f\"Model directory {model_path} does not contain model_index.json. \"\n            \"Only Hugging Face diffusers format is supported.\")\n\n    # Check for transformer and vae directories\n    transformer_dir = os.path.join(model_path, \"transformer\")\n    vae_dir = os.path.join(model_path, \"vae\")\n\n    if not os.path.exists(transformer_dir):\n        raise ValueError(\n            f\"Model directory {model_path} does not contain a transformer/ directory.\"\n        )\n\n    if not os.path.exists(vae_dir):\n        raise ValueError(\n            f\"Model directory {model_path} does not contain a vae/ directory.\")\n\n    # Load the config\n    with open(config_path) as f:\n        config = json.load(f)\n\n    # Verify diffusers version exists\n    if \"_diffusers_version\" not in config:\n        raise ValueError(\"model_index.json does not contain _diffusers_version\")\n\n    logger.info(\"Diffusers version: %s\", config[\"_diffusers_version\"])\n    return cast(dict[str, Any], config)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.utils.warn_for_unimplemented_methods","title":"fastvideo.utils.warn_for_unimplemented_methods","text":"<pre><code>warn_for_unimplemented_methods(cls: type[T]) -&gt; type[T]\n</code></pre> <p>A replacement for <code>abc.ABC</code>. When we use <code>abc.ABC</code>, subclasses will fail to instantiate if they do not implement all abstract methods. Here, we only require <code>raise NotImplementedError</code> in the base class, and log a warning if the method is not implemented in the subclass.</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def warn_for_unimplemented_methods(cls: type[T]) -&gt; type[T]:\n    \"\"\"\n    A replacement for `abc.ABC`.\n    When we use `abc.ABC`, subclasses will fail to instantiate\n    if they do not implement all abstract methods.\n    Here, we only require `raise NotImplementedError` in the\n    base class, and log a warning if the method is not implemented\n    in the subclass.\n    \"\"\"\n\n    original_init = cls.__init__\n\n    def find_unimplemented_methods(self: object):\n        unimplemented_methods = []\n        for attr_name in dir(self):\n            # bypass inner method\n            if attr_name.startswith('_'):\n                continue\n\n            try:\n                attr = getattr(self, attr_name)\n                # get the func of callable method\n                if callable(attr):\n                    attr_func = attr.__func__\n            except AttributeError:\n                continue\n            src = inspect.getsource(attr_func)\n            if \"NotImplementedError\" in src:\n                unimplemented_methods.append(attr_name)\n        if unimplemented_methods:\n            method_names = ','.join(unimplemented_methods)\n            msg = (f\"Methods {method_names} not implemented in {self}\")\n            logger.warning(msg)\n\n    @wraps(original_init)\n    def wrapped_init(self, *args, **kwargs) -&gt; None:\n        original_init(self, *args, **kwargs)\n        find_unimplemented_methods(self)\n\n    type.__setattr__(cls, '__init__', wrapped_init)\n    return cls\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.utils.xpu_is_initialized","title":"fastvideo.utils.xpu_is_initialized","text":"<pre><code>xpu_is_initialized() -&gt; bool\n</code></pre> <p>Check if XPU is initialized.</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def xpu_is_initialized() -&gt; bool:\n    \"\"\"Check if XPU is initialized.\"\"\"\n    if not torch.xpu._is_compiled():\n        return False\n    return torch.xpu.is_initialized()\n</code></pre>"},{"location":"api/fastvideo/#fastvideoworker","title":"fastvideo.worker","text":""},{"location":"api/fastvideo/#fastvideo.worker","title":"worker","text":""},{"location":"api/fastvideo/#fastvideo.worker-classes","title":"Classes","text":""},{"location":"api/fastvideo/#fastvideo.worker.Executor","title":"fastvideo.worker.Executor","text":"<pre><code>Executor(fastvideo_args: FastVideoArgs)\n</code></pre> <p>               Bases: <code>ABC</code></p> Source code in <code>fastvideo/worker/executor.py</code> <pre><code>def __init__(self, fastvideo_args: FastVideoArgs):\n    self.fastvideo_args = fastvideo_args\n\n    self._init_executor()\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.worker.Executor-functions","title":"Functions","text":"fastvideo.worker.Executor.collective_rpc <code>abstractmethod</code> \u00b6 <pre><code>collective_rpc(\n    method: str | Callable[..., _R],\n    timeout: float | None = None,\n    args: tuple = (),\n    kwargs: dict[str, Any] | None = None,\n) -&gt; list[_R]\n</code></pre> <p>Execute an RPC call on all workers.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str | Callable[..., _R]</code> <p>Name of the worker method to execute, or a callable that is serialized and sent to all workers to execute.</p> <p>If the method is a callable, it should accept an additional <code>self</code> argument, in addition to the arguments passed in <code>args</code> and <code>kwargs</code>. The <code>self</code> argument will be the worker object.</p> required <code>timeout</code> <code>float | None</code> <p>Maximum time in seconds to wait for execution. Raises a :exc:<code>TimeoutError</code> on timeout. <code>None</code> means wait indefinitely.</p> <code>None</code> <code>args</code> <code>tuple</code> <p>Positional arguments to pass to the worker method.</p> <code>()</code> <code>kwargs</code> <code>dict[str, Any] | None</code> <p>Keyword arguments to pass to the worker method.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[_R]</code> <p>A list containing the results from each worker.</p> Note <p>It is recommended to use this API to only pass control messages, and set up data-plane communication to pass data.</p> Source code in <code>fastvideo/worker/executor.py</code> <pre><code>@abstractmethod\ndef collective_rpc(self,\n                   method: str | Callable[..., _R],\n                   timeout: float | None = None,\n                   args: tuple = (),\n                   kwargs: dict[str, Any] | None = None) -&gt; list[_R]:\n    \"\"\"\n    Execute an RPC call on all workers.\n\n    Args:\n        method: Name of the worker method to execute, or a callable that\n            is serialized and sent to all workers to execute.\n\n            If the method is a callable, it should accept an additional\n            `self` argument, in addition to the arguments passed in `args`\n            and `kwargs`. The `self` argument will be the worker object.\n        timeout: Maximum time in seconds to wait for execution. Raises a\n            :exc:`TimeoutError` on timeout. `None` means wait indefinitely.\n        args: Positional arguments to pass to the worker method.\n        kwargs: Keyword arguments to pass to the worker method.\n\n    Returns:\n        A list containing the results from each worker.\n\n    Note:\n        It is recommended to use this API to only pass control messages,\n        and set up data-plane communication to pass data.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.worker.Executor.merge_lora_weights <code>abstractmethod</code> \u00b6 <pre><code>merge_lora_weights() -&gt; None\n</code></pre> <p>Merge the LoRA weights for the workers.</p> Source code in <code>fastvideo/worker/executor.py</code> <pre><code>@abstractmethod\ndef merge_lora_weights(self) -&gt; None:\n    \"\"\"\n    Merge the LoRA weights for the workers.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.worker.Executor.set_lora_adapter <code>abstractmethod</code> \u00b6 <pre><code>set_lora_adapter(\n    lora_nickname: str, lora_path: str | None = None\n) -&gt; None\n</code></pre> <p>Set the LoRA adapter for the workers.</p> Source code in <code>fastvideo/worker/executor.py</code> <pre><code>@abstractmethod\ndef set_lora_adapter(self,\n                     lora_nickname: str,\n                     lora_path: str | None = None) -&gt; None:\n    \"\"\"\n    Set the LoRA adapter for the workers.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.worker.Executor.shutdown <code>abstractmethod</code> \u00b6 <pre><code>shutdown() -&gt; None\n</code></pre> <p>Shutdown the executor.</p> Source code in <code>fastvideo/worker/executor.py</code> <pre><code>@abstractmethod\ndef shutdown(self) -&gt; None:\n    \"\"\"\n    Shutdown the executor.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.worker.Executor.unmerge_lora_weights <code>abstractmethod</code> \u00b6 <pre><code>unmerge_lora_weights() -&gt; None\n</code></pre> <p>Unmerge the LoRA weights for the workers.</p> Source code in <code>fastvideo/worker/executor.py</code> <pre><code>@abstractmethod\ndef unmerge_lora_weights(self) -&gt; None:\n    \"\"\"\n    Unmerge the LoRA weights for the workers.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.worker.MultiprocExecutor","title":"fastvideo.worker.MultiprocExecutor","text":"<pre><code>MultiprocExecutor(fastvideo_args: FastVideoArgs)\n</code></pre> <p>               Bases: <code>Executor</code></p> Source code in <code>fastvideo/worker/executor.py</code> <pre><code>def __init__(self, fastvideo_args: FastVideoArgs):\n    self.fastvideo_args = fastvideo_args\n\n    self._init_executor()\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.worker.MultiprocExecutor-functions","title":"Functions","text":"fastvideo.worker.MultiprocExecutor.__del__ \u00b6 <pre><code>__del__()\n</code></pre> <p>Ensure cleanup on garbage collection</p> Source code in <code>fastvideo/worker/multiproc_executor.py</code> <pre><code>def __del__(self):\n    \"\"\"Ensure cleanup on garbage collection\"\"\"\n    self.shutdown()\n</code></pre> fastvideo.worker.MultiprocExecutor.__enter__ \u00b6 <pre><code>__enter__()\n</code></pre> <p>Support for context manager protocol</p> Source code in <code>fastvideo/worker/multiproc_executor.py</code> <pre><code>def __enter__(self):\n    \"\"\"Support for context manager protocol\"\"\"\n    return self\n</code></pre> fastvideo.worker.MultiprocExecutor.__exit__ \u00b6 <pre><code>__exit__(exc_type, exc_val, exc_tb)\n</code></pre> <p>Ensure cleanup when exiting context</p> Source code in <code>fastvideo/worker/multiproc_executor.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Ensure cleanup when exiting context\"\"\"\n    self.shutdown()\n</code></pre> fastvideo.worker.MultiprocExecutor.shutdown \u00b6 <pre><code>shutdown() -&gt; None\n</code></pre> <p>Properly shut down the executor and its workers</p> Source code in <code>fastvideo/worker/multiproc_executor.py</code> <pre><code>def shutdown(self) -&gt; None:\n    \"\"\"Properly shut down the executor and its workers\"\"\"\n    if hasattr(self, 'shutting_down') and self.shutting_down:\n        return  # Prevent multiple shutdown calls\n\n    logger.info(\"Shutting down MultiprocExecutor...\")\n    self.shutting_down = True\n\n    # First try gentle termination\n    try:\n        # Send termination message to all workers\n        for worker in self.workers:\n            with contextlib.suppress(Exception):\n                worker.pipe.send({\n                    \"method\": \"shutdown\",\n                    \"args\": (),\n                    \"kwargs\": {}\n                })\n\n        # Give workers some time to exit gracefully\n        start_time = time.perf_counter()\n        while time.perf_counter() - start_time &lt; 5.0:  # 5 seconds timeout\n            if all(not worker.proc.is_alive() for worker in self.workers):\n                break\n            time.sleep(0.1)\n\n        # Force terminate any remaining workers\n        for worker in self.workers:\n            if worker.proc.is_alive():\n                worker.proc.terminate()\n\n        # Final timeout for terminate\n        start_time = time.perf_counter()\n        while time.perf_counter() - start_time &lt; 2.0:  # 2 seconds timeout\n            if all(not worker.proc.is_alive() for worker in self.workers):\n                break\n            time.sleep(0.1)\n\n        # Kill if still alive\n        for worker in self.workers:\n            if worker.proc.is_alive():\n                worker.proc.kill()\n            worker.proc.join(timeout=1.0)\n\n    except Exception as e:\n        logger.error(\"Error during shutdown: %s\", e)\n        # Last resort, try to kill all workers\n        for worker in self.workers:\n            with contextlib.suppress(Exception):\n                if worker.proc.is_alive():\n                    worker.proc.kill()\n\n    # Clean up pipes\n    for worker in self.workers:\n        with contextlib.suppress(Exception):\n            worker.pipe.close()\n\n    self.workers = []\n    logger.info(\"MultiprocExecutor shutdown complete\")\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.worker-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideo.worker.initialize_ray_cluster","title":"fastvideo.worker.initialize_ray_cluster","text":"<pre><code>initialize_ray_cluster(\n    fastvideo_args: FastVideoArgs,\n    ray_address: str | None = None,\n)\n</code></pre> <p>Initialize the distributed cluster with Ray.</p> <p>it will connect to the Ray cluster and create a placement group for the workers, which includes the specification of the resources for each distributed worker.</p> <p>Parameters:</p> Name Type Description Default <code>parallel_config</code> <p>The configurations for parallel execution.</p> required <code>ray_address</code> <code>str | None</code> <p>The address of the Ray cluster. If None, uses the default Ray cluster address.</p> <code>None</code> Source code in <code>fastvideo/worker/ray_utils.py</code> <pre><code>def initialize_ray_cluster(\n    fastvideo_args: FastVideoArgs,\n    ray_address: str | None = None,\n):\n    \"\"\"Initialize the distributed cluster with Ray.\n\n    it will connect to the Ray cluster and create a placement group\n    for the workers, which includes the specification of the resources\n    for each distributed worker.\n\n    Args:\n        parallel_config: The configurations for parallel execution.\n        ray_address: The address of the Ray cluster. If None, uses\n            the default Ray cluster address.\n    \"\"\"\n    assert_ray_available()\n    from fastvideo.platforms import current_platform\n\n    if ray.is_initialized():\n        logger.info(\"Ray is already initialized. Skipping Ray initialization.\")\n    elif current_platform.is_rocm() or current_platform.is_xpu():\n        # Try to connect existing ray instance and create a new one if not found\n        try:\n            ray.init(\"auto\")\n        except ConnectionError:\n            logger.warning(\n                \"No existing RAY instance detected. \"\n                \"A new instance will be launched with current node resources.\")\n            ray.init(address=ray_address,\n                     num_gpus=fastvideo_args.num_gpus,\n                     runtime_env=fastvideo_args.ray_runtime_env)\n    else:\n        ray.init(address=ray_address,\n                 runtime_env=fastvideo_args.ray_runtime_env)\n\n    device_str = current_platform.ray_device_key\n    if not device_str:\n        raise ValueError(\n            f\"current platform {current_platform.device_name} does not \"\n            \"support ray.\")\n\n    # Create or get the placement group for worker processes\n    if fastvideo_args.ray_placement_group:\n        current_placement_group = fastvideo_args.ray_placement_group\n    else:\n        current_placement_group = ray.util.get_current_placement_group()\n\n    if current_placement_group:\n        logger.info(\"Using the existing placement group\")\n\n        # We are in a placement group\n        bundles = current_placement_group.bundle_specs\n        # Verify that we can use the placement group.\n        device_bundles = 0\n        for bundle in bundles:\n            bundle_devices = bundle.get(device_str, 0)\n            if bundle_devices &gt; 1:\n                raise ValueError(\n                    \"Placement group bundle cannot have more than 1 \"\n                    f\"{device_str}.\")\n            if bundle_devices:\n                device_bundles += 1\n        if fastvideo_args.num_gpus &gt; device_bundles:\n            raise ValueError(\n                f\"The number of required {device_str}s exceeds the total \"\n                f\"number of available {device_str}s in the placement group. \"\n                f\"Required number of devices: {fastvideo_args.num_gpus}. \"\n                f\"Total number of devices: {device_bundles}.\")\n    else:\n        logger.info(\"No current placement group found. \"\n                    \"Creating a new placement group.\")\n        num_devices_in_cluster = ray.cluster_resources().get(device_str, 0)\n        # Log a warning message and delay resource allocation failure response.\n        # Avoid immediate rejection to allow user-initiated placement group\n        # created and wait cluster to be ready\n        if fastvideo_args.num_gpus &gt; num_devices_in_cluster:\n            logger.warning(\n                \"The number of required %ss exceeds the total \"\n                \"number of available %ss in the placement group.\", device_str,\n                device_str)\n        # Create a new placement group\n        placement_group_specs: list[dict[str, float]] = ([{\n            device_str: 1.0\n        } for _ in range(fastvideo_args.num_gpus)])\n\n        # FastVideo engine is also a worker to execute model with an accelerator,\n        # so it requires to have the device in a current node. Check if\n        # the current node has at least one device.\n        current_ip = get_ip()\n        current_node_id = ray.get_runtime_context().get_node_id()\n        current_node_resource = available_resources_per_node()[current_node_id]\n        if current_node_resource.get(device_str, 0) &lt; 1:\n            raise ValueError(\n                f\"Current node has no {device_str} available. \"\n                f\"{current_node_resource=}. FastVideo engine cannot start without \"\n                f\"{device_str}. Make sure you have at least 1 {device_str} \"\n                f\"available in a node {current_node_id=} {current_ip=}.\")\n        # This way, at least bundle is required to be created in a current\n        # node.\n        placement_group_specs[0][f\"node:{current_ip}\"] = 0.001\n\n        # By default, Ray packs resources as much as possible.\n        current_placement_group = ray.util.placement_group(\n            placement_group_specs, strategy=\"PACK\")\n        _wait_until_pg_ready(current_placement_group)\n\n    assert current_placement_group is not None\n    _verify_bundles(current_placement_group, fastvideo_args, device_str)\n    # Set the placement group in the fastvideo args\n    fastvideo_args.ray_placement_group = current_placement_group\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.worker-modules","title":"Modules","text":""},{"location":"api/fastvideo/#fastvideo.worker.executor","title":"fastvideo.worker.executor","text":""},{"location":"api/fastvideo/#fastvideo.worker.executor-classes","title":"Classes","text":"fastvideo.worker.executor.Executor \u00b6 <pre><code>Executor(fastvideo_args: FastVideoArgs)\n</code></pre> <p>               Bases: <code>ABC</code></p> Source code in <code>fastvideo/worker/executor.py</code> <pre><code>def __init__(self, fastvideo_args: FastVideoArgs):\n    self.fastvideo_args = fastvideo_args\n\n    self._init_executor()\n</code></pre> Functions\u00b6 fastvideo.worker.executor.Executor.collective_rpc <code>abstractmethod</code> \u00b6 <pre><code>collective_rpc(\n    method: str | Callable[..., _R],\n    timeout: float | None = None,\n    args: tuple = (),\n    kwargs: dict[str, Any] | None = None,\n) -&gt; list[_R]\n</code></pre> <p>Execute an RPC call on all workers.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str | Callable[..., _R]</code> <p>Name of the worker method to execute, or a callable that is serialized and sent to all workers to execute.</p> <p>If the method is a callable, it should accept an additional <code>self</code> argument, in addition to the arguments passed in <code>args</code> and <code>kwargs</code>. The <code>self</code> argument will be the worker object.</p> required <code>timeout</code> <code>float | None</code> <p>Maximum time in seconds to wait for execution. Raises a :exc:<code>TimeoutError</code> on timeout. <code>None</code> means wait indefinitely.</p> <code>None</code> <code>args</code> <code>tuple</code> <p>Positional arguments to pass to the worker method.</p> <code>()</code> <code>kwargs</code> <code>dict[str, Any] | None</code> <p>Keyword arguments to pass to the worker method.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[_R]</code> <p>A list containing the results from each worker.</p> Note <p>It is recommended to use this API to only pass control messages, and set up data-plane communication to pass data.</p> Source code in <code>fastvideo/worker/executor.py</code> <pre><code>@abstractmethod\ndef collective_rpc(self,\n                   method: str | Callable[..., _R],\n                   timeout: float | None = None,\n                   args: tuple = (),\n                   kwargs: dict[str, Any] | None = None) -&gt; list[_R]:\n    \"\"\"\n    Execute an RPC call on all workers.\n\n    Args:\n        method: Name of the worker method to execute, or a callable that\n            is serialized and sent to all workers to execute.\n\n            If the method is a callable, it should accept an additional\n            `self` argument, in addition to the arguments passed in `args`\n            and `kwargs`. The `self` argument will be the worker object.\n        timeout: Maximum time in seconds to wait for execution. Raises a\n            :exc:`TimeoutError` on timeout. `None` means wait indefinitely.\n        args: Positional arguments to pass to the worker method.\n        kwargs: Keyword arguments to pass to the worker method.\n\n    Returns:\n        A list containing the results from each worker.\n\n    Note:\n        It is recommended to use this API to only pass control messages,\n        and set up data-plane communication to pass data.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.worker.executor.Executor.merge_lora_weights <code>abstractmethod</code> \u00b6 <pre><code>merge_lora_weights() -&gt; None\n</code></pre> <p>Merge the LoRA weights for the workers.</p> Source code in <code>fastvideo/worker/executor.py</code> <pre><code>@abstractmethod\ndef merge_lora_weights(self) -&gt; None:\n    \"\"\"\n    Merge the LoRA weights for the workers.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.worker.executor.Executor.set_lora_adapter <code>abstractmethod</code> \u00b6 <pre><code>set_lora_adapter(\n    lora_nickname: str, lora_path: str | None = None\n) -&gt; None\n</code></pre> <p>Set the LoRA adapter for the workers.</p> Source code in <code>fastvideo/worker/executor.py</code> <pre><code>@abstractmethod\ndef set_lora_adapter(self,\n                     lora_nickname: str,\n                     lora_path: str | None = None) -&gt; None:\n    \"\"\"\n    Set the LoRA adapter for the workers.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.worker.executor.Executor.shutdown <code>abstractmethod</code> \u00b6 <pre><code>shutdown() -&gt; None\n</code></pre> <p>Shutdown the executor.</p> Source code in <code>fastvideo/worker/executor.py</code> <pre><code>@abstractmethod\ndef shutdown(self) -&gt; None:\n    \"\"\"\n    Shutdown the executor.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.worker.executor.Executor.unmerge_lora_weights <code>abstractmethod</code> \u00b6 <pre><code>unmerge_lora_weights() -&gt; None\n</code></pre> <p>Unmerge the LoRA weights for the workers.</p> Source code in <code>fastvideo/worker/executor.py</code> <pre><code>@abstractmethod\ndef unmerge_lora_weights(self) -&gt; None:\n    \"\"\"\n    Unmerge the LoRA weights for the workers.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.worker.executor-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideo.worker.gpu_worker","title":"fastvideo.worker.gpu_worker","text":""},{"location":"api/fastvideo/#fastvideo.worker.gpu_worker-classes","title":"Classes","text":"fastvideo.worker.gpu_worker.Worker \u00b6 <pre><code>Worker(\n    fastvideo_args: FastVideoArgs,\n    local_rank: int,\n    rank: int,\n    distributed_init_method: str,\n)\n</code></pre> Source code in <code>fastvideo/worker/gpu_worker.py</code> <pre><code>def __init__(self, fastvideo_args: FastVideoArgs, local_rank: int,\n             rank: int, distributed_init_method: str):\n    self.fastvideo_args = fastvideo_args\n    self.local_rank = local_rank\n    self.rank = rank\n    self.distributed_init_method = distributed_init_method\n</code></pre> Functions\u00b6 fastvideo.worker.gpu_worker.Worker.init_device \u00b6 <pre><code>init_device() -&gt; None\n</code></pre> <p>Initialize the device for the worker.</p> Source code in <code>fastvideo/worker/gpu_worker.py</code> <pre><code>def init_device(self) -&gt; None:\n    \"\"\"Initialize the device for the worker.\"\"\"\n\n    # torch.distributed.all_reduce does not free the input tensor until\n    # the synchronization point. This causes the memory usage to grow\n    # as the number of all_reduce calls increases. This env var disables\n    # this behavior.\n    # Related issue:\n    # https://discuss.pytorch.org/t/cuda-allocation-lifetime-for-inputs-to-distributed-all-reduce/191573\n    os.environ[\"TORCH_NCCL_AVOID_RECORD_STREAMS\"] = \"1\"\n    # This env var set by Ray causes exceptions with graph building.\n    os.environ.pop(\"NCCL_ASYNC_ERROR_HANDLING\", None)\n\n    # Platform-agnostic device initialization\n    self.device = get_local_torch_device()\n\n    from fastvideo.platforms import current_platform\n\n    # _check_if_gpu_supports_dtype(self.model_config.dtype)\n    if current_platform.is_cuda_alike():\n        self.init_gpu_memory = torch.cuda.mem_get_info()[0]\n    else:\n        # For MPS, we can't get memory info the same way\n        self.init_gpu_memory = 0\n\n    if self.fastvideo_args.distributed_executor_backend == \"mp\":\n        os.environ[\"LOCAL_RANK\"] = str(self.local_rank)\n    os.environ[\"RANK\"] = str(self.rank)\n    os.environ[\"WORLD_SIZE\"] = str(self.fastvideo_args.num_gpus)\n\n    # Initialize the distributed environment.\n    maybe_init_distributed_environment_and_model_parallel(\n        self.fastvideo_args.tp_size, self.fastvideo_args.sp_size,\n        self.distributed_init_method)\n\n    self.pipeline = build_pipeline(self.fastvideo_args)\n</code></pre> fastvideo.worker.gpu_worker.Worker.shutdown \u00b6 <pre><code>shutdown() -&gt; dict[str, Any]\n</code></pre> <p>Gracefully shut down the worker process</p> Source code in <code>fastvideo/worker/gpu_worker.py</code> <pre><code>def shutdown(self) -&gt; dict[str, Any]:\n    \"\"\"Gracefully shut down the worker process\"\"\"\n    logger.info(\"Worker %d shutting down...\",\n                self.rank,\n                local_main_process_only=False)\n    # Clean up resources\n    if hasattr(self, 'pipeline') and self.pipeline is not None:\n        # Clean up pipeline resources if needed\n        pass\n\n    # Destroy the distributed environment\n    cleanup_dist_env_and_memory(shutdown_ray=False)\n\n    logger.info(\"Worker %d shutdown complete\",\n                self.rank,\n                local_main_process_only=False)\n    return {\"status\": \"shutdown_complete\"}\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.worker.gpu_worker-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideo.worker.multiproc_executor","title":"fastvideo.worker.multiproc_executor","text":""},{"location":"api/fastvideo/#fastvideo.worker.multiproc_executor-classes","title":"Classes","text":"fastvideo.worker.multiproc_executor.MultiprocExecutor \u00b6 <pre><code>MultiprocExecutor(fastvideo_args: FastVideoArgs)\n</code></pre> <p>               Bases: <code>Executor</code></p> Source code in <code>fastvideo/worker/executor.py</code> <pre><code>def __init__(self, fastvideo_args: FastVideoArgs):\n    self.fastvideo_args = fastvideo_args\n\n    self._init_executor()\n</code></pre> Functions\u00b6 fastvideo.worker.multiproc_executor.MultiprocExecutor.__del__ \u00b6 <pre><code>__del__()\n</code></pre> <p>Ensure cleanup on garbage collection</p> Source code in <code>fastvideo/worker/multiproc_executor.py</code> <pre><code>def __del__(self):\n    \"\"\"Ensure cleanup on garbage collection\"\"\"\n    self.shutdown()\n</code></pre> fastvideo.worker.multiproc_executor.MultiprocExecutor.__enter__ \u00b6 <pre><code>__enter__()\n</code></pre> <p>Support for context manager protocol</p> Source code in <code>fastvideo/worker/multiproc_executor.py</code> <pre><code>def __enter__(self):\n    \"\"\"Support for context manager protocol\"\"\"\n    return self\n</code></pre> fastvideo.worker.multiproc_executor.MultiprocExecutor.__exit__ \u00b6 <pre><code>__exit__(exc_type, exc_val, exc_tb)\n</code></pre> <p>Ensure cleanup when exiting context</p> Source code in <code>fastvideo/worker/multiproc_executor.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Ensure cleanup when exiting context\"\"\"\n    self.shutdown()\n</code></pre> fastvideo.worker.multiproc_executor.MultiprocExecutor.shutdown \u00b6 <pre><code>shutdown() -&gt; None\n</code></pre> <p>Properly shut down the executor and its workers</p> Source code in <code>fastvideo/worker/multiproc_executor.py</code> <pre><code>def shutdown(self) -&gt; None:\n    \"\"\"Properly shut down the executor and its workers\"\"\"\n    if hasattr(self, 'shutting_down') and self.shutting_down:\n        return  # Prevent multiple shutdown calls\n\n    logger.info(\"Shutting down MultiprocExecutor...\")\n    self.shutting_down = True\n\n    # First try gentle termination\n    try:\n        # Send termination message to all workers\n        for worker in self.workers:\n            with contextlib.suppress(Exception):\n                worker.pipe.send({\n                    \"method\": \"shutdown\",\n                    \"args\": (),\n                    \"kwargs\": {}\n                })\n\n        # Give workers some time to exit gracefully\n        start_time = time.perf_counter()\n        while time.perf_counter() - start_time &lt; 5.0:  # 5 seconds timeout\n            if all(not worker.proc.is_alive() for worker in self.workers):\n                break\n            time.sleep(0.1)\n\n        # Force terminate any remaining workers\n        for worker in self.workers:\n            if worker.proc.is_alive():\n                worker.proc.terminate()\n\n        # Final timeout for terminate\n        start_time = time.perf_counter()\n        while time.perf_counter() - start_time &lt; 2.0:  # 2 seconds timeout\n            if all(not worker.proc.is_alive() for worker in self.workers):\n                break\n            time.sleep(0.1)\n\n        # Kill if still alive\n        for worker in self.workers:\n            if worker.proc.is_alive():\n                worker.proc.kill()\n            worker.proc.join(timeout=1.0)\n\n    except Exception as e:\n        logger.error(\"Error during shutdown: %s\", e)\n        # Last resort, try to kill all workers\n        for worker in self.workers:\n            with contextlib.suppress(Exception):\n                if worker.proc.is_alive():\n                    worker.proc.kill()\n\n    # Clean up pipes\n    for worker in self.workers:\n        with contextlib.suppress(Exception):\n            worker.pipe.close()\n\n    self.workers = []\n    logger.info(\"MultiprocExecutor shutdown complete\")\n</code></pre> fastvideo.worker.multiproc_executor.UnreadyWorkerProcHandle <code>dataclass</code> \u00b6 <pre><code>UnreadyWorkerProcHandle(\n    proc: BaseProcess,\n    rank: int,\n    pipe: Connection,\n    ready_pipe: Connection,\n)\n</code></pre> <p>WorkerProcess handle before READY.</p> fastvideo.worker.multiproc_executor.WorkerMultiprocProc \u00b6 <pre><code>WorkerMultiprocProc(\n    fastvideo_args: FastVideoArgs,\n    local_rank: int,\n    rank: int,\n    distributed_init_method: str,\n    pipe: Connection,\n)\n</code></pre> <p>Adapter that runs one Worker in busy loop.</p> Source code in <code>fastvideo/worker/multiproc_executor.py</code> <pre><code>def __init__(\n    self,\n    fastvideo_args: FastVideoArgs,\n    local_rank: int,\n    rank: int,\n    distributed_init_method: str,\n    pipe: Connection,\n):\n    self.rank = rank\n    self.pipe = pipe\n    wrapper = WorkerWrapperBase(fastvideo_args=fastvideo_args,\n                                rpc_rank=rank)\n\n    all_kwargs: list[dict] = [{} for _ in range(fastvideo_args.num_gpus)]\n    all_kwargs[rank] = {\n        \"fastvideo_args\": fastvideo_args,\n        \"local_rank\": local_rank,\n        \"rank\": rank,\n        \"distributed_init_method\": distributed_init_method,\n    }\n    wrapper.init_worker(all_kwargs)\n    self.worker = wrapper\n\n    # Initialize device\n    self.worker.init_device()\n\n    # Set process title and log prefix\n    self.setup_proc_title_and_log_prefix()\n</code></pre> Functions\u00b6 fastvideo.worker.multiproc_executor.WorkerMultiprocProc.worker_busy_loop \u00b6 <pre><code>worker_busy_loop() -&gt; None\n</code></pre> <p>Main busy loop for Multiprocessing Workers</p> Source code in <code>fastvideo/worker/multiproc_executor.py</code> <pre><code>def worker_busy_loop(self) -&gt; None:\n    \"\"\"Main busy loop for Multiprocessing Workers\"\"\"\n    while True:\n        logger.info(\"Worker %d starting event loop...\", self.rank)\n        try:\n            rpc_call = self.pipe.recv()\n            method = rpc_call.get(\"method\")\n            args = rpc_call.get(\"args\", ())\n            kwargs = rpc_call.get(\"kwargs\", {})\n\n            if isinstance(method, str):\n                if method == \"shutdown\":\n                    response = self.shutdown()\n                    with contextlib.suppress(Exception):\n                        self.pipe.send(response)\n                    break\n                if method == 'execute_forward':\n                    forward_batch = kwargs['forward_batch']\n                    fastvideo_args = kwargs['fastvideo_args']\n                    output_batch = self.worker.execute_forward(\n                        forward_batch, fastvideo_args)\n                    logging_info = None\n                    if envs.FASTVIDEO_STAGE_LOGGING:\n                        logging_info = output_batch.logging_info\n                    self.pipe.send({\n                        \"output_batch\": output_batch.output.cpu(),\n                        \"logging_info\": logging_info\n                    })\n            else:\n                result = self.worker.execute_method(method, *args, **kwargs)\n                self.pipe.send(result)\n        except KeyboardInterrupt:\n            logger.error(\n                \"Worker %d in loop received KeyboardInterrupt, aborting forward pass\",\n                self.rank)\n            try:\n                self.pipe.send(\n                    {\"error\": \"Operation aborted by KeyboardInterrupt\"})\n                logger.info(\"Worker %d sent error response after interrupt\",\n                            self.rank)\n            except Exception as e:\n                logger.error(\"Worker %d failed to send error response: %s\",\n                             self.rank, str(e))\n            continue\n</code></pre> fastvideo.worker.multiproc_executor.WorkerMultiprocProc.worker_main <code>staticmethod</code> \u00b6 <pre><code>worker_main(*args, **kwargs)\n</code></pre> <p>Worker initialization and execution loops. This runs a background process</p> Source code in <code>fastvideo/worker/multiproc_executor.py</code> <pre><code>@staticmethod\ndef worker_main(*args, **kwargs):\n    \"\"\" Worker initialization and execution loops.\n    This runs a background process \"\"\"\n\n    # Signal handler used for graceful termination.\n    # SystemExit exception is only raised once to allow this and worker\n    # processes to terminate without error\n    shutdown_requested = False\n\n    def signal_handler(signum, frame):\n        nonlocal shutdown_requested\n        if not shutdown_requested:\n            shutdown_requested = True\n            raise SystemExit()\n\n    # Either SIGTERM or SIGINT will terminate the worker\n    signal.signal(signal.SIGTERM, signal_handler)\n    signal.signal(signal.SIGINT, signal_handler)\n    kill_itself_when_parent_died()\n    faulthandler.enable()\n    parent_process = psutil.Process().parent()\n\n    worker = None\n    ready_pipe = kwargs.pop(\"ready_pipe\")\n    rank = kwargs.get(\"rank\")\n\n    try:\n        worker = WorkerMultiprocProc(*args, **kwargs)\n\n        # Send READY once we know everything is loaded\n        ready_pipe.send({\n            \"status\": WorkerMultiprocProc.READY_STR,\n        })\n\n        ready_pipe.close()\n        ready_pipe = None\n\n        worker.worker_busy_loop()\n\n    except Exception:\n        if ready_pipe is not None:\n            logger.exception(\"WorkerMultiprocProc failed to start.\")\n        else:\n            logger.exception(\"WorkerMultiprocProc failed.\")\n\n        # The parent sends a SIGTERM to all worker processes if\n        # any worker dies. Set this value so we don't re-throw\n        # SystemExit() to avoid zmq exceptions in __del__.\n        shutdown_requested = True\n        traceback = get_exception_traceback()\n        logger.error(\"Worker %d hit an exception: %s\", rank, traceback)\n        parent_process.send_signal(signal.SIGQUIT)\n\n    finally:\n        if ready_pipe is not None:\n            ready_pipe.close()\n        # Clean up once worker exits busy loop\n        if worker is not None:\n            worker.shutdown()\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.worker.multiproc_executor-functions","title":"Functions","text":"fastvideo.worker.multiproc_executor.set_multiproc_executor_envs \u00b6 <pre><code>set_multiproc_executor_envs() -&gt; None\n</code></pre> <p>Set up environment variables that should be used when there are workers in a multiprocessing environment. This should be called by the parent  process before worker processes are created</p> Source code in <code>fastvideo/worker/multiproc_executor.py</code> <pre><code>def set_multiproc_executor_envs() -&gt; None:\n    \"\"\" Set up environment variables that should be used when there are workers\n    in a multiprocessing environment. This should be called by the parent \n    process before worker processes are created\"\"\"\n\n    force_spawn()\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.worker.ray_distributed_executor","title":"fastvideo.worker.ray_distributed_executor","text":""},{"location":"api/fastvideo/#fastvideo.worker.ray_distributed_executor-classes","title":"Classes","text":"fastvideo.worker.ray_distributed_executor.RayDistributedExecutor \u00b6 <pre><code>RayDistributedExecutor(fastvideo_args: FastVideoArgs)\n</code></pre> <p>               Bases: <code>Executor</code></p> <p>Ray-based distributed executor</p> Source code in <code>fastvideo/worker/executor.py</code> <pre><code>def __init__(self, fastvideo_args: FastVideoArgs):\n    self.fastvideo_args = fastvideo_args\n\n    self._init_executor()\n</code></pre> fastvideo.worker.ray_distributed_executor.RayWorkerMetaData <code>dataclass</code> \u00b6 <pre><code>RayWorkerMetaData(\n    worker: ActorHandle,\n    created_rank: int,\n    adjusted_rank: int = -1,\n    ip: str = \"\",\n)\n</code></pre> <p>Metadata for a Ray worker. The order of ray worker creation can be random, and we need to reset the rank after creating all workers.</p>"},{"location":"api/fastvideo/#fastvideo.worker.ray_distributed_executor-functions","title":"Functions","text":""},{"location":"api/fastvideo/#fastvideo.worker.ray_env","title":"fastvideo.worker.ray_env","text":""},{"location":"api/fastvideo/#fastvideo.worker.ray_env-functions","title":"Functions","text":"fastvideo.worker.ray_env.get_env_vars_to_copy \u00b6 <pre><code>get_env_vars_to_copy(\n    exclude_vars: set[str] | None = None,\n    additional_vars: set[str] | None = None,\n    destination: str | None = None,\n) -&gt; set[str]\n</code></pre> <p>Get the environment variables to copy to downstream Ray actors.</p> <p>Example use cases: - Copy environment variables from RayDistributedExecutor to Ray workers. - Copy environment variables from RayDPClient to Ray DPEngineCoreActor.</p> <p>Parameters:</p> Name Type Description Default <code>exclude_vars</code> <code>set[str] | None</code> <p>A set of FastVideo defined environment variables to exclude from copying.</p> <code>None</code> <code>additional_vars</code> <code>set[str] | None</code> <p>A set of additional environment variables to copy. If a variable is in both exclude_vars and additional_vars, it will be excluded.</p> <code>None</code> <code>destination</code> <code>str | None</code> <p>The destination of the environment variables.</p> <code>None</code> <p>Returns:     A set of environment variables to copy.</p> Source code in <code>fastvideo/worker/ray_env.py</code> <pre><code>def get_env_vars_to_copy(\n    exclude_vars: set[str] | None = None,\n    additional_vars: set[str] | None = None,\n    destination: str | None = None,\n) -&gt; set[str]:\n    \"\"\"\n    Get the environment variables to copy to downstream Ray actors.\n\n    Example use cases:\n    - Copy environment variables from RayDistributedExecutor to Ray workers.\n    - Copy environment variables from RayDPClient to Ray DPEngineCoreActor.\n\n    Args:\n        exclude_vars: A set of FastVideo defined environment variables to exclude\n            from copying.\n        additional_vars: A set of additional environment variables to copy.\n            If a variable is in both exclude_vars and additional_vars, it will\n            be excluded.\n        destination: The destination of the environment variables.\n    Returns:\n        A set of environment variables to copy.\n    \"\"\"\n    exclude_vars = exclude_vars or set()\n    additional_vars = additional_vars or set()\n\n    env_vars_to_copy = {\n        v\n        for v in set(envs.environment_variables).union(additional_vars)\n        if v not in exclude_vars and v not in RAY_NON_CARRY_OVER_ENV_VARS\n    }\n\n    to_destination = \" to \" + destination if destination is not None else \"\"\n\n    logger.info(\n        \"RAY_NON_CARRY_OVER_ENV_VARS from config: %s\",\n        RAY_NON_CARRY_OVER_ENV_VARS,\n    )\n    logger.info(\n        \"Copying the following environment variables%s: %s\",\n        to_destination,\n        [v for v in env_vars_to_copy if v in os.environ],\n    )\n    logger.info(\n        \"If certain env vars should NOT be copied, add them to %s file\",\n        RAY_NON_CARRY_OVER_ENV_VARS_FILE,\n    )\n\n    return env_vars_to_copy\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.worker.ray_utils","title":"fastvideo.worker.ray_utils","text":""},{"location":"api/fastvideo/#fastvideo.worker.ray_utils-classes","title":"Classes","text":"fastvideo.worker.ray_utils.RayWorkerWrapper \u00b6 <pre><code>RayWorkerWrapper(*args, **kwargs)\n</code></pre> <p>               Bases: <code>WorkerWrapperBase</code></p> <p>Ray wrapper for fastvideo.worker.Worker, allowing Worker to be lazily initialized after Ray sets CUDA_VISIBLE_DEVICES.</p> Source code in <code>fastvideo/worker/ray_utils.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.worker.ray_utils-functions","title":"Functions","text":"fastvideo.worker.ray_utils.assert_ray_available \u00b6 <pre><code>assert_ray_available() -&gt; None\n</code></pre> <p>Raise an exception if Ray is not available.</p> Source code in <code>fastvideo/worker/ray_utils.py</code> <pre><code>def assert_ray_available() -&gt; None:\n    \"\"\"Raise an exception if Ray is not available.\"\"\"\n    if ray is None:\n        raise ValueError(f\"Failed to import Ray: {ray_import_err}.\"\n                         \"Please install Ray with `pip install ray`.\")\n</code></pre> fastvideo.worker.ray_utils.initialize_ray_cluster \u00b6 <pre><code>initialize_ray_cluster(\n    fastvideo_args: FastVideoArgs,\n    ray_address: str | None = None,\n)\n</code></pre> <p>Initialize the distributed cluster with Ray.</p> <p>it will connect to the Ray cluster and create a placement group for the workers, which includes the specification of the resources for each distributed worker.</p> <p>Parameters:</p> Name Type Description Default <code>parallel_config</code> <p>The configurations for parallel execution.</p> required <code>ray_address</code> <code>str | None</code> <p>The address of the Ray cluster. If None, uses the default Ray cluster address.</p> <code>None</code> Source code in <code>fastvideo/worker/ray_utils.py</code> <pre><code>def initialize_ray_cluster(\n    fastvideo_args: FastVideoArgs,\n    ray_address: str | None = None,\n):\n    \"\"\"Initialize the distributed cluster with Ray.\n\n    it will connect to the Ray cluster and create a placement group\n    for the workers, which includes the specification of the resources\n    for each distributed worker.\n\n    Args:\n        parallel_config: The configurations for parallel execution.\n        ray_address: The address of the Ray cluster. If None, uses\n            the default Ray cluster address.\n    \"\"\"\n    assert_ray_available()\n    from fastvideo.platforms import current_platform\n\n    if ray.is_initialized():\n        logger.info(\"Ray is already initialized. Skipping Ray initialization.\")\n    elif current_platform.is_rocm() or current_platform.is_xpu():\n        # Try to connect existing ray instance and create a new one if not found\n        try:\n            ray.init(\"auto\")\n        except ConnectionError:\n            logger.warning(\n                \"No existing RAY instance detected. \"\n                \"A new instance will be launched with current node resources.\")\n            ray.init(address=ray_address,\n                     num_gpus=fastvideo_args.num_gpus,\n                     runtime_env=fastvideo_args.ray_runtime_env)\n    else:\n        ray.init(address=ray_address,\n                 runtime_env=fastvideo_args.ray_runtime_env)\n\n    device_str = current_platform.ray_device_key\n    if not device_str:\n        raise ValueError(\n            f\"current platform {current_platform.device_name} does not \"\n            \"support ray.\")\n\n    # Create or get the placement group for worker processes\n    if fastvideo_args.ray_placement_group:\n        current_placement_group = fastvideo_args.ray_placement_group\n    else:\n        current_placement_group = ray.util.get_current_placement_group()\n\n    if current_placement_group:\n        logger.info(\"Using the existing placement group\")\n\n        # We are in a placement group\n        bundles = current_placement_group.bundle_specs\n        # Verify that we can use the placement group.\n        device_bundles = 0\n        for bundle in bundles:\n            bundle_devices = bundle.get(device_str, 0)\n            if bundle_devices &gt; 1:\n                raise ValueError(\n                    \"Placement group bundle cannot have more than 1 \"\n                    f\"{device_str}.\")\n            if bundle_devices:\n                device_bundles += 1\n        if fastvideo_args.num_gpus &gt; device_bundles:\n            raise ValueError(\n                f\"The number of required {device_str}s exceeds the total \"\n                f\"number of available {device_str}s in the placement group. \"\n                f\"Required number of devices: {fastvideo_args.num_gpus}. \"\n                f\"Total number of devices: {device_bundles}.\")\n    else:\n        logger.info(\"No current placement group found. \"\n                    \"Creating a new placement group.\")\n        num_devices_in_cluster = ray.cluster_resources().get(device_str, 0)\n        # Log a warning message and delay resource allocation failure response.\n        # Avoid immediate rejection to allow user-initiated placement group\n        # created and wait cluster to be ready\n        if fastvideo_args.num_gpus &gt; num_devices_in_cluster:\n            logger.warning(\n                \"The number of required %ss exceeds the total \"\n                \"number of available %ss in the placement group.\", device_str,\n                device_str)\n        # Create a new placement group\n        placement_group_specs: list[dict[str, float]] = ([{\n            device_str: 1.0\n        } for _ in range(fastvideo_args.num_gpus)])\n\n        # FastVideo engine is also a worker to execute model with an accelerator,\n        # so it requires to have the device in a current node. Check if\n        # the current node has at least one device.\n        current_ip = get_ip()\n        current_node_id = ray.get_runtime_context().get_node_id()\n        current_node_resource = available_resources_per_node()[current_node_id]\n        if current_node_resource.get(device_str, 0) &lt; 1:\n            raise ValueError(\n                f\"Current node has no {device_str} available. \"\n                f\"{current_node_resource=}. FastVideo engine cannot start without \"\n                f\"{device_str}. Make sure you have at least 1 {device_str} \"\n                f\"available in a node {current_node_id=} {current_ip=}.\")\n        # This way, at least bundle is required to be created in a current\n        # node.\n        placement_group_specs[0][f\"node:{current_ip}\"] = 0.001\n\n        # By default, Ray packs resources as much as possible.\n        current_placement_group = ray.util.placement_group(\n            placement_group_specs, strategy=\"PACK\")\n        _wait_until_pg_ready(current_placement_group)\n\n    assert current_placement_group is not None\n    _verify_bundles(current_placement_group, fastvideo_args, device_str)\n    # Set the placement group in the fastvideo args\n    fastvideo_args.ray_placement_group = current_placement_group\n</code></pre> fastvideo.worker.ray_utils.is_in_ray_actor \u00b6 <pre><code>is_in_ray_actor()\n</code></pre> <p>Check if we are in a Ray actor.</p> Source code in <code>fastvideo/worker/ray_utils.py</code> <pre><code>def is_in_ray_actor():\n    \"\"\"Check if we are in a Ray actor.\"\"\"\n\n    try:\n        import ray\n        return (ray.is_initialized()\n                and ray.get_runtime_context().get_actor_id() is not None)\n    except ImportError:\n        return False\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.worker.worker_base","title":"fastvideo.worker.worker_base","text":""},{"location":"api/fastvideo/#fastvideo.worker.worker_base-classes","title":"Classes","text":"fastvideo.worker.worker_base.WorkerWrapperBase \u00b6 <pre><code>WorkerWrapperBase(\n    fastvideo_args: FastVideoArgs, rpc_rank: int = 0\n)\n</code></pre> <p>This class represents one process in an executor/engine. It is responsible for lazily initializing the worker and handling the worker's lifecycle. We first instantiate the WorkerWrapper, which remembers the worker module and class name. Then, when we call <code>update_environment_variables</code>, and the real initialization happens in <code>init_worker</code>.</p> <p>Initialize the worker wrapper with the given fastvideo_args and rpc_rank. Note: rpc_rank is the rank of the worker in the executor. In most cases, it is also the rank of the worker in the distributed group. However, when multiple executors work together, they can be different. e.g. in the case of SPMD-style offline inference with TP=2, users can launch 2 engines/executors, each with only 1 worker. All workers have rpc_rank=0, but they have different ranks in the TP group.</p> Source code in <code>fastvideo/worker/worker_base.py</code> <pre><code>def __init__(\n    self,\n    fastvideo_args: FastVideoArgs,\n    rpc_rank: int = 0,\n) -&gt; None:\n    \"\"\"\n    Initialize the worker wrapper with the given fastvideo_args and rpc_rank.\n    Note: rpc_rank is the rank of the worker in the executor. In most cases,\n    it is also the rank of the worker in the distributed group. However,\n    when multiple executors work together, they can be different.\n    e.g. in the case of SPMD-style offline inference with TP=2,\n    users can launch 2 engines/executors, each with only 1 worker.\n    All workers have rpc_rank=0, but they have different ranks in the TP\n    group.\n    \"\"\"\n    self.rpc_rank = rpc_rank\n    self.worker: Worker | None = None\n    self.fastvideo_args: FastVideoArgs | None = None\n</code></pre> Functions\u00b6 fastvideo.worker.worker_base.WorkerWrapperBase.adjust_rank \u00b6 <pre><code>adjust_rank(rank_mapping: dict[int, int]) -&gt; None\n</code></pre> <p>Adjust the rpc_rank based on the given mapping. It is only used during the initialization of the executor, to adjust the rpc_rank of workers after we create all workers.</p> Source code in <code>fastvideo/worker/worker_base.py</code> <pre><code>def adjust_rank(self, rank_mapping: dict[int, int]) -&gt; None:\n    \"\"\"\n    Adjust the rpc_rank based on the given mapping.\n    It is only used during the initialization of the executor,\n    to adjust the rpc_rank of workers after we create all workers.\n    \"\"\"\n    if self.rpc_rank in rank_mapping:\n        self.rpc_rank = rank_mapping[self.rpc_rank]\n</code></pre> fastvideo.worker.worker_base.WorkerWrapperBase.init_worker \u00b6 <pre><code>init_worker(all_kwargs: list[dict[str, Any]]) -&gt; None\n</code></pre> <p>Here we inject some common logic before initializing the worker. Arguments are passed to the worker class constructor.</p> Source code in <code>fastvideo/worker/worker_base.py</code> <pre><code>def init_worker(self, all_kwargs: list[dict[str, Any]]) -&gt; None:\n    \"\"\"\n    Here we inject some common logic before initializing the worker.\n    Arguments are passed to the worker class constructor.\n    \"\"\"\n    kwargs = all_kwargs[self.rpc_rank]\n    self.fastvideo_args = kwargs.get(\"fastvideo_args\")\n    assert self.fastvideo_args is not None, (\n        \"fastvideo_args is required to initialize the worker\")\n\n    self.worker = Worker(**kwargs)\n    assert self.worker is not None\n</code></pre>"},{"location":"api/fastvideo/#fastvideo.worker.worker_base-functions","title":"Functions","text":""},{"location":"api/summary/","title":"Summary","text":""},{"location":"api/summary/#video-generator","title":"Video Generator","text":"<pre><code>    fastvideo.VideoGenerator\n</code></pre>"},{"location":"api/summary/#initialization-configuration","title":"Initialization Configuration","text":"<pre><code>    fastvideo.configs.pipelines.PipelineConfig\n</code></pre>"},{"location":"api/summary/#sampling-configuration","title":"Sampling Configuration","text":"<pre><code>    fastvideo.configs.sample.SamplingParam\n</code></pre>"},{"location":"api/fastvideo/STA_configuration/","title":"STA_configuration","text":""},{"location":"api/fastvideo/STA_configuration/#fastvideo.STA_configuration","title":"STA_configuration","text":""},{"location":"api/fastvideo/STA_configuration/#fastvideo.STA_configuration-functions","title":"Functions","text":""},{"location":"api/fastvideo/STA_configuration/#fastvideo.STA_configuration.average_head_losses","title":"fastvideo.STA_configuration.average_head_losses","text":"<pre><code>average_head_losses(\n    results: list[dict[str, Any]],\n    selected_masks: list[list[int]],\n) -&gt; dict[str, dict[str, np.ndarray]]\n</code></pre> <p>Average losses across all prompts for each mask strategy.</p> Source code in <code>fastvideo/STA_configuration.py</code> <pre><code>def average_head_losses(\n        results: list[dict[str, Any]],\n        selected_masks: list[list[int]]) -&gt; dict[str, dict[str, np.ndarray]]:\n    \"\"\"Average losses across all prompts for each mask strategy.\"\"\"\n    # Initialize a dictionary to store the averaged results\n    averaged_losses: dict[str, dict[str, np.ndarray]] = {}\n    loss_type = 'L2_loss'\n    # Get all loss types (e.g., 'L2_loss')\n    averaged_losses[loss_type] = {}\n\n    for mask in selected_masks:\n        mask_str = str(mask)\n        data_shape = np.array(results[0][loss_type][mask_str]).shape\n        accumulated_data = np.zeros(data_shape)\n\n        # Sum across all prompts\n        for prompt_result in results:\n            accumulated_data += np.array(prompt_result[loss_type][mask_str])\n\n        # Average by dividing by number of prompts\n        averaged_data = accumulated_data / len(results)\n        averaged_losses[loss_type][mask_str] = averaged_data\n\n    return averaged_losses\n</code></pre>"},{"location":"api/fastvideo/STA_configuration/#fastvideo.STA_configuration.configure_sta","title":"fastvideo.STA_configuration.configure_sta","text":"<pre><code>configure_sta(\n    mode: str = \"STA_searching\",\n    layer_num: int = 40,\n    time_step_num: int = 50,\n    head_num: int = 40,\n    **kwargs\n) -&gt; list[list[list[Any]]]\n</code></pre> <p>Configure Sliding Tile Attention (STA) parameters based on the specified mode.</p>"},{"location":"api/fastvideo/STA_configuration/#fastvideo.STA_configuration.configure_sta--parameters","title":"Parameters:","text":"<p>mode : str     The STA mode to use. Options are:     - 'STA_searching': Generate a set of mask candidates for initial search     - 'STA_tuning': Select best mask strategy based on previously saved results     - 'STA_inference': Load and use a previously tuned mask strategy layer_num: int, number of layers time_step_num: int, number of timesteps head_num: int, number of heads</p> <p>**kwargs : dict     Mode-specific parameters:</p> <pre><code>For 'STA_searching':\n- mask_candidates: list of str, optional, mask candidates to use\n- mask_selected: list of int, optional, indices of selected masks\n\nFor 'STA_tuning':\n- mask_search_files_path: str, required, path to mask search results\n- mask_candidates: list of str, optional, mask candidates to use\n- mask_selected: list of int, optional, indices of selected masks\n- skip_time_steps: int, optional, number of time steps to use full attention (default 12)\n- save_dir: str, optional, directory to save mask strategy (default \"mask_candidates\")\n\nFor 'STA_inference':\n- load_path: str, optional, path to load mask strategy (default \"mask_candidates/mask_strategy.json\")\n</code></pre> Source code in <code>fastvideo/STA_configuration.py</code> <pre><code>def configure_sta(mode: str = 'STA_searching',\n                  layer_num: int = 40,\n                  time_step_num: int = 50,\n                  head_num: int = 40,\n                  **kwargs) -&gt; list[list[list[Any]]]:\n    \"\"\"\n    Configure Sliding Tile Attention (STA) parameters based on the specified mode.\n\n    Parameters:\n    ----------\n    mode : str\n        The STA mode to use. Options are:\n        - 'STA_searching': Generate a set of mask candidates for initial search\n        - 'STA_tuning': Select best mask strategy based on previously saved results\n        - 'STA_inference': Load and use a previously tuned mask strategy\n    layer_num: int, number of layers\n    time_step_num: int, number of timesteps\n    head_num: int, number of heads\n\n    **kwargs : dict\n        Mode-specific parameters:\n\n        For 'STA_searching':\n        - mask_candidates: list of str, optional, mask candidates to use\n        - mask_selected: list of int, optional, indices of selected masks\n\n        For 'STA_tuning':\n        - mask_search_files_path: str, required, path to mask search results\n        - mask_candidates: list of str, optional, mask candidates to use\n        - mask_selected: list of int, optional, indices of selected masks\n        - skip_time_steps: int, optional, number of time steps to use full attention (default 12)\n        - save_dir: str, optional, directory to save mask strategy (default \"mask_candidates\")\n\n        For 'STA_inference':\n        - load_path: str, optional, path to load mask strategy (default \"mask_candidates/mask_strategy.json\")\n    \"\"\"\n    valid_modes = [\n        'STA_searching', 'STA_tuning', 'STA_inference', 'STA_tuning_cfg'\n    ]\n    if mode not in valid_modes:\n        raise ValueError(f\"Mode must be one of {valid_modes}, got {mode}\")\n\n    if mode == 'STA_searching':\n        # Get parameters with defaults\n        mask_candidates: list[str] | None = kwargs.get('mask_candidates')\n        if mask_candidates is None:\n            raise ValueError(\n                \"mask_candidates is required for STA_searching mode\")\n        mask_selected: list[int] = kwargs.get('mask_selected',\n                                              list(range(len(mask_candidates))))\n\n        # Parse selected masks\n        selected_masks: list[list[int]] = []\n        for index in mask_selected:\n            mask = mask_candidates[index]\n            masks_list = [int(x) for x in mask.split(',')]\n            selected_masks.append(masks_list)\n\n        # Create 3D mask structure with fixed dimensions (t=50, l=60)\n        masks_3d: list[list[list[list[int]]]] = []\n        for i in range(time_step_num):  # Fixed t dimension = 50\n            row = []\n            for j in range(layer_num):  # Fixed l dimension = 60\n                row.append(selected_masks)  # Add all masks at each position\n            masks_3d.append(row)\n\n        return masks_3d\n\n    elif mode == 'STA_tuning':\n        # Get required parameters\n        mask_search_files_path: str | None = kwargs.get(\n            'mask_search_files_path')\n        if not mask_search_files_path:\n            raise ValueError(\n                \"mask_search_files_path is required for STA_tuning mode\")\n\n        # Get optional parameters with defaults\n        mask_candidates_tuning: list[str] | None = kwargs.get('mask_candidates')\n        if mask_candidates_tuning is None:\n            raise ValueError(\"mask_candidates is required for STA_tuning mode\")\n        mask_selected_tuning: list[int] = kwargs.get(\n            'mask_selected', list(range(len(mask_candidates_tuning))))\n        skip_time_steps_tuning: int | None = kwargs.get('skip_time_steps')\n        save_dir_tuning: str | None = kwargs.get('save_dir', \"mask_candidates\")\n\n        # Parse selected masks\n        selected_masks_tuning: list[list[int]] = []\n        for index in mask_selected_tuning:\n            mask = mask_candidates_tuning[index]\n            masks_list = [int(x) for x in mask.split(',')]\n            selected_masks_tuning.append(masks_list)\n\n        # Read JSON results\n        results = read_specific_json_files(mask_search_files_path)\n        averaged_results = average_head_losses(results, selected_masks_tuning)\n\n        # Add full attention mask for specific cases\n        full_attention_mask_tuning: list[int] | None = kwargs.get(\n            'full_attention_mask')\n        if full_attention_mask_tuning is not None:\n            selected_masks_tuning.append(full_attention_mask_tuning)\n\n        # Select best mask strategy\n        timesteps_tuning: int = kwargs.get('timesteps', time_step_num)\n        if skip_time_steps_tuning is None:\n            skip_time_steps_tuning = 12\n        mask_strategy, sparsity, strategy_counts = select_best_mask_strategy(\n            averaged_results, selected_masks_tuning, skip_time_steps_tuning,\n            timesteps_tuning, head_num)\n\n        # Save mask strategy\n        if save_dir_tuning is not None:\n            os.makedirs(save_dir_tuning, exist_ok=True)\n            file_path = os.path.join(\n                save_dir_tuning,\n                f'mask_strategy_s{skip_time_steps_tuning}.json')\n            with open(file_path, 'w') as f:\n                json.dump(mask_strategy, f, indent=4)\n            print(f\"Successfully saved mask_strategy to {file_path}\")\n\n        # Print sparsity and strategy counts for information\n        print(f\"Overall sparsity: {sparsity:.4f}\")\n        print(\"\\nStrategy usage counts:\")\n        total_heads = time_step_num * layer_num * head_num  # Fixed dimensions\n        for strategy, count in strategy_counts.items():\n            print(\n                f\"Strategy {strategy}: {count} heads ({count/total_heads*100:.2f}%)\"\n            )\n\n        # Convert dictionary to 3D list with fixed dimensions\n        mask_strategy_3d = dict_to_3d_list(mask_strategy,\n                                           t_max=time_step_num,\n                                           l_max=layer_num,\n                                           h_max=head_num)\n\n        return mask_strategy_3d\n    elif mode == 'STA_tuning_cfg':\n        # Get required parameters for both positive and negative paths\n        mask_search_files_path_pos: str | None = kwargs.get(\n            'mask_search_files_path_pos')\n        mask_search_files_path_neg: str | None = kwargs.get(\n            'mask_search_files_path_neg')\n        save_dir_cfg: str | None = kwargs.get('save_dir')\n\n        if not mask_search_files_path_pos or not mask_search_files_path_neg or not save_dir_cfg:\n            raise ValueError(\n                \"mask_search_files_path_pos, mask_search_files_path_neg, and save_dir are required for STA_tuning_cfg mode\"\n            )\n\n        # Get optional parameters with defaults\n        mask_candidates_cfg: list[str] | None = kwargs.get('mask_candidates')\n        if mask_candidates_cfg is None:\n            raise ValueError(\n                \"mask_candidates is required for STA_tuning_cfg mode\")\n        mask_selected_cfg: list[int] = kwargs.get(\n            'mask_selected', list(range(len(mask_candidates_cfg))))\n        skip_time_steps_cfg: int | None = kwargs.get('skip_time_steps')\n\n        # Parse selected masks\n        selected_masks_cfg: list[list[int]] = []\n        for index in mask_selected_cfg:\n            mask = mask_candidates_cfg[index]\n            masks_list = [int(x) for x in mask.split(',')]\n            selected_masks_cfg.append(masks_list)\n\n        # Read JSON results for both positive and negative paths\n        pos_results = read_specific_json_files(mask_search_files_path_pos)\n        neg_results = read_specific_json_files(mask_search_files_path_neg)\n        # Combine positive and negative results into one list\n        combined_results = pos_results + neg_results\n\n        # Average the combined results\n        averaged_results = average_head_losses(combined_results,\n                                               selected_masks_cfg)\n\n        # Add full attention mask for specific cases\n        full_attention_mask_cfg: list[int] | None = kwargs.get(\n            'full_attention_mask')\n        if full_attention_mask_cfg is not None:\n            selected_masks_cfg.append(full_attention_mask_cfg)\n\n        timesteps_cfg: int = kwargs.get('timesteps', time_step_num)\n        if skip_time_steps_cfg is None:\n            skip_time_steps_cfg = 12\n        # Select best mask strategy using combined results\n        mask_strategy, sparsity, strategy_counts = select_best_mask_strategy(\n            averaged_results, selected_masks_cfg, skip_time_steps_cfg,\n            timesteps_cfg, head_num)\n\n        # Save mask strategy\n        os.makedirs(save_dir_cfg, exist_ok=True)\n        file_path = os.path.join(save_dir_cfg,\n                                 f'mask_strategy_s{skip_time_steps_cfg}.json')\n        with open(file_path, 'w') as f:\n            json.dump(mask_strategy, f, indent=4)\n        print(f\"Successfully saved mask_strategy to {file_path}\")\n\n        # Print sparsity and strategy counts for information\n        print(f\"Overall sparsity: {sparsity:.4f}\")\n        print(\"\\nStrategy usage counts:\")\n        total_heads = time_step_num * layer_num * head_num  # Fixed dimensions\n        for strategy, count in strategy_counts.items():\n            print(\n                f\"Strategy {strategy}: {count} heads ({count/total_heads*100:.2f}%)\"\n            )\n\n        # Convert dictionary to 3D list with fixed dimensions\n        mask_strategy_3d = dict_to_3d_list(mask_strategy,\n                                           t_max=time_step_num,\n                                           l_max=layer_num,\n                                           h_max=head_num)\n\n        return mask_strategy_3d\n\n    else:  # STA_inference\n        # Get parameters with defaults\n        load_path: str | None = kwargs.get(\n            'load_path', \"mask_candidates/mask_strategy.json\")\n        if load_path is None:\n            raise ValueError(\"load_path is required for STA_inference mode\")\n\n        # Load previously saved mask strategy\n        with open(load_path) as f:\n            mask_strategy = json.load(f)\n\n        # Convert dictionary to 3D list with fixed dimensions\n        mask_strategy_3d = dict_to_3d_list(mask_strategy,\n                                           t_max=time_step_num,\n                                           l_max=layer_num,\n                                           h_max=head_num)\n\n        return mask_strategy_3d\n</code></pre>"},{"location":"api/fastvideo/STA_configuration/#fastvideo.STA_configuration.read_specific_json_files","title":"fastvideo.STA_configuration.read_specific_json_files","text":"<pre><code>read_specific_json_files(\n    folder_path: str,\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>Read and parse JSON files containing mask search results.</p> Source code in <code>fastvideo/STA_configuration.py</code> <pre><code>def read_specific_json_files(folder_path: str) -&gt; list[dict[str, Any]]:\n    \"\"\"Read and parse JSON files containing mask search results.\"\"\"\n    json_contents: list[dict[str, Any]] = []\n\n    # List files only in the current directory (no walk)\n    files = os.listdir(folder_path)\n    # Filter files\n    matching_files = [f for f in files if 'mask' in f and f.endswith('.json')]\n    print(f\"Found {len(matching_files)} matching files: {matching_files}\")\n\n    for file_name in matching_files:\n        file_path = os.path.join(folder_path, file_name)\n        with open(file_path) as file:\n            data = json.load(file)\n            json_contents.append(data)\n\n    return json_contents\n</code></pre>"},{"location":"api/fastvideo/STA_configuration/#fastvideo.STA_configuration.select_best_mask_strategy","title":"fastvideo.STA_configuration.select_best_mask_strategy","text":"<pre><code>select_best_mask_strategy(\n    averaged_results: dict[str, dict[str, ndarray]],\n    selected_masks: list[list[int]],\n    skip_time_steps: int = 12,\n    timesteps: int = 50,\n    head_num: int = 40,\n) -&gt; tuple[dict[str, list[int]], float, dict[str, int]]\n</code></pre> <p>Select the best mask strategy for each head based on loss minimization.</p> Source code in <code>fastvideo/STA_configuration.py</code> <pre><code>def select_best_mask_strategy(\n        averaged_results: dict[str, dict[str, np.ndarray]],\n        selected_masks: list[list[int]],\n        skip_time_steps: int = 12,\n        timesteps: int = 50,\n        head_num: int = 40\n) -&gt; tuple[dict[str, list[int]], float, dict[str, int]]:\n    \"\"\"Select the best mask strategy for each head based on loss minimization.\"\"\"\n    best_mask_strategy: dict[str, list[int]] = {}\n    loss_type = 'L2_loss'\n    # Get the shape of time steps and layers\n    layers = len(averaged_results[loss_type][str(selected_masks[0])][0])\n\n    # Counter for sparsity calculation\n    total_tokens = 0  # total number of masked tokens\n    total_length = 0  # total sequence length\n\n    strategy_counts: dict[str, int] = {\n        str(strategy): 0\n        for strategy in selected_masks\n    }\n    full_attn_strategy = selected_masks[-1]  # Last strategy is full attention\n    print(f\"Strategy {full_attn_strategy}, skip first {skip_time_steps} steps \")\n\n    for t in range(timesteps):\n        for layer_idx in range(layers):\n            for h in range(head_num):\n                if t &lt; skip_time_steps:  # First steps use full attention\n                    strategy = full_attn_strategy\n                else:\n                    # Get losses for this head across all strategies\n                    head_losses = []\n                    for strategy in selected_masks[:\n                                                   -1]:  # Exclude full attention\n                        head_losses.append(averaged_results[loss_type][str(\n                            strategy)][t][layer_idx][h])\n\n                    # Find which strategy gives minimum loss\n                    best_strategy_idx = np.argmin(head_losses)\n                    strategy = selected_masks[best_strategy_idx]\n\n                best_mask_strategy[f'{t}_{layer_idx}_{h}'] = strategy\n\n                # Calculate sparsity\n                nums = strategy  # strategy is already a list of numbers\n                total_tokens += nums[0] * nums[1] * nums[\n                    2]  # masked tokens for chosen strategy\n                total_length += full_attn_strategy[0] * full_attn_strategy[\n                    1] * full_attn_strategy[2]\n\n                # Count strategy usage\n                strategy_counts[str(strategy)] += 1\n\n    overall_sparsity = 1 - total_tokens / total_length\n\n    return best_mask_strategy, overall_sparsity, strategy_counts\n</code></pre>"},{"location":"api/fastvideo/attention/","title":"attention","text":""},{"location":"api/fastvideo/attention/#fastvideo.attention","title":"attention","text":""},{"location":"api/fastvideo/attention/#fastvideo.attention-classes","title":"Classes","text":""},{"location":"api/fastvideo/attention/#fastvideo.attention.AttentionBackend","title":"fastvideo.attention.AttentionBackend","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class for attention backends.</p>"},{"location":"api/fastvideo/attention/#fastvideo.attention.AttentionMetadata","title":"fastvideo.attention.AttentionMetadata  <code>dataclass</code>","text":"<pre><code>AttentionMetadata(current_timestep: int)\n</code></pre> <p>Attention metadata for prefill and decode batched together.</p>"},{"location":"api/fastvideo/attention/#fastvideo.attention.AttentionMetadata-functions","title":"Functions","text":""},{"location":"api/fastvideo/attention/#fastvideo.attention.AttentionMetadata.asdict_zerocopy","title":"fastvideo.attention.AttentionMetadata.asdict_zerocopy","text":"<pre><code>asdict_zerocopy(\n    skip_fields: set[str] | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Similar to dataclasses.asdict, but avoids deepcopying.</p> Source code in <code>fastvideo/attention/backends/abstract.py</code> <pre><code>def asdict_zerocopy(self,\n                    skip_fields: set[str] | None = None) -&gt; dict[str, Any]:\n    \"\"\"Similar to dataclasses.asdict, but avoids deepcopying.\"\"\"\n    if skip_fields is None:\n        skip_fields = set()\n    # Note that if we add dataclasses as fields, they will need\n    # similar handling.\n    return {\n        field.name: getattr(self, field.name)\n        for field in fields(self) if field.name not in skip_fields\n    }\n</code></pre>"},{"location":"api/fastvideo/attention/#fastvideo.attention.AttentionMetadataBuilder","title":"fastvideo.attention.AttentionMetadataBuilder","text":"<pre><code>AttentionMetadataBuilder()\n</code></pre> <p>               Bases: <code>ABC</code>, <code>Generic[T]</code></p> <p>Abstract class for attention metadata builders.</p> <p>Create the builder, remember some configuration and parameters.</p> Source code in <code>fastvideo/attention/backends/abstract.py</code> <pre><code>@abstractmethod\ndef __init__(self) -&gt; None:\n    \"\"\"Create the builder, remember some configuration and parameters.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fastvideo/attention/#fastvideo.attention.AttentionMetadataBuilder-functions","title":"Functions","text":""},{"location":"api/fastvideo/attention/#fastvideo.attention.AttentionMetadataBuilder.build","title":"fastvideo.attention.AttentionMetadataBuilder.build  <code>abstractmethod</code>","text":"<pre><code>build(**kwargs: dict[str, Any]) -&gt; AttentionMetadata\n</code></pre> <p>Build attention metadata with on-device tensors.</p> Source code in <code>fastvideo/attention/backends/abstract.py</code> <pre><code>@abstractmethod\ndef build(\n    self,\n    **kwargs: dict[str, Any],\n) -&gt; AttentionMetadata:\n    \"\"\"Build attention metadata with on-device tensors.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fastvideo/attention/#fastvideo.attention.AttentionMetadataBuilder.prepare","title":"fastvideo.attention.AttentionMetadataBuilder.prepare  <code>abstractmethod</code>","text":"<pre><code>prepare() -&gt; None\n</code></pre> <p>Prepare for one batch.</p> Source code in <code>fastvideo/attention/backends/abstract.py</code> <pre><code>@abstractmethod\ndef prepare(self) -&gt; None:\n    \"\"\"Prepare for one batch.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fastvideo/attention/#fastvideo.attention.DistributedAttention","title":"fastvideo.attention.DistributedAttention","text":"<pre><code>DistributedAttention(\n    num_heads: int,\n    head_size: int,\n    num_kv_heads: int | None = None,\n    softmax_scale: float | None = None,\n    causal: bool = False,\n    supported_attention_backends: tuple[\n        AttentionBackendEnum, ...\n    ]\n    | None = None,\n    prefix: str = \"\",\n    **extra_impl_args\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Distributed attention layer.</p> Source code in <code>fastvideo/attention/layer.py</code> <pre><code>def __init__(self,\n             num_heads: int,\n             head_size: int,\n             num_kv_heads: int | None = None,\n             softmax_scale: float | None = None,\n             causal: bool = False,\n             supported_attention_backends: tuple[AttentionBackendEnum, ...]\n             | None = None,\n             prefix: str = \"\",\n             **extra_impl_args) -&gt; None:\n    super().__init__()\n    if softmax_scale is None:\n        self.softmax_scale = head_size**-0.5\n    else:\n        self.softmax_scale = softmax_scale\n\n    if num_kv_heads is None:\n        num_kv_heads = num_heads\n\n    dtype = get_compute_dtype()\n    attn_backend = get_attn_backend(\n        head_size,\n        dtype,\n        supported_attention_backends=supported_attention_backends)\n    impl_cls = attn_backend.get_impl_cls()\n    self.attn_impl = impl_cls(num_heads=num_heads,\n                              head_size=head_size,\n                              causal=causal,\n                              softmax_scale=self.softmax_scale,\n                              num_kv_heads=num_kv_heads,\n                              prefix=f\"{prefix}.impl\",\n                              **extra_impl_args)\n    self.num_heads = num_heads\n    self.head_size = head_size\n    self.num_kv_heads = num_kv_heads\n    self.backend = backend_name_to_enum(attn_backend.get_name())\n    self.dtype = dtype\n</code></pre>"},{"location":"api/fastvideo/attention/#fastvideo.attention.DistributedAttention-functions","title":"Functions","text":""},{"location":"api/fastvideo/attention/#fastvideo.attention.DistributedAttention.forward","title":"fastvideo.attention.DistributedAttention.forward","text":"<pre><code>forward(\n    q: Tensor,\n    k: Tensor,\n    v: Tensor,\n    replicated_q: Tensor | None = None,\n    replicated_k: Tensor | None = None,\n    replicated_v: Tensor | None = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor | None]\n</code></pre> <p>Forward pass for distributed attention.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>Tensor</code> <p>Query tensor [batch_size, seq_len, num_heads, head_dim]</p> required <code>k</code> <code>Tensor</code> <p>Key tensor [batch_size, seq_len, num_heads, head_dim]</p> required <code>v</code> <code>Tensor</code> <p>Value tensor [batch_size, seq_len, num_heads, head_dim]</p> required <code>replicated_q</code> <code>Optional[Tensor]</code> <p>Replicated query tensor, typically for text tokens</p> <code>None</code> <code>replicated_k</code> <code>Optional[Tensor]</code> <p>Replicated key tensor</p> <code>None</code> <code>replicated_v</code> <code>Optional[Tensor]</code> <p>Replicated value tensor</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor | None]</code> <p>Tuple[torch.Tensor, Optional[torch.Tensor]]: A tuple containing: - o (torch.Tensor): Output tensor after attention for the main sequence - replicated_o (Optional[torch.Tensor]): Output tensor for replicated tokens, if provided</p> Source code in <code>fastvideo/attention/layer.py</code> <pre><code>@torch.compiler.disable\ndef forward(\n    self,\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    replicated_q: torch.Tensor | None = None,\n    replicated_k: torch.Tensor | None = None,\n    replicated_v: torch.Tensor | None = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor | None]:\n    \"\"\"Forward pass for distributed attention.\n\n    Args:\n        q (torch.Tensor): Query tensor [batch_size, seq_len, num_heads, head_dim]\n        k (torch.Tensor): Key tensor [batch_size, seq_len, num_heads, head_dim]\n        v (torch.Tensor): Value tensor [batch_size, seq_len, num_heads, head_dim]\n        replicated_q (Optional[torch.Tensor]): Replicated query tensor, typically for text tokens\n        replicated_k (Optional[torch.Tensor]): Replicated key tensor\n        replicated_v (Optional[torch.Tensor]): Replicated value tensor\n\n    Returns:\n        Tuple[torch.Tensor, Optional[torch.Tensor]]: A tuple containing:\n            - o (torch.Tensor): Output tensor after attention for the main sequence\n            - replicated_o (Optional[torch.Tensor]): Output tensor for replicated tokens, if provided\n    \"\"\"\n    # Check input shapes\n    assert q.dim() == 4 and k.dim() == 4 and v.dim(\n    ) == 4, \"Expected 4D tensors\"\n    batch_size, seq_len, num_heads, head_dim = q.shape\n    local_rank = get_sp_parallel_rank()\n    world_size = get_sp_world_size()\n\n    forward_context: ForwardContext = get_forward_context()\n    ctx_attn_metadata = forward_context.attn_metadata\n\n    # Stack QKV\n    qkv = torch.cat([q, k, v], dim=0)  # [3, seq_len, num_heads, head_dim]\n\n    # Redistribute heads across sequence dimension\n    qkv = sequence_model_parallel_all_to_all_4D(qkv,\n                                                scatter_dim=2,\n                                                gather_dim=1)\n    # Apply backend-specific preprocess_qkv\n    qkv = self.attn_impl.preprocess_qkv(qkv, ctx_attn_metadata)\n\n    # Concatenate with replicated QKV if provided\n    if replicated_q is not None:\n        assert replicated_k is not None and replicated_v is not None\n        replicated_qkv = torch.cat(\n            [replicated_q, replicated_k, replicated_v],\n            dim=0)  # [3, seq_len, num_heads, head_dim]\n        heads_per_rank = num_heads // world_size\n        replicated_qkv = replicated_qkv[:, :, local_rank *\n                                        heads_per_rank:(local_rank + 1) *\n                                        heads_per_rank]\n        qkv = torch.cat([qkv, replicated_qkv], dim=1)\n\n    q, k, v = qkv.chunk(3, dim=0)\n\n    output = self.attn_impl.forward(q, k, v, ctx_attn_metadata)\n\n    # Redistribute back if using sequence parallelism\n    replicated_output = None\n    if replicated_q is not None:\n        replicated_output = output[:, seq_len * world_size:]\n        output = output[:, :seq_len * world_size]\n        # TODO: make this asynchronous\n        replicated_output = sequence_model_parallel_all_gather(\n            replicated_output.contiguous(), dim=2)\n    # Apply backend-specific postprocess_output\n    output = self.attn_impl.postprocess_output(output, ctx_attn_metadata)\n\n    output = sequence_model_parallel_all_to_all_4D(output,\n                                                   scatter_dim=1,\n                                                   gather_dim=2)\n    return output, replicated_output\n</code></pre>"},{"location":"api/fastvideo/attention/#fastvideo.attention.DistributedAttention_VSA","title":"fastvideo.attention.DistributedAttention_VSA","text":"<pre><code>DistributedAttention_VSA(\n    num_heads: int,\n    head_size: int,\n    num_kv_heads: int | None = None,\n    softmax_scale: float | None = None,\n    causal: bool = False,\n    supported_attention_backends: tuple[\n        AttentionBackendEnum, ...\n    ]\n    | None = None,\n    prefix: str = \"\",\n    **extra_impl_args\n)\n</code></pre> <p>               Bases: <code>DistributedAttention</code></p> <p>Distributed attention layer with VSA support.</p> Source code in <code>fastvideo/attention/layer.py</code> <pre><code>def __init__(self,\n             num_heads: int,\n             head_size: int,\n             num_kv_heads: int | None = None,\n             softmax_scale: float | None = None,\n             causal: bool = False,\n             supported_attention_backends: tuple[AttentionBackendEnum, ...]\n             | None = None,\n             prefix: str = \"\",\n             **extra_impl_args) -&gt; None:\n    super().__init__()\n    if softmax_scale is None:\n        self.softmax_scale = head_size**-0.5\n    else:\n        self.softmax_scale = softmax_scale\n\n    if num_kv_heads is None:\n        num_kv_heads = num_heads\n\n    dtype = get_compute_dtype()\n    attn_backend = get_attn_backend(\n        head_size,\n        dtype,\n        supported_attention_backends=supported_attention_backends)\n    impl_cls = attn_backend.get_impl_cls()\n    self.attn_impl = impl_cls(num_heads=num_heads,\n                              head_size=head_size,\n                              causal=causal,\n                              softmax_scale=self.softmax_scale,\n                              num_kv_heads=num_kv_heads,\n                              prefix=f\"{prefix}.impl\",\n                              **extra_impl_args)\n    self.num_heads = num_heads\n    self.head_size = head_size\n    self.num_kv_heads = num_kv_heads\n    self.backend = backend_name_to_enum(attn_backend.get_name())\n    self.dtype = dtype\n</code></pre>"},{"location":"api/fastvideo/attention/#fastvideo.attention.DistributedAttention_VSA-functions","title":"Functions","text":""},{"location":"api/fastvideo/attention/#fastvideo.attention.DistributedAttention_VSA.forward","title":"fastvideo.attention.DistributedAttention_VSA.forward","text":"<pre><code>forward(\n    q: Tensor,\n    k: Tensor,\n    v: Tensor,\n    replicated_q: Tensor | None = None,\n    replicated_k: Tensor | None = None,\n    replicated_v: Tensor | None = None,\n    gate_compress: Tensor | None = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor | None]\n</code></pre> <p>Forward pass for distributed attention.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>Tensor</code> <p>Query tensor [batch_size, seq_len, num_heads, head_dim]</p> required <code>k</code> <code>Tensor</code> <p>Key tensor [batch_size, seq_len, num_heads, head_dim]</p> required <code>v</code> <code>Tensor</code> <p>Value tensor [batch_size, seq_len, num_heads, head_dim]</p> required <code>gate_compress</code> <code>Tensor</code> <p>Gate compress tensor [batch_size, seq_len, num_heads, head_dim]</p> <code>None</code> <code>replicated_q</code> <code>Optional[Tensor]</code> <p>Replicated query tensor, typically for text tokens</p> <code>None</code> <code>replicated_k</code> <code>Optional[Tensor]</code> <p>Replicated key tensor</p> <code>None</code> <code>replicated_v</code> <code>Optional[Tensor]</code> <p>Replicated value tensor</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor | None]</code> <p>Tuple[torch.Tensor, Optional[torch.Tensor]]: A tuple containing: - o (torch.Tensor): Output tensor after attention for the main sequence - replicated_o (Optional[torch.Tensor]): Output tensor for replicated tokens, if provided</p> Source code in <code>fastvideo/attention/layer.py</code> <pre><code>@torch.compiler.disable\ndef forward(\n    self,\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    replicated_q: torch.Tensor | None = None,\n    replicated_k: torch.Tensor | None = None,\n    replicated_v: torch.Tensor | None = None,\n    gate_compress: torch.Tensor | None = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor | None]:\n    \"\"\"Forward pass for distributed attention.\n\n    Args:\n        q (torch.Tensor): Query tensor [batch_size, seq_len, num_heads, head_dim]\n        k (torch.Tensor): Key tensor [batch_size, seq_len, num_heads, head_dim]\n        v (torch.Tensor): Value tensor [batch_size, seq_len, num_heads, head_dim]\n        gate_compress (torch.Tensor): Gate compress tensor [batch_size, seq_len, num_heads, head_dim]\n        replicated_q (Optional[torch.Tensor]): Replicated query tensor, typically for text tokens\n        replicated_k (Optional[torch.Tensor]): Replicated key tensor\n        replicated_v (Optional[torch.Tensor]): Replicated value tensor\n\n    Returns:\n        Tuple[torch.Tensor, Optional[torch.Tensor]]: A tuple containing:\n            - o (torch.Tensor): Output tensor after attention for the main sequence\n            - replicated_o (Optional[torch.Tensor]): Output tensor for replicated tokens, if provided\n    \"\"\"\n    # Check text tokens are not supported for VSA now\n    assert replicated_q is None and replicated_k is None and replicated_v is None, \"Replicated QKV is not supported for VSA now\"\n    # Check input shapes\n    assert q.dim() == 4 and k.dim() == 4 and v.dim(\n    ) == 4, \"Expected 4D tensors\"\n\n    forward_context: ForwardContext = get_forward_context()\n    ctx_attn_metadata = forward_context.attn_metadata\n\n    # Stack QKV\n    qkvg = torch.cat([q, k, v, gate_compress],\n                     dim=0)  # [3, seq_len, num_heads, head_dim]\n\n    # Redistribute heads across sequence dimension\n    qkvg = sequence_model_parallel_all_to_all_4D(qkvg,\n                                                 scatter_dim=2,\n                                                 gather_dim=1)\n\n    qkvg = self.attn_impl.preprocess_qkv(qkvg, ctx_attn_metadata)\n\n    q, k, v, gate_compress = qkvg.chunk(4, dim=0)\n    output = self.attn_impl.forward(\n        q, k, v, gate_compress, ctx_attn_metadata)  # type: ignore[call-arg]\n\n    # Redistribute back if using sequence parallelism\n    replicated_output = None\n\n    # Apply backend-specific postprocess_output\n    output = self.attn_impl.postprocess_output(output, ctx_attn_metadata)\n\n    output = sequence_model_parallel_all_to_all_4D(output,\n                                                   scatter_dim=1,\n                                                   gather_dim=2)\n    return output, replicated_output\n</code></pre>"},{"location":"api/fastvideo/attention/#fastvideo.attention.LocalAttention","title":"fastvideo.attention.LocalAttention","text":"<pre><code>LocalAttention(\n    num_heads: int,\n    head_size: int,\n    num_kv_heads: int | None = None,\n    softmax_scale: float | None = None,\n    causal: bool = False,\n    supported_attention_backends: tuple[\n        AttentionBackendEnum, ...\n    ]\n    | None = None,\n    **extra_impl_args\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Attention layer.</p> Source code in <code>fastvideo/attention/layer.py</code> <pre><code>def __init__(self,\n             num_heads: int,\n             head_size: int,\n             num_kv_heads: int | None = None,\n             softmax_scale: float | None = None,\n             causal: bool = False,\n             supported_attention_backends: tuple[AttentionBackendEnum, ...]\n             | None = None,\n             **extra_impl_args) -&gt; None:\n    super().__init__()\n    if softmax_scale is None:\n        self.softmax_scale = head_size**-0.5\n    else:\n        self.softmax_scale = softmax_scale\n    if num_kv_heads is None:\n        num_kv_heads = num_heads\n\n    dtype = get_compute_dtype()\n    attn_backend = get_attn_backend(\n        head_size,\n        dtype,\n        supported_attention_backends=supported_attention_backends)\n    impl_cls = attn_backend.get_impl_cls()\n    self.attn_impl = impl_cls(num_heads=num_heads,\n                              head_size=head_size,\n                              softmax_scale=self.softmax_scale,\n                              num_kv_heads=num_kv_heads,\n                              causal=causal,\n                              **extra_impl_args)\n    self.num_heads = num_heads\n    self.head_size = head_size\n    self.num_kv_heads = num_kv_heads\n    self.backend = backend_name_to_enum(attn_backend.get_name())\n    self.dtype = dtype\n</code></pre>"},{"location":"api/fastvideo/attention/#fastvideo.attention.LocalAttention-functions","title":"Functions","text":""},{"location":"api/fastvideo/attention/#fastvideo.attention.LocalAttention.forward","title":"fastvideo.attention.LocalAttention.forward","text":"<pre><code>forward(q: Tensor, k: Tensor, v: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Apply local attention between query, key and value tensors.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>Tensor</code> <p>Query tensor of shape [batch_size, seq_len, num_heads, head_dim]</p> required <code>k</code> <code>Tensor</code> <p>Key tensor of shape [batch_size, seq_len, num_heads, head_dim] </p> required <code>v</code> <code>Tensor</code> <p>Value tensor of shape [batch_size, seq_len, num_heads, head_dim]</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output tensor after local attention</p> Source code in <code>fastvideo/attention/layer.py</code> <pre><code>def forward(\n    self,\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Apply local attention between query, key and value tensors.\n\n    Args:\n        q (torch.Tensor): Query tensor of shape [batch_size, seq_len, num_heads, head_dim]\n        k (torch.Tensor): Key tensor of shape [batch_size, seq_len, num_heads, head_dim] \n        v (torch.Tensor): Value tensor of shape [batch_size, seq_len, num_heads, head_dim]\n\n    Returns:\n        torch.Tensor: Output tensor after local attention\n    \"\"\"\n    # Check input shapes\n    assert q.dim() == 4 and k.dim() == 4 and v.dim(\n    ) == 4, \"Expected 4D tensors\"\n\n    forward_context: ForwardContext = get_forward_context()\n    ctx_attn_metadata = forward_context.attn_metadata\n\n    output = self.attn_impl.forward(q, k, v, ctx_attn_metadata)\n    return output\n</code></pre>"},{"location":"api/fastvideo/attention/#fastvideo.attention-modules","title":"Modules","text":""},{"location":"api/fastvideo/attention/#fastvideo.attention.backends","title":"fastvideo.attention.backends","text":""},{"location":"api/fastvideo/attention/#fastvideo.attention.backends-modules","title":"Modules","text":""},{"location":"api/fastvideo/attention/#fastvideo.attention.backends.abstract","title":"fastvideo.attention.backends.abstract","text":"Classes\u00b6 fastvideo.attention.backends.abstract.AttentionBackend \u00b6 <p>               Bases: <code>ABC</code></p> <p>Abstract class for attention backends.</p> fastvideo.attention.backends.abstract.AttentionImpl \u00b6 <pre><code>AttentionImpl(\n    num_heads: int,\n    head_size: int,\n    softmax_scale: float,\n    causal: bool = False,\n    num_kv_heads: int | None = None,\n    prefix: str = \"\",\n    **extra_impl_args\n)\n</code></pre> <p>               Bases: <code>ABC</code>, <code>Generic[T]</code></p> Source code in <code>fastvideo/attention/backends/abstract.py</code> <pre><code>@abstractmethod\ndef __init__(\n    self,\n    num_heads: int,\n    head_size: int,\n    softmax_scale: float,\n    causal: bool = False,\n    num_kv_heads: int | None = None,\n    prefix: str = \"\",\n    **extra_impl_args,\n) -&gt; None:\n    raise NotImplementedError\n</code></pre> Functions\u00b6 fastvideo.attention.backends.abstract.AttentionImpl.postprocess_output \u00b6 <pre><code>postprocess_output(\n    output: Tensor, attn_metadata: T\n) -&gt; torch.Tensor\n</code></pre> <p>Postprocess the output tensor after the attention operation.</p> <p>Default implementation returns the tensor unchanged. Subclasses can override this to implement custom postprocessing like untiling, scaling, or other transformations.</p> <p>Called BEFORE all_to_all for distributed attention</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Tensor</code> <p>The output tensor from the attention operation</p> required <code>attn_metadata</code> <code>T</code> <p>Metadata for the attention operation</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Postprocessed output tensor</p> Source code in <code>fastvideo/attention/backends/abstract.py</code> <pre><code>def postprocess_output(\n    self,\n    output: torch.Tensor,\n    attn_metadata: T,\n) -&gt; torch.Tensor:\n    \"\"\"Postprocess the output tensor after the attention operation.\n\n    Default implementation returns the tensor unchanged.\n    Subclasses can override this to implement custom postprocessing\n    like untiling, scaling, or other transformations.\n\n    Called BEFORE all_to_all for distributed attention\n\n    Args:\n        output: The output tensor from the attention operation\n        attn_metadata: Metadata for the attention operation\n\n    Returns:\n        Postprocessed output tensor\n    \"\"\"\n\n    return output\n</code></pre> fastvideo.attention.backends.abstract.AttentionImpl.preprocess_qkv \u00b6 <pre><code>preprocess_qkv(\n    qkv: Tensor, attn_metadata: T\n) -&gt; torch.Tensor\n</code></pre> <p>Preprocess QKV tensor before performing attention operation.</p> <p>Default implementation returns the tensor unchanged. Subclasses can override this to implement custom preprocessing like reshaping, tiling, scaling, or other transformations.</p> <p>Called AFTER all_to_all for distributed attention</p> <p>Parameters:</p> Name Type Description Default <code>qkv</code> <code>Tensor</code> <p>The query-key-value tensor</p> required <code>attn_metadata</code> <code>T</code> <p>Metadata for the attention operation</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Processed QKV tensor</p> Source code in <code>fastvideo/attention/backends/abstract.py</code> <pre><code>def preprocess_qkv(self, qkv: torch.Tensor,\n                   attn_metadata: T) -&gt; torch.Tensor:\n    \"\"\"Preprocess QKV tensor before performing attention operation.\n\n    Default implementation returns the tensor unchanged.\n    Subclasses can override this to implement custom preprocessing\n    like reshaping, tiling, scaling, or other transformations.\n\n    Called AFTER all_to_all for distributed attention\n\n    Args:\n        qkv: The query-key-value tensor\n        attn_metadata: Metadata for the attention operation\n\n    Returns:\n        Processed QKV tensor\n    \"\"\"\n    return qkv\n</code></pre> fastvideo.attention.backends.abstract.AttentionMetadata <code>dataclass</code> \u00b6 <pre><code>AttentionMetadata(current_timestep: int)\n</code></pre> <p>Attention metadata for prefill and decode batched together.</p> Functions\u00b6 fastvideo.attention.backends.abstract.AttentionMetadata.asdict_zerocopy \u00b6 <pre><code>asdict_zerocopy(\n    skip_fields: set[str] | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Similar to dataclasses.asdict, but avoids deepcopying.</p> Source code in <code>fastvideo/attention/backends/abstract.py</code> <pre><code>def asdict_zerocopy(self,\n                    skip_fields: set[str] | None = None) -&gt; dict[str, Any]:\n    \"\"\"Similar to dataclasses.asdict, but avoids deepcopying.\"\"\"\n    if skip_fields is None:\n        skip_fields = set()\n    # Note that if we add dataclasses as fields, they will need\n    # similar handling.\n    return {\n        field.name: getattr(self, field.name)\n        for field in fields(self) if field.name not in skip_fields\n    }\n</code></pre> fastvideo.attention.backends.abstract.AttentionMetadataBuilder \u00b6 <pre><code>AttentionMetadataBuilder()\n</code></pre> <p>               Bases: <code>ABC</code>, <code>Generic[T]</code></p> <p>Abstract class for attention metadata builders.</p> <p>Create the builder, remember some configuration and parameters.</p> Source code in <code>fastvideo/attention/backends/abstract.py</code> <pre><code>@abstractmethod\ndef __init__(self) -&gt; None:\n    \"\"\"Create the builder, remember some configuration and parameters.\"\"\"\n    raise NotImplementedError\n</code></pre> Functions\u00b6 fastvideo.attention.backends.abstract.AttentionMetadataBuilder.build <code>abstractmethod</code> \u00b6 <pre><code>build(**kwargs: dict[str, Any]) -&gt; AttentionMetadata\n</code></pre> <p>Build attention metadata with on-device tensors.</p> Source code in <code>fastvideo/attention/backends/abstract.py</code> <pre><code>@abstractmethod\ndef build(\n    self,\n    **kwargs: dict[str, Any],\n) -&gt; AttentionMetadata:\n    \"\"\"Build attention metadata with on-device tensors.\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.attention.backends.abstract.AttentionMetadataBuilder.prepare <code>abstractmethod</code> \u00b6 <pre><code>prepare() -&gt; None\n</code></pre> <p>Prepare for one batch.</p> Source code in <code>fastvideo/attention/backends/abstract.py</code> <pre><code>@abstractmethod\ndef prepare(self) -&gt; None:\n    \"\"\"Prepare for one batch.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fastvideo/attention/#fastvideo.attention.backends.video_sparse_attn","title":"fastvideo.attention.backends.video_sparse_attn","text":"Classes\u00b6 Functions\u00b6 fastvideo.attention.backends.video_sparse_attn.construct_variable_block_sizes <code>cached</code> \u00b6 <pre><code>construct_variable_block_sizes(\n    dit_seq_shape: tuple[int, int, int],\n    num_tiles: tuple[int, int, int],\n    device: device,\n) -&gt; torch.LongTensor\n</code></pre> <p>Compute the number of valid (non\u2011padded) tokens inside every (ts_t\u00a0\u00d7\u00a0ts_h\u00a0\u00d7\u00a0ts_w) tile after padding \u2011\u2011 flattened in the order (t\u2011tile, h\u2011tile, w\u2011tile) that <code>rearrange</code> uses.</p>"},{"location":"api/fastvideo/attention/#fastvideo.attention.backends.video_sparse_attn.construct_variable_block_sizes--returns","title":"Returns","text":"<p>torch.LongTensor  # shape: [\u220f full_window_size]</p> Source code in <code>fastvideo/attention/backends/video_sparse_attn.py</code> <pre><code>@functools.lru_cache(maxsize=10)\ndef construct_variable_block_sizes(\n    dit_seq_shape: tuple[int, int, int],\n    num_tiles: tuple[int, int, int],\n    device: torch.device,\n) -&gt; torch.LongTensor:\n    \"\"\"\n    Compute the number of valid (non\u2011padded) tokens inside every\n    (ts_t\u00a0\u00d7\u00a0ts_h\u00a0\u00d7\u00a0ts_w) tile after padding \u2011\u2011 flattened in the order\n    (t\u2011tile, h\u2011tile, w\u2011tile) that `rearrange` uses.\n\n    Returns\n    -------\n    torch.LongTensor  # shape: [\u220f full_window_size]\n    \"\"\"\n    # unpack\n    t, h, w = dit_seq_shape\n    ts_t, ts_h, ts_w = VSA_TILE_SIZE\n    n_t, n_h, n_w = num_tiles\n\n    def _sizes(dim_len: int, tile: int, n_tiles: int) -&gt; torch.LongTensor:\n        \"\"\"Vector with the size of each tile along one dimension.\"\"\"\n        sizes = torch.full((n_tiles, ), tile, dtype=torch.int, device=device)\n        # size of last (possibly partial) tile\n        remainder = dim_len - (n_tiles - 1) * tile\n        sizes[-1] = remainder if remainder &gt; 0 else tile\n        return sizes\n\n    t_sizes = _sizes(t, ts_t, n_t)  # [n_t]\n    h_sizes = _sizes(h, ts_h, n_h)  # [n_h]\n    w_sizes = _sizes(w, ts_w, n_w)  # [n_w]\n\n    # broadcast\u2011multiply to get voxels per tile, then flatten\n    block_sizes = (\n        t_sizes[:, None, None]  # [n_t, 1,   1]\n        * h_sizes[None, :, None]  # [1,   n_h, 1]\n        * w_sizes[None, None, :]  # [1,   1,   n_w]\n    ).reshape(-1)  # [n_t * n_h * n_w]\n\n    return block_sizes\n</code></pre>"},{"location":"api/fastvideo/attention/#fastvideo.attention.backends.vmoba","title":"fastvideo.attention.backends.vmoba","text":"Classes\u00b6 fastvideo.attention.backends.vmoba.VMOBAAttentionImpl \u00b6 <pre><code>VMOBAAttentionImpl(\n    num_heads,\n    head_size,\n    softmax_scale,\n    causal=False,\n    num_kv_heads=None,\n    prefix=\"\",\n    **extra_impl_args\n)\n</code></pre> <p>               Bases: <code>AttentionImpl</code></p> Source code in <code>fastvideo/attention/backends/vmoba.py</code> <pre><code>def __init__(self,\n             num_heads,\n             head_size,\n             softmax_scale,\n             causal=False,\n             num_kv_heads=None,\n             prefix=\"\",\n             **extra_impl_args) -&gt; None:\n    self.prefix = prefix\n    self.layer_idx = self._get_layer_idx(prefix)\n    from flash_attn.bert_padding import pad_input\n    self.pad_input = pad_input\n</code></pre> Functions\u00b6 fastvideo.attention.backends.vmoba.VMOBAAttentionImpl.forward \u00b6 <pre><code>forward(\n    query: Tensor,\n    key: Tensor,\n    value: Tensor,\n    attn_metadata: AttentionMetadata,\n) -&gt; torch.Tensor\n</code></pre> <p>query: [B, L, H, D] key:   [B, L, H, D] value: [B, L, H, D] attn_metadata: AttentionMetadata</p> Source code in <code>fastvideo/attention/backends/vmoba.py</code> <pre><code>def forward(\n    self,\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    attn_metadata: AttentionMetadata,\n) -&gt; torch.Tensor:\n    \"\"\"\n    query: [B, L, H, D]\n    key:   [B, L, H, D]\n    value: [B, L, H, D]\n    attn_metadata: AttentionMetadata\n    \"\"\"\n    batch_size, sequence_length, num_heads, head_dim = query.shape\n\n    # select chunk type according to layer idx:\n    loop_layer_num = attn_metadata.temporal_layer + attn_metadata.spatial_layer + attn_metadata.st_layer\n    moba_layer = self.layer_idx - attn_metadata.first_full_layer\n    if moba_layer % loop_layer_num &lt; attn_metadata.temporal_layer:\n        moba_chunk_size = attn_metadata.temporal_chunk_size\n        moba_topk = attn_metadata.temporal_topk\n    elif moba_layer % loop_layer_num &lt; attn_metadata.temporal_layer + attn_metadata.spatial_layer:\n        moba_chunk_size = attn_metadata.spatial_chunk_size\n        moba_topk = attn_metadata.spatial_topk\n    elif moba_layer % loop_layer_num &lt; attn_metadata.temporal_layer + attn_metadata.spatial_layer + attn_metadata.st_layer:\n        moba_chunk_size = attn_metadata.st_chunk_size\n        moba_topk = attn_metadata.st_topk\n\n    query, chunk_size = process_moba_input(query,\n                                           attn_metadata.patch_resolution,\n                                           moba_chunk_size)\n    key, chunk_size = process_moba_input(key,\n                                         attn_metadata.patch_resolution,\n                                         moba_chunk_size)\n    value, chunk_size = process_moba_input(value,\n                                           attn_metadata.patch_resolution,\n                                           moba_chunk_size)\n    max_seqlen = query.shape[1]\n    indices_q = torch.arange(0,\n                             query.shape[0] * query.shape[1],\n                             device=query.device)\n    cu_seqlens = torch.arange(0,\n                              query.shape[0] * query.shape[1] + 1,\n                              query.shape[1],\n                              dtype=torch.int32,\n                              device=query.device)\n    query = rearrange(query, \"b s ... -&gt; (b s) ...\")\n    key = rearrange(key, \"b s ... -&gt; (b s) ...\")\n    value = rearrange(value, \"b s ... -&gt; (b s) ...\")\n\n    # current_timestep=attn_metadata.current_timestep\n    hidden_states = moba_attn_varlen(\n        query,\n        key,\n        value,\n        cu_seqlens=cu_seqlens,\n        max_seqlen=max_seqlen,\n        moba_chunk_size=chunk_size,\n        moba_topk=moba_topk,\n        select_mode=attn_metadata.moba_select_mode,\n        simsum_threshold=attn_metadata.moba_threshold,\n        threshold_type=attn_metadata.moba_threshold_type,\n    )\n    hidden_states = self.pad_input(hidden_states, indices_q, batch_size,\n                                   sequence_length)\n    hidden_states = process_moba_output(hidden_states,\n                                        attn_metadata.patch_resolution,\n                                        moba_chunk_size)\n\n    return hidden_states\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/attention/#fastvideo.attention.layer","title":"fastvideo.attention.layer","text":""},{"location":"api/fastvideo/attention/#fastvideo.attention.layer-classes","title":"Classes","text":""},{"location":"api/fastvideo/attention/#fastvideo.attention.layer.DistributedAttention","title":"fastvideo.attention.layer.DistributedAttention","text":"<pre><code>DistributedAttention(\n    num_heads: int,\n    head_size: int,\n    num_kv_heads: int | None = None,\n    softmax_scale: float | None = None,\n    causal: bool = False,\n    supported_attention_backends: tuple[\n        AttentionBackendEnum, ...\n    ]\n    | None = None,\n    prefix: str = \"\",\n    **extra_impl_args\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Distributed attention layer.</p> Source code in <code>fastvideo/attention/layer.py</code> <pre><code>def __init__(self,\n             num_heads: int,\n             head_size: int,\n             num_kv_heads: int | None = None,\n             softmax_scale: float | None = None,\n             causal: bool = False,\n             supported_attention_backends: tuple[AttentionBackendEnum, ...]\n             | None = None,\n             prefix: str = \"\",\n             **extra_impl_args) -&gt; None:\n    super().__init__()\n    if softmax_scale is None:\n        self.softmax_scale = head_size**-0.5\n    else:\n        self.softmax_scale = softmax_scale\n\n    if num_kv_heads is None:\n        num_kv_heads = num_heads\n\n    dtype = get_compute_dtype()\n    attn_backend = get_attn_backend(\n        head_size,\n        dtype,\n        supported_attention_backends=supported_attention_backends)\n    impl_cls = attn_backend.get_impl_cls()\n    self.attn_impl = impl_cls(num_heads=num_heads,\n                              head_size=head_size,\n                              causal=causal,\n                              softmax_scale=self.softmax_scale,\n                              num_kv_heads=num_kv_heads,\n                              prefix=f\"{prefix}.impl\",\n                              **extra_impl_args)\n    self.num_heads = num_heads\n    self.head_size = head_size\n    self.num_kv_heads = num_kv_heads\n    self.backend = backend_name_to_enum(attn_backend.get_name())\n    self.dtype = dtype\n</code></pre> Functions\u00b6 fastvideo.attention.layer.DistributedAttention.forward \u00b6 <pre><code>forward(\n    q: Tensor,\n    k: Tensor,\n    v: Tensor,\n    replicated_q: Tensor | None = None,\n    replicated_k: Tensor | None = None,\n    replicated_v: Tensor | None = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor | None]\n</code></pre> <p>Forward pass for distributed attention.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>Tensor</code> <p>Query tensor [batch_size, seq_len, num_heads, head_dim]</p> required <code>k</code> <code>Tensor</code> <p>Key tensor [batch_size, seq_len, num_heads, head_dim]</p> required <code>v</code> <code>Tensor</code> <p>Value tensor [batch_size, seq_len, num_heads, head_dim]</p> required <code>replicated_q</code> <code>Optional[Tensor]</code> <p>Replicated query tensor, typically for text tokens</p> <code>None</code> <code>replicated_k</code> <code>Optional[Tensor]</code> <p>Replicated key tensor</p> <code>None</code> <code>replicated_v</code> <code>Optional[Tensor]</code> <p>Replicated value tensor</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor | None]</code> <p>Tuple[torch.Tensor, Optional[torch.Tensor]]: A tuple containing: - o (torch.Tensor): Output tensor after attention for the main sequence - replicated_o (Optional[torch.Tensor]): Output tensor for replicated tokens, if provided</p> Source code in <code>fastvideo/attention/layer.py</code> <pre><code>@torch.compiler.disable\ndef forward(\n    self,\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    replicated_q: torch.Tensor | None = None,\n    replicated_k: torch.Tensor | None = None,\n    replicated_v: torch.Tensor | None = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor | None]:\n    \"\"\"Forward pass for distributed attention.\n\n    Args:\n        q (torch.Tensor): Query tensor [batch_size, seq_len, num_heads, head_dim]\n        k (torch.Tensor): Key tensor [batch_size, seq_len, num_heads, head_dim]\n        v (torch.Tensor): Value tensor [batch_size, seq_len, num_heads, head_dim]\n        replicated_q (Optional[torch.Tensor]): Replicated query tensor, typically for text tokens\n        replicated_k (Optional[torch.Tensor]): Replicated key tensor\n        replicated_v (Optional[torch.Tensor]): Replicated value tensor\n\n    Returns:\n        Tuple[torch.Tensor, Optional[torch.Tensor]]: A tuple containing:\n            - o (torch.Tensor): Output tensor after attention for the main sequence\n            - replicated_o (Optional[torch.Tensor]): Output tensor for replicated tokens, if provided\n    \"\"\"\n    # Check input shapes\n    assert q.dim() == 4 and k.dim() == 4 and v.dim(\n    ) == 4, \"Expected 4D tensors\"\n    batch_size, seq_len, num_heads, head_dim = q.shape\n    local_rank = get_sp_parallel_rank()\n    world_size = get_sp_world_size()\n\n    forward_context: ForwardContext = get_forward_context()\n    ctx_attn_metadata = forward_context.attn_metadata\n\n    # Stack QKV\n    qkv = torch.cat([q, k, v], dim=0)  # [3, seq_len, num_heads, head_dim]\n\n    # Redistribute heads across sequence dimension\n    qkv = sequence_model_parallel_all_to_all_4D(qkv,\n                                                scatter_dim=2,\n                                                gather_dim=1)\n    # Apply backend-specific preprocess_qkv\n    qkv = self.attn_impl.preprocess_qkv(qkv, ctx_attn_metadata)\n\n    # Concatenate with replicated QKV if provided\n    if replicated_q is not None:\n        assert replicated_k is not None and replicated_v is not None\n        replicated_qkv = torch.cat(\n            [replicated_q, replicated_k, replicated_v],\n            dim=0)  # [3, seq_len, num_heads, head_dim]\n        heads_per_rank = num_heads // world_size\n        replicated_qkv = replicated_qkv[:, :, local_rank *\n                                        heads_per_rank:(local_rank + 1) *\n                                        heads_per_rank]\n        qkv = torch.cat([qkv, replicated_qkv], dim=1)\n\n    q, k, v = qkv.chunk(3, dim=0)\n\n    output = self.attn_impl.forward(q, k, v, ctx_attn_metadata)\n\n    # Redistribute back if using sequence parallelism\n    replicated_output = None\n    if replicated_q is not None:\n        replicated_output = output[:, seq_len * world_size:]\n        output = output[:, :seq_len * world_size]\n        # TODO: make this asynchronous\n        replicated_output = sequence_model_parallel_all_gather(\n            replicated_output.contiguous(), dim=2)\n    # Apply backend-specific postprocess_output\n    output = self.attn_impl.postprocess_output(output, ctx_attn_metadata)\n\n    output = sequence_model_parallel_all_to_all_4D(output,\n                                                   scatter_dim=1,\n                                                   gather_dim=2)\n    return output, replicated_output\n</code></pre>"},{"location":"api/fastvideo/attention/#fastvideo.attention.layer.DistributedAttention_VSA","title":"fastvideo.attention.layer.DistributedAttention_VSA","text":"<pre><code>DistributedAttention_VSA(\n    num_heads: int,\n    head_size: int,\n    num_kv_heads: int | None = None,\n    softmax_scale: float | None = None,\n    causal: bool = False,\n    supported_attention_backends: tuple[\n        AttentionBackendEnum, ...\n    ]\n    | None = None,\n    prefix: str = \"\",\n    **extra_impl_args\n)\n</code></pre> <p>               Bases: <code>DistributedAttention</code></p> <p>Distributed attention layer with VSA support.</p> Source code in <code>fastvideo/attention/layer.py</code> <pre><code>def __init__(self,\n             num_heads: int,\n             head_size: int,\n             num_kv_heads: int | None = None,\n             softmax_scale: float | None = None,\n             causal: bool = False,\n             supported_attention_backends: tuple[AttentionBackendEnum, ...]\n             | None = None,\n             prefix: str = \"\",\n             **extra_impl_args) -&gt; None:\n    super().__init__()\n    if softmax_scale is None:\n        self.softmax_scale = head_size**-0.5\n    else:\n        self.softmax_scale = softmax_scale\n\n    if num_kv_heads is None:\n        num_kv_heads = num_heads\n\n    dtype = get_compute_dtype()\n    attn_backend = get_attn_backend(\n        head_size,\n        dtype,\n        supported_attention_backends=supported_attention_backends)\n    impl_cls = attn_backend.get_impl_cls()\n    self.attn_impl = impl_cls(num_heads=num_heads,\n                              head_size=head_size,\n                              causal=causal,\n                              softmax_scale=self.softmax_scale,\n                              num_kv_heads=num_kv_heads,\n                              prefix=f\"{prefix}.impl\",\n                              **extra_impl_args)\n    self.num_heads = num_heads\n    self.head_size = head_size\n    self.num_kv_heads = num_kv_heads\n    self.backend = backend_name_to_enum(attn_backend.get_name())\n    self.dtype = dtype\n</code></pre> Functions\u00b6 fastvideo.attention.layer.DistributedAttention_VSA.forward \u00b6 <pre><code>forward(\n    q: Tensor,\n    k: Tensor,\n    v: Tensor,\n    replicated_q: Tensor | None = None,\n    replicated_k: Tensor | None = None,\n    replicated_v: Tensor | None = None,\n    gate_compress: Tensor | None = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor | None]\n</code></pre> <p>Forward pass for distributed attention.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>Tensor</code> <p>Query tensor [batch_size, seq_len, num_heads, head_dim]</p> required <code>k</code> <code>Tensor</code> <p>Key tensor [batch_size, seq_len, num_heads, head_dim]</p> required <code>v</code> <code>Tensor</code> <p>Value tensor [batch_size, seq_len, num_heads, head_dim]</p> required <code>gate_compress</code> <code>Tensor</code> <p>Gate compress tensor [batch_size, seq_len, num_heads, head_dim]</p> <code>None</code> <code>replicated_q</code> <code>Optional[Tensor]</code> <p>Replicated query tensor, typically for text tokens</p> <code>None</code> <code>replicated_k</code> <code>Optional[Tensor]</code> <p>Replicated key tensor</p> <code>None</code> <code>replicated_v</code> <code>Optional[Tensor]</code> <p>Replicated value tensor</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor | None]</code> <p>Tuple[torch.Tensor, Optional[torch.Tensor]]: A tuple containing: - o (torch.Tensor): Output tensor after attention for the main sequence - replicated_o (Optional[torch.Tensor]): Output tensor for replicated tokens, if provided</p> Source code in <code>fastvideo/attention/layer.py</code> <pre><code>@torch.compiler.disable\ndef forward(\n    self,\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    replicated_q: torch.Tensor | None = None,\n    replicated_k: torch.Tensor | None = None,\n    replicated_v: torch.Tensor | None = None,\n    gate_compress: torch.Tensor | None = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor | None]:\n    \"\"\"Forward pass for distributed attention.\n\n    Args:\n        q (torch.Tensor): Query tensor [batch_size, seq_len, num_heads, head_dim]\n        k (torch.Tensor): Key tensor [batch_size, seq_len, num_heads, head_dim]\n        v (torch.Tensor): Value tensor [batch_size, seq_len, num_heads, head_dim]\n        gate_compress (torch.Tensor): Gate compress tensor [batch_size, seq_len, num_heads, head_dim]\n        replicated_q (Optional[torch.Tensor]): Replicated query tensor, typically for text tokens\n        replicated_k (Optional[torch.Tensor]): Replicated key tensor\n        replicated_v (Optional[torch.Tensor]): Replicated value tensor\n\n    Returns:\n        Tuple[torch.Tensor, Optional[torch.Tensor]]: A tuple containing:\n            - o (torch.Tensor): Output tensor after attention for the main sequence\n            - replicated_o (Optional[torch.Tensor]): Output tensor for replicated tokens, if provided\n    \"\"\"\n    # Check text tokens are not supported for VSA now\n    assert replicated_q is None and replicated_k is None and replicated_v is None, \"Replicated QKV is not supported for VSA now\"\n    # Check input shapes\n    assert q.dim() == 4 and k.dim() == 4 and v.dim(\n    ) == 4, \"Expected 4D tensors\"\n\n    forward_context: ForwardContext = get_forward_context()\n    ctx_attn_metadata = forward_context.attn_metadata\n\n    # Stack QKV\n    qkvg = torch.cat([q, k, v, gate_compress],\n                     dim=0)  # [3, seq_len, num_heads, head_dim]\n\n    # Redistribute heads across sequence dimension\n    qkvg = sequence_model_parallel_all_to_all_4D(qkvg,\n                                                 scatter_dim=2,\n                                                 gather_dim=1)\n\n    qkvg = self.attn_impl.preprocess_qkv(qkvg, ctx_attn_metadata)\n\n    q, k, v, gate_compress = qkvg.chunk(4, dim=0)\n    output = self.attn_impl.forward(\n        q, k, v, gate_compress, ctx_attn_metadata)  # type: ignore[call-arg]\n\n    # Redistribute back if using sequence parallelism\n    replicated_output = None\n\n    # Apply backend-specific postprocess_output\n    output = self.attn_impl.postprocess_output(output, ctx_attn_metadata)\n\n    output = sequence_model_parallel_all_to_all_4D(output,\n                                                   scatter_dim=1,\n                                                   gather_dim=2)\n    return output, replicated_output\n</code></pre>"},{"location":"api/fastvideo/attention/#fastvideo.attention.layer.LocalAttention","title":"fastvideo.attention.layer.LocalAttention","text":"<pre><code>LocalAttention(\n    num_heads: int,\n    head_size: int,\n    num_kv_heads: int | None = None,\n    softmax_scale: float | None = None,\n    causal: bool = False,\n    supported_attention_backends: tuple[\n        AttentionBackendEnum, ...\n    ]\n    | None = None,\n    **extra_impl_args\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Attention layer.</p> Source code in <code>fastvideo/attention/layer.py</code> <pre><code>def __init__(self,\n             num_heads: int,\n             head_size: int,\n             num_kv_heads: int | None = None,\n             softmax_scale: float | None = None,\n             causal: bool = False,\n             supported_attention_backends: tuple[AttentionBackendEnum, ...]\n             | None = None,\n             **extra_impl_args) -&gt; None:\n    super().__init__()\n    if softmax_scale is None:\n        self.softmax_scale = head_size**-0.5\n    else:\n        self.softmax_scale = softmax_scale\n    if num_kv_heads is None:\n        num_kv_heads = num_heads\n\n    dtype = get_compute_dtype()\n    attn_backend = get_attn_backend(\n        head_size,\n        dtype,\n        supported_attention_backends=supported_attention_backends)\n    impl_cls = attn_backend.get_impl_cls()\n    self.attn_impl = impl_cls(num_heads=num_heads,\n                              head_size=head_size,\n                              softmax_scale=self.softmax_scale,\n                              num_kv_heads=num_kv_heads,\n                              causal=causal,\n                              **extra_impl_args)\n    self.num_heads = num_heads\n    self.head_size = head_size\n    self.num_kv_heads = num_kv_heads\n    self.backend = backend_name_to_enum(attn_backend.get_name())\n    self.dtype = dtype\n</code></pre> Functions\u00b6 fastvideo.attention.layer.LocalAttention.forward \u00b6 <pre><code>forward(q: Tensor, k: Tensor, v: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Apply local attention between query, key and value tensors.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>Tensor</code> <p>Query tensor of shape [batch_size, seq_len, num_heads, head_dim]</p> required <code>k</code> <code>Tensor</code> <p>Key tensor of shape [batch_size, seq_len, num_heads, head_dim] </p> required <code>v</code> <code>Tensor</code> <p>Value tensor of shape [batch_size, seq_len, num_heads, head_dim]</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output tensor after local attention</p> Source code in <code>fastvideo/attention/layer.py</code> <pre><code>def forward(\n    self,\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Apply local attention between query, key and value tensors.\n\n    Args:\n        q (torch.Tensor): Query tensor of shape [batch_size, seq_len, num_heads, head_dim]\n        k (torch.Tensor): Key tensor of shape [batch_size, seq_len, num_heads, head_dim] \n        v (torch.Tensor): Value tensor of shape [batch_size, seq_len, num_heads, head_dim]\n\n    Returns:\n        torch.Tensor: Output tensor after local attention\n    \"\"\"\n    # Check input shapes\n    assert q.dim() == 4 and k.dim() == 4 and v.dim(\n    ) == 4, \"Expected 4D tensors\"\n\n    forward_context: ForwardContext = get_forward_context()\n    ctx_attn_metadata = forward_context.attn_metadata\n\n    output = self.attn_impl.forward(q, k, v, ctx_attn_metadata)\n    return output\n</code></pre>"},{"location":"api/fastvideo/attention/#fastvideo.attention.layer-functions","title":"Functions","text":""},{"location":"api/fastvideo/attention/#fastvideo.attention.selector","title":"fastvideo.attention.selector","text":""},{"location":"api/fastvideo/attention/#fastvideo.attention.selector-classes","title":"Classes","text":""},{"location":"api/fastvideo/attention/#fastvideo.attention.selector-functions","title":"Functions","text":""},{"location":"api/fastvideo/attention/#fastvideo.attention.selector.backend_name_to_enum","title":"fastvideo.attention.selector.backend_name_to_enum","text":"<pre><code>backend_name_to_enum(\n    backend_name: str,\n) -&gt; AttentionBackendEnum | None\n</code></pre> <p>Convert a string backend name to a _Backend enum value.</p> <p>Returns: * _Backend: enum value if backend_name is a valid in-tree type * None: otherwise it's an invalid in-tree type or an out-of-tree platform is         loaded.</p> Source code in <code>fastvideo/attention/selector.py</code> <pre><code>def backend_name_to_enum(backend_name: str) -&gt; AttentionBackendEnum | None:\n    \"\"\"\n    Convert a string backend name to a _Backend enum value.\n\n    Returns:\n    * _Backend: enum value if backend_name is a valid in-tree type\n    * None: otherwise it's an invalid in-tree type or an out-of-tree platform is\n            loaded.\n    \"\"\"\n    assert backend_name is not None\n    return AttentionBackendEnum[backend_name] if backend_name in AttentionBackendEnum.__members__ else \\\n          None\n</code></pre>"},{"location":"api/fastvideo/attention/#fastvideo.attention.selector.get_env_variable_attn_backend","title":"fastvideo.attention.selector.get_env_variable_attn_backend","text":"<pre><code>get_env_variable_attn_backend() -&gt; AttentionBackendEnum | None\n</code></pre> <p>Get the backend override specified by the FastVideo attention backend environment variable, if one is specified.</p> <p>Returns:</p> <ul> <li>_Backend enum value if an override is specified</li> <li>None otherwise</li> </ul> Source code in <code>fastvideo/attention/selector.py</code> <pre><code>def get_env_variable_attn_backend() -&gt; AttentionBackendEnum | None:\n    '''\n    Get the backend override specified by the FastVideo attention\n    backend environment variable, if one is specified.\n\n    Returns:\n\n    * _Backend enum value if an override is specified\n    * None otherwise\n    '''\n    backend_name = os.environ.get(STR_BACKEND_ENV_VAR)\n    return (None\n            if backend_name is None else backend_name_to_enum(backend_name))\n</code></pre>"},{"location":"api/fastvideo/attention/#fastvideo.attention.selector.get_global_forced_attn_backend","title":"fastvideo.attention.selector.get_global_forced_attn_backend","text":"<pre><code>get_global_forced_attn_backend() -&gt; AttentionBackendEnum | None\n</code></pre> <p>Get the currently-forced choice of attention backend, or None if auto-selection is currently enabled.</p> Source code in <code>fastvideo/attention/selector.py</code> <pre><code>def get_global_forced_attn_backend() -&gt; AttentionBackendEnum | None:\n    '''\n    Get the currently-forced choice of attention backend,\n    or None if auto-selection is currently enabled.\n    '''\n    return forced_attn_backend\n</code></pre>"},{"location":"api/fastvideo/attention/#fastvideo.attention.selector.global_force_attn_backend","title":"fastvideo.attention.selector.global_force_attn_backend","text":"<pre><code>global_force_attn_backend(\n    attn_backend: AttentionBackendEnum | None,\n) -&gt; None\n</code></pre> <p>Force all attention operations to use a specified backend.</p> <p>Passing <code>None</code> for the argument re-enables automatic backend selection.,</p> <p>Arguments:</p> <ul> <li>attn_backend: backend selection (None to revert to auto)</li> </ul> Source code in <code>fastvideo/attention/selector.py</code> <pre><code>def global_force_attn_backend(\n        attn_backend: AttentionBackendEnum | None) -&gt; None:\n    '''\n    Force all attention operations to use a specified backend.\n\n    Passing `None` for the argument re-enables automatic\n    backend selection.,\n\n    Arguments:\n\n    * attn_backend: backend selection (None to revert to auto)\n    '''\n    global forced_attn_backend\n    forced_attn_backend = attn_backend\n</code></pre>"},{"location":"api/fastvideo/attention/#fastvideo.attention.selector.global_force_attn_backend_context_manager","title":"fastvideo.attention.selector.global_force_attn_backend_context_manager","text":"<pre><code>global_force_attn_backend_context_manager(\n    attn_backend: AttentionBackendEnum,\n) -&gt; Generator[None, None, None]\n</code></pre> <p>Globally force a FastVideo attention backend override within a context manager, reverting the global attention backend override to its prior state upon exiting the context manager.</p> <p>Arguments:</p> <ul> <li>attn_backend: attention backend to force</li> </ul> <p>Returns:</p> <ul> <li>Generator</li> </ul> Source code in <code>fastvideo/attention/selector.py</code> <pre><code>@contextmanager\ndef global_force_attn_backend_context_manager(\n        attn_backend: AttentionBackendEnum) -&gt; Generator[None, None, None]:\n    '''\n    Globally force a FastVideo attention backend override within a\n    context manager, reverting the global attention backend\n    override to its prior state upon exiting the context\n    manager.\n\n    Arguments:\n\n    * attn_backend: attention backend to force\n\n    Returns:\n\n    * Generator\n    '''\n\n    # Save the current state of the global backend override (if any)\n    original_value = get_global_forced_attn_backend()\n\n    # Globally force the new backend override\n    global_force_attn_backend(attn_backend)\n\n    # Yield control back to the enclosed code block\n    try:\n        yield\n    finally:\n        # Revert the original global backend override, if any\n        global_force_attn_backend(original_value)\n</code></pre>"},{"location":"api/fastvideo/configs/","title":"configs","text":""},{"location":"api/fastvideo/configs/#fastvideo.configs","title":"configs","text":""},{"location":"api/fastvideo/configs/#fastvideo.configs-modules","title":"Modules","text":""},{"location":"api/fastvideo/configs/#fastvideo.configs.configs","title":"fastvideo.configs.configs","text":""},{"location":"api/fastvideo/configs/#fastvideo.configs.configs-classes","title":"Classes","text":""},{"location":"api/fastvideo/configs/#fastvideo.configs.configs.DatasetType","title":"fastvideo.configs.configs.DatasetType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration for different dataset types.</p> Functions\u00b6 fastvideo.configs.configs.DatasetType.choices <code>classmethod</code> \u00b6 <pre><code>choices() -&gt; list[str]\n</code></pre> <p>Get all available choices as strings for argparse.</p> Source code in <code>fastvideo/configs/configs.py</code> <pre><code>@classmethod\ndef choices(cls) -&gt; list[str]:\n    \"\"\"Get all available choices as strings for argparse.\"\"\"\n    return [dataset_type.value for dataset_type in cls]\n</code></pre> fastvideo.configs.configs.DatasetType.from_string <code>classmethod</code> \u00b6 <pre><code>from_string(value: str) -&gt; DatasetType\n</code></pre> <p>Convert string to DatasetType enum.</p> Source code in <code>fastvideo/configs/configs.py</code> <pre><code>@classmethod\ndef from_string(cls, value: str) -&gt; \"DatasetType\":\n    \"\"\"Convert string to DatasetType enum.\"\"\"\n    try:\n        return cls(value.lower())\n    except ValueError:\n        raise ValueError(\n            f\"Invalid dataset type: {value}. Must be one of: {', '.join([m.value for m in cls])}\"\n        ) from None\n</code></pre>"},{"location":"api/fastvideo/configs/#fastvideo.configs.configs.PreprocessConfig","title":"fastvideo.configs.configs.PreprocessConfig  <code>dataclass</code>","text":"<pre><code>PreprocessConfig(\n    model_path: str = \"\",\n    dataset_path: str = \"\",\n    dataset_type: DatasetType = DatasetType.HF,\n    dataset_output_dir: str = \"./output\",\n    dataloader_num_workers: int = 1,\n    preprocess_video_batch_size: int = 2,\n    samples_per_file: int = 64,\n    flush_frequency: int = 256,\n    video_loader_type: VideoLoaderType = VideoLoaderType.TORCHCODEC,\n    max_height: int = 480,\n    max_width: int = 848,\n    num_frames: int = 163,\n    video_length_tolerance_range: float = 2.0,\n    train_fps: int = 30,\n    speed_factor: float = 1.0,\n    drop_short_ratio: float = 1.0,\n    do_temporal_sample: bool = False,\n    training_cfg_rate: float = 0.0,\n    seed: int = 42,\n)\n</code></pre> <p>Configuration for preprocessing operations.</p> Functions\u00b6 fastvideo.configs.configs.PreprocessConfig.add_cli_args <code>staticmethod</code> \u00b6 <pre><code>add_cli_args(\n    parser: FlexibleArgumentParser,\n    prefix: str = \"preprocess\",\n) -&gt; FlexibleArgumentParser\n</code></pre> <p>Add preprocessing configuration arguments to the parser.</p> Source code in <code>fastvideo/configs/configs.py</code> <pre><code>@staticmethod\ndef add_cli_args(parser: FlexibleArgumentParser,\n                 prefix: str = \"preprocess\") -&gt; FlexibleArgumentParser:\n    \"\"\"Add preprocessing configuration arguments to the parser.\"\"\"\n    prefix_with_dot = f\"{prefix}.\" if (prefix.strip() != \"\") else \"\"\n\n    preprocess_args = parser.add_argument_group(\"Preprocessing Arguments\")\n    # Model &amp; Dataset\n    preprocess_args.add_argument(f\"--{prefix_with_dot}model-path\",\n                                 type=str,\n                                 default=PreprocessConfig.model_path,\n                                 help=\"Path to the model for preprocessing\")\n    preprocess_args.add_argument(\n        f\"--{prefix_with_dot}dataset-path\",\n        type=str,\n        default=PreprocessConfig.dataset_path,\n        help=\"Path to the dataset directory for preprocessing\")\n    preprocess_args.add_argument(\n        f\"--{prefix_with_dot}dataset-type\",\n        type=str,\n        choices=DatasetType.choices(),\n        default=PreprocessConfig.dataset_type.value,\n        help=\"Type of the dataset\")\n    preprocess_args.add_argument(\n        f\"--{prefix_with_dot}dataset-output-dir\",\n        type=str,\n        default=PreprocessConfig.dataset_output_dir,\n        help=\"The output directory where the dataset will be written.\")\n\n    # Dataloader\n    preprocess_args.add_argument(\n        f\"--{prefix_with_dot}dataloader-num-workers\",\n        type=int,\n        default=PreprocessConfig.dataloader_num_workers,\n        help=\n        \"Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\"\n    )\n    preprocess_args.add_argument(\n        f\"--{prefix_with_dot}preprocess-video-batch-size\",\n        type=int,\n        default=PreprocessConfig.preprocess_video_batch_size,\n        help=\"Batch size (per device) for the training dataloader.\")\n\n    # Saver\n    preprocess_args.add_argument(f\"--{prefix_with_dot}samples-per-file\",\n                                 type=int,\n                                 default=PreprocessConfig.samples_per_file,\n                                 help=\"Number of samples per output file\")\n    preprocess_args.add_argument(f\"--{prefix_with_dot}flush-frequency\",\n                                 type=int,\n                                 default=PreprocessConfig.flush_frequency,\n                                 help=\"How often to save to parquet files\")\n\n    # Video processing parameters\n    preprocess_args.add_argument(\n        f\"--{prefix_with_dot}video-loader-type\",\n        type=str,\n        choices=VideoLoaderType.choices(),\n        default=PreprocessConfig.video_loader_type.value,\n        help=\"Type of the video loader\")\n    preprocess_args.add_argument(f\"--{prefix_with_dot}max-height\",\n                                 type=int,\n                                 default=PreprocessConfig.max_height,\n                                 help=\"Maximum height for video processing\")\n    preprocess_args.add_argument(f\"--{prefix_with_dot}max-width\",\n                                 type=int,\n                                 default=PreprocessConfig.max_width,\n                                 help=\"Maximum width for video processing\")\n    preprocess_args.add_argument(f\"--{prefix_with_dot}num-frames\",\n                                 type=int,\n                                 default=PreprocessConfig.num_frames,\n                                 help=\"Number of frames to process\")\n    preprocess_args.add_argument(\n        f\"--{prefix_with_dot}video-length-tolerance-range\",\n        type=float,\n        default=PreprocessConfig.video_length_tolerance_range,\n        help=\"Video length tolerance range\")\n    preprocess_args.add_argument(f\"--{prefix_with_dot}train-fps\",\n                                 type=int,\n                                 default=PreprocessConfig.train_fps,\n                                 help=\"Training FPS\")\n    preprocess_args.add_argument(f\"--{prefix_with_dot}speed-factor\",\n                                 type=float,\n                                 default=PreprocessConfig.speed_factor,\n                                 help=\"Speed factor for video processing\")\n    preprocess_args.add_argument(f\"--{prefix_with_dot}drop-short-ratio\",\n                                 type=float,\n                                 default=PreprocessConfig.drop_short_ratio,\n                                 help=\"Ratio for dropping short videos\")\n    preprocess_args.add_argument(\n        f\"--{prefix_with_dot}do-temporal-sample\",\n        action=StoreBoolean,\n        default=PreprocessConfig.do_temporal_sample,\n        help=\"Whether to do temporal sampling\")\n\n    # Model Training configuration\n    preprocess_args.add_argument(f\"--{prefix_with_dot}training-cfg-rate\",\n                                 type=float,\n                                 default=PreprocessConfig.training_cfg_rate,\n                                 help=\"Training CFG rate\")\n    preprocess_args.add_argument(f\"--{prefix_with_dot}seed\",\n                                 type=int,\n                                 default=PreprocessConfig.seed,\n                                 help=\"Seed for random number generator\")\n\n    return parser\n</code></pre> fastvideo.configs.configs.PreprocessConfig.from_kwargs <code>classmethod</code> \u00b6 <pre><code>from_kwargs(\n    kwargs: dict[str, Any]\n) -&gt; Optional[PreprocessConfig]\n</code></pre> <p>Create PreprocessConfig from keyword arguments.</p> Source code in <code>fastvideo/configs/configs.py</code> <pre><code>@classmethod\ndef from_kwargs(cls, kwargs: dict[str,\n                                  Any]) -&gt; Optional[\"PreprocessConfig\"]:\n    \"\"\"Create PreprocessConfig from keyword arguments.\"\"\"\n    if 'dataset_type' in kwargs and isinstance(kwargs['dataset_type'], str):\n        kwargs['dataset_type'] = DatasetType.from_string(\n            kwargs['dataset_type'])\n    if 'video_loader_type' in kwargs and isinstance(\n            kwargs['video_loader_type'], str):\n        kwargs['video_loader_type'] = VideoLoaderType.from_string(\n            kwargs['video_loader_type'])\n\n    preprocess_config = cls()\n    if not update_config_from_args(\n            preprocess_config, kwargs, prefix=\"preprocess\", pop_args=True):\n        return None\n    return preprocess_config\n</code></pre>"},{"location":"api/fastvideo/configs/#fastvideo.configs.configs.VideoLoaderType","title":"fastvideo.configs.configs.VideoLoaderType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration for different video loaders.</p> Functions\u00b6 fastvideo.configs.configs.VideoLoaderType.choices <code>classmethod</code> \u00b6 <pre><code>choices() -&gt; list[str]\n</code></pre> <p>Get all available choices as strings for argparse.</p> Source code in <code>fastvideo/configs/configs.py</code> <pre><code>@classmethod\ndef choices(cls) -&gt; list[str]:\n    \"\"\"Get all available choices as strings for argparse.\"\"\"\n    return [video_loader.value for video_loader in cls]\n</code></pre> fastvideo.configs.configs.VideoLoaderType.from_string <code>classmethod</code> \u00b6 <pre><code>from_string(value: str) -&gt; VideoLoaderType\n</code></pre> <p>Convert string to VideoLoader enum.</p> Source code in <code>fastvideo/configs/configs.py</code> <pre><code>@classmethod\ndef from_string(cls, value: str) -&gt; \"VideoLoaderType\":\n    \"\"\"Convert string to VideoLoader enum.\"\"\"\n    try:\n        return cls(value.lower())\n    except ValueError:\n        raise ValueError(\n            f\"Invalid video loader: {value}. Must be one of: {', '.join([m.value for m in cls])}\"\n        ) from None\n</code></pre>"},{"location":"api/fastvideo/configs/#fastvideo.configs.configs-functions","title":"Functions","text":""},{"location":"api/fastvideo/configs/#fastvideo.configs.models","title":"fastvideo.configs.models","text":""},{"location":"api/fastvideo/configs/#fastvideo.configs.models-classes","title":"Classes","text":""},{"location":"api/fastvideo/configs/#fastvideo.configs.models.DiTConfig","title":"fastvideo.configs.models.DiTConfig  <code>dataclass</code>","text":"<pre><code>DiTConfig(\n    arch_config: DiTArchConfig = DiTArchConfig(),\n    prefix: str = \"\",\n    quant_config: QuantizationConfig | None = None,\n)\n</code></pre> <p>               Bases: <code>ModelConfig</code></p> Functions\u00b6 fastvideo.configs.models.DiTConfig.add_cli_args <code>staticmethod</code> \u00b6 <pre><code>add_cli_args(\n    parser: Any, prefix: str = \"dit-config\"\n) -&gt; Any\n</code></pre> <p>Add CLI arguments for DiTConfig fields</p> Source code in <code>fastvideo/configs/models/dits/base.py</code> <pre><code>@staticmethod\ndef add_cli_args(parser: Any, prefix: str = \"dit-config\") -&gt; Any:\n    \"\"\"Add CLI arguments for DiTConfig fields\"\"\"\n    parser.add_argument(\n        f\"--{prefix}.prefix\",\n        type=str,\n        dest=f\"{prefix.replace('-', '_')}.prefix\",\n        default=DiTConfig.prefix,\n        help=\"Prefix for the DiT model\",\n    )\n\n    parser.add_argument(\n        f\"--{prefix}.quant-config\",\n        type=str,\n        dest=f\"{prefix.replace('-', '_')}.quant_config\",\n        default=None,\n        help=\"Quantization configuration for the DiT model\",\n    )\n\n    return parser\n</code></pre>"},{"location":"api/fastvideo/configs/#fastvideo.configs.models.VAEConfig","title":"fastvideo.configs.models.VAEConfig  <code>dataclass</code>","text":"<pre><code>VAEConfig(\n    arch_config: VAEArchConfig = VAEArchConfig(),\n    load_encoder: bool = True,\n    load_decoder: bool = True,\n    tile_sample_min_height: int = 256,\n    tile_sample_min_width: int = 256,\n    tile_sample_min_num_frames: int = 16,\n    tile_sample_stride_height: int = 192,\n    tile_sample_stride_width: int = 192,\n    tile_sample_stride_num_frames: int = 12,\n    blend_num_frames: int = 0,\n    use_tiling: bool = True,\n    use_temporal_tiling: bool = True,\n    use_parallel_tiling: bool = True,\n    use_temporal_scaling_frames: bool = True,\n)\n</code></pre> <p>               Bases: <code>ModelConfig</code></p> Functions\u00b6 fastvideo.configs.models.VAEConfig.add_cli_args <code>staticmethod</code> \u00b6 <pre><code>add_cli_args(\n    parser: Any, prefix: str = \"vae-config\"\n) -&gt; Any\n</code></pre> <p>Add CLI arguments for VAEConfig fields</p> Source code in <code>fastvideo/configs/models/vaes/base.py</code> <pre><code>@staticmethod\ndef add_cli_args(parser: Any, prefix: str = \"vae-config\") -&gt; Any:\n    \"\"\"Add CLI arguments for VAEConfig fields\"\"\"\n    parser.add_argument(\n        f\"--{prefix}.load-encoder\",\n        action=StoreBoolean,\n        dest=f\"{prefix.replace('-', '_')}.load_encoder\",\n        default=VAEConfig.load_encoder,\n        help=\"Whether to load the VAE encoder\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.load-decoder\",\n        action=StoreBoolean,\n        dest=f\"{prefix.replace('-', '_')}.load_decoder\",\n        default=VAEConfig.load_decoder,\n        help=\"Whether to load the VAE decoder\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-min-height\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_min_height\",\n        default=VAEConfig.tile_sample_min_height,\n        help=\"Minimum height for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-min-width\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_min_width\",\n        default=VAEConfig.tile_sample_min_width,\n        help=\"Minimum width for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-min-num-frames\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_min_num_frames\",\n        default=VAEConfig.tile_sample_min_num_frames,\n        help=\"Minimum number of frames for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-stride-height\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_stride_height\",\n        default=VAEConfig.tile_sample_stride_height,\n        help=\"Stride height for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-stride-width\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_stride_width\",\n        default=VAEConfig.tile_sample_stride_width,\n        help=\"Stride width for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-stride-num-frames\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_stride_num_frames\",\n        default=VAEConfig.tile_sample_stride_num_frames,\n        help=\"Stride number of frames for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.blend-num-frames\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.blend_num_frames\",\n        default=VAEConfig.blend_num_frames,\n        help=\"Number of frames to blend for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.use-tiling\",\n        action=StoreBoolean,\n        dest=f\"{prefix.replace('-', '_')}.use_tiling\",\n        default=VAEConfig.use_tiling,\n        help=\"Whether to use tiling for VAE\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.use-temporal-tiling\",\n        action=StoreBoolean,\n        dest=f\"{prefix.replace('-', '_')}.use_temporal_tiling\",\n        default=VAEConfig.use_temporal_tiling,\n        help=\"Whether to use temporal tiling for VAE\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.use-parallel-tiling\",\n        action=StoreBoolean,\n        dest=f\"{prefix.replace('-', '_')}.use_parallel_tiling\",\n        default=VAEConfig.use_parallel_tiling,\n        help=\"Whether to use parallel tiling for VAE\",\n    )\n\n    return parser\n</code></pre>"},{"location":"api/fastvideo/configs/#fastvideo.configs.models-modules","title":"Modules","text":""},{"location":"api/fastvideo/configs/#fastvideo.configs.models.dits","title":"fastvideo.configs.models.dits","text":"Modules\u00b6 fastvideo.configs.models.dits.base \u00b6 Classes\u00b6 fastvideo.configs.models.dits.base.DiTConfig <code>dataclass</code> \u00b6 <pre><code>DiTConfig(\n    arch_config: DiTArchConfig = DiTArchConfig(),\n    prefix: str = \"\",\n    quant_config: QuantizationConfig | None = None,\n)\n</code></pre> <p>               Bases: <code>ModelConfig</code></p> Functions\u00b6 fastvideo.configs.models.dits.base.DiTConfig.add_cli_args <code>staticmethod</code> \u00b6 <pre><code>add_cli_args(\n    parser: Any, prefix: str = \"dit-config\"\n) -&gt; Any\n</code></pre> <p>Add CLI arguments for DiTConfig fields</p> Source code in <code>fastvideo/configs/models/dits/base.py</code> <pre><code>@staticmethod\ndef add_cli_args(parser: Any, prefix: str = \"dit-config\") -&gt; Any:\n    \"\"\"Add CLI arguments for DiTConfig fields\"\"\"\n    parser.add_argument(\n        f\"--{prefix}.prefix\",\n        type=str,\n        dest=f\"{prefix.replace('-', '_')}.prefix\",\n        default=DiTConfig.prefix,\n        help=\"Prefix for the DiT model\",\n    )\n\n    parser.add_argument(\n        f\"--{prefix}.quant-config\",\n        type=str,\n        dest=f\"{prefix.replace('-', '_')}.quant_config\",\n        default=None,\n        help=\"Quantization configuration for the DiT model\",\n    )\n\n    return parser\n</code></pre>"},{"location":"api/fastvideo/configs/#fastvideo.configs.models.vaes","title":"fastvideo.configs.models.vaes","text":"Modules\u00b6 fastvideo.configs.models.vaes.base \u00b6 Classes\u00b6 fastvideo.configs.models.vaes.base.VAEConfig <code>dataclass</code> \u00b6 <pre><code>VAEConfig(\n    arch_config: VAEArchConfig = VAEArchConfig(),\n    load_encoder: bool = True,\n    load_decoder: bool = True,\n    tile_sample_min_height: int = 256,\n    tile_sample_min_width: int = 256,\n    tile_sample_min_num_frames: int = 16,\n    tile_sample_stride_height: int = 192,\n    tile_sample_stride_width: int = 192,\n    tile_sample_stride_num_frames: int = 12,\n    blend_num_frames: int = 0,\n    use_tiling: bool = True,\n    use_temporal_tiling: bool = True,\n    use_parallel_tiling: bool = True,\n    use_temporal_scaling_frames: bool = True,\n)\n</code></pre> <p>               Bases: <code>ModelConfig</code></p> Functions\u00b6 fastvideo.configs.models.vaes.base.VAEConfig.add_cli_args <code>staticmethod</code> \u00b6 <pre><code>add_cli_args(\n    parser: Any, prefix: str = \"vae-config\"\n) -&gt; Any\n</code></pre> <p>Add CLI arguments for VAEConfig fields</p> Source code in <code>fastvideo/configs/models/vaes/base.py</code> <pre><code>@staticmethod\ndef add_cli_args(parser: Any, prefix: str = \"vae-config\") -&gt; Any:\n    \"\"\"Add CLI arguments for VAEConfig fields\"\"\"\n    parser.add_argument(\n        f\"--{prefix}.load-encoder\",\n        action=StoreBoolean,\n        dest=f\"{prefix.replace('-', '_')}.load_encoder\",\n        default=VAEConfig.load_encoder,\n        help=\"Whether to load the VAE encoder\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.load-decoder\",\n        action=StoreBoolean,\n        dest=f\"{prefix.replace('-', '_')}.load_decoder\",\n        default=VAEConfig.load_decoder,\n        help=\"Whether to load the VAE decoder\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-min-height\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_min_height\",\n        default=VAEConfig.tile_sample_min_height,\n        help=\"Minimum height for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-min-width\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_min_width\",\n        default=VAEConfig.tile_sample_min_width,\n        help=\"Minimum width for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-min-num-frames\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_min_num_frames\",\n        default=VAEConfig.tile_sample_min_num_frames,\n        help=\"Minimum number of frames for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-stride-height\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_stride_height\",\n        default=VAEConfig.tile_sample_stride_height,\n        help=\"Stride height for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-stride-width\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_stride_width\",\n        default=VAEConfig.tile_sample_stride_width,\n        help=\"Stride width for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.tile-sample-stride-num-frames\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.tile_sample_stride_num_frames\",\n        default=VAEConfig.tile_sample_stride_num_frames,\n        help=\"Stride number of frames for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.blend-num-frames\",\n        type=int,\n        dest=f\"{prefix.replace('-', '_')}.blend_num_frames\",\n        default=VAEConfig.blend_num_frames,\n        help=\"Number of frames to blend for VAE tile sampling\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.use-tiling\",\n        action=StoreBoolean,\n        dest=f\"{prefix.replace('-', '_')}.use_tiling\",\n        default=VAEConfig.use_tiling,\n        help=\"Whether to use tiling for VAE\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.use-temporal-tiling\",\n        action=StoreBoolean,\n        dest=f\"{prefix.replace('-', '_')}.use_temporal_tiling\",\n        default=VAEConfig.use_temporal_tiling,\n        help=\"Whether to use temporal tiling for VAE\",\n    )\n    parser.add_argument(\n        f\"--{prefix}.use-parallel-tiling\",\n        action=StoreBoolean,\n        dest=f\"{prefix.replace('-', '_')}.use_parallel_tiling\",\n        default=VAEConfig.use_parallel_tiling,\n        help=\"Whether to use parallel tiling for VAE\",\n    )\n\n    return parser\n</code></pre>"},{"location":"api/fastvideo/configs/#fastvideo.configs.pipelines","title":"fastvideo.configs.pipelines","text":""},{"location":"api/fastvideo/configs/#fastvideo.configs.pipelines-classes","title":"Classes","text":""},{"location":"api/fastvideo/configs/#fastvideo.configs.pipelines.FastHunyuanConfig","title":"fastvideo.configs.pipelines.FastHunyuanConfig  <code>dataclass</code>","text":"<pre><code>FastHunyuanConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: int = 6,\n    flow_shift: int = 17,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = HunyuanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = HunyuanVAEConfig(),\n    vae_precision: str = \"fp16\",\n    vae_tiling: bool = True,\n    vae_sp: bool = True,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (LlamaConfig(), CLIPTextConfig())\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp16\", \"fp16\")\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (\n        lambda: (\n            llama_preprocess_text,\n            clip_preprocess_text,\n        )\n    )(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], tensor], ...\n    ] = (\n        lambda: (\n            llama_postprocess_text,\n            clip_postprocess_text,\n        )\n    )(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n)\n</code></pre> <p>               Bases: <code>HunyuanConfig</code></p> <p>Configuration specifically optimized for FastHunyuan weights.</p>"},{"location":"api/fastvideo/configs/#fastvideo.configs.pipelines.HunyuanConfig","title":"fastvideo.configs.pipelines.HunyuanConfig  <code>dataclass</code>","text":"<pre><code>HunyuanConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: int = 6,\n    flow_shift: int = 7,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = HunyuanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = HunyuanVAEConfig(),\n    vae_precision: str = \"fp16\",\n    vae_tiling: bool = True,\n    vae_sp: bool = True,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (LlamaConfig(), CLIPTextConfig())\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp16\", \"fp16\")\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (\n        lambda: (\n            llama_preprocess_text,\n            clip_preprocess_text,\n        )\n    )(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], tensor], ...\n    ] = (\n        lambda: (\n            llama_postprocess_text,\n            clip_postprocess_text,\n        )\n    )(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n)\n</code></pre> <p>               Bases: <code>PipelineConfig</code></p> <p>Base configuration for HunYuan pipeline architecture.</p>"},{"location":"api/fastvideo/configs/#fastvideo.configs.pipelines.PipelineConfig","title":"fastvideo.configs.pipelines.PipelineConfig  <code>dataclass</code>","text":"<pre><code>PipelineConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = None,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = DiTConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = VAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = True,\n    vae_sp: bool = True,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (EncoderConfig(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], tensor], ...\n    ] = (lambda: (postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n)\n</code></pre> <p>Base configuration for all pipeline architectures.</p> Functions\u00b6 fastvideo.configs.pipelines.PipelineConfig.from_kwargs <code>classmethod</code> \u00b6 <pre><code>from_kwargs(\n    kwargs: dict[str, Any], config_cli_prefix: str = \"\"\n) -&gt; PipelineConfig\n</code></pre> <p>Load PipelineConfig from kwargs Dictionary. kwargs: dictionary of kwargs config_cli_prefix: prefix of CLI arguments for this PipelineConfig instance</p> Source code in <code>fastvideo/configs/pipelines/base.py</code> <pre><code>@classmethod\ndef from_kwargs(cls,\n                kwargs: dict[str, Any],\n                config_cli_prefix: str = \"\") -&gt; \"PipelineConfig\":\n    \"\"\"\n    Load PipelineConfig from kwargs Dictionary.\n    kwargs: dictionary of kwargs\n    config_cli_prefix: prefix of CLI arguments for this PipelineConfig instance\n    \"\"\"\n    from fastvideo.configs.pipelines.registry import (\n        get_pipeline_config_cls_from_name)\n\n    prefix_with_dot = f\"{config_cli_prefix}.\" if (config_cli_prefix.strip()\n                                                  != \"\") else \"\"\n    model_path: str | None = kwargs.get(prefix_with_dot + 'model_path',\n                                        None) or kwargs.get('model_path')\n    pipeline_config_or_path: str | PipelineConfig | dict[\n        str, Any] | None = kwargs.get(prefix_with_dot + 'pipeline_config',\n                                      None) or kwargs.get('pipeline_config')\n    if model_path is None:\n        raise ValueError(\"model_path is required in kwargs\")\n\n    # 1. Get the pipeline config class from the registry\n    pipeline_config_cls = get_pipeline_config_cls_from_name(model_path)\n\n    # 2. Instantiate PipelineConfig\n    if pipeline_config_cls is None:\n        logger.warning(\n            \"Couldn't find pipeline config for %s. Using the default pipeline config.\",\n            model_path)\n        pipeline_config = cls()\n    else:\n        pipeline_config = pipeline_config_cls()\n\n    # 3. Load PipelineConfig from a json file or a PipelineConfig object if provided\n    if isinstance(pipeline_config_or_path, str):\n        pipeline_config.load_from_json(pipeline_config_or_path)\n        kwargs[prefix_with_dot +\n               'pipeline_config_path'] = pipeline_config_or_path\n    elif isinstance(pipeline_config_or_path, PipelineConfig):\n        pipeline_config = pipeline_config_or_path\n    elif isinstance(pipeline_config_or_path, dict):\n        pipeline_config.update_pipeline_config(pipeline_config_or_path)\n\n    # 4. Update PipelineConfig from CLI arguments if provided\n    kwargs[prefix_with_dot + 'model_path'] = model_path\n    pipeline_config.update_config_from_dict(kwargs, config_cli_prefix)\n    return pipeline_config\n</code></pre> fastvideo.configs.pipelines.PipelineConfig.from_pretrained <code>classmethod</code> \u00b6 <pre><code>from_pretrained(model_path: str) -&gt; PipelineConfig\n</code></pre> <p>use the pipeline class setting from model_path to match the pipeline config</p> Source code in <code>fastvideo/configs/pipelines/base.py</code> <pre><code>@classmethod\ndef from_pretrained(cls, model_path: str) -&gt; \"PipelineConfig\":\n    \"\"\"\n    use the pipeline class setting from model_path to match the pipeline config\n    \"\"\"\n    from fastvideo.configs.pipelines.registry import (\n        get_pipeline_config_cls_from_name)\n    pipeline_config_cls = get_pipeline_config_cls_from_name(model_path)\n\n    return cast(PipelineConfig, pipeline_config_cls(model_path=model_path))\n</code></pre>"},{"location":"api/fastvideo/configs/#fastvideo.configs.pipelines.SlidingTileAttnConfig","title":"fastvideo.configs.pipelines.SlidingTileAttnConfig  <code>dataclass</code>","text":"<pre><code>SlidingTileAttnConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = None,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = DiTConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = VAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = True,\n    vae_sp: bool = True,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (EncoderConfig(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], tensor], ...\n    ] = (lambda: (postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    window_size: int = 16,\n    stride: int = 8,\n    height: int = 576,\n    width: int = 1024,\n    pad_to_square: bool = False,\n    use_overlap_optimization: bool = True,\n)\n</code></pre> <p>               Bases: <code>PipelineConfig</code></p> <p>Configuration for sliding tile attention.</p>"},{"location":"api/fastvideo/configs/#fastvideo.configs.pipelines.StepVideoT2VConfig","title":"fastvideo.configs.pipelines.StepVideoT2VConfig  <code>dataclass</code>","text":"<pre><code>StepVideoT2VConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: int = 13,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = StepVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = StepVideoVAEConfig(),\n    vae_precision: str = \"bf16\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (EncoderConfig(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], tensor], ...\n    ] = (lambda: (postprocess_text,))(),\n    pos_magic: str = \"\u200b\u8d85\u9ad8\u200b\u6e05\u200b\u3001HDR \u200b\u89c6\u9891\u200b\u3001\u200b\u73af\u5883\u5149\u200b\u3001\u200b\u675c\u6bd4\u200b\u5168\u666f\u200b\u58f0\u200b\u3001\u200b\u753b\u9762\u200b\u7a33\u5b9a\u200b\u3001\u200b\u6d41\u7545\u200b\u52a8\u4f5c\u200b\u3001\u200b\u903c\u771f\u200b\u7684\u200b\u7ec6\u8282\u200b\u3001\u200b\u4e13\u4e1a\u7ea7\u200b\u6784\u56fe\u200b\u3001\u200b\u8d85\u73b0\u5b9e\u4e3b\u4e49\u200b\u3001\u200b\u81ea\u7136\u200b\u3001\u200b\u751f\u52a8\u200b\u3001\u200b\u8d85\u200b\u7ec6\u8282\u200b\u3001\u200b\u6e05\u6670\u200b\u3002\",\n    neg_magic: str = \"\u200b\u753b\u9762\u200b\u6697\u200b\u3001\u200b\u4f4e\u5206\u8fa8\u7387\u200b\u3001\u200b\u4e0d\u826f\u200b\u624b\u200b\u3001\u200b\u6587\u672c\u200b\u3001\u200b\u7f3a\u5c11\u200b\u624b\u6307\u200b\u3001\u200b\u591a\u4f59\u200b\u7684\u200b\u624b\u6307\u200b\u3001\u200b\u88c1\u526a\u200b\u3001\u200b\u4f4e\u8d28\u91cf\u200b\u3001\u200b\u9897\u7c92\u72b6\u200b\u3001\u200b\u7b7e\u540d\u200b\u3001\u200b\u6c34\u5370\u200b\u3001\u200b\u7528\u6237\u540d\u200b\u3001\u200b\u6a21\u7cca\u200b\u3002\",\n    timesteps_scale: bool = False,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n)\n</code></pre> <p>               Bases: <code>PipelineConfig</code></p> <p>Base configuration for StepVideo pipeline architecture.</p>"},{"location":"api/fastvideo/configs/#fastvideo.configs.pipelines.WanI2V480PConfig","title":"fastvideo.configs.pipelines.WanI2V480PConfig  <code>dataclass</code>","text":"<pre><code>WanI2V480PConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = 3.0,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = WanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = WanVAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = CLIPVisionConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (T5Config(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], Tensor], ...\n    ] = (lambda: (t5_postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n    warp_denoising_step: bool = True,\n)\n</code></pre> <p>               Bases: <code>WanT2V480PConfig</code></p> <p>Base configuration for Wan I2V 14B 480P pipeline architecture.</p>"},{"location":"api/fastvideo/configs/#fastvideo.configs.pipelines.WanI2V720PConfig","title":"fastvideo.configs.pipelines.WanI2V720PConfig  <code>dataclass</code>","text":"<pre><code>WanI2V720PConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = 5.0,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = WanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = WanVAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = CLIPVisionConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (T5Config(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], Tensor], ...\n    ] = (lambda: (t5_postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n    warp_denoising_step: bool = True,\n)\n</code></pre> <p>               Bases: <code>WanI2V480PConfig</code></p> <p>Base configuration for Wan I2V 14B 720P pipeline architecture.</p>"},{"location":"api/fastvideo/configs/#fastvideo.configs.pipelines.WanT2V480PConfig","title":"fastvideo.configs.pipelines.WanT2V480PConfig  <code>dataclass</code>","text":"<pre><code>WanT2V480PConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = 3.0,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = WanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = WanVAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (T5Config(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], Tensor], ...\n    ] = (lambda: (t5_postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n    warp_denoising_step: bool = True,\n)\n</code></pre> <p>               Bases: <code>PipelineConfig</code></p> <p>Base configuration for Wan T2V 1.3B pipeline architecture.</p>"},{"location":"api/fastvideo/configs/#fastvideo.configs.pipelines.WanT2V720PConfig","title":"fastvideo.configs.pipelines.WanT2V720PConfig  <code>dataclass</code>","text":"<pre><code>WanT2V720PConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = 5.0,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = WanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = WanVAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (T5Config(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], Tensor], ...\n    ] = (lambda: (t5_postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n    warp_denoising_step: bool = True,\n)\n</code></pre> <p>               Bases: <code>WanT2V480PConfig</code></p> <p>Base configuration for Wan T2V 14B 720P pipeline architecture.</p>"},{"location":"api/fastvideo/configs/#fastvideo.configs.pipelines-functions","title":"Functions","text":""},{"location":"api/fastvideo/configs/#fastvideo.configs.pipelines.get_pipeline_config_cls_from_name","title":"fastvideo.configs.pipelines.get_pipeline_config_cls_from_name","text":"<pre><code>get_pipeline_config_cls_from_name(\n    pipeline_name_or_path: str,\n) -&gt; type[PipelineConfig]\n</code></pre> <p>Get the appropriate configuration class for a given pipeline name or path.</p> <p>This function implements a multi-step lookup process to find the most suitable configuration class for a given pipeline. It follows this order: 1. Exact match in the PIPE_NAME_TO_CONFIG 2. Partial match in the PIPE_NAME_TO_CONFIG 3. Fallback to class name in the model_index.json 4. else raise an error</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_name_or_path</code> <code>str</code> <p>The name or path of the pipeline. This can be: - A registered model ID (e.g., \"FastVideo/FastHunyuan-diffusers\") - A local path to a model directory - A model ID that will be downloaded</p> required <p>Returns:</p> Type Description <code>type[PipelineConfig]</code> <p>Type[PipelineConfig]: The configuration class that best matches the pipeline. This will be one of: - A specific weight configuration class if an exact match is found - A fallback configuration class based on the pipeline architecture - The base PipelineConfig class if no matches are found</p> Note <ul> <li>For local paths, the function will verify the model configuration</li> <li>For remote models, it will attempt to download the model index</li> <li>Warning messages are logged when falling back to less specific configurations</li> </ul> Source code in <code>fastvideo/configs/pipelines/registry.py</code> <pre><code>def get_pipeline_config_cls_from_name(\n        pipeline_name_or_path: str) -&gt; type[PipelineConfig]:\n    \"\"\"Get the appropriate configuration class for a given pipeline name or path.\n\n    This function implements a multi-step lookup process to find the most suitable\n    configuration class for a given pipeline. It follows this order:\n    1. Exact match in the PIPE_NAME_TO_CONFIG\n    2. Partial match in the PIPE_NAME_TO_CONFIG\n    3. Fallback to class name in the model_index.json\n    4. else raise an error\n\n    Args:\n        pipeline_name_or_path (str): The name or path of the pipeline. This can be:\n            - A registered model ID (e.g., \"FastVideo/FastHunyuan-diffusers\")\n            - A local path to a model directory\n            - A model ID that will be downloaded\n\n    Returns:\n        Type[PipelineConfig]: The configuration class that best matches the pipeline.\n            This will be one of:\n            - A specific weight configuration class if an exact match is found\n            - A fallback configuration class based on the pipeline architecture\n            - The base PipelineConfig class if no matches are found\n\n    Note:\n        - For local paths, the function will verify the model configuration\n        - For remote models, it will attempt to download the model index\n        - Warning messages are logged when falling back to less specific configurations\n    \"\"\"\n\n    pipeline_config_cls: type[PipelineConfig] | None = None\n\n    # First try exact match for specific weights\n    if pipeline_name_or_path in PIPE_NAME_TO_CONFIG:\n        pipeline_config_cls = PIPE_NAME_TO_CONFIG[pipeline_name_or_path]\n\n    # Try partial matches (for local paths that might include the weight ID)\n    for registered_id, config_class in PIPE_NAME_TO_CONFIG.items():\n        if registered_id in pipeline_name_or_path:\n            pipeline_config_cls = config_class\n            break\n\n    # If no match, try to use the fallback config\n    if pipeline_config_cls is None:\n        if os.path.exists(pipeline_name_or_path):\n            config = verify_model_config_and_directory(pipeline_name_or_path)\n        else:\n            config = maybe_download_model_index(pipeline_name_or_path)\n        logger.warning(\n            \"Trying to use the config from the model_index.json. FastVideo may not correctly identify the optimal config for this model in this situation.\"\n        )\n\n        pipeline_name = config[\"_class_name\"]\n        # Try to determine pipeline architecture for fallback\n        for pipeline_type, detector in PIPELINE_DETECTOR.items():\n            if detector(pipeline_name.lower()):\n                pipeline_config_cls = PIPELINE_FALLBACK_CONFIG.get(\n                    pipeline_type)\n                break\n\n        if pipeline_config_cls is not None:\n            logger.warning(\n                \"No match found for pipeline %s, using fallback config %s.\",\n                pipeline_name_or_path, pipeline_config_cls)\n\n    if pipeline_config_cls is None:\n        raise ValueError(\n            f\"No match found for pipeline {pipeline_name_or_path}, please check the pipeline name or path.\"\n        )\n\n    return pipeline_config_cls\n</code></pre>"},{"location":"api/fastvideo/configs/#fastvideo.configs.pipelines-modules","title":"Modules","text":""},{"location":"api/fastvideo/configs/#fastvideo.configs.pipelines.base","title":"fastvideo.configs.pipelines.base","text":"Classes\u00b6 fastvideo.configs.pipelines.base.PipelineConfig <code>dataclass</code> \u00b6 <pre><code>PipelineConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = None,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = DiTConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = VAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = True,\n    vae_sp: bool = True,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (EncoderConfig(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], tensor], ...\n    ] = (lambda: (postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n)\n</code></pre> <p>Base configuration for all pipeline architectures.</p> Functions\u00b6 fastvideo.configs.pipelines.base.PipelineConfig.from_kwargs <code>classmethod</code> \u00b6 <pre><code>from_kwargs(\n    kwargs: dict[str, Any], config_cli_prefix: str = \"\"\n) -&gt; PipelineConfig\n</code></pre> <p>Load PipelineConfig from kwargs Dictionary. kwargs: dictionary of kwargs config_cli_prefix: prefix of CLI arguments for this PipelineConfig instance</p> Source code in <code>fastvideo/configs/pipelines/base.py</code> <pre><code>@classmethod\ndef from_kwargs(cls,\n                kwargs: dict[str, Any],\n                config_cli_prefix: str = \"\") -&gt; \"PipelineConfig\":\n    \"\"\"\n    Load PipelineConfig from kwargs Dictionary.\n    kwargs: dictionary of kwargs\n    config_cli_prefix: prefix of CLI arguments for this PipelineConfig instance\n    \"\"\"\n    from fastvideo.configs.pipelines.registry import (\n        get_pipeline_config_cls_from_name)\n\n    prefix_with_dot = f\"{config_cli_prefix}.\" if (config_cli_prefix.strip()\n                                                  != \"\") else \"\"\n    model_path: str | None = kwargs.get(prefix_with_dot + 'model_path',\n                                        None) or kwargs.get('model_path')\n    pipeline_config_or_path: str | PipelineConfig | dict[\n        str, Any] | None = kwargs.get(prefix_with_dot + 'pipeline_config',\n                                      None) or kwargs.get('pipeline_config')\n    if model_path is None:\n        raise ValueError(\"model_path is required in kwargs\")\n\n    # 1. Get the pipeline config class from the registry\n    pipeline_config_cls = get_pipeline_config_cls_from_name(model_path)\n\n    # 2. Instantiate PipelineConfig\n    if pipeline_config_cls is None:\n        logger.warning(\n            \"Couldn't find pipeline config for %s. Using the default pipeline config.\",\n            model_path)\n        pipeline_config = cls()\n    else:\n        pipeline_config = pipeline_config_cls()\n\n    # 3. Load PipelineConfig from a json file or a PipelineConfig object if provided\n    if isinstance(pipeline_config_or_path, str):\n        pipeline_config.load_from_json(pipeline_config_or_path)\n        kwargs[prefix_with_dot +\n               'pipeline_config_path'] = pipeline_config_or_path\n    elif isinstance(pipeline_config_or_path, PipelineConfig):\n        pipeline_config = pipeline_config_or_path\n    elif isinstance(pipeline_config_or_path, dict):\n        pipeline_config.update_pipeline_config(pipeline_config_or_path)\n\n    # 4. Update PipelineConfig from CLI arguments if provided\n    kwargs[prefix_with_dot + 'model_path'] = model_path\n    pipeline_config.update_config_from_dict(kwargs, config_cli_prefix)\n    return pipeline_config\n</code></pre> fastvideo.configs.pipelines.base.PipelineConfig.from_pretrained <code>classmethod</code> \u00b6 <pre><code>from_pretrained(model_path: str) -&gt; PipelineConfig\n</code></pre> <p>use the pipeline class setting from model_path to match the pipeline config</p> Source code in <code>fastvideo/configs/pipelines/base.py</code> <pre><code>@classmethod\ndef from_pretrained(cls, model_path: str) -&gt; \"PipelineConfig\":\n    \"\"\"\n    use the pipeline class setting from model_path to match the pipeline config\n    \"\"\"\n    from fastvideo.configs.pipelines.registry import (\n        get_pipeline_config_cls_from_name)\n    pipeline_config_cls = get_pipeline_config_cls_from_name(model_path)\n\n    return cast(PipelineConfig, pipeline_config_cls(model_path=model_path))\n</code></pre> fastvideo.configs.pipelines.base.STA_Mode \u00b6 <p>               Bases: <code>str</code>, <code>Enum</code></p> <p>STA (Sliding Tile Attention) modes.</p> fastvideo.configs.pipelines.base.SlidingTileAttnConfig <code>dataclass</code> \u00b6 <pre><code>SlidingTileAttnConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = None,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = DiTConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = VAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = True,\n    vae_sp: bool = True,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (EncoderConfig(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], tensor], ...\n    ] = (lambda: (postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    window_size: int = 16,\n    stride: int = 8,\n    height: int = 576,\n    width: int = 1024,\n    pad_to_square: bool = False,\n    use_overlap_optimization: bool = True,\n)\n</code></pre> <p>               Bases: <code>PipelineConfig</code></p> <p>Configuration for sliding tile attention.</p> Functions\u00b6 fastvideo.configs.pipelines.base.parse_int_list \u00b6 <pre><code>parse_int_list(value: str) -&gt; list[int]\n</code></pre> <p>Parse a comma-separated string of integers into a list.</p> Source code in <code>fastvideo/configs/pipelines/base.py</code> <pre><code>def parse_int_list(value: str) -&gt; list[int]:\n    \"\"\"Parse a comma-separated string of integers into a list.\"\"\"\n    if not value:\n        return []\n    return [int(x.strip()) for x in value.split(\",\")]\n</code></pre>"},{"location":"api/fastvideo/configs/#fastvideo.configs.pipelines.hunyuan","title":"fastvideo.configs.pipelines.hunyuan","text":"Classes\u00b6 fastvideo.configs.pipelines.hunyuan.FastHunyuanConfig <code>dataclass</code> \u00b6 <pre><code>FastHunyuanConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: int = 6,\n    flow_shift: int = 17,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = HunyuanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = HunyuanVAEConfig(),\n    vae_precision: str = \"fp16\",\n    vae_tiling: bool = True,\n    vae_sp: bool = True,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (LlamaConfig(), CLIPTextConfig())\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp16\", \"fp16\")\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (\n        lambda: (\n            llama_preprocess_text,\n            clip_preprocess_text,\n        )\n    )(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], tensor], ...\n    ] = (\n        lambda: (\n            llama_postprocess_text,\n            clip_postprocess_text,\n        )\n    )(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n)\n</code></pre> <p>               Bases: <code>HunyuanConfig</code></p> <p>Configuration specifically optimized for FastHunyuan weights.</p> fastvideo.configs.pipelines.hunyuan.HunyuanConfig <code>dataclass</code> \u00b6 <pre><code>HunyuanConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: int = 6,\n    flow_shift: int = 7,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = HunyuanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = HunyuanVAEConfig(),\n    vae_precision: str = \"fp16\",\n    vae_tiling: bool = True,\n    vae_sp: bool = True,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (LlamaConfig(), CLIPTextConfig())\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp16\", \"fp16\")\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (\n        lambda: (\n            llama_preprocess_text,\n            clip_preprocess_text,\n        )\n    )(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], tensor], ...\n    ] = (\n        lambda: (\n            llama_postprocess_text,\n            clip_postprocess_text,\n        )\n    )(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n)\n</code></pre> <p>               Bases: <code>PipelineConfig</code></p> <p>Base configuration for HunYuan pipeline architecture.</p>"},{"location":"api/fastvideo/configs/#fastvideo.configs.pipelines.registry","title":"fastvideo.configs.pipelines.registry","text":"<p>Registry for pipeline weight-specific configurations.</p> Classes\u00b6 Functions\u00b6 fastvideo.configs.pipelines.registry.get_pipeline_config_cls_from_name \u00b6 <pre><code>get_pipeline_config_cls_from_name(\n    pipeline_name_or_path: str,\n) -&gt; type[PipelineConfig]\n</code></pre> <p>Get the appropriate configuration class for a given pipeline name or path.</p> <p>This function implements a multi-step lookup process to find the most suitable configuration class for a given pipeline. It follows this order: 1. Exact match in the PIPE_NAME_TO_CONFIG 2. Partial match in the PIPE_NAME_TO_CONFIG 3. Fallback to class name in the model_index.json 4. else raise an error</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_name_or_path</code> <code>str</code> <p>The name or path of the pipeline. This can be: - A registered model ID (e.g., \"FastVideo/FastHunyuan-diffusers\") - A local path to a model directory - A model ID that will be downloaded</p> required <p>Returns:</p> Type Description <code>type[PipelineConfig]</code> <p>Type[PipelineConfig]: The configuration class that best matches the pipeline. This will be one of: - A specific weight configuration class if an exact match is found - A fallback configuration class based on the pipeline architecture - The base PipelineConfig class if no matches are found</p> Note <ul> <li>For local paths, the function will verify the model configuration</li> <li>For remote models, it will attempt to download the model index</li> <li>Warning messages are logged when falling back to less specific configurations</li> </ul> Source code in <code>fastvideo/configs/pipelines/registry.py</code> <pre><code>def get_pipeline_config_cls_from_name(\n        pipeline_name_or_path: str) -&gt; type[PipelineConfig]:\n    \"\"\"Get the appropriate configuration class for a given pipeline name or path.\n\n    This function implements a multi-step lookup process to find the most suitable\n    configuration class for a given pipeline. It follows this order:\n    1. Exact match in the PIPE_NAME_TO_CONFIG\n    2. Partial match in the PIPE_NAME_TO_CONFIG\n    3. Fallback to class name in the model_index.json\n    4. else raise an error\n\n    Args:\n        pipeline_name_or_path (str): The name or path of the pipeline. This can be:\n            - A registered model ID (e.g., \"FastVideo/FastHunyuan-diffusers\")\n            - A local path to a model directory\n            - A model ID that will be downloaded\n\n    Returns:\n        Type[PipelineConfig]: The configuration class that best matches the pipeline.\n            This will be one of:\n            - A specific weight configuration class if an exact match is found\n            - A fallback configuration class based on the pipeline architecture\n            - The base PipelineConfig class if no matches are found\n\n    Note:\n        - For local paths, the function will verify the model configuration\n        - For remote models, it will attempt to download the model index\n        - Warning messages are logged when falling back to less specific configurations\n    \"\"\"\n\n    pipeline_config_cls: type[PipelineConfig] | None = None\n\n    # First try exact match for specific weights\n    if pipeline_name_or_path in PIPE_NAME_TO_CONFIG:\n        pipeline_config_cls = PIPE_NAME_TO_CONFIG[pipeline_name_or_path]\n\n    # Try partial matches (for local paths that might include the weight ID)\n    for registered_id, config_class in PIPE_NAME_TO_CONFIG.items():\n        if registered_id in pipeline_name_or_path:\n            pipeline_config_cls = config_class\n            break\n\n    # If no match, try to use the fallback config\n    if pipeline_config_cls is None:\n        if os.path.exists(pipeline_name_or_path):\n            config = verify_model_config_and_directory(pipeline_name_or_path)\n        else:\n            config = maybe_download_model_index(pipeline_name_or_path)\n        logger.warning(\n            \"Trying to use the config from the model_index.json. FastVideo may not correctly identify the optimal config for this model in this situation.\"\n        )\n\n        pipeline_name = config[\"_class_name\"]\n        # Try to determine pipeline architecture for fallback\n        for pipeline_type, detector in PIPELINE_DETECTOR.items():\n            if detector(pipeline_name.lower()):\n                pipeline_config_cls = PIPELINE_FALLBACK_CONFIG.get(\n                    pipeline_type)\n                break\n\n        if pipeline_config_cls is not None:\n            logger.warning(\n                \"No match found for pipeline %s, using fallback config %s.\",\n                pipeline_name_or_path, pipeline_config_cls)\n\n    if pipeline_config_cls is None:\n        raise ValueError(\n            f\"No match found for pipeline {pipeline_name_or_path}, please check the pipeline name or path.\"\n        )\n\n    return pipeline_config_cls\n</code></pre>"},{"location":"api/fastvideo/configs/#fastvideo.configs.pipelines.stepvideo","title":"fastvideo.configs.pipelines.stepvideo","text":"Classes\u00b6 fastvideo.configs.pipelines.stepvideo.StepVideoT2VConfig <code>dataclass</code> \u00b6 <pre><code>StepVideoT2VConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: int = 13,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = StepVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = StepVideoVAEConfig(),\n    vae_precision: str = \"bf16\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (EncoderConfig(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], tensor], ...\n    ] = (lambda: (postprocess_text,))(),\n    pos_magic: str = \"\u200b\u8d85\u9ad8\u200b\u6e05\u200b\u3001HDR \u200b\u89c6\u9891\u200b\u3001\u200b\u73af\u5883\u5149\u200b\u3001\u200b\u675c\u6bd4\u200b\u5168\u666f\u200b\u58f0\u200b\u3001\u200b\u753b\u9762\u200b\u7a33\u5b9a\u200b\u3001\u200b\u6d41\u7545\u200b\u52a8\u4f5c\u200b\u3001\u200b\u903c\u771f\u200b\u7684\u200b\u7ec6\u8282\u200b\u3001\u200b\u4e13\u4e1a\u7ea7\u200b\u6784\u56fe\u200b\u3001\u200b\u8d85\u73b0\u5b9e\u4e3b\u4e49\u200b\u3001\u200b\u81ea\u7136\u200b\u3001\u200b\u751f\u52a8\u200b\u3001\u200b\u8d85\u200b\u7ec6\u8282\u200b\u3001\u200b\u6e05\u6670\u200b\u3002\",\n    neg_magic: str = \"\u200b\u753b\u9762\u200b\u6697\u200b\u3001\u200b\u4f4e\u5206\u8fa8\u7387\u200b\u3001\u200b\u4e0d\u826f\u200b\u624b\u200b\u3001\u200b\u6587\u672c\u200b\u3001\u200b\u7f3a\u5c11\u200b\u624b\u6307\u200b\u3001\u200b\u591a\u4f59\u200b\u7684\u200b\u624b\u6307\u200b\u3001\u200b\u88c1\u526a\u200b\u3001\u200b\u4f4e\u8d28\u91cf\u200b\u3001\u200b\u9897\u7c92\u72b6\u200b\u3001\u200b\u7b7e\u540d\u200b\u3001\u200b\u6c34\u5370\u200b\u3001\u200b\u7528\u6237\u540d\u200b\u3001\u200b\u6a21\u7cca\u200b\u3002\",\n    timesteps_scale: bool = False,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n)\n</code></pre> <p>               Bases: <code>PipelineConfig</code></p> <p>Base configuration for StepVideo pipeline architecture.</p>"},{"location":"api/fastvideo/configs/#fastvideo.configs.pipelines.wan","title":"fastvideo.configs.pipelines.wan","text":"Classes\u00b6 fastvideo.configs.pipelines.wan.FastWan2_1_T2V_480P_Config <code>dataclass</code> \u00b6 <pre><code>FastWan2_1_T2V_480P_Config(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = 8.0,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = WanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = WanVAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (T5Config(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], Tensor], ...\n    ] = (lambda: (t5_postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int]\n    | None = (lambda: [1000, 757, 522])(),\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n    warp_denoising_step: bool = True,\n)\n</code></pre> <p>               Bases: <code>WanT2V480PConfig</code></p> <p>Base configuration for FastWan T2V 1.3B 480P pipeline architecture with DMD</p> fastvideo.configs.pipelines.wan.WANV2VConfig <code>dataclass</code> \u00b6 <pre><code>WANV2VConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = 3.0,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = WanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = WanVAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = WAN2_1ControlCLIPVisionConfig(),\n    image_encoder_precision: str = \"bf16\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (T5Config(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], Tensor], ...\n    ] = (lambda: (t5_postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n    warp_denoising_step: bool = True,\n)\n</code></pre> <p>               Bases: <code>WanI2V480PConfig</code></p> <p>Configuration for WAN2.1 1.3B Control pipeline.</p> fastvideo.configs.pipelines.wan.WanI2V480PConfig <code>dataclass</code> \u00b6 <pre><code>WanI2V480PConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = 3.0,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = WanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = WanVAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = CLIPVisionConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (T5Config(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], Tensor], ...\n    ] = (lambda: (t5_postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n    warp_denoising_step: bool = True,\n)\n</code></pre> <p>               Bases: <code>WanT2V480PConfig</code></p> <p>Base configuration for Wan I2V 14B 480P pipeline architecture.</p> fastvideo.configs.pipelines.wan.WanI2V720PConfig <code>dataclass</code> \u00b6 <pre><code>WanI2V720PConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = 5.0,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = WanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = WanVAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = CLIPVisionConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (T5Config(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], Tensor], ...\n    ] = (lambda: (t5_postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n    warp_denoising_step: bool = True,\n)\n</code></pre> <p>               Bases: <code>WanI2V480PConfig</code></p> <p>Base configuration for Wan I2V 14B 720P pipeline architecture.</p> fastvideo.configs.pipelines.wan.WanT2V480PConfig <code>dataclass</code> \u00b6 <pre><code>WanT2V480PConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = 3.0,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = WanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = WanVAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (T5Config(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], Tensor], ...\n    ] = (lambda: (t5_postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n    warp_denoising_step: bool = True,\n)\n</code></pre> <p>               Bases: <code>PipelineConfig</code></p> <p>Base configuration for Wan T2V 1.3B pipeline architecture.</p> fastvideo.configs.pipelines.wan.WanT2V720PConfig <code>dataclass</code> \u00b6 <pre><code>WanT2V720PConfig(\n    model_path: str = \"\",\n    pipeline_config_path: str | None = None,\n    embedded_cfg_scale: float = 6.0,\n    flow_shift: float | None = 5.0,\n    disable_autocast: bool = False,\n    dit_config: DiTConfig = WanVideoConfig(),\n    dit_precision: str = \"bf16\",\n    vae_config: VAEConfig = WanVAEConfig(),\n    vae_precision: str = \"fp32\",\n    vae_tiling: bool = False,\n    vae_sp: bool = False,\n    image_encoder_config: EncoderConfig = EncoderConfig(),\n    image_encoder_precision: str = \"fp32\",\n    text_encoder_configs: tuple[EncoderConfig, ...] = (\n        lambda: (T5Config(),)\n    )(),\n    text_encoder_precisions: tuple[str, ...] = (\n        lambda: (\"fp32\",)\n    )(),\n    preprocess_text_funcs: tuple[\n        Callable[[str], str], ...\n    ] = (lambda: (preprocess_text,))(),\n    postprocess_text_funcs: tuple[\n        Callable[[BaseEncoderOutput], Tensor], ...\n    ] = (lambda: (t5_postprocess_text,))(),\n    pos_magic: str | None = None,\n    neg_magic: str | None = None,\n    timesteps_scale: bool | None = None,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    dmd_denoising_steps: list[int] | None = None,\n    ti2v_task: bool = False,\n    boundary_ratio: float | None = None,\n    precision: str = \"bf16\",\n    warp_denoising_step: bool = True,\n)\n</code></pre> <p>               Bases: <code>WanT2V480PConfig</code></p> <p>Base configuration for Wan T2V 14B 720P pipeline architecture.</p>"},{"location":"api/fastvideo/configs/#fastvideo.configs.sample","title":"fastvideo.configs.sample","text":""},{"location":"api/fastvideo/configs/#fastvideo.configs.sample-classes","title":"Classes","text":""},{"location":"api/fastvideo/configs/#fastvideo.configs.sample.SamplingParam","title":"fastvideo.configs.sample.SamplingParam  <code>dataclass</code>","text":"<pre><code>SamplingParam(\n    data_type: str = \"video\",\n    image_path: str | None = None,\n    video_path: str | None = None,\n    prompt: str | list[str] | None = None,\n    negative_prompt: str = \"Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\",\n    prompt_path: str | None = None,\n    output_path: str = \"outputs/\",\n    output_video_name: str | None = None,\n    num_videos_per_prompt: int = 1,\n    seed: int = 1024,\n    num_frames: int = 125,\n    num_frames_round_down: bool = False,\n    height: int = 720,\n    width: int = 1280,\n    fps: int = 24,\n    num_inference_steps: int = 50,\n    guidance_scale: float = 1.0,\n    guidance_rescale: float = 0.0,\n    boundary_ratio: float | None = None,\n    enable_teacache: bool = False,\n    save_video: bool = True,\n    return_frames: bool = False,\n    return_trajectory_latents: bool = False,\n    return_trajectory_decoded: bool = False,\n)\n</code></pre> <p>Sampling parameters for video generation.</p> Functions\u00b6 fastvideo.configs.sample.SamplingParam.add_cli_args <code>staticmethod</code> \u00b6 <pre><code>add_cli_args(parser: Any) -&gt; Any\n</code></pre> <p>Add CLI arguments for SamplingParam fields</p> Source code in <code>fastvideo/configs/sample/base.py</code> <pre><code>@staticmethod\ndef add_cli_args(parser: Any) -&gt; Any:\n    \"\"\"Add CLI arguments for SamplingParam fields\"\"\"\n    parser.add_argument(\n        \"--prompt\",\n        type=str,\n        default=SamplingParam.prompt,\n        help=\"Text prompt for video generation\",\n    )\n    parser.add_argument(\n        \"--negative-prompt\",\n        type=str,\n        default=SamplingParam.negative_prompt,\n        help=\"Negative text prompt for video generation\",\n    )\n    parser.add_argument(\n        \"--prompt-path\",\n        type=str,\n        default=SamplingParam.prompt_path,\n        help=\"Path to a text file containing the prompt\",\n    )\n    parser.add_argument(\n        \"--output-path\",\n        type=str,\n        default=SamplingParam.output_path,\n        help=\"Path to save the generated video\",\n    )\n    parser.add_argument(\n        \"--output-video-name\",\n        type=str,\n        default=SamplingParam.output_video_name,\n        help=\"Name of the output video\",\n    )\n    parser.add_argument(\n        \"--num-videos-per-prompt\",\n        type=int,\n        default=SamplingParam.num_videos_per_prompt,\n        help=\"Number of videos to generate per prompt\",\n    )\n    parser.add_argument(\n        \"--seed\",\n        type=int,\n        default=SamplingParam.seed,\n        help=\"Random seed for generation\",\n    )\n    parser.add_argument(\n        \"--num-frames\",\n        type=int,\n        default=SamplingParam.num_frames,\n        help=\"Number of frames to generate\",\n    )\n    parser.add_argument(\n        \"--height\",\n        type=int,\n        default=SamplingParam.height,\n        help=\"Height of generated video\",\n    )\n    parser.add_argument(\n        \"--width\",\n        type=int,\n        default=SamplingParam.width,\n        help=\"Width of generated video\",\n    )\n    parser.add_argument(\n        \"--fps\",\n        type=int,\n        default=SamplingParam.fps,\n        help=\"Frames per second for saved video\",\n    )\n    parser.add_argument(\n        \"--num-inference-steps\",\n        type=int,\n        default=SamplingParam.num_inference_steps,\n        help=\"Number of denoising steps\",\n    )\n    parser.add_argument(\n        \"--guidance-scale\",\n        type=float,\n        default=SamplingParam.guidance_scale,\n        help=\"Classifier-free guidance scale\",\n    )\n    parser.add_argument(\n        \"--guidance-rescale\",\n        type=float,\n        default=SamplingParam.guidance_rescale,\n        help=\"Guidance rescale factor\",\n    )\n    parser.add_argument(\n        \"--boundary-ratio\",\n        type=float,\n        default=SamplingParam.boundary_ratio,\n        help=\"Boundary timestep ratio\",\n    )\n    parser.add_argument(\n        \"--save-video\",\n        action=\"store_true\",\n        default=SamplingParam.save_video,\n        help=\"Whether to save the video to disk\",\n    )\n    parser.add_argument(\n        \"--no-save-video\",\n        action=\"store_false\",\n        dest=\"save_video\",\n        help=\"Don't save the video to disk\",\n    )\n    parser.add_argument(\n        \"--return-frames\",\n        action=\"store_true\",\n        default=SamplingParam.return_frames,\n        help=\"Whether to return the raw frames\",\n    )\n    parser.add_argument(\n        \"--image-path\",\n        type=str,\n        default=SamplingParam.image_path,\n        help=\"Path to input image for image-to-video generation\",\n    )\n    parser.add_argument(\n        \"--video_path\",\n        type=str,\n        default=SamplingParam.video_path,\n        help=\"Path to input video for video-to-video generation\",\n    )\n    parser.add_argument(\n        \"--moba-config-path\",\n        type=str,\n        default=None,\n        help=\n        \"Path to a JSON file containing V-MoBA specific configurations.\",\n    )\n    parser.add_argument(\n        \"--return-trajectory-latents\",\n        action=\"store_true\",\n        default=SamplingParam.return_trajectory_latents,\n        help=\"Whether to return the trajectory\",\n    )\n    parser.add_argument(\n        \"--return-trajectory-decoded\",\n        action=\"store_true\",\n        default=SamplingParam.return_trajectory_decoded,\n        help=\"Whether to return the decoded trajectory\",\n    )\n    return parser\n</code></pre>"},{"location":"api/fastvideo/configs/#fastvideo.configs.sample-modules","title":"Modules","text":""},{"location":"api/fastvideo/configs/#fastvideo.configs.sample.base","title":"fastvideo.configs.sample.base","text":"Classes\u00b6 fastvideo.configs.sample.base.SamplingParam <code>dataclass</code> \u00b6 <pre><code>SamplingParam(\n    data_type: str = \"video\",\n    image_path: str | None = None,\n    video_path: str | None = None,\n    prompt: str | list[str] | None = None,\n    negative_prompt: str = \"Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\",\n    prompt_path: str | None = None,\n    output_path: str = \"outputs/\",\n    output_video_name: str | None = None,\n    num_videos_per_prompt: int = 1,\n    seed: int = 1024,\n    num_frames: int = 125,\n    num_frames_round_down: bool = False,\n    height: int = 720,\n    width: int = 1280,\n    fps: int = 24,\n    num_inference_steps: int = 50,\n    guidance_scale: float = 1.0,\n    guidance_rescale: float = 0.0,\n    boundary_ratio: float | None = None,\n    enable_teacache: bool = False,\n    save_video: bool = True,\n    return_frames: bool = False,\n    return_trajectory_latents: bool = False,\n    return_trajectory_decoded: bool = False,\n)\n</code></pre> <p>Sampling parameters for video generation.</p> Functions\u00b6 fastvideo.configs.sample.base.SamplingParam.add_cli_args <code>staticmethod</code> \u00b6 <pre><code>add_cli_args(parser: Any) -&gt; Any\n</code></pre> <p>Add CLI arguments for SamplingParam fields</p> Source code in <code>fastvideo/configs/sample/base.py</code> <pre><code>@staticmethod\ndef add_cli_args(parser: Any) -&gt; Any:\n    \"\"\"Add CLI arguments for SamplingParam fields\"\"\"\n    parser.add_argument(\n        \"--prompt\",\n        type=str,\n        default=SamplingParam.prompt,\n        help=\"Text prompt for video generation\",\n    )\n    parser.add_argument(\n        \"--negative-prompt\",\n        type=str,\n        default=SamplingParam.negative_prompt,\n        help=\"Negative text prompt for video generation\",\n    )\n    parser.add_argument(\n        \"--prompt-path\",\n        type=str,\n        default=SamplingParam.prompt_path,\n        help=\"Path to a text file containing the prompt\",\n    )\n    parser.add_argument(\n        \"--output-path\",\n        type=str,\n        default=SamplingParam.output_path,\n        help=\"Path to save the generated video\",\n    )\n    parser.add_argument(\n        \"--output-video-name\",\n        type=str,\n        default=SamplingParam.output_video_name,\n        help=\"Name of the output video\",\n    )\n    parser.add_argument(\n        \"--num-videos-per-prompt\",\n        type=int,\n        default=SamplingParam.num_videos_per_prompt,\n        help=\"Number of videos to generate per prompt\",\n    )\n    parser.add_argument(\n        \"--seed\",\n        type=int,\n        default=SamplingParam.seed,\n        help=\"Random seed for generation\",\n    )\n    parser.add_argument(\n        \"--num-frames\",\n        type=int,\n        default=SamplingParam.num_frames,\n        help=\"Number of frames to generate\",\n    )\n    parser.add_argument(\n        \"--height\",\n        type=int,\n        default=SamplingParam.height,\n        help=\"Height of generated video\",\n    )\n    parser.add_argument(\n        \"--width\",\n        type=int,\n        default=SamplingParam.width,\n        help=\"Width of generated video\",\n    )\n    parser.add_argument(\n        \"--fps\",\n        type=int,\n        default=SamplingParam.fps,\n        help=\"Frames per second for saved video\",\n    )\n    parser.add_argument(\n        \"--num-inference-steps\",\n        type=int,\n        default=SamplingParam.num_inference_steps,\n        help=\"Number of denoising steps\",\n    )\n    parser.add_argument(\n        \"--guidance-scale\",\n        type=float,\n        default=SamplingParam.guidance_scale,\n        help=\"Classifier-free guidance scale\",\n    )\n    parser.add_argument(\n        \"--guidance-rescale\",\n        type=float,\n        default=SamplingParam.guidance_rescale,\n        help=\"Guidance rescale factor\",\n    )\n    parser.add_argument(\n        \"--boundary-ratio\",\n        type=float,\n        default=SamplingParam.boundary_ratio,\n        help=\"Boundary timestep ratio\",\n    )\n    parser.add_argument(\n        \"--save-video\",\n        action=\"store_true\",\n        default=SamplingParam.save_video,\n        help=\"Whether to save the video to disk\",\n    )\n    parser.add_argument(\n        \"--no-save-video\",\n        action=\"store_false\",\n        dest=\"save_video\",\n        help=\"Don't save the video to disk\",\n    )\n    parser.add_argument(\n        \"--return-frames\",\n        action=\"store_true\",\n        default=SamplingParam.return_frames,\n        help=\"Whether to return the raw frames\",\n    )\n    parser.add_argument(\n        \"--image-path\",\n        type=str,\n        default=SamplingParam.image_path,\n        help=\"Path to input image for image-to-video generation\",\n    )\n    parser.add_argument(\n        \"--video_path\",\n        type=str,\n        default=SamplingParam.video_path,\n        help=\"Path to input video for video-to-video generation\",\n    )\n    parser.add_argument(\n        \"--moba-config-path\",\n        type=str,\n        default=None,\n        help=\n        \"Path to a JSON file containing V-MoBA specific configurations.\",\n    )\n    parser.add_argument(\n        \"--return-trajectory-latents\",\n        action=\"store_true\",\n        default=SamplingParam.return_trajectory_latents,\n        help=\"Whether to return the trajectory\",\n    )\n    parser.add_argument(\n        \"--return-trajectory-decoded\",\n        action=\"store_true\",\n        default=SamplingParam.return_trajectory_decoded,\n        help=\"Whether to return the decoded trajectory\",\n    )\n    return parser\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/configs/#fastvideo.configs.sample.registry","title":"fastvideo.configs.sample.registry","text":"Classes\u00b6 Functions\u00b6 fastvideo.configs.sample.registry.get_sampling_param_cls_for_name \u00b6 <pre><code>get_sampling_param_cls_for_name(\n    pipeline_name_or_path: str,\n) -&gt; Any | None\n</code></pre> <p>Get the appropriate sampling param for specific pretrained weights.</p> Source code in <code>fastvideo/configs/sample/registry.py</code> <pre><code>def get_sampling_param_cls_for_name(pipeline_name_or_path: str) -&gt; Any | None:\n    \"\"\"Get the appropriate sampling param for specific pretrained weights.\"\"\"\n\n    if os.path.exists(pipeline_name_or_path):\n        config = verify_model_config_and_directory(pipeline_name_or_path)\n        logger.warning(\n            \"FastVideo may not correctly identify the optimal sampling param for this model, as the local directory may have been renamed.\"\n        )\n    else:\n        config = maybe_download_model_index(pipeline_name_or_path)\n\n    pipeline_name = config[\"_class_name\"]\n\n    # First try exact match for specific weights\n    if pipeline_name_or_path in SAMPLING_PARAM_REGISTRY:\n        return SAMPLING_PARAM_REGISTRY[pipeline_name_or_path]\n\n    # Try partial matches (for local paths that might include the weight ID)\n    for registered_id, config_class in SAMPLING_PARAM_REGISTRY.items():\n        if registered_id in pipeline_name_or_path:\n            return config_class\n\n    # If no match, try to use the fallback config\n    fallback_config = None\n    # Try to determine pipeline architecture for fallback\n    for pipeline_type, detector in SAMPLING_PARAM_DETECTOR.items():\n        if detector(pipeline_name.lower()):\n            fallback_config = SAMPLING_FALLBACK_PARAM.get(pipeline_type)\n            break\n\n    logger.warning(\n        \"No match found for pipeline %s, using fallback sampling param %s.\",\n        pipeline_name_or_path, fallback_config)\n    return fallback_config\n</code></pre>"},{"location":"api/fastvideo/configs/#fastvideo.configs.sample.wan","title":"fastvideo.configs.sample.wan","text":"Classes\u00b6 fastvideo.configs.sample.wan.Wan2_1_Fun_1_3B_InP_SamplingParam <code>dataclass</code> \u00b6 <pre><code>Wan2_1_Fun_1_3B_InP_SamplingParam(\n    data_type: str = \"video\",\n    image_path: str | None = None,\n    video_path: str | None = None,\n    prompt: str | list[str] | None = None,\n    negative_prompt: str\n    | None = \"\u200b\u8272\u8c03\u200b\u8273\u4e3d\u200b\uff0c\u200b\u8fc7\u200b\u66dd\u200b\uff0c\u200b\u9759\u6001\u200b\uff0c\u200b\u7ec6\u8282\u200b\u6a21\u7cca\u4e0d\u6e05\u200b\uff0c\u200b\u5b57\u5e55\u200b\uff0c\u200b\u98ce\u683c\u200b\uff0c\u200b\u4f5c\u54c1\u200b\uff0c\u200b\u753b\u4f5c\u200b\uff0c\u200b\u753b\u9762\u200b\uff0c\u200b\u9759\u6b62\u200b\uff0c\u200b\u6574\u4f53\u200b\u53d1\u7070\u200b\uff0c\u200b\u6700\u5dee\u200b\u8d28\u91cf\u200b\uff0c\u200b\u4f4e\u8d28\u91cf\u200b\uff0cJPEG\u200b\u538b\u7f29\u200b\u6b8b\u7559\u200b\uff0c\u200b\u4e11\u964b\u200b\u7684\u200b\uff0c\u200b\u6b8b\u7f3a\u200b\u7684\u200b\uff0c\u200b\u591a\u4f59\u200b\u7684\u200b\u624b\u6307\u200b\uff0c\u200b\u753b\u5f97\u200b\u4e0d\u597d\u200b\u7684\u200b\u624b\u90e8\u200b\uff0c\u200b\u753b\u5f97\u200b\u4e0d\u597d\u200b\u7684\u200b\u8138\u90e8\u200b\uff0c\u200b\u7578\u5f62\u200b\u7684\u200b\uff0c\u200b\u6bc1\u5bb9\u200b\u7684\u200b\uff0c\u200b\u5f62\u6001\u200b\u7578\u5f62\u200b\u7684\u200b\u80a2\u4f53\u200b\uff0c\u200b\u624b\u6307\u200b\u878d\u5408\u200b\uff0c\u200b\u9759\u6b62\u4e0d\u52a8\u200b\u7684\u200b\u753b\u9762\u200b\uff0c\u200b\u6742\u4e71\u200b\u7684\u200b\u80cc\u666f\u200b\uff0c\u200b\u4e09\u6761\u200b\u817f\u200b\uff0c\u200b\u80cc\u666f\u200b\u4eba\u200b\u5f88\u591a\u200b\uff0c\u200b\u5012\u200b\u7740\u200b\u8d70\u200b\",\n    prompt_path: str | None = None,\n    output_path: str = \"outputs/\",\n    output_video_name: str | None = None,\n    num_videos_per_prompt: int = 1,\n    seed: int = 1024,\n    num_frames: int = 81,\n    num_frames_round_down: bool = False,\n    height: int = 480,\n    width: int = 832,\n    fps: int = 16,\n    num_inference_steps: int = 50,\n    guidance_scale: float = 6.0,\n    guidance_rescale: float = 0.0,\n    boundary_ratio: float | None = None,\n    enable_teacache: bool = False,\n    save_video: bool = True,\n    return_frames: bool = False,\n    return_trajectory_latents: bool = False,\n    return_trajectory_decoded: bool = False,\n)\n</code></pre> <p>               Bases: <code>SamplingParam</code></p> <p>Sampling parameters for Wan2.1 Fun 1.3B InP model.</p> fastvideo.configs.sample.wan.Wan2_2_Base_SamplingParam <code>dataclass</code> \u00b6 <pre><code>Wan2_2_Base_SamplingParam(\n    data_type: str = \"video\",\n    image_path: str | None = None,\n    video_path: str | None = None,\n    prompt: str | list[str] | None = None,\n    negative_prompt: str\n    | None = \"\u200b\u8272\u8c03\u200b\u8273\u4e3d\u200b\uff0c\u200b\u8fc7\u200b\u66dd\u200b\uff0c\u200b\u9759\u6001\u200b\uff0c\u200b\u7ec6\u8282\u200b\u6a21\u7cca\u4e0d\u6e05\u200b\uff0c\u200b\u5b57\u5e55\u200b\uff0c\u200b\u98ce\u683c\u200b\uff0c\u200b\u4f5c\u54c1\u200b\uff0c\u200b\u753b\u4f5c\u200b\uff0c\u200b\u753b\u9762\u200b\uff0c\u200b\u9759\u6b62\u200b\uff0c\u200b\u6574\u4f53\u200b\u53d1\u7070\u200b\uff0c\u200b\u6700\u5dee\u200b\u8d28\u91cf\u200b\uff0c\u200b\u4f4e\u8d28\u91cf\u200b\uff0cJPEG\u200b\u538b\u7f29\u200b\u6b8b\u7559\u200b\uff0c\u200b\u4e11\u964b\u200b\u7684\u200b\uff0c\u200b\u6b8b\u7f3a\u200b\u7684\u200b\uff0c\u200b\u591a\u4f59\u200b\u7684\u200b\u624b\u6307\u200b\uff0c\u200b\u753b\u5f97\u200b\u4e0d\u597d\u200b\u7684\u200b\u624b\u90e8\u200b\uff0c\u200b\u753b\u5f97\u200b\u4e0d\u597d\u200b\u7684\u200b\u8138\u90e8\u200b\uff0c\u200b\u7578\u5f62\u200b\u7684\u200b\uff0c\u200b\u6bc1\u5bb9\u200b\u7684\u200b\uff0c\u200b\u5f62\u6001\u200b\u7578\u5f62\u200b\u7684\u200b\u80a2\u4f53\u200b\uff0c\u200b\u624b\u6307\u200b\u878d\u5408\u200b\uff0c\u200b\u9759\u6b62\u4e0d\u52a8\u200b\u7684\u200b\u753b\u9762\u200b\uff0c\u200b\u6742\u4e71\u200b\u7684\u200b\u80cc\u666f\u200b\uff0c\u200b\u4e09\u6761\u200b\u817f\u200b\uff0c\u200b\u80cc\u666f\u200b\u4eba\u200b\u5f88\u591a\u200b\uff0c\u200b\u5012\u200b\u7740\u200b\u8d70\u200b\",\n    prompt_path: str | None = None,\n    output_path: str = \"outputs/\",\n    output_video_name: str | None = None,\n    num_videos_per_prompt: int = 1,\n    seed: int = 1024,\n    num_frames: int = 125,\n    num_frames_round_down: bool = False,\n    height: int = 720,\n    width: int = 1280,\n    fps: int = 24,\n    num_inference_steps: int = 50,\n    guidance_scale: float = 1.0,\n    guidance_rescale: float = 0.0,\n    boundary_ratio: float | None = None,\n    enable_teacache: bool = False,\n    save_video: bool = True,\n    return_frames: bool = False,\n    return_trajectory_latents: bool = False,\n    return_trajectory_decoded: bool = False,\n)\n</code></pre> <p>               Bases: <code>SamplingParam</code></p> <p>Sampling parameters for Wan2.2 TI2V 5B model.</p> fastvideo.configs.sample.wan.Wan2_2_TI2V_5B_SamplingParam <code>dataclass</code> \u00b6 <pre><code>Wan2_2_TI2V_5B_SamplingParam(\n    data_type: str = \"video\",\n    image_path: str | None = None,\n    video_path: str | None = None,\n    prompt: str | list[str] | None = None,\n    negative_prompt: str\n    | None = \"\u200b\u8272\u8c03\u200b\u8273\u4e3d\u200b\uff0c\u200b\u8fc7\u200b\u66dd\u200b\uff0c\u200b\u9759\u6001\u200b\uff0c\u200b\u7ec6\u8282\u200b\u6a21\u7cca\u4e0d\u6e05\u200b\uff0c\u200b\u5b57\u5e55\u200b\uff0c\u200b\u98ce\u683c\u200b\uff0c\u200b\u4f5c\u54c1\u200b\uff0c\u200b\u753b\u4f5c\u200b\uff0c\u200b\u753b\u9762\u200b\uff0c\u200b\u9759\u6b62\u200b\uff0c\u200b\u6574\u4f53\u200b\u53d1\u7070\u200b\uff0c\u200b\u6700\u5dee\u200b\u8d28\u91cf\u200b\uff0c\u200b\u4f4e\u8d28\u91cf\u200b\uff0cJPEG\u200b\u538b\u7f29\u200b\u6b8b\u7559\u200b\uff0c\u200b\u4e11\u964b\u200b\u7684\u200b\uff0c\u200b\u6b8b\u7f3a\u200b\u7684\u200b\uff0c\u200b\u591a\u4f59\u200b\u7684\u200b\u624b\u6307\u200b\uff0c\u200b\u753b\u5f97\u200b\u4e0d\u597d\u200b\u7684\u200b\u624b\u90e8\u200b\uff0c\u200b\u753b\u5f97\u200b\u4e0d\u597d\u200b\u7684\u200b\u8138\u90e8\u200b\uff0c\u200b\u7578\u5f62\u200b\u7684\u200b\uff0c\u200b\u6bc1\u5bb9\u200b\u7684\u200b\uff0c\u200b\u5f62\u6001\u200b\u7578\u5f62\u200b\u7684\u200b\u80a2\u4f53\u200b\uff0c\u200b\u624b\u6307\u200b\u878d\u5408\u200b\uff0c\u200b\u9759\u6b62\u4e0d\u52a8\u200b\u7684\u200b\u753b\u9762\u200b\uff0c\u200b\u6742\u4e71\u200b\u7684\u200b\u80cc\u666f\u200b\uff0c\u200b\u4e09\u6761\u200b\u817f\u200b\uff0c\u200b\u80cc\u666f\u200b\u4eba\u200b\u5f88\u591a\u200b\uff0c\u200b\u5012\u200b\u7740\u200b\u8d70\u200b\",\n    prompt_path: str | None = None,\n    output_path: str = \"outputs/\",\n    output_video_name: str | None = None,\n    num_videos_per_prompt: int = 1,\n    seed: int = 1024,\n    num_frames: int = 121,\n    num_frames_round_down: bool = False,\n    height: int = 704,\n    width: int = 1280,\n    fps: int = 24,\n    num_inference_steps: int = 50,\n    guidance_scale: float = 5.0,\n    guidance_rescale: float = 0.0,\n    boundary_ratio: float | None = None,\n    enable_teacache: bool = False,\n    save_video: bool = True,\n    return_frames: bool = False,\n    return_trajectory_latents: bool = False,\n    return_trajectory_decoded: bool = False,\n)\n</code></pre> <p>               Bases: <code>Wan2_2_Base_SamplingParam</code></p> <p>Sampling parameters for Wan2.2 TI2V 5B model.</p>"},{"location":"api/fastvideo/configs/#fastvideo.configs.utils","title":"fastvideo.configs.utils","text":""},{"location":"api/fastvideo/configs/#fastvideo.configs.utils-functions","title":"Functions","text":""},{"location":"api/fastvideo/configs/#fastvideo.configs.utils.clean_cli_args","title":"fastvideo.configs.utils.clean_cli_args","text":"<pre><code>clean_cli_args(args: Namespace) -&gt; dict[str, Any]\n</code></pre> <p>Clean the arguments by removing the ones that not explicitly provided by the user.</p> Source code in <code>fastvideo/configs/utils.py</code> <pre><code>def clean_cli_args(args: argparse.Namespace) -&gt; dict[str, Any]:\n    \"\"\"\n    Clean the arguments by removing the ones that not explicitly provided by the user.\n    \"\"\"\n    provided_args = {}\n    for k, v in vars(args).items():\n        if (v is not None and hasattr(args, '_provided')\n                and k in args._provided):\n            provided_args[k] = v\n\n    return provided_args\n</code></pre>"},{"location":"api/fastvideo/configs/#fastvideo.configs.utils.update_config_from_args","title":"fastvideo.configs.utils.update_config_from_args","text":"<pre><code>update_config_from_args(\n    config: Any,\n    args_dict: dict[str, Any],\n    prefix: str = \"\",\n    pop_args: bool = False,\n) -&gt; bool\n</code></pre> <p>Update configuration object from arguments dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Any</code> <p>The configuration object to update</p> required <code>args_dict</code> <code>dict[str, Any]</code> <p>Dictionary containing arguments</p> required <code>prefix</code> <code>str</code> <p>Prefix for the configuration parameters in the args_dict.    If None, assumes direct attribute mapping without prefix.</p> <code>''</code> Source code in <code>fastvideo/configs/utils.py</code> <pre><code>def update_config_from_args(config: Any,\n                            args_dict: dict[str, Any],\n                            prefix: str = \"\",\n                            pop_args: bool = False) -&gt; bool:\n    \"\"\"\n    Update configuration object from arguments dictionary.\n\n    Args:\n        config: The configuration object to update\n        args_dict: Dictionary containing arguments\n        prefix: Prefix for the configuration parameters in the args_dict.\n               If None, assumes direct attribute mapping without prefix.\n    \"\"\"\n    # Handle top-level attributes (no prefix)\n    args_not_to_remove = [\n        'model_path',\n    ]\n    args_to_remove = []\n    if prefix.strip() == \"\":\n        for key, value in args_dict.items():\n            if hasattr(config, key) and value is not None:\n                if key == \"text_encoder_precisions\" and isinstance(value, list):\n                    setattr(config, key, tuple(value))\n                else:\n                    setattr(config, key, value)\n                if pop_args:\n                    args_to_remove.append(key)\n    else:\n        # Handle nested attributes with prefix\n        prefix_with_dot = f\"{prefix}.\"\n        for key, value in args_dict.items():\n            if key.startswith(prefix_with_dot) and value is not None:\n                attr_name = key[len(prefix_with_dot):]\n                if hasattr(config, attr_name):\n                    setattr(config, attr_name, value)\n                if pop_args:\n                    args_to_remove.append(key)\n\n    if pop_args:\n        for key in args_to_remove:\n            if key not in args_not_to_remove:\n                args_dict.pop(key)\n\n    return len(args_to_remove) &gt; 0\n</code></pre>"},{"location":"api/fastvideo/dataset/","title":"dataset","text":""},{"location":"api/fastvideo/dataset/#fastvideo.dataset","title":"dataset","text":""},{"location":"api/fastvideo/dataset/#fastvideo.dataset-classes","title":"Classes","text":""},{"location":"api/fastvideo/dataset/#fastvideo.dataset.TextDataset","title":"fastvideo.dataset.TextDataset","text":"<pre><code>TextDataset(\n    data_merge_path: str,\n    args,\n    start_idx: int = 0,\n    seed: int = 42,\n)\n</code></pre> <p>               Bases: <code>IterableDataset</code>, <code>Stateful</code></p> <p>Text-only dataset for processing prompts from a simple text file.</p> <p>Assumes that data_merge_path is a text file with one prompt per line: A cat playing with a ball A dog running in the park A person cooking dinner ...</p> <p>This dataset processes text data through text encoding stages only.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def __init__(self,\n             data_merge_path: str,\n             args,\n             start_idx: int = 0,\n             seed: int = 42):\n    self.data_merge_path = data_merge_path\n    self.start_idx = start_idx\n    self.args = args\n    self.seed = seed\n\n    # Initialize tokenizer\n    tokenizer_path = os.path.join(args.model_path, \"tokenizer\")\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path,\n                                              cache_dir=args.cache_dir)\n\n    # Initialize text encoding stage\n    self.text_encoding_stage = TextEncodingStage(\n        tokenizer=tokenizer,\n        text_max_length=args.text_max_length,\n        cfg_rate=getattr(args, 'training_cfg_rate', 0.0),\n        seed=self.seed)\n\n    # Process text data\n    self.processed_batches = self._process_text_data()\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.TextDataset-functions","title":"Functions","text":""},{"location":"api/fastvideo/dataset/#fastvideo.dataset.TextDataset.__iter__","title":"fastvideo.dataset.TextDataset.__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Iterator for the dataset.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def __iter__(self):\n    \"\"\"Iterator for the dataset.\"\"\"\n    # Set up distributed sampling if needed\n    if torch.distributed.is_available() and torch.distributed.is_initialized():\n        rank = torch.distributed.get_rank()\n        world_size = torch.distributed.get_world_size()\n    else:\n        rank = 0\n        world_size = 1\n\n    # Calculate chunk for this rank\n    total_items = len(self.processed_batches)\n    items_per_rank = math.ceil(total_items / world_size)\n    start_idx = rank * items_per_rank + self.start_idx\n    end_idx = min(start_idx + items_per_rank, total_items)\n\n    # Yield items for this rank\n    for idx in range(start_idx, end_idx):\n        if idx &lt; len(self.processed_batches):\n            yield self._get_item(idx)\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.TextDataset.load_state_dict","title":"fastvideo.dataset.TextDataset.load_state_dict","text":"<pre><code>load_state_dict(state_dict: dict[str, Any]) -&gt; None\n</code></pre> <p>Load state dict from checkpoint.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def load_state_dict(self, state_dict: dict[str, Any]) -&gt; None:\n    \"\"\"Load state dict from checkpoint.\"\"\"\n    self.processed_batches = state_dict[\"processed_batches\"]\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.TextDataset.state_dict","title":"fastvideo.dataset.TextDataset.state_dict","text":"<pre><code>state_dict() -&gt; dict[str, Any]\n</code></pre> <p>Return state dict for checkpointing.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def state_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return state dict for checkpointing.\"\"\"\n    return {\"processed_batches\": self.processed_batches}\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.ValidationDataset","title":"fastvideo.dataset.ValidationDataset","text":"<pre><code>ValidationDataset(filename: str)\n</code></pre> <p>               Bases: <code>IterableDataset</code></p> Source code in <code>fastvideo/dataset/validation_dataset.py</code> <pre><code>def __init__(self, filename: str):\n    super().__init__()\n\n    self.filename = pathlib.Path(filename)\n    # get directory of filename\n    self.dir = os.path.abspath(self.filename.parent)\n\n    if not self.filename.exists():\n        raise FileNotFoundError(\n            f\"File {self.filename.as_posix()} does not exist\")\n\n    if self.filename.suffix == \".csv\":\n        data = datasets.load_dataset(\"csv\",\n                                     data_files=self.filename.as_posix(),\n                                     split=\"train\")\n    elif self.filename.suffix == \".json\":\n        data = datasets.load_dataset(\"json\",\n                                     data_files=self.filename.as_posix(),\n                                     split=\"train\",\n                                     field=\"data\")\n    elif self.filename.suffix == \".parquet\":\n        data = datasets.load_dataset(\"parquet\",\n                                     data_files=self.filename.as_posix(),\n                                     split=\"train\")\n    elif self.filename.suffix == \".arrow\":\n        data = datasets.load_dataset(\"arrow\",\n                                     data_files=self.filename.as_posix(),\n                                     split=\"train\")\n    else:\n        _SUPPORTED_FILE_FORMATS = [\".csv\", \".json\", \".parquet\", \".arrow\"]\n        raise ValueError(\n            f\"Unsupported file format {self.filename.suffix} for validation dataset. Supported formats are: {_SUPPORTED_FILE_FORMATS}\"\n        )\n\n    # Get distributed training info\n    self.global_rank = get_world_rank()\n    self.world_size = get_world_size()\n    self.sp_world_size = get_sp_world_size()\n    self.num_sp_groups = self.world_size // self.sp_world_size\n\n    # Convert to list to get total samples\n    self.all_samples = list(data)\n    self.original_total_samples = len(self.all_samples)\n\n    # Extend samples to be a multiple of DP degree (num_sp_groups)\n    remainder = self.original_total_samples % self.num_sp_groups\n    if remainder != 0:\n        samples_to_add = self.num_sp_groups - remainder\n\n        # Duplicate samples cyclically to reach the target\n        additional_samples = []\n        for i in range(samples_to_add):\n            additional_samples.append(\n                self.all_samples[i % self.original_total_samples])\n\n        self.all_samples.extend(additional_samples)\n\n    self.total_samples = len(self.all_samples)\n\n    # Calculate which SP group this rank belongs to\n    self.sp_group_id = self.global_rank // self.sp_world_size\n\n    # Now all SP groups will have equal number of samples\n    self.samples_per_sp_group = self.total_samples // self.num_sp_groups\n\n    # Calculate start and end indices for this SP group\n    self.start_idx = self.sp_group_id * self.samples_per_sp_group\n    self.end_idx = self.start_idx + self.samples_per_sp_group\n\n    # Get samples for this SP group\n    self.sp_group_samples = self.all_samples[self.start_idx:self.end_idx]\n\n    logger.info(\n        \"Rank %s (SP group %s): \"\n        \"Original samples: %s, \"\n        \"Extended samples: %s, \"\n        \"SP group samples: %s, \"\n        \"Range: [%s:%s]\",\n        self.global_rank,\n        self.sp_group_id,\n        self.original_total_samples,\n        self.total_samples,\n        len(self.sp_group_samples),\n        self.start_idx,\n        self.end_idx,\n        local_main_process_only=False)\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.ValidationDataset-functions","title":"Functions","text":""},{"location":"api/fastvideo/dataset/#fastvideo.dataset.ValidationDataset.__len__","title":"fastvideo.dataset.ValidationDataset.__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the number of samples for this SP group.</p> Source code in <code>fastvideo/dataset/validation_dataset.py</code> <pre><code>def __len__(self):\n    \"\"\"Return the number of samples for this SP group.\"\"\"\n    return len(self.sp_group_samples)\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.VideoCaptionMergedDataset","title":"fastvideo.dataset.VideoCaptionMergedDataset","text":"<pre><code>VideoCaptionMergedDataset(\n    data_merge_path: str,\n    args,\n    transform,\n    temporal_sample,\n    transform_topcrop,\n    start_idx: int = 0,\n    seed: int = 42,\n)\n</code></pre> <p>               Bases: <code>IterableDataset</code>, <code>Stateful</code></p> <p>Merged dataset for video and caption data with stage-based processing. Assumes that data_merge_path is a txt file with the following format: , <pre><code>The folder should contain videos.\n\nThe json file should be a list of dictionaries with the following format:\n[\n{\n    \"path\": \"1gGQy4nxyUo-Scene-016.mp4\",\n    \"resolution\": {\n    \"width\": 1920,\n    \"height\": 1080\n    },\n    \"size\": 2439112,\n    \"fps\": 25.0,\n    \"duration\": 6.88,\n    \"num_frames\": 172,\n    \"cap\": [\n    \"A watermelon wearing a helmet is crushed by a hydraulic press, causing it to flatten and burst open.\"\n    ]\n},\n...\n]\n</code></pre> <p>This dataset processes video and image data through a series of stages: - Data validation - Resolution filtering - Frame sampling - Transformation - Text encoding</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def __init__(self,\n             data_merge_path: str,\n             args,\n             transform,\n             temporal_sample,\n             transform_topcrop,\n             start_idx: int = 0,\n             seed: int = 42):\n    self.data_merge_path = data_merge_path\n    self.start_idx = start_idx\n    self.args = args\n    self.temporal_sample = temporal_sample\n    self.seed = seed\n\n    # Initialize tokenizer\n    tokenizer_path = os.path.join(args.model_path, \"tokenizer\")\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path,\n                                              cache_dir=args.cache_dir)\n\n    # Initialize processing stages\n    self._init_stages(args, transform, transform_topcrop, tokenizer)\n\n    # Process metadata\n    self.processed_batches = self._process_metadata()\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.VideoCaptionMergedDataset-functions","title":"Functions","text":""},{"location":"api/fastvideo/dataset/#fastvideo.dataset.VideoCaptionMergedDataset.__iter__","title":"fastvideo.dataset.VideoCaptionMergedDataset.__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Iterate through processed data items.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def __iter__(self):\n    \"\"\"Iterate through processed data items.\"\"\"\n    for idx in range(len(self.processed_batches)):\n        yield self._get_item(idx)\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.VideoCaptionMergedDataset.load_state_dict","title":"fastvideo.dataset.VideoCaptionMergedDataset.load_state_dict","text":"<pre><code>load_state_dict(state_dict: dict[str, Any]) -&gt; None\n</code></pre> <p>Load state dict from checkpoint.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def load_state_dict(self, state_dict: dict[str, Any]) -&gt; None:\n    \"\"\"Load state dict from checkpoint.\"\"\"\n    self.processed_batches = state_dict[\"processed_batches\"]\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.VideoCaptionMergedDataset.state_dict","title":"fastvideo.dataset.VideoCaptionMergedDataset.state_dict","text":"<pre><code>state_dict() -&gt; dict[str, Any]\n</code></pre> <p>Return state dict for checkpointing.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def state_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return state dict for checkpointing.\"\"\"\n    return {\"processed_batches\": self.processed_batches}\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset-modules","title":"Modules","text":""},{"location":"api/fastvideo/dataset/#fastvideo.dataset.parquet_dataset_iterable_style","title":"fastvideo.dataset.parquet_dataset_iterable_style","text":""},{"location":"api/fastvideo/dataset/#fastvideo.dataset.parquet_dataset_iterable_style-classes","title":"Classes","text":""},{"location":"api/fastvideo/dataset/#fastvideo.dataset.parquet_dataset_iterable_style.LatentsParquetIterStyleDataset","title":"fastvideo.dataset.parquet_dataset_iterable_style.LatentsParquetIterStyleDataset","text":"<pre><code>LatentsParquetIterStyleDataset(\n    path: str,\n    batch_size: int = 1024,\n    cfg_rate: float = 0.1,\n    num_workers: int = 1,\n    drop_last: bool = True,\n    text_padding_length: int = 512,\n    seed: int = 42,\n    read_batch_size: int = 32,\n    parquet_schema: Schema = None,\n)\n</code></pre> <p>               Bases: <code>IterableDataset</code></p> <p>Efficient loader for video-text data from a directory of Parquet files.</p> Source code in <code>fastvideo/dataset/parquet_dataset_iterable_style.py</code> <pre><code>def __init__(self,\n             path: str,\n             batch_size: int = 1024,\n             cfg_rate: float = 0.1,\n             num_workers: int = 1,\n             drop_last: bool = True,\n             text_padding_length: int = 512,\n             seed: int = 42,\n             read_batch_size: int = 32,\n             parquet_schema: pa.Schema = None):\n    super().__init__()\n    self.path = str(path)\n    self.batch_size = batch_size\n    self.parquet_schema = parquet_schema\n    self.cfg_rate = cfg_rate\n    self.text_padding_length = text_padding_length\n    self.seed = seed\n    self.read_batch_size = read_batch_size\n    # Get distributed training info\n    self.global_rank = get_world_rank()\n    self.world_size = get_world_size()\n    self.sp_world_size = get_sp_world_size()\n    self.num_sp_groups = self.world_size // self.sp_world_size\n    num_workers = 1 if num_workers == 0 else num_workers\n    # Get sharding info\n    shard_parquet_files, shard_total_samples, shard_parquet_lengths = shard_parquet_files_across_sp_groups_and_workers(\n        self.path, self.num_sp_groups, num_workers, seed)\n\n    if drop_last:\n        self.worker_num_samples = min(\n            shard_total_samples) // batch_size * batch_size\n        # Assign files to current rank's SP group\n        ith_sp_group = self.global_rank // self.sp_world_size\n        self.sp_group_parquet_files = shard_parquet_files[ith_sp_group::self\n                                                          .num_sp_groups]\n        self.sp_group_parquet_lengths = shard_parquet_lengths[\n            ith_sp_group::self.num_sp_groups]\n        self.sp_group_num_samples = shard_total_samples[ith_sp_group::self.\n                                                        num_sp_groups]\n        logger.info(\n            \"In total %d parquet files, %d samples, after sharding we retain %d samples due to drop_last\",\n            sum([len(shard) for shard in shard_parquet_files]),\n            sum(shard_total_samples),\n            self.worker_num_samples * self.num_sp_groups * num_workers)\n    else:\n        raise ValueError(\"drop_last must be True\")\n    logger.info(\"Each dataloader worker will load %d samples\",\n                self.worker_num_samples)\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.parquet_dataset_iterable_style-functions","title":"Functions","text":""},{"location":"api/fastvideo/dataset/#fastvideo.dataset.parquet_dataset_iterable_style.build_parquet_iterable_style_dataloader","title":"fastvideo.dataset.parquet_dataset_iterable_style.build_parquet_iterable_style_dataloader","text":"<pre><code>build_parquet_iterable_style_dataloader(\n    path: str,\n    batch_size: int,\n    num_data_workers: int,\n    cfg_rate: float = 0.0,\n    drop_last: bool = True,\n    text_padding_length: int = 512,\n    seed: int = 42,\n    read_batch_size: int = 32,\n) -&gt; tuple[\n    LatentsParquetIterStyleDataset, StatefulDataLoader\n]\n</code></pre> <p>Build a dataloader for the LatentsParquetIterStyleDataset.</p> Source code in <code>fastvideo/dataset/parquet_dataset_iterable_style.py</code> <pre><code>def build_parquet_iterable_style_dataloader(\n    path: str,\n    batch_size: int,\n    num_data_workers: int,\n    cfg_rate: float = 0.0,\n    drop_last: bool = True,\n    text_padding_length: int = 512,\n    seed: int = 42,\n    read_batch_size: int = 32\n) -&gt; tuple[LatentsParquetIterStyleDataset, StatefulDataLoader]:\n    \"\"\"Build a dataloader for the LatentsParquetIterStyleDataset.\"\"\"\n    dataset = LatentsParquetIterStyleDataset(\n        path=path,\n        batch_size=batch_size,\n        cfg_rate=cfg_rate,\n        num_workers=num_data_workers,\n        drop_last=drop_last,\n        text_padding_length=text_padding_length,\n        seed=seed,\n        read_batch_size=read_batch_size)\n\n    loader = StatefulDataLoader(\n        dataset,\n        batch_size=1,\n        num_workers=num_data_workers,\n        pin_memory=True,\n    )\n    return dataset, loader\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.parquet_dataset_iterable_style.shard_parquet_files_across_sp_groups_and_workers","title":"fastvideo.dataset.parquet_dataset_iterable_style.shard_parquet_files_across_sp_groups_and_workers","text":"<pre><code>shard_parquet_files_across_sp_groups_and_workers(\n    path: str,\n    num_sp_groups: int,\n    num_workers: int,\n    seed: int = 42,\n) -&gt; tuple[\n    list[list[str]], list[int], list[dict[str, int]]\n]\n</code></pre> <p>Shard parquet files across SP groups and workers in a balanced way.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Directory containing parquet files</p> required <code>num_sp_groups</code> <code>int</code> <p>Number of SP groups to shard across</p> required <code>num_workers</code> <code>int</code> <p>Number of workers per SP group</p> required <code>seed</code> <code>int</code> <p>Random seed for shuffling</p> <code>42</code> <p>Returns:</p> Type Description <code>list[list[str]]</code> <p>Tuple containing:</p> <code>list[int]</code> <ul> <li>List of lists of parquet files for each shard</li> </ul> <code>list[dict[str, int]]</code> <ul> <li>List of total samples per shard</li> </ul> <code>tuple[list[list[str]], list[int], list[dict[str, int]]]</code> <ul> <li>List of dictionaries mapping file paths to their lengths</li> </ul> Source code in <code>fastvideo/dataset/parquet_dataset_iterable_style.py</code> <pre><code>def shard_parquet_files_across_sp_groups_and_workers(\n    path: str,\n    num_sp_groups: int,\n    num_workers: int,\n    seed: int = 42,\n) -&gt; tuple[list[list[str]], list[int], list[dict[str, int]]]:\n    \"\"\"\n    Shard parquet files across SP groups and workers in a balanced way.\n\n    Args:\n        path: Directory containing parquet files\n        num_sp_groups: Number of SP groups to shard across\n        num_workers: Number of workers per SP group\n        seed: Random seed for shuffling\n\n    Returns:\n        Tuple containing:\n        - List of lists of parquet files for each shard\n        - List of total samples per shard\n        - List of dictionaries mapping file paths to their lengths\n    \"\"\"\n    # Check if sharding plan already exists\n    sharding_info_dir = os.path.join(\n        path, f\"sharding_info_{num_sp_groups}_sp_groups_{num_workers}_workers\")\n\n    # Only rank 0 handles cache checking and file scanning\n    if get_world_rank() == 0:\n        cache_loaded = False\n        shard_parquet_files = None\n        shard_total_samples = None\n        shard_parquet_lengths = None\n\n        # First try to load existing sharding plan\n        if os.path.exists(sharding_info_dir):\n            logger.info(\"Loading sharding plan from %s\", sharding_info_dir)\n            try:\n                with open(\n                        os.path.join(sharding_info_dir,\n                                     \"shard_parquet_files.pkl\"), \"rb\") as f:\n                    shard_parquet_files = pickle.load(f)\n                with open(\n                        os.path.join(sharding_info_dir,\n                                     \"shard_total_samples.pkl\"), \"rb\") as f:\n                    shard_total_samples = pickle.load(f)\n                with open(\n                        os.path.join(sharding_info_dir,\n                                     \"shard_parquet_lengths.pkl\"), \"rb\") as f:\n                    shard_parquet_lengths = pickle.load(f)\n                cache_loaded = True\n                logger.info(\"Successfully loaded sharding plan\")\n            except Exception as e:\n                logger.error(\"Error loading sharding plan: %s\", str(e))\n                logger.info(\"Falling back to creating new sharding plan\")\n                cache_loaded = False\n\n        # If cache not loaded (either doesn't exist or failed to load), create sharding plan\n        if not cache_loaded:\n            logger.info(\"Creating new sharding plan\")\n            logger.info(\"Scanning for parquet files in %s\", path)\n\n            # Find all parquet files\n            parquet_files = []\n\n            for root, _, files in os.walk(path):\n                for file in files:\n                    if file.endswith('.parquet'):\n                        parquet_files.append(os.path.join(root, file))\n\n            if not parquet_files:\n                raise ValueError(\"No parquet files found in %s\", path)\n\n            # Calculate file lengths efficiently using a single pass\n            logger.info(\"Calculating file lengths...\")\n            lengths = []\n            for file in tqdm.tqdm(parquet_files, desc=\"Reading parquet files\"):\n                lengths.append(pq.ParquetFile(file).metadata.num_rows)\n\n            total_samples = sum(lengths)\n            logger.info(\"Found %d files with %d total samples\",\n                        len(parquet_files), total_samples)\n\n            # Sort files by length for better balancing\n            sorted_indices = np.argsort(lengths)\n            sorted_files = [parquet_files[i] for i in sorted_indices]\n            sorted_lengths = [lengths[i] for i in sorted_indices]\n\n            # Create shards\n            num_shards = num_sp_groups * num_workers\n            shard_parquet_files = [[] for _ in range(num_shards)]\n            shard_total_samples = [0] * num_shards\n            shard_parquet_lengths = [{} for _ in range(num_shards)]\n\n            # Distribute files to shards using a greedy approach\n            logger.info(\"Distributing files to shards...\")\n            for file, length in zip(reversed(sorted_files),\n                                    reversed(sorted_lengths),\n                                    strict=True):\n                # Find shard with minimum current length\n                target_shard = np.argmin(shard_total_samples)\n                shard_parquet_files[target_shard].append(file)\n                shard_total_samples[target_shard] += length\n                shard_parquet_lengths[target_shard][file] = length\n            #randomize each shard\n            for shard in shard_parquet_files:\n                rng = random.Random(seed)\n                rng.shuffle(shard)\n\n            # Save the sharding plan\n            os.makedirs(sharding_info_dir, exist_ok=True)\n            with open(\n                    os.path.join(sharding_info_dir, \"shard_parquet_files.pkl\"),\n                    \"wb\") as f:\n                pickle.dump(shard_parquet_files, f)\n            with open(\n                    os.path.join(sharding_info_dir, \"shard_total_samples.pkl\"),\n                    \"wb\") as f:\n                pickle.dump(shard_total_samples, f)\n            with open(\n                    os.path.join(sharding_info_dir,\n                                 \"shard_parquet_lengths.pkl\"), \"wb\") as f:\n                pickle.dump(shard_parquet_lengths, f)\n            logger.info(\"Saved sharding info to %s\", sharding_info_dir)\n\n    # Wait for rank 0 to finish creating/loading sharding plan\n    world_group = get_world_group()\n    world_group.barrier()\n\n    # Now all ranks load the sharding plan (it should exist and be valid now)\n    logger.info(\"Loading sharding plan from %s after barrier\",\n                sharding_info_dir)\n    with open(os.path.join(sharding_info_dir, \"shard_parquet_files.pkl\"),\n              \"rb\") as f:\n        shard_parquet_files = pickle.load(f)\n    with open(os.path.join(sharding_info_dir, \"shard_total_samples.pkl\"),\n              \"rb\") as f:\n        shard_total_samples = pickle.load(f)\n    with open(os.path.join(sharding_info_dir, \"shard_parquet_lengths.pkl\"),\n              \"rb\") as f:\n        shard_parquet_lengths = pickle.load(f)\n\n    return shard_parquet_files, shard_total_samples, shard_parquet_lengths\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.parquet_dataset_map_style","title":"fastvideo.dataset.parquet_dataset_map_style","text":""},{"location":"api/fastvideo/dataset/#fastvideo.dataset.parquet_dataset_map_style-classes","title":"Classes","text":""},{"location":"api/fastvideo/dataset/#fastvideo.dataset.parquet_dataset_map_style.DP_SP_BatchSampler","title":"fastvideo.dataset.parquet_dataset_map_style.DP_SP_BatchSampler","text":"<pre><code>DP_SP_BatchSampler(\n    batch_size: int,\n    dataset_size: int,\n    num_sp_groups: int,\n    sp_world_size: int,\n    global_rank: int,\n    drop_last: bool = True,\n    drop_first_row: bool = False,\n    seed: int = 0,\n)\n</code></pre> <p>               Bases: <code>Sampler[list[int]]</code></p> <p>A simple sequential batch sampler that yields batches of indices.</p> Source code in <code>fastvideo/dataset/parquet_dataset_map_style.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int,\n    dataset_size: int,\n    num_sp_groups: int,\n    sp_world_size: int,\n    global_rank: int,\n    drop_last: bool = True,\n    drop_first_row: bool = False,\n    seed: int = 0,\n):\n    self.batch_size = batch_size\n    self.dataset_size = dataset_size\n    self.drop_last = drop_last\n    self.seed = seed\n    self.num_sp_groups = num_sp_groups\n    self.global_rank = global_rank\n    self.sp_world_size = sp_world_size\n\n    # \u2500\u2500 epoch-level RNG \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    rng = torch.Generator().manual_seed(self.seed)\n    # Create a random permutation of all indices\n    global_indices = torch.randperm(self.dataset_size, generator=rng)\n\n    if drop_first_row:\n        # drop 0 in global_indices\n        global_indices = global_indices[global_indices != 0]\n        self.dataset_size = self.dataset_size - 1\n\n    if self.drop_last:\n        # For drop_last=True, we:\n        # 1. Ensure total samples is divisible by (batch_size * num_sp_groups)\n        # 2. This guarantees each SP group gets same number of complete batches\n        # 3. Prevents uneven batch sizes across SP groups at end of epoch\n        num_batches = self.dataset_size // self.batch_size\n        num_global_batches = num_batches // self.num_sp_groups\n        global_indices = global_indices[:num_global_batches *\n                                        self.num_sp_groups *\n                                        self.batch_size]\n    else:\n        if self.dataset_size % (self.num_sp_groups * self.batch_size) != 0:\n            # add more indices to make it divisible by (batch_size * num_sp_groups)\n            padding_size = self.num_sp_groups * self.batch_size - (\n                self.dataset_size % (self.num_sp_groups * self.batch_size))\n            logger.info(\"Padding the dataset from %d to %d\",\n                        self.dataset_size, self.dataset_size + padding_size)\n            global_indices = torch.cat(\n                [global_indices, global_indices[:padding_size]])\n\n    # shard the indices to each sp group\n    ith_sp_group = self.global_rank // self.sp_world_size\n    sp_group_local_indices = global_indices[ith_sp_group::self.\n                                            num_sp_groups]\n    self.sp_group_local_indices = sp_group_local_indices\n    logger.info(\"Dataset size for each sp group: %d\",\n                len(sp_group_local_indices))\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.parquet_dataset_map_style.LatentsParquetMapStyleDataset","title":"fastvideo.dataset.parquet_dataset_map_style.LatentsParquetMapStyleDataset","text":"<pre><code>LatentsParquetMapStyleDataset(\n    path: str,\n    batch_size: int,\n    parquet_schema: Schema,\n    cfg_rate: float = 0.0,\n    seed: int = 42,\n    drop_last: bool = True,\n    drop_first_row: bool = False,\n    text_padding_length: int = 512,\n)\n</code></pre> <p>               Bases: <code>Dataset</code></p> <p>Return latents[B,C,T,H,W] and embeddings[B,L,D] in pinned CPU memory. Note:  Using parquet for map style dataset is not efficient, we mainly keep it for backward compatibility and debugging.</p> Source code in <code>fastvideo/dataset/parquet_dataset_map_style.py</code> <pre><code>def __init__(\n    self,\n    path: str,\n    batch_size: int,\n    parquet_schema: pa.Schema,\n    cfg_rate: float = 0.0,\n    seed: int = 42,\n    drop_last: bool = True,\n    drop_first_row: bool = False,\n    text_padding_length: int = 512,\n):\n    super().__init__()\n    self.path = path\n    self.cfg_rate = cfg_rate\n    self.parquet_schema = parquet_schema\n    self.seed = seed\n    # Create a seeded random generator for deterministic CFG\n    self.rng = random.Random(seed)\n    logger.info(\"Initializing LatentsParquetMapStyleDataset with path: %s\",\n                path)\n    self.parquet_files, self.lengths = get_parquet_files_and_length(path)\n    self.batch = batch_size\n    self.text_padding_length = text_padding_length\n    self.sampler = DP_SP_BatchSampler(\n        batch_size=batch_size,\n        dataset_size=sum(self.lengths),\n        num_sp_groups=get_world_size() // get_sp_world_size(),\n        sp_world_size=get_sp_world_size(),\n        global_rank=get_world_rank(),\n        drop_last=drop_last,\n        drop_first_row=drop_first_row,\n        seed=seed,\n    )\n    logger.info(\"Dataset initialized with %d parquet files and %d rows\",\n                len(self.parquet_files), sum(self.lengths))\n</code></pre> Functions\u00b6 fastvideo.dataset.parquet_dataset_map_style.LatentsParquetMapStyleDataset.__getitems__ \u00b6 <pre><code>__getitems__(indices: list[int]) -&gt; dict[str, Any]\n</code></pre> <p>Batch fetch using read_row_from_parquet_file for each index.</p> Source code in <code>fastvideo/dataset/parquet_dataset_map_style.py</code> <pre><code>def __getitems__(self, indices: list[int]) -&gt; dict[str, Any]:\n    \"\"\"\n    Batch fetch using read_row_from_parquet_file for each index.\n    \"\"\"\n    rows = [\n        read_row_from_parquet_file(self.parquet_files, idx, self.lengths)\n        for idx in indices\n    ]\n\n    batch = collate_rows_from_parquet_schema(rows,\n                                             self.parquet_schema,\n                                             self.text_padding_length,\n                                             cfg_rate=self.cfg_rate,\n                                             rng=self.rng)\n    return batch\n</code></pre> fastvideo.dataset.parquet_dataset_map_style.LatentsParquetMapStyleDataset.get_validation_negative_prompt \u00b6 <pre><code>get_validation_negative_prompt() -&gt; tuple[\n    torch.Tensor, torch.Tensor, str\n]\n</code></pre> <p>Get the negative prompt for validation.  This method ensures the negative prompt is loaded and cached properly. Returns the processed negative prompt data (latents, embeddings, masks, info).</p> Source code in <code>fastvideo/dataset/parquet_dataset_map_style.py</code> <pre><code>def get_validation_negative_prompt(\n        self) -&gt; tuple[torch.Tensor, torch.Tensor, str]:\n    \"\"\"\n    Get the negative prompt for validation. \n    This method ensures the negative prompt is loaded and cached properly.\n    Returns the processed negative prompt data (latents, embeddings, masks, info).\n    \"\"\"\n\n    # Read first row from first parquet file\n    file_path = self.parquet_files[0]\n    row_idx = 0\n    # Read the negative prompt data\n    row_dict = read_row_from_parquet_file([file_path], row_idx,\n                                          [self.lengths[0]])\n\n    batch = collate_rows_from_parquet_schema([row_dict],\n                                             self.parquet_schema,\n                                             self.text_padding_length,\n                                             cfg_rate=0.0,\n                                             rng=self.rng)\n    negative_prompt = batch['info_list'][0]['prompt']\n    negative_prompt_embedding = batch['text_embedding']\n    negative_prompt_attention_mask = batch['text_attention_mask']\n    if len(negative_prompt_embedding.shape) == 2:\n        negative_prompt_embedding = negative_prompt_embedding.unsqueeze(0)\n    if len(negative_prompt_attention_mask.shape) == 1:\n        negative_prompt_attention_mask = negative_prompt_attention_mask.unsqueeze(\n            0).unsqueeze(0)\n\n    return negative_prompt_embedding, negative_prompt_attention_mask, negative_prompt\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.parquet_dataset_map_style-functions","title":"Functions","text":""},{"location":"api/fastvideo/dataset/#fastvideo.dataset.parquet_dataset_map_style.read_row_from_parquet_file","title":"fastvideo.dataset.parquet_dataset_map_style.read_row_from_parquet_file","text":"<pre><code>read_row_from_parquet_file(\n    parquet_files: list[str],\n    global_row_idx: int,\n    lengths: list[int],\n) -&gt; dict[str, Any]\n</code></pre> <p>Read a row from a parquet file. Args:     parquet_files: List[str]     global_row_idx: int     lengths: List[int] Returns:</p> Source code in <code>fastvideo/dataset/parquet_dataset_map_style.py</code> <pre><code>def read_row_from_parquet_file(parquet_files: list[str], global_row_idx: int,\n                               lengths: list[int]) -&gt; dict[str, Any]:\n    '''\n    Read a row from a parquet file.\n    Args:\n        parquet_files: List[str]\n        global_row_idx: int\n        lengths: List[int]\n    Returns:\n    '''\n    # find the parquet file and local row index\n    cumulative = 0\n    file_index = 0\n    local_row_idx = 0\n\n    for file_index in range(len(lengths)):\n        if cumulative + lengths[file_index] &gt; global_row_idx:\n            local_row_idx = global_row_idx - cumulative\n            break\n        cumulative += lengths[file_index]\n    else:\n        # If we reach here, global_row_idx is out of bounds\n        raise IndexError(\n            f\"global_row_idx {global_row_idx} is out of bounds for dataset\")\n\n    parquet_file = pq.ParquetFile(parquet_files[file_index])\n\n    # Calculate the row group to read into memory and the local idx\n    # This way we can avoid reading in the entire parquet file\n    cumulative = 0\n    row_group_index = 0\n    local_index = 0\n\n    for i in range(parquet_file.num_row_groups):\n        num_rows = parquet_file.metadata.row_group(i).num_rows\n        if cumulative + num_rows &gt; local_row_idx:\n            row_group_index = i\n            local_index = local_row_idx - cumulative\n            break\n        cumulative += num_rows\n    else:\n        # If we reach here, local_row_idx is out of bounds for this parquet file\n        raise IndexError(\n            f\"local_row_idx {local_row_idx} is out of bounds for parquet file {parquet_files[file_index]}\"\n        )\n\n    row_group = parquet_file.read_row_group(row_group_index).to_pydict()\n    row_dict = {k: v[local_index] for k, v in row_group.items()}\n    del row_group\n\n    return row_dict\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.preprocessing_datasets","title":"fastvideo.dataset.preprocessing_datasets","text":""},{"location":"api/fastvideo/dataset/#fastvideo.dataset.preprocessing_datasets-classes","title":"Classes","text":""},{"location":"api/fastvideo/dataset/#fastvideo.dataset.preprocessing_datasets.DataValidationStage","title":"fastvideo.dataset.preprocessing_datasets.DataValidationStage","text":"<p>               Bases: <code>DatasetFilterStage</code></p> <p>Stage for validating data items.</p> Functions\u00b6 fastvideo.dataset.preprocessing_datasets.DataValidationStage.process \u00b6 <pre><code>process(\n    batch: PreprocessBatch, **kwargs\n) -&gt; PreprocessBatch\n</code></pre> <p>Process does nothing for validation - filtering is handled by should_keep.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def process(self, batch: PreprocessBatch, **kwargs) -&gt; PreprocessBatch:\n    \"\"\"Process does nothing for validation - filtering is handled by should_keep.\"\"\"\n    return batch\n</code></pre> fastvideo.dataset.preprocessing_datasets.DataValidationStage.should_keep \u00b6 <pre><code>should_keep(batch: PreprocessBatch, **kwargs) -&gt; bool\n</code></pre> <p>Validate data item.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>PreprocessBatch</code> <p>Dataset batch to validate</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if valid, False if invalid</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def should_keep(self, batch: PreprocessBatch, **kwargs) -&gt; bool:\n    \"\"\"\n    Validate data item.\n\n    Args:\n        batch: Dataset batch to validate\n\n    Returns:\n        True if valid, False if invalid\n    \"\"\"\n    # Check for caption\n    if batch.cap is None:\n        return False\n\n    if batch.is_video:\n        # Validate video-specific fields\n        if batch.duration is None or batch.fps is None:\n            return False\n    elif not batch.is_image:\n        return False\n\n    return True\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.preprocessing_datasets.DatasetFilterStage","title":"fastvideo.dataset.preprocessing_datasets.DatasetFilterStage","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for dataset filtering stages.</p> <p>These stages can filter out items during metadata processing.</p> Functions\u00b6 fastvideo.dataset.preprocessing_datasets.DatasetFilterStage.process <code>abstractmethod</code> \u00b6 <pre><code>process(\n    batch: PreprocessBatch, **kwargs\n) -&gt; PreprocessBatch\n</code></pre> <p>Process the dataset batch (for non-filtering operations).</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>PreprocessBatch</code> <p>Dataset batch to process</p> required <code>**kwargs</code> <p>Additional processing parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>PreprocessBatch</code> <p>Processed batch</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>@abstractmethod\ndef process(self, batch: PreprocessBatch, **kwargs) -&gt; PreprocessBatch:\n    \"\"\"\n    Process the dataset batch (for non-filtering operations).\n\n    Args:\n        batch: Dataset batch to process\n        **kwargs: Additional processing parameters\n\n    Returns:\n        Processed batch\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.dataset.preprocessing_datasets.DatasetFilterStage.should_keep <code>abstractmethod</code> \u00b6 <pre><code>should_keep(batch: PreprocessBatch, **kwargs) -&gt; bool\n</code></pre> <p>Check if batch should be kept.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>PreprocessBatch</code> <p>Dataset batch to check</p> required <code>**kwargs</code> <p>Additional parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if batch should be kept, False otherwise</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>@abstractmethod\ndef should_keep(self, batch: PreprocessBatch, **kwargs) -&gt; bool:\n    \"\"\"\n    Check if batch should be kept.\n\n    Args:\n        batch: Dataset batch to check\n        **kwargs: Additional parameters\n\n    Returns:\n        True if batch should be kept, False otherwise\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.preprocessing_datasets.DatasetStage","title":"fastvideo.dataset.preprocessing_datasets.DatasetStage","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for dataset processing stages.</p> <p>Similar to PipelineStage but designed for dataset preprocessing operations.</p> Functions\u00b6 fastvideo.dataset.preprocessing_datasets.DatasetStage.process <code>abstractmethod</code> \u00b6 <pre><code>process(\n    batch: PreprocessBatch, **kwargs\n) -&gt; PreprocessBatch\n</code></pre> <p>Process the dataset batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>PreprocessBatch</code> <p>Dataset batch to process</p> required <code>**kwargs</code> <p>Additional processing parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>PreprocessBatch</code> <p>Processed batch</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>@abstractmethod\ndef process(self, batch: PreprocessBatch, **kwargs) -&gt; PreprocessBatch:\n    \"\"\"\n    Process the dataset batch.\n\n    Args:\n        batch: Dataset batch to process\n        **kwargs: Additional processing parameters\n\n    Returns:\n        Processed batch\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.preprocessing_datasets.FrameSamplingStage","title":"fastvideo.dataset.preprocessing_datasets.FrameSamplingStage","text":"<pre><code>FrameSamplingStage(\n    num_frames: int,\n    train_fps: int,\n    speed_factor: int = 1,\n    video_length_tolerance_range: float = 5.0,\n    drop_short_ratio: float = 0.0,\n    seed: int = 42,\n)\n</code></pre> <p>               Bases: <code>DatasetFilterStage</code></p> <p>Stage for temporal frame sampling and indexing.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def __init__(self,\n             num_frames: int,\n             train_fps: int,\n             speed_factor: int = 1,\n             video_length_tolerance_range: float = 5.0,\n             drop_short_ratio: float = 0.0,\n             seed: int = 42):\n    self.num_frames = num_frames\n    self.train_fps = train_fps\n    self.speed_factor = speed_factor\n    self.video_length_tolerance_range = video_length_tolerance_range\n    self.drop_short_ratio = drop_short_ratio\n    # Create a seeded random generator for deterministic sampling\n    self.rng = random.Random(seed)\n</code></pre> Functions\u00b6 fastvideo.dataset.preprocessing_datasets.FrameSamplingStage.process \u00b6 <pre><code>process(\n    batch: PreprocessBatch,\n    temporal_sample_fn=None,\n    **kwargs\n) -&gt; PreprocessBatch\n</code></pre> <p>Process frame sampling for video data items.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>PreprocessBatch</code> <p>Dataset batch</p> required <code>temporal_sample_fn</code> <p>Function for temporal sampling</p> <code>None</code> <p>Returns:</p> Type Description <code>PreprocessBatch</code> <p>Updated batch with frame sampling info</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def process(self,\n            batch: PreprocessBatch,\n            temporal_sample_fn=None,\n            **kwargs) -&gt; PreprocessBatch:\n    \"\"\"\n    Process frame sampling for video data items.\n\n    Args:\n        batch: Dataset batch\n        temporal_sample_fn: Function for temporal sampling\n\n    Returns:\n        Updated batch with frame sampling info\n    \"\"\"\n    if batch.is_image:\n        # For images, just add sample info\n        batch.sample_frame_index = [0]\n        batch.sample_num_frames = 1\n        return batch\n\n    assert batch.duration is not None and batch.fps is not None\n    batch.num_frames = math.ceil(batch.fps * batch.duration)\n\n    # Resample frame indices\n    frame_interval = batch.fps / self.train_fps\n    start_frame_idx = 0\n    frame_indices = np.arange(start_frame_idx, batch.num_frames,\n                              frame_interval).astype(int)\n\n    # Temporal crop if too long\n    if len(frame_indices) &gt; self.num_frames:\n        if temporal_sample_fn is not None:\n            begin_index, end_index = temporal_sample_fn(len(frame_indices))\n            frame_indices = frame_indices[begin_index:end_index]\n        else:\n            frame_indices = frame_indices[:self.num_frames]\n\n    batch.sample_frame_index = frame_indices.tolist()\n    batch.sample_num_frames = len(frame_indices)\n\n    return batch\n</code></pre> fastvideo.dataset.preprocessing_datasets.FrameSamplingStage.should_keep \u00b6 <pre><code>should_keep(batch: PreprocessBatch, **kwargs) -&gt; bool\n</code></pre> <p>Check if video should be kept based on length constraints.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>PreprocessBatch</code> <p>Dataset batch</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if should be kept, False otherwise</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def should_keep(self, batch: PreprocessBatch, **kwargs) -&gt; bool:\n    \"\"\"\n    Check if video should be kept based on length constraints.\n\n    Args:\n        batch: Dataset batch\n\n    Returns:\n        True if should be kept, False otherwise\n    \"\"\"\n    if batch.is_image:\n        return True\n\n    if batch.duration is None or batch.fps is None:\n        return False\n\n    num_frames = math.ceil(batch.fps * batch.duration)\n\n    # Check if video is too long\n    if (num_frames / batch.fps &gt; self.video_length_tolerance_range *\n        (self.num_frames / self.train_fps * self.speed_factor)):\n        return False\n\n    # Resample frame indices to check length\n    frame_interval = batch.fps / self.train_fps\n    start_frame_idx = 0\n    frame_indices = np.arange(start_frame_idx, num_frames,\n                              frame_interval).astype(int)\n\n    # Filter short videos\n    return not (len(frame_indices) &lt; self.num_frames\n                and self.rng.random() &lt; self.drop_short_ratio)\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.preprocessing_datasets.ImageTransformStage","title":"fastvideo.dataset.preprocessing_datasets.ImageTransformStage","text":"<pre><code>ImageTransformStage(transform, transform_topcrop)\n</code></pre> <p>               Bases: <code>DatasetStage</code></p> <p>Stage for image data transformation.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def __init__(self, transform, transform_topcrop) -&gt; None:\n    self.transform = transform\n    self.transform_topcrop = transform_topcrop\n</code></pre> Functions\u00b6 fastvideo.dataset.preprocessing_datasets.ImageTransformStage.process \u00b6 <pre><code>process(\n    batch: PreprocessBatch, **kwargs\n) -&gt; PreprocessBatch\n</code></pre> <p>Transform image data.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>PreprocessBatch</code> <p>Dataset batch with image information</p> required <p>Returns:</p> Type Description <code>PreprocessBatch</code> <p>Batch with transformed image tensor</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def process(self, batch: PreprocessBatch, **kwargs) -&gt; PreprocessBatch:\n    \"\"\"\n    Transform image data.\n\n    Args:\n        batch: Dataset batch with image information\n\n    Returns:\n        Batch with transformed image tensor\n    \"\"\"\n    if not batch.is_image:\n        return batch\n\n    image = Image.open(batch.path).convert(\"RGB\")\n    image = torch.from_numpy(np.array(image))\n    image = rearrange(image, \"h w c -&gt; c h w\").unsqueeze(0)\n\n    if self.transform_topcrop is not None:\n        image = self.transform_topcrop(image)\n    elif self.transform is not None:\n        image = self.transform(image)\n\n    image = image.transpose(0, 1)  # [1 C H W] -&gt; [C 1 H W]\n    image = image.float() / 127.5 - 1.0\n    batch.pixel_values = image\n    return batch\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.preprocessing_datasets.PreprocessBatch","title":"fastvideo.dataset.preprocessing_datasets.PreprocessBatch  <code>dataclass</code>","text":"<pre><code>PreprocessBatch(\n    path: str,\n    cap: str | list[str],\n    resolution: dict | None = None,\n    fps: float | None = None,\n    duration: float | None = None,\n    num_frames: int | None = None,\n    sample_frame_index: list[int] | None = None,\n    sample_num_frames: int | None = None,\n    pixel_values: Tensor | None = None,\n    text: str | None = None,\n    input_ids: Tensor | None = None,\n    cond_mask: Tensor | None = None,\n)\n</code></pre> <p>Batch information for dataset processing stages.</p> <p>This class holds all the information about a video-caption or image-caption pair as it moves through the processing pipeline. Fields are populated by different stages.</p> Attributes\u00b6 fastvideo.dataset.preprocessing_datasets.PreprocessBatch.is_image <code>property</code> \u00b6 <pre><code>is_image: bool\n</code></pre> <p>Check if this is an image item.</p> fastvideo.dataset.preprocessing_datasets.PreprocessBatch.is_video <code>property</code> \u00b6 <pre><code>is_video: bool\n</code></pre> <p>Check if this is a video item.</p>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.preprocessing_datasets.ResolutionFilterStage","title":"fastvideo.dataset.preprocessing_datasets.ResolutionFilterStage","text":"<pre><code>ResolutionFilterStage(\n    max_h_div_w_ratio: float = 17 / 16,\n    min_h_div_w_ratio: float = 8 / 16,\n    max_height: int = 1024,\n    max_width: int = 1024,\n)\n</code></pre> <p>               Bases: <code>DatasetFilterStage</code></p> <p>Stage for filtering data items based on resolution constraints.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def __init__(self,\n             max_h_div_w_ratio: float = 17 / 16,\n             min_h_div_w_ratio: float = 8 / 16,\n             max_height: int = 1024,\n             max_width: int = 1024):\n    self.max_h_div_w_ratio = max_h_div_w_ratio\n    self.min_h_div_w_ratio = min_h_div_w_ratio\n    self.max_height = max_height\n    self.max_width = max_width\n</code></pre> Functions\u00b6 fastvideo.dataset.preprocessing_datasets.ResolutionFilterStage.filter_resolution \u00b6 <pre><code>filter_resolution(\n    h: int,\n    w: int,\n    max_h_div_w_ratio: float,\n    min_h_div_w_ratio: float,\n) -&gt; bool\n</code></pre> <p>Filter based on height/width ratio.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def filter_resolution(self, h: int, w: int, max_h_div_w_ratio: float,\n                      min_h_div_w_ratio: float) -&gt; bool:\n    \"\"\"Filter based on height/width ratio.\"\"\"\n    return h / w &lt;= max_h_div_w_ratio and h / w &gt;= min_h_div_w_ratio\n</code></pre> fastvideo.dataset.preprocessing_datasets.ResolutionFilterStage.process \u00b6 <pre><code>process(\n    batch: PreprocessBatch, **kwargs\n) -&gt; PreprocessBatch\n</code></pre> <p>Process does nothing for resolution filtering - filtering is handled by should_keep.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def process(self, batch: PreprocessBatch, **kwargs) -&gt; PreprocessBatch:\n    \"\"\"Process does nothing for resolution filtering - filtering is handled by should_keep.\"\"\"\n    return batch\n</code></pre> fastvideo.dataset.preprocessing_datasets.ResolutionFilterStage.should_keep \u00b6 <pre><code>should_keep(batch: PreprocessBatch, **kwargs) -&gt; bool\n</code></pre> <p>Check if data item passes resolution filtering.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>PreprocessBatch</code> <p>Dataset batch with resolution information</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if passes filter, False otherwise</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def should_keep(self, batch: PreprocessBatch, **kwargs) -&gt; bool:\n    \"\"\"\n    Check if data item passes resolution filtering.\n\n    Args:\n        batch: Dataset batch with resolution information\n\n    Returns:\n        True if passes filter, False otherwise\n    \"\"\"\n    # Only apply to videos\n    if not batch.is_video:\n        return True\n\n    if batch.resolution is None:\n        return False\n\n    height = batch.resolution.get(\"height\", None)\n    width = batch.resolution.get(\"width\", None)\n    if height is None or width is None:\n        return False\n\n    # Check aspect ratio\n    aspect = self.max_height / self.max_width\n    hw_aspect_thr = 1.5\n\n    return self.filter_resolution(\n        height,\n        width,\n        max_h_div_w_ratio=hw_aspect_thr * aspect,\n        min_h_div_w_ratio=1 / hw_aspect_thr * aspect,\n    )\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.preprocessing_datasets.TextDataset","title":"fastvideo.dataset.preprocessing_datasets.TextDataset","text":"<pre><code>TextDataset(\n    data_merge_path: str,\n    args,\n    start_idx: int = 0,\n    seed: int = 42,\n)\n</code></pre> <p>               Bases: <code>IterableDataset</code>, <code>Stateful</code></p> <p>Text-only dataset for processing prompts from a simple text file.</p> <p>Assumes that data_merge_path is a text file with one prompt per line: A cat playing with a ball A dog running in the park A person cooking dinner ...</p> <p>This dataset processes text data through text encoding stages only.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def __init__(self,\n             data_merge_path: str,\n             args,\n             start_idx: int = 0,\n             seed: int = 42):\n    self.data_merge_path = data_merge_path\n    self.start_idx = start_idx\n    self.args = args\n    self.seed = seed\n\n    # Initialize tokenizer\n    tokenizer_path = os.path.join(args.model_path, \"tokenizer\")\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path,\n                                              cache_dir=args.cache_dir)\n\n    # Initialize text encoding stage\n    self.text_encoding_stage = TextEncodingStage(\n        tokenizer=tokenizer,\n        text_max_length=args.text_max_length,\n        cfg_rate=getattr(args, 'training_cfg_rate', 0.0),\n        seed=self.seed)\n\n    # Process text data\n    self.processed_batches = self._process_text_data()\n</code></pre> Functions\u00b6 fastvideo.dataset.preprocessing_datasets.TextDataset.__iter__ \u00b6 <pre><code>__iter__()\n</code></pre> <p>Iterator for the dataset.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def __iter__(self):\n    \"\"\"Iterator for the dataset.\"\"\"\n    # Set up distributed sampling if needed\n    if torch.distributed.is_available() and torch.distributed.is_initialized():\n        rank = torch.distributed.get_rank()\n        world_size = torch.distributed.get_world_size()\n    else:\n        rank = 0\n        world_size = 1\n\n    # Calculate chunk for this rank\n    total_items = len(self.processed_batches)\n    items_per_rank = math.ceil(total_items / world_size)\n    start_idx = rank * items_per_rank + self.start_idx\n    end_idx = min(start_idx + items_per_rank, total_items)\n\n    # Yield items for this rank\n    for idx in range(start_idx, end_idx):\n        if idx &lt; len(self.processed_batches):\n            yield self._get_item(idx)\n</code></pre> fastvideo.dataset.preprocessing_datasets.TextDataset.load_state_dict \u00b6 <pre><code>load_state_dict(state_dict: dict[str, Any]) -&gt; None\n</code></pre> <p>Load state dict from checkpoint.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def load_state_dict(self, state_dict: dict[str, Any]) -&gt; None:\n    \"\"\"Load state dict from checkpoint.\"\"\"\n    self.processed_batches = state_dict[\"processed_batches\"]\n</code></pre> fastvideo.dataset.preprocessing_datasets.TextDataset.state_dict \u00b6 <pre><code>state_dict() -&gt; dict[str, Any]\n</code></pre> <p>Return state dict for checkpointing.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def state_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return state dict for checkpointing.\"\"\"\n    return {\"processed_batches\": self.processed_batches}\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.preprocessing_datasets.TextEncodingStage","title":"fastvideo.dataset.preprocessing_datasets.TextEncodingStage","text":"<pre><code>TextEncodingStage(\n    tokenizer,\n    text_max_length: int,\n    cfg_rate: float = 0.0,\n    seed: int = 42,\n)\n</code></pre> <p>               Bases: <code>DatasetStage</code></p> <p>Stage for text tokenization and encoding.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def __init__(self,\n             tokenizer,\n             text_max_length: int,\n             cfg_rate: float = 0.0,\n             seed: int = 42):\n    self.tokenizer = tokenizer\n    self.text_max_length = text_max_length\n    self.cfg_rate = cfg_rate\n    # Create a seeded random generator for deterministic CFG\n    self.rng = random.Random(seed)\n</code></pre> Functions\u00b6 fastvideo.dataset.preprocessing_datasets.TextEncodingStage.process \u00b6 <pre><code>process(\n    batch: PreprocessBatch, **kwargs\n) -&gt; PreprocessBatch\n</code></pre> <p>Process text data.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>PreprocessBatch</code> <p>Dataset batch with caption information</p> required <p>Returns:</p> Type Description <code>PreprocessBatch</code> <p>Batch with encoded text information</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def process(self, batch: PreprocessBatch, **kwargs) -&gt; PreprocessBatch:\n    \"\"\"\n    Process text data.\n\n    Args:\n        batch: Dataset batch with caption information\n\n    Returns:\n        Batch with encoded text information\n    \"\"\"\n    text = batch.cap\n    if not isinstance(text, list):\n        text = [text]\n    text = [self.rng.choice(text)]\n\n    text = text[0] if self.rng.random() &gt; self.cfg_rate else \"\"\n    text_tokens_and_mask = self.tokenizer(\n        text,\n        max_length=self.text_max_length,\n        padding=\"max_length\",\n        truncation=True,\n        return_attention_mask=True,\n        add_special_tokens=True,\n        return_tensors=\"pt\",\n    )\n\n    batch.text = text\n    batch.input_ids = text_tokens_and_mask[\"input_ids\"]\n    batch.cond_mask = text_tokens_and_mask[\"attention_mask\"]\n    return batch\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.preprocessing_datasets.VideoCaptionMergedDataset","title":"fastvideo.dataset.preprocessing_datasets.VideoCaptionMergedDataset","text":"<pre><code>VideoCaptionMergedDataset(\n    data_merge_path: str,\n    args,\n    transform,\n    temporal_sample,\n    transform_topcrop,\n    start_idx: int = 0,\n    seed: int = 42,\n)\n</code></pre> <p>               Bases: <code>IterableDataset</code>, <code>Stateful</code></p> <p>Merged dataset for video and caption data with stage-based processing. Assumes that data_merge_path is a txt file with the following format: , <pre><code>The folder should contain videos.\n\nThe json file should be a list of dictionaries with the following format:\n[\n{\n    \"path\": \"1gGQy4nxyUo-Scene-016.mp4\",\n    \"resolution\": {\n    \"width\": 1920,\n    \"height\": 1080\n    },\n    \"size\": 2439112,\n    \"fps\": 25.0,\n    \"duration\": 6.88,\n    \"num_frames\": 172,\n    \"cap\": [\n    \"A watermelon wearing a helmet is crushed by a hydraulic press, causing it to flatten and burst open.\"\n    ]\n},\n...\n]\n</code></pre> <p>This dataset processes video and image data through a series of stages: - Data validation - Resolution filtering - Frame sampling - Transformation - Text encoding</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def __init__(self,\n             data_merge_path: str,\n             args,\n             transform,\n             temporal_sample,\n             transform_topcrop,\n             start_idx: int = 0,\n             seed: int = 42):\n    self.data_merge_path = data_merge_path\n    self.start_idx = start_idx\n    self.args = args\n    self.temporal_sample = temporal_sample\n    self.seed = seed\n\n    # Initialize tokenizer\n    tokenizer_path = os.path.join(args.model_path, \"tokenizer\")\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path,\n                                              cache_dir=args.cache_dir)\n\n    # Initialize processing stages\n    self._init_stages(args, transform, transform_topcrop, tokenizer)\n\n    # Process metadata\n    self.processed_batches = self._process_metadata()\n</code></pre> Functions\u00b6 fastvideo.dataset.preprocessing_datasets.VideoCaptionMergedDataset.__iter__ \u00b6 <pre><code>__iter__()\n</code></pre> <p>Iterate through processed data items.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def __iter__(self):\n    \"\"\"Iterate through processed data items.\"\"\"\n    for idx in range(len(self.processed_batches)):\n        yield self._get_item(idx)\n</code></pre> fastvideo.dataset.preprocessing_datasets.VideoCaptionMergedDataset.load_state_dict \u00b6 <pre><code>load_state_dict(state_dict: dict[str, Any]) -&gt; None\n</code></pre> <p>Load state dict from checkpoint.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def load_state_dict(self, state_dict: dict[str, Any]) -&gt; None:\n    \"\"\"Load state dict from checkpoint.\"\"\"\n    self.processed_batches = state_dict[\"processed_batches\"]\n</code></pre> fastvideo.dataset.preprocessing_datasets.VideoCaptionMergedDataset.state_dict \u00b6 <pre><code>state_dict() -&gt; dict[str, Any]\n</code></pre> <p>Return state dict for checkpointing.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def state_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return state dict for checkpointing.\"\"\"\n    return {\"processed_batches\": self.processed_batches}\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.preprocessing_datasets.VideoTransformStage","title":"fastvideo.dataset.preprocessing_datasets.VideoTransformStage","text":"<pre><code>VideoTransformStage(transform)\n</code></pre> <p>               Bases: <code>DatasetStage</code></p> <p>Stage for video data transformation.</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def __init__(self, transform) -&gt; None:\n    self.transform = transform\n</code></pre> Functions\u00b6 fastvideo.dataset.preprocessing_datasets.VideoTransformStage.process \u00b6 <pre><code>process(\n    batch: PreprocessBatch, **kwargs\n) -&gt; PreprocessBatch\n</code></pre> <p>Transform video data.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>PreprocessBatch</code> <p>Dataset batch with video information</p> required <p>Returns:</p> Type Description <code>PreprocessBatch</code> <p>Batch with transformed video tensor</p> Source code in <code>fastvideo/dataset/preprocessing_datasets.py</code> <pre><code>def process(self, batch: PreprocessBatch, **kwargs) -&gt; PreprocessBatch:\n    \"\"\"\n    Transform video data.\n\n    Args:\n        batch: Dataset batch with video information\n\n    Returns:\n        Batch with transformed video tensor\n    \"\"\"\n    if not batch.is_video:\n        return batch\n\n    assert os.path.exists(batch.path), f\"file {batch.path} do not exist!\"\n    assert batch.sample_frame_index is not None, \"Frame indices must be set before transformation\"\n\n    torchvision_video, _, metadata = torchvision.io.read_video(\n        batch.path, output_format=\"TCHW\")\n    video = torchvision_video[batch.sample_frame_index]\n    if self.transform is not None:\n        video = self.transform(video)\n    video = rearrange(video, \"t c h w -&gt; c t h w\")\n    video = video.to(torch.uint8)\n\n    h, w = video.shape[-2:]\n    assert (\n        h / w &lt;= 17 / 16 and h / w &gt;= 8 / 16\n    ), f\"Only videos with a ratio (h/w) less than 17/16 and more than 8/16 are supported. But video ({batch.path}) found ratio is {round(h / w, 2)} with the shape of {video.shape}\"\n\n    video = video.float() / 127.5 - 1.0\n    batch.pixel_values = video\n    return batch\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.preprocessing_datasets-functions","title":"Functions","text":""},{"location":"api/fastvideo/dataset/#fastvideo.dataset.transform","title":"fastvideo.dataset.transform","text":""},{"location":"api/fastvideo/dataset/#fastvideo.dataset.transform-classes","title":"Classes","text":""},{"location":"api/fastvideo/dataset/#fastvideo.dataset.transform.CenterCropResizeVideo","title":"fastvideo.dataset.transform.CenterCropResizeVideo","text":"<pre><code>CenterCropResizeVideo(\n    size, top_crop=False, interpolation_mode=\"bilinear\"\n)\n</code></pre> <p>First use the short side for cropping length, center crop video, then resize to the specified size</p> Source code in <code>fastvideo/dataset/transform.py</code> <pre><code>def __init__(\n    self,\n    size,\n    top_crop=False,\n    interpolation_mode=\"bilinear\",\n) -&gt; None:\n    if len(size) != 2:\n        raise ValueError(\n            f\"size should be tuple (height, width), instead got {size}\")\n    self.size = size\n    self.top_crop = top_crop\n    self.interpolation_mode = interpolation_mode\n</code></pre> Functions\u00b6 fastvideo.dataset.transform.CenterCropResizeVideo.__call__ \u00b6 <pre><code>__call__(clip) -&gt; torch.Tensor\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>clip</code> <code>tensor</code> <p>Video clip to be cropped. Size is (T, C, H, W)</p> required <p>Returns:     torch.tensor: scale resized / center cropped video clip.         size is (T, C, crop_size, crop_size)</p> Source code in <code>fastvideo/dataset/transform.py</code> <pre><code>def __call__(self, clip) -&gt; torch.Tensor:\n    \"\"\"\n    Args:\n        clip (torch.tensor): Video clip to be cropped. Size is (T, C, H, W)\n    Returns:\n        torch.tensor: scale resized / center cropped video clip.\n            size is (T, C, crop_size, crop_size)\n    \"\"\"\n    clip_center_crop = center_crop_th_tw(clip,\n                                         self.size[0],\n                                         self.size[1],\n                                         top_crop=self.top_crop)\n    clip_center_crop_resize = resize(\n        clip_center_crop,\n        target_size=self.size,\n        interpolation_mode=self.interpolation_mode,\n    )\n    return clip_center_crop_resize\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.transform.Normalize255","title":"fastvideo.dataset.transform.Normalize255","text":"<pre><code>Normalize255()\n</code></pre> <p>Convert tensor data type from uint8 to float, divide value by 255.0 and</p> Source code in <code>fastvideo/dataset/transform.py</code> <pre><code>def __init__(self) -&gt; None:\n    pass\n</code></pre> Functions\u00b6 fastvideo.dataset.transform.Normalize255.__call__ \u00b6 <pre><code>__call__(clip) -&gt; torch.Tensor\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>clip</code> <code>torch.tensor, dtype=torch.uint8</code> <p>Size is (T, C, H, W)</p> required <p>Return:     clip (torch.tensor, dtype=torch.float): Size is (T, C, H, W)</p> Source code in <code>fastvideo/dataset/transform.py</code> <pre><code>def __call__(self, clip) -&gt; torch.Tensor:\n    \"\"\"\n    Args:\n        clip (torch.tensor, dtype=torch.uint8): Size is (T, C, H, W)\n    Return:\n        clip (torch.tensor, dtype=torch.float): Size is (T, C, H, W)\n    \"\"\"\n    return normalize_video(clip)\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.transform.TemporalRandomCrop","title":"fastvideo.dataset.transform.TemporalRandomCrop","text":"<pre><code>TemporalRandomCrop(size)\n</code></pre> <p>Temporally crop the given frame indices at a random location.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>Desired length of frames will be seen in the model.</p> required Source code in <code>fastvideo/dataset/transform.py</code> <pre><code>def __init__(self, size) -&gt; None:\n    self.size = size\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.transform-functions","title":"Functions","text":""},{"location":"api/fastvideo/dataset/#fastvideo.dataset.transform.crop","title":"fastvideo.dataset.transform.crop","text":"<pre><code>crop(clip, i, j, h, w) -&gt; torch.Tensor\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>clip</code> <code>tensor</code> <p>Video clip to be cropped. Size is (T, C, H, W)</p> required Source code in <code>fastvideo/dataset/transform.py</code> <pre><code>def crop(clip, i, j, h, w) -&gt; torch.Tensor:\n    \"\"\"\n    Args:\n        clip (torch.tensor): Video clip to be cropped. Size is (T, C, H, W)\n    \"\"\"\n    if len(clip.size()) != 4:\n        raise ValueError(\"clip should be a 4D tensor\")\n    return clip[..., i:i + h, j:j + w]\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.transform.normalize_video","title":"fastvideo.dataset.transform.normalize_video","text":"<pre><code>normalize_video(clip) -&gt; torch.Tensor\n</code></pre> <p>Convert tensor data type from uint8 to float, divide value by 255.0 and permute the dimensions of clip tensor Args:     clip (torch.tensor, dtype=torch.uint8): Size is (T, C, H, W) Return:     clip (torch.tensor, dtype=torch.float): Size is (T, C, H, W)</p> Source code in <code>fastvideo/dataset/transform.py</code> <pre><code>def normalize_video(clip) -&gt; torch.Tensor:\n    \"\"\"\n    Convert tensor data type from uint8 to float, divide value by 255.0 and\n    permute the dimensions of clip tensor\n    Args:\n        clip (torch.tensor, dtype=torch.uint8): Size is (T, C, H, W)\n    Return:\n        clip (torch.tensor, dtype=torch.float): Size is (T, C, H, W)\n    \"\"\"\n    _is_tensor_video_clip(clip)\n    if not clip.dtype == torch.uint8:\n        raise TypeError(\n            f\"clip tensor should have data type uint8. Got {clip.dtype}\")\n    # return clip.float().permute(3, 0, 1, 2) / 255.0\n    return clip.float() / 255.0\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.utils","title":"fastvideo.dataset.utils","text":""},{"location":"api/fastvideo/dataset/#fastvideo.dataset.utils-functions","title":"Functions","text":""},{"location":"api/fastvideo/dataset/#fastvideo.dataset.utils.collate_rows_from_parquet_schema","title":"fastvideo.dataset.utils.collate_rows_from_parquet_schema","text":"<pre><code>collate_rows_from_parquet_schema(\n    rows,\n    parquet_schema,\n    text_padding_length,\n    cfg_rate=0.0,\n    rng=None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Collate rows from parquet files based on the provided schema. Dynamically processes tensor fields based on schema and returns batched data.</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <p>List of row dictionaries from parquet files</p> required <code>parquet_schema</code> <p>PyArrow schema defining the structure of the data</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict containing batched tensors and metadata</p> Source code in <code>fastvideo/dataset/utils.py</code> <pre><code>def collate_rows_from_parquet_schema(rows,\n                                     parquet_schema,\n                                     text_padding_length,\n                                     cfg_rate=0.0,\n                                     rng=None) -&gt; dict[str, Any]:\n    \"\"\"\n    Collate rows from parquet files based on the provided schema.\n    Dynamically processes tensor fields based on schema and returns batched data.\n\n    Args:\n        rows: List of row dictionaries from parquet files\n        parquet_schema: PyArrow schema defining the structure of the data\n\n    Returns:\n        Dict containing batched tensors and metadata\n    \"\"\"\n    if not rows:\n        return cast(dict[str, Any], {})\n\n    # Initialize containers for different data types\n    batch_data: dict[str, Any] = {}\n\n    # Get tensor and metadata field names from schema (fields ending with '_bytes')\n    tensor_fields = []\n    metadata_fields = []\n    for field in parquet_schema.names:\n        if field.endswith('_bytes'):\n            shape_field = field.replace('_bytes', '_shape')\n            dtype_field = field.replace('_bytes', '_dtype')\n            tensor_name = field.replace('_bytes', '')\n            tensor_fields.append(tensor_name)\n            assert shape_field in parquet_schema.names, f\"Shape field {shape_field} not found in schema for field {field}. Currently we only support *_bytes fields for tensors.\"\n            assert dtype_field in parquet_schema.names, f\"Dtype field {dtype_field} not found in schema for field {field}. Currently we only support *_bytes fields for tensors.\"\n        elif not field.endswith('_shape') and not field.endswith('_dtype'):\n            # Only add actual metadata fields, not the shape/dtype helper fields\n            metadata_fields.append(field)\n\n    # Process each tensor field\n    for tensor_name in tensor_fields:\n        tensor_list = []\n\n        for row in rows:\n            # Get tensor data from row using the existing helper function pattern\n            shape_key = f\"{tensor_name}_shape\"\n            bytes_key = f\"{tensor_name}_bytes\"\n\n            if shape_key in row and bytes_key in row:\n                shape = row[shape_key]\n                bytes_data = row[bytes_key]\n\n                if len(bytes_data) == 0:\n                    tensor = torch.zeros(0, dtype=torch.bfloat16)\n                else:\n                    # Convert bytes to tensor using float32 as default\n                    if tensor_name == 'text_embedding' and (rng.random(\n                    ) if rng else random.random()) &lt; cfg_rate:\n                        data = np.zeros((512, 4096), dtype=np.float32)\n                    else:\n                        data = np.frombuffer(\n                            bytes_data, dtype=np.float32).reshape(shape).copy()\n                    tensor = torch.from_numpy(data)\n                    # if len(data.shape) == 3:\n                    #     B, L, D = tensor.shape\n                    #     assert B == 1, \"Batch size must be 1\"\n                    #     tensor = tensor.squeeze(0)\n\n                tensor_list.append(tensor)\n            else:\n                # Handle missing tensor data\n                tensor_list.append(torch.zeros(0, dtype=torch.bfloat16))\n\n        # Stack tensors with special handling for text embeddings\n        if tensor_name == 'text_embedding':\n            # Handle text embeddings with padding\n            padded_tensors = []\n            attention_masks = []\n\n            for tensor in tensor_list:\n                if tensor.numel() &gt; 0:\n                    padded_tensor, mask = pad(tensor, text_padding_length)\n                    padded_tensors.append(padded_tensor)\n                    attention_masks.append(mask)\n                else:\n                    # Handle empty embeddings - assume default embedding dimension\n                    padded_tensors.append(\n                        torch.zeros(text_padding_length,\n                                    768,\n                                    dtype=torch.bfloat16))\n                    attention_masks.append(torch.zeros(text_padding_length))\n\n            batch_data[tensor_name] = torch.stack(padded_tensors)\n            batch_data['text_attention_mask'] = torch.stack(attention_masks)\n        else:\n            # Stack all tensors to preserve batch consistency\n            # Don't filter out None or empty tensors as this breaks batch sizing\n            try:\n                batch_data[tensor_name] = torch.stack(tensor_list)\n            except ValueError as e:\n                shapes = [\n                    t.shape\n                    if t is not None and hasattr(t, 'shape') else 'None/Invalid'\n                    for t in tensor_list\n                ]\n                raise ValueError(\n                    f\"Failed to stack tensors for field '{tensor_name}'. \"\n                    f\"Tensor shapes: {shapes}. \"\n                    f\"All tensors in a batch must have compatible shapes. \"\n                    f\"Original error: {e}\") from e\n\n    # Process metadata fields into info_list\n    info_list = []\n    for row in rows:\n        info = {}\n        for field in metadata_fields:\n            info[field] = row.get(field, \"\")\n\n        # Add prompt field for backward compatibility\n        info[\"prompt\"] = info.get(\"caption\", \"\")\n        info_list.append(info)\n\n    batch_data['info_list'] = info_list\n\n    # Add caption_text for backward compatibility\n    if info_list and 'caption' in info_list[0]:\n        batch_data['caption_text'] = [info['caption'] for info in info_list]\n\n    return batch_data\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.utils.get_torch_tensors_from_row_dict","title":"fastvideo.dataset.utils.get_torch_tensors_from_row_dict","text":"<pre><code>get_torch_tensors_from_row_dict(\n    row_dict, keys, cfg_rate, rng=None\n) -&gt; dict[str, Any]\n</code></pre> <p>Get the latents and prompts from a row dictionary.</p> Source code in <code>fastvideo/dataset/utils.py</code> <pre><code>def get_torch_tensors_from_row_dict(row_dict,\n                                    keys,\n                                    cfg_rate,\n                                    rng=None) -&gt; dict[str, Any]:\n    \"\"\"\n    Get the latents and prompts from a row dictionary.\n    \"\"\"\n    return_dict = {}\n    for key in keys:\n        shape, bytes = None, None\n        if isinstance(key, tuple):\n            for k in key:\n                try:\n                    shape = row_dict[f\"{k}_shape\"]\n                    bytes = row_dict[f\"{k}_bytes\"]\n                except KeyError:\n                    continue\n            key = key[0]\n            if shape is None or bytes is None:\n                raise ValueError(f\"Key {key} not found in row_dict\")\n        else:\n            shape = row_dict[f\"{key}_shape\"]\n            bytes = row_dict[f\"{key}_bytes\"]\n\n        # TODO (peiyuan): read precision\n        if key == 'text_embedding' and (rng.random()\n                                        if rng else random.random()) &lt; cfg_rate:\n            data = np.zeros((512, 4096), dtype=np.float32)\n        else:\n            data = np.frombuffer(bytes, dtype=np.float32).reshape(shape).copy()\n        data = torch.from_numpy(data)\n        if len(data.shape) == 3:\n            B, L, D = data.shape\n            assert B == 1, \"Batch size must be 1\"\n            data = data.squeeze(0)\n        return_dict[key] = data\n    return return_dict\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.utils.pad","title":"fastvideo.dataset.utils.pad","text":"<pre><code>pad(\n    t: Tensor, padding_length: int\n) -&gt; tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Pad or crop an embedding [L, D] to exactly padding_length tokens. Return: - [L, D] tensor in pinned CPU memory - [L] attention mask in pinned CPU memory</p> Source code in <code>fastvideo/dataset/utils.py</code> <pre><code>def pad(t: torch.Tensor, padding_length: int) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Pad or crop an embedding [L, D] to exactly padding_length tokens.\n    Return:\n    - [L, D] tensor in pinned CPU memory\n    - [L] attention mask in pinned CPU memory\n    \"\"\"\n    L, D = t.shape\n    if padding_length &gt; L:  # pad\n        pad = torch.zeros(padding_length - L, D, dtype=t.dtype, device=t.device)\n        return torch.cat([t, pad], 0), torch.cat(\n            [torch.ones(L), torch.zeros(padding_length - L)], 0)\n    else:  # crop\n        return t[:padding_length], torch.ones(padding_length)\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.validation_dataset","title":"fastvideo.dataset.validation_dataset","text":""},{"location":"api/fastvideo/dataset/#fastvideo.dataset.validation_dataset-classes","title":"Classes","text":""},{"location":"api/fastvideo/dataset/#fastvideo.dataset.validation_dataset.ValidationDataset","title":"fastvideo.dataset.validation_dataset.ValidationDataset","text":"<pre><code>ValidationDataset(filename: str)\n</code></pre> <p>               Bases: <code>IterableDataset</code></p> Source code in <code>fastvideo/dataset/validation_dataset.py</code> <pre><code>def __init__(self, filename: str):\n    super().__init__()\n\n    self.filename = pathlib.Path(filename)\n    # get directory of filename\n    self.dir = os.path.abspath(self.filename.parent)\n\n    if not self.filename.exists():\n        raise FileNotFoundError(\n            f\"File {self.filename.as_posix()} does not exist\")\n\n    if self.filename.suffix == \".csv\":\n        data = datasets.load_dataset(\"csv\",\n                                     data_files=self.filename.as_posix(),\n                                     split=\"train\")\n    elif self.filename.suffix == \".json\":\n        data = datasets.load_dataset(\"json\",\n                                     data_files=self.filename.as_posix(),\n                                     split=\"train\",\n                                     field=\"data\")\n    elif self.filename.suffix == \".parquet\":\n        data = datasets.load_dataset(\"parquet\",\n                                     data_files=self.filename.as_posix(),\n                                     split=\"train\")\n    elif self.filename.suffix == \".arrow\":\n        data = datasets.load_dataset(\"arrow\",\n                                     data_files=self.filename.as_posix(),\n                                     split=\"train\")\n    else:\n        _SUPPORTED_FILE_FORMATS = [\".csv\", \".json\", \".parquet\", \".arrow\"]\n        raise ValueError(\n            f\"Unsupported file format {self.filename.suffix} for validation dataset. Supported formats are: {_SUPPORTED_FILE_FORMATS}\"\n        )\n\n    # Get distributed training info\n    self.global_rank = get_world_rank()\n    self.world_size = get_world_size()\n    self.sp_world_size = get_sp_world_size()\n    self.num_sp_groups = self.world_size // self.sp_world_size\n\n    # Convert to list to get total samples\n    self.all_samples = list(data)\n    self.original_total_samples = len(self.all_samples)\n\n    # Extend samples to be a multiple of DP degree (num_sp_groups)\n    remainder = self.original_total_samples % self.num_sp_groups\n    if remainder != 0:\n        samples_to_add = self.num_sp_groups - remainder\n\n        # Duplicate samples cyclically to reach the target\n        additional_samples = []\n        for i in range(samples_to_add):\n            additional_samples.append(\n                self.all_samples[i % self.original_total_samples])\n\n        self.all_samples.extend(additional_samples)\n\n    self.total_samples = len(self.all_samples)\n\n    # Calculate which SP group this rank belongs to\n    self.sp_group_id = self.global_rank // self.sp_world_size\n\n    # Now all SP groups will have equal number of samples\n    self.samples_per_sp_group = self.total_samples // self.num_sp_groups\n\n    # Calculate start and end indices for this SP group\n    self.start_idx = self.sp_group_id * self.samples_per_sp_group\n    self.end_idx = self.start_idx + self.samples_per_sp_group\n\n    # Get samples for this SP group\n    self.sp_group_samples = self.all_samples[self.start_idx:self.end_idx]\n\n    logger.info(\n        \"Rank %s (SP group %s): \"\n        \"Original samples: %s, \"\n        \"Extended samples: %s, \"\n        \"SP group samples: %s, \"\n        \"Range: [%s:%s]\",\n        self.global_rank,\n        self.sp_group_id,\n        self.original_total_samples,\n        self.total_samples,\n        len(self.sp_group_samples),\n        self.start_idx,\n        self.end_idx,\n        local_main_process_only=False)\n</code></pre> Functions\u00b6 fastvideo.dataset.validation_dataset.ValidationDataset.__len__ \u00b6 <pre><code>__len__()\n</code></pre> <p>Return the number of samples for this SP group.</p> Source code in <code>fastvideo/dataset/validation_dataset.py</code> <pre><code>def __len__(self):\n    \"\"\"Return the number of samples for this SP group.\"\"\"\n    return len(self.sp_group_samples)\n</code></pre>"},{"location":"api/fastvideo/dataset/#fastvideo.dataset.validation_dataset-functions","title":"Functions","text":""},{"location":"api/fastvideo/distributed/","title":"distributed","text":""},{"location":"api/fastvideo/distributed/#fastvideo.distributed","title":"distributed","text":""},{"location":"api/fastvideo/distributed/#fastvideo.distributed-classes","title":"Classes","text":""},{"location":"api/fastvideo/distributed/#fastvideo.distributed.StatelessProcessGroup","title":"fastvideo.distributed.StatelessProcessGroup  <code>dataclass</code>","text":"<pre><code>StatelessProcessGroup(\n    rank: int,\n    world_size: int,\n    store: Store,\n    data_expiration_seconds: int = 3600,\n    send_dst_counter: dict[int, int] = dict(),\n    recv_src_counter: dict[int, int] = dict(),\n    broadcast_send_counter: int = 0,\n    broadcast_recv_src_counter: dict[int, int] = dict(),\n    entries: deque[tuple[str, float]] = deque(),\n)\n</code></pre> <p>A dataclass to hold a metadata store, and the rank, world_size of the group. Only use it to communicate metadata between processes. For data-plane communication, create NCCL-related objects.</p>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.StatelessProcessGroup-functions","title":"Functions","text":""},{"location":"api/fastvideo/distributed/#fastvideo.distributed.StatelessProcessGroup.all_gather_obj","title":"fastvideo.distributed.StatelessProcessGroup.all_gather_obj","text":"<pre><code>all_gather_obj(obj: Any) -&gt; list[Any]\n</code></pre> <p>All gather an object from all ranks.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def all_gather_obj(self, obj: Any) -&gt; list[Any]:\n    \"\"\"All gather an object from all ranks.\"\"\"\n    gathered_objs = []\n    for i in range(self.world_size):\n        if i == self.rank:\n            gathered_objs.append(obj)\n            self.broadcast_obj(obj, src=self.rank)\n        else:\n            recv_obj = self.broadcast_obj(None, src=i)\n            gathered_objs.append(recv_obj)\n    return gathered_objs\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.StatelessProcessGroup.barrier","title":"fastvideo.distributed.StatelessProcessGroup.barrier","text":"<pre><code>barrier()\n</code></pre> <p>A barrier to synchronize all ranks.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def barrier(self):\n    \"\"\"A barrier to synchronize all ranks.\"\"\"\n    for i in range(self.world_size):\n        if i == self.rank:\n            self.broadcast_obj(None, src=self.rank)\n        else:\n            self.broadcast_obj(None, src=i)\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.StatelessProcessGroup.broadcast_obj","title":"fastvideo.distributed.StatelessProcessGroup.broadcast_obj","text":"<pre><code>broadcast_obj(obj: Any | None, src: int) -&gt; Any\n</code></pre> <p>Broadcast an object from a source rank to all other ranks. It does not clean up after all ranks have received the object. Use it for limited times, e.g., for initialization.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def broadcast_obj(self, obj: Any | None, src: int) -&gt; Any:\n    \"\"\"Broadcast an object from a source rank to all other ranks.\n    It does not clean up after all ranks have received the object.\n    Use it for limited times, e.g., for initialization.\n    \"\"\"\n    if self.rank == src:\n        self.expire_data()\n        key = (f\"broadcast_from/{src}/\"\n               f\"{self.broadcast_send_counter}\")\n        self.store.set(key, pickle.dumps(obj))\n        self.broadcast_send_counter += 1\n        self.entries.append((key, time.perf_counter()))\n        return obj\n    else:\n        key = (f\"broadcast_from/{src}/\"\n               f\"{self.broadcast_recv_src_counter[src]}\")\n        recv_obj = pickle.loads(self.store.get(key))\n        self.broadcast_recv_src_counter[src] += 1\n        return recv_obj\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.StatelessProcessGroup.create","title":"fastvideo.distributed.StatelessProcessGroup.create  <code>staticmethod</code>","text":"<pre><code>create(\n    host: str,\n    port: int,\n    rank: int,\n    world_size: int,\n    data_expiration_seconds: int = 3600,\n) -&gt; StatelessProcessGroup\n</code></pre> <p>A replacement for <code>torch.distributed.init_process_group</code> that does not pollute the global state.</p> <p>If we have process A and process B called <code>torch.distributed.init_process_group</code> to form a group, and then we want to form another group with process A, B, C, D, it is not possible in PyTorch, because process A and process B have already formed a group, and process C and process D cannot join that group. This function is a workaround for this issue.</p> <p><code>torch.distributed.init_process_group</code> is a global call, while this function is a stateless call. It will return a <code>StatelessProcessGroup</code> object that can be used for exchanging metadata. With this function, process A and process B can call <code>StatelessProcessGroup.create</code> to form a group, and then process A, B, C, and D can call <code>StatelessProcessGroup.create</code> to form another group.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>@staticmethod\ndef create(\n    host: str,\n    port: int,\n    rank: int,\n    world_size: int,\n    data_expiration_seconds: int = 3600,\n) -&gt; \"StatelessProcessGroup\":\n    \"\"\"A replacement for `torch.distributed.init_process_group` that does not\n    pollute the global state.\n\n    If we have process A and process B called `torch.distributed.init_process_group`\n    to form a group, and then we want to form another group with process A, B, C,\n    D, it is not possible in PyTorch, because process A and process B have already\n    formed a group, and process C and process D cannot join that group. This\n    function is a workaround for this issue.\n\n    `torch.distributed.init_process_group` is a global call, while this function\n    is a stateless call. It will return a `StatelessProcessGroup` object that can be\n    used for exchanging metadata. With this function, process A and process B\n    can call `StatelessProcessGroup.create` to form a group, and then process A, B,\n    C, and D can call `StatelessProcessGroup.create` to form another group.\n    \"\"\" # noqa\n    store = TCPStore(\n        host_name=host,\n        port=port,\n        world_size=world_size,\n        is_master=(rank == 0),\n    )\n\n    return StatelessProcessGroup(\n        rank=rank,\n        world_size=world_size,\n        store=store,\n        data_expiration_seconds=data_expiration_seconds)\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.StatelessProcessGroup.expire_data","title":"fastvideo.distributed.StatelessProcessGroup.expire_data","text":"<pre><code>expire_data() -&gt; None\n</code></pre> <p>Expire data that is older than <code>data_expiration_seconds</code> seconds.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def expire_data(self) -&gt; None:\n    \"\"\"Expire data that is older than `data_expiration_seconds` seconds.\"\"\"\n    while self.entries:\n        # check the oldest entry\n        key, timestamp = self.entries[0]\n        if time.perf_counter() - timestamp &gt; self.data_expiration_seconds:\n            self.store.delete_key(key)\n            self.entries.popleft()\n        else:\n            break\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.StatelessProcessGroup.recv_obj","title":"fastvideo.distributed.StatelessProcessGroup.recv_obj","text":"<pre><code>recv_obj(src: int) -&gt; Any\n</code></pre> <p>Receive an object from a source rank.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def recv_obj(self, src: int) -&gt; Any:\n    \"\"\"Receive an object from a source rank.\"\"\"\n    obj = pickle.loads(\n        self.store.get(f\"send_to/{self.rank}/{self.recv_src_counter[src]}\"))\n    self.recv_src_counter[src] += 1\n    return obj\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.StatelessProcessGroup.send_obj","title":"fastvideo.distributed.StatelessProcessGroup.send_obj","text":"<pre><code>send_obj(obj: Any, dst: int)\n</code></pre> <p>Send an object to a destination rank.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def send_obj(self, obj: Any, dst: int):\n    \"\"\"Send an object to a destination rank.\"\"\"\n    self.expire_data()\n    key = f\"send_to/{dst}/{self.send_dst_counter[dst]}\"\n    self.store.set(key, pickle.dumps(obj))\n    self.send_dst_counter[dst] += 1\n    self.entries.append((key, time.perf_counter()))\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed-functions","title":"Functions","text":""},{"location":"api/fastvideo/distributed/#fastvideo.distributed.divide","title":"fastvideo.distributed.divide","text":"<pre><code>divide(numerator: int, denominator: int) -&gt; int\n</code></pre> <p>Ensure that numerator is divisible by the denominator and return the division value.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def divide(numerator: int, denominator: int) -&gt; int:\n    \"\"\"Ensure that numerator is divisible by the denominator and return\n    the division value.\"\"\"\n    ensure_divisibility(numerator, denominator)\n    return numerator // denominator\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.ensure_divisibility","title":"fastvideo.distributed.ensure_divisibility","text":"<pre><code>ensure_divisibility(numerator, denominator) -&gt; None\n</code></pre> <p>Ensure that numerator is divisible by the denominator.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def ensure_divisibility(numerator, denominator) -&gt; None:\n    \"\"\"Ensure that numerator is divisible by the denominator.\"\"\"\n    assert numerator % denominator == 0, \"{} is not divisible by {}\".format(\n        numerator, denominator)\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.get_dp_rank","title":"fastvideo.distributed.get_dp_rank","text":"<pre><code>get_dp_rank() -&gt; int\n</code></pre> <p>Return my rank for the data parallel group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_dp_rank() -&gt; int:\n    \"\"\"Return my rank for the data parallel group.\"\"\"\n    return get_dp_group().rank_in_group\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.get_dp_world_size","title":"fastvideo.distributed.get_dp_world_size","text":"<pre><code>get_dp_world_size() -&gt; int\n</code></pre> <p>Return world size for the data parallel group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_dp_world_size() -&gt; int:\n    \"\"\"Return world size for the data parallel group.\"\"\"\n    return get_dp_group().world_size\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.get_local_torch_device","title":"fastvideo.distributed.get_local_torch_device","text":"<pre><code>get_local_torch_device() -&gt; torch.device\n</code></pre> <p>Return the torch device for the current rank.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_local_torch_device() -&gt; torch.device:\n    \"\"\"Return the torch device for the current rank.\"\"\"\n    from fastvideo.platforms import current_platform\n    if current_platform.is_npu():\n        device = torch.device(f\"npu:{envs.LOCAL_RANK}\")\n    elif current_platform.is_cuda_alike() or current_platform.is_cuda():\n        device = torch.device(f\"cuda:{envs.LOCAL_RANK}\")\n    else:\n        device = torch.device(\"mps\")\n    return device\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.get_sp_parallel_rank","title":"fastvideo.distributed.get_sp_parallel_rank","text":"<pre><code>get_sp_parallel_rank() -&gt; int\n</code></pre> <p>Return my rank for the sequence model parallel group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_sp_parallel_rank() -&gt; int:\n    \"\"\"Return my rank for the sequence model parallel group.\"\"\"\n    return get_sp_group().rank_in_group\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.get_sp_world_size","title":"fastvideo.distributed.get_sp_world_size","text":"<pre><code>get_sp_world_size() -&gt; int\n</code></pre> <p>Return world size for the sequence model parallel group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_sp_world_size() -&gt; int:\n    \"\"\"Return world size for the sequence model parallel group.\"\"\"\n    return get_sp_group().world_size\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.get_tp_rank","title":"fastvideo.distributed.get_tp_rank","text":"<pre><code>get_tp_rank() -&gt; int\n</code></pre> <p>Return my rank for the tensor model parallel group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_tp_rank() -&gt; int:\n    \"\"\"Return my rank for the tensor model parallel group.\"\"\"\n    return get_tp_group().rank_in_group\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.get_tp_world_size","title":"fastvideo.distributed.get_tp_world_size","text":"<pre><code>get_tp_world_size() -&gt; int\n</code></pre> <p>Return world size for the tensor model parallel group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_tp_world_size() -&gt; int:\n    \"\"\"Return world size for the tensor model parallel group.\"\"\"\n    return get_tp_group().world_size\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.get_world_rank","title":"fastvideo.distributed.get_world_rank","text":"<pre><code>get_world_rank() -&gt; int\n</code></pre> <p>Return my rank for the world group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_world_rank() -&gt; int:\n    \"\"\"Return my rank for the world group.\"\"\"\n    return get_world_group().rank\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.get_world_size","title":"fastvideo.distributed.get_world_size","text":"<pre><code>get_world_size() -&gt; int\n</code></pre> <p>Return world size for the world group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_world_size() -&gt; int:\n    \"\"\"Return world size for the world group.\"\"\"\n    return get_world_group().world_size\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.init_logger","title":"fastvideo.distributed.init_logger","text":"<pre><code>init_logger(name: str) -&gt; _FastvideoLogger\n</code></pre> <p>The main purpose of this function is to ensure that loggers are retrieved in such a way that we can be sure the root fastvideo logger has already been configured.</p> Source code in <code>fastvideo/logger.py</code> <pre><code>def init_logger(name: str) -&gt; _FastvideoLogger:\n    \"\"\"The main purpose of this function is to ensure that loggers are\n    retrieved in such a way that we can be sure the root fastvideo logger has\n    already been configured.\"\"\"\n\n    logger = logging.getLogger(name)\n\n    methods_to_patch = {\n        \"info_once\": _print_info_once,\n        \"warning_once\": _print_warning_once,\n        \"info\": _info,\n    }\n\n    for method_name, method in methods_to_patch.items():\n        setattr(logger, method_name,\n                MethodType(method, logger))  # type: ignore[arg-type]\n\n    return cast(_FastvideoLogger, logger)\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.initialize_model_parallel","title":"fastvideo.distributed.initialize_model_parallel","text":"<pre><code>initialize_model_parallel(\n    tensor_model_parallel_size: int = 1,\n    sequence_model_parallel_size: int = 1,\n    data_parallel_size: int = 1,\n    backend: str | None = None,\n) -&gt; None\n</code></pre> <p>Initialize model parallel groups.</p> <p>Parameters:</p> Name Type Description Default <code>tensor_model_parallel_size</code> <code>int</code> <p>number of GPUs used for tensor model parallelism (used for language encoder).</p> <code>1</code> <code>sequence_model_parallel_size</code> <code>int</code> <p>number of GPUs used for sequence model parallelism (used for DiT).</p> <code>1</code> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def initialize_model_parallel(\n    tensor_model_parallel_size: int = 1,\n    sequence_model_parallel_size: int = 1,\n    data_parallel_size: int = 1,\n    backend: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Initialize model parallel groups.\n\n    Arguments:\n        tensor_model_parallel_size: number of GPUs used for tensor model\n            parallelism (used for language encoder).\n        sequence_model_parallel_size: number of GPUs used for sequence model\n            parallelism (used for DiT).\n    \"\"\"\n    # Get world size and rank. Ensure some consistencies.\n    assert _WORLD is not None, \"world group is not initialized, please call init_distributed_environment first\"\n    world_size: int = get_world_size()\n    backend = backend or torch.distributed.get_backend(\n        get_world_group().device_group)\n    assert world_size &gt;= tensor_model_parallel_size, f\"world_size({world_size}) must be greater than or equal to tensor_model_parallel_size({tensor_model_parallel_size})\"\n    num_tensor_model_parallel_groups: int = (world_size //\n                                             tensor_model_parallel_size)\n    global _TP\n    assert _TP is None, (\"tensor model parallel group is already initialized\")\n    group_ranks = []\n    for i in range(num_tensor_model_parallel_groups):\n        ranks = list(\n            range(i * tensor_model_parallel_size,\n                  (i + 1) * tensor_model_parallel_size))\n        group_ranks.append(ranks)\n\n    # message queue broadcaster is only used in tensor model parallel group\n    _TP = init_model_parallel_group(group_ranks,\n                                    get_world_group().local_rank,\n                                    backend,\n                                    use_message_queue_broadcaster=True,\n                                    group_name=\"tp\")\n\n    # Build the sequence model-parallel groups.\n    num_sequence_model_parallel_groups: int = (world_size //\n                                               sequence_model_parallel_size)\n    global _SP\n    assert _SP is None, (\"sequence model parallel group is already initialized\")\n    group_ranks = []\n\n    # Since SP is incompatible with TP and PP, we can use a simpler group creation logic\n    for i in range(num_sequence_model_parallel_groups):\n        # Create groups of consecutive ranks\n        ranks = list(\n            range(i * sequence_model_parallel_size,\n                  (i + 1) * sequence_model_parallel_size))\n        group_ranks.append(ranks)\n\n    _SP = init_model_parallel_group(group_ranks,\n                                    get_world_group().local_rank,\n                                    backend,\n                                    group_name=\"sp\")\n\n    # Build the data parallel groups.\n    num_data_parallel_groups: int = sequence_model_parallel_size\n    global _DP\n    assert _DP is None, (\"data parallel group is already initialized\")\n    group_ranks = []\n\n    for i in range(num_data_parallel_groups):\n        ranks = list(range(i, world_size, num_data_parallel_groups))\n        group_ranks.append(ranks)\n\n    _DP = init_model_parallel_group(group_ranks,\n                                    get_world_group().local_rank,\n                                    backend,\n                                    group_name=\"dp\")\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.model_parallel_is_initialized","title":"fastvideo.distributed.model_parallel_is_initialized","text":"<pre><code>model_parallel_is_initialized() -&gt; bool\n</code></pre> <p>Check if tensor, sequence parallel groups are initialized.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def model_parallel_is_initialized() -&gt; bool:\n    \"\"\"Check if tensor, sequence parallel groups are initialized.\"\"\"\n    return _TP is not None and _SP is not None and _DP is not None\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.sequence_model_parallel_all_gather","title":"fastvideo.distributed.sequence_model_parallel_all_gather","text":"<pre><code>sequence_model_parallel_all_gather(\n    input_: Tensor, dim: int = -1\n) -&gt; torch.Tensor\n</code></pre> <p>All-gather the input tensor across model parallel group.</p> Source code in <code>fastvideo/distributed/communication_op.py</code> <pre><code>def sequence_model_parallel_all_gather(input_: torch.Tensor,\n                                       dim: int = -1) -&gt; torch.Tensor:\n    \"\"\"All-gather the input tensor across model parallel group.\"\"\"\n    return get_sp_group().all_gather(input_, dim)\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.sequence_model_parallel_all_to_all_4D","title":"fastvideo.distributed.sequence_model_parallel_all_to_all_4D","text":"<pre><code>sequence_model_parallel_all_to_all_4D(\n    input_: Tensor,\n    scatter_dim: int = 2,\n    gather_dim: int = 1,\n) -&gt; torch.Tensor\n</code></pre> <p>All-to-all communication of 4D tensors (e.g. QKV matrices) across sequence parallel group.</p> Source code in <code>fastvideo/distributed/communication_op.py</code> <pre><code>def sequence_model_parallel_all_to_all_4D(input_: torch.Tensor,\n                                          scatter_dim: int = 2,\n                                          gather_dim: int = 1) -&gt; torch.Tensor:\n    \"\"\"All-to-all communication of 4D tensors (e.g. QKV matrices) across sequence parallel group.\"\"\"\n    return get_sp_group().all_to_all_4D(input_, scatter_dim, gather_dim)\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.split_tensor_along_last_dim","title":"fastvideo.distributed.split_tensor_along_last_dim","text":"<pre><code>split_tensor_along_last_dim(\n    tensor: Tensor,\n    num_partitions: int,\n    contiguous_split_chunks: bool = False,\n) -&gt; Sequence[torch.Tensor]\n</code></pre> <p>Split a tensor along its last dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>input tensor.</p> required <code>num_partitions</code> <code>int</code> <p>number of partitions to split the tensor</p> required <code>contiguous_split_chunks</code> <code>bool</code> <p>If True, make each chunk contiguous                      in memory.</p> <code>False</code> <p>Returns:</p> Type Description <code>Sequence[Tensor]</code> <p>A list of Tensors</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def split_tensor_along_last_dim(\n    tensor: torch.Tensor,\n    num_partitions: int,\n    contiguous_split_chunks: bool = False,\n) -&gt; Sequence[torch.Tensor]:\n    \"\"\" Split a tensor along its last dimension.\n\n        Arguments:\n            tensor: input tensor.\n            num_partitions: number of partitions to split the tensor\n            contiguous_split_chunks: If True, make each chunk contiguous\n                                     in memory.\n\n        Returns:\n            A list of Tensors\n    \"\"\"\n    # Get the size and dimension.\n    last_dim = tensor.dim() - 1\n    last_dim_size = divide(tensor.size()[last_dim], num_partitions)\n    # Split.\n    tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)\n    # NOTE: torch.split does not create contiguous tensors by default.\n    if contiguous_split_chunks:\n        return tuple(chunk.contiguous() for chunk in tensor_list)\n\n    return tuple(tensor_list)\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.tensor_model_parallel_all_gather","title":"fastvideo.distributed.tensor_model_parallel_all_gather","text":"<pre><code>tensor_model_parallel_all_gather(\n    input_: Tensor, dim: int = -1\n) -&gt; torch.Tensor\n</code></pre> <p>All-gather the input tensor across model parallel group.</p> Source code in <code>fastvideo/distributed/communication_op.py</code> <pre><code>def tensor_model_parallel_all_gather(input_: torch.Tensor,\n                                     dim: int = -1) -&gt; torch.Tensor:\n    \"\"\"All-gather the input tensor across model parallel group.\"\"\"\n    return get_tp_group().all_gather(input_, dim)\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.tensor_model_parallel_all_reduce","title":"fastvideo.distributed.tensor_model_parallel_all_reduce","text":"<pre><code>tensor_model_parallel_all_reduce(\n    input_: Tensor,\n) -&gt; torch.Tensor\n</code></pre> <p>All-reduce the input tensor across model parallel group.</p> Source code in <code>fastvideo/distributed/communication_op.py</code> <pre><code>def tensor_model_parallel_all_reduce(input_: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"All-reduce the input tensor across model parallel group.\"\"\"\n    return get_tp_group().all_reduce(input_)\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed-modules","title":"Modules","text":""},{"location":"api/fastvideo/distributed/#fastvideo.distributed.communication_op","title":"fastvideo.distributed.communication_op","text":""},{"location":"api/fastvideo/distributed/#fastvideo.distributed.communication_op-functions","title":"Functions","text":""},{"location":"api/fastvideo/distributed/#fastvideo.distributed.communication_op.sequence_model_parallel_all_gather","title":"fastvideo.distributed.communication_op.sequence_model_parallel_all_gather","text":"<pre><code>sequence_model_parallel_all_gather(\n    input_: Tensor, dim: int = -1\n) -&gt; torch.Tensor\n</code></pre> <p>All-gather the input tensor across model parallel group.</p> Source code in <code>fastvideo/distributed/communication_op.py</code> <pre><code>def sequence_model_parallel_all_gather(input_: torch.Tensor,\n                                       dim: int = -1) -&gt; torch.Tensor:\n    \"\"\"All-gather the input tensor across model parallel group.\"\"\"\n    return get_sp_group().all_gather(input_, dim)\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.communication_op.sequence_model_parallel_all_to_all_4D","title":"fastvideo.distributed.communication_op.sequence_model_parallel_all_to_all_4D","text":"<pre><code>sequence_model_parallel_all_to_all_4D(\n    input_: Tensor,\n    scatter_dim: int = 2,\n    gather_dim: int = 1,\n) -&gt; torch.Tensor\n</code></pre> <p>All-to-all communication of 4D tensors (e.g. QKV matrices) across sequence parallel group.</p> Source code in <code>fastvideo/distributed/communication_op.py</code> <pre><code>def sequence_model_parallel_all_to_all_4D(input_: torch.Tensor,\n                                          scatter_dim: int = 2,\n                                          gather_dim: int = 1) -&gt; torch.Tensor:\n    \"\"\"All-to-all communication of 4D tensors (e.g. QKV matrices) across sequence parallel group.\"\"\"\n    return get_sp_group().all_to_all_4D(input_, scatter_dim, gather_dim)\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.communication_op.tensor_model_parallel_all_gather","title":"fastvideo.distributed.communication_op.tensor_model_parallel_all_gather","text":"<pre><code>tensor_model_parallel_all_gather(\n    input_: Tensor, dim: int = -1\n) -&gt; torch.Tensor\n</code></pre> <p>All-gather the input tensor across model parallel group.</p> Source code in <code>fastvideo/distributed/communication_op.py</code> <pre><code>def tensor_model_parallel_all_gather(input_: torch.Tensor,\n                                     dim: int = -1) -&gt; torch.Tensor:\n    \"\"\"All-gather the input tensor across model parallel group.\"\"\"\n    return get_tp_group().all_gather(input_, dim)\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.communication_op.tensor_model_parallel_all_reduce","title":"fastvideo.distributed.communication_op.tensor_model_parallel_all_reduce","text":"<pre><code>tensor_model_parallel_all_reduce(\n    input_: Tensor,\n) -&gt; torch.Tensor\n</code></pre> <p>All-reduce the input tensor across model parallel group.</p> Source code in <code>fastvideo/distributed/communication_op.py</code> <pre><code>def tensor_model_parallel_all_reduce(input_: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"All-reduce the input tensor across model parallel group.\"\"\"\n    return get_tp_group().all_reduce(input_)\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.device_communicators","title":"fastvideo.distributed.device_communicators","text":""},{"location":"api/fastvideo/distributed/#fastvideo.distributed.device_communicators-modules","title":"Modules","text":""},{"location":"api/fastvideo/distributed/#fastvideo.distributed.device_communicators.base_device_communicator","title":"fastvideo.distributed.device_communicators.base_device_communicator","text":"Classes\u00b6 fastvideo.distributed.device_communicators.base_device_communicator.DeviceCommunicatorBase \u00b6 <pre><code>DeviceCommunicatorBase(\n    cpu_group: ProcessGroup,\n    device: device | None = None,\n    device_group: ProcessGroup | None = None,\n    unique_name: str = \"\",\n)\n</code></pre> <p>Base class for device-specific communicator with autograd support. It can use the <code>cpu_group</code> to initialize the communicator. If the device has PyTorch integration (PyTorch can recognize its communication backend), the <code>device_group</code> will also be given.</p> Source code in <code>fastvideo/distributed/device_communicators/base_device_communicator.py</code> <pre><code>def __init__(self,\n             cpu_group: ProcessGroup,\n             device: torch.device | None = None,\n             device_group: ProcessGroup | None = None,\n             unique_name: str = \"\"):\n    self.device = device or torch.device(\"cpu\")\n    self.cpu_group = cpu_group\n    self.device_group = device_group\n    self.unique_name = unique_name\n    self.rank = dist.get_rank(cpu_group)\n    self.world_size = dist.get_world_size(cpu_group)\n    self.ranks = dist.get_process_group_ranks(cpu_group)\n    self.global_rank = dist.get_rank()\n    self.global_world_size = dist.get_world_size()\n    self.rank_in_group = dist.get_group_rank(self.cpu_group,\n                                             self.global_rank)\n</code></pre> Functions\u00b6 fastvideo.distributed.device_communicators.base_device_communicator.DeviceCommunicatorBase.all_gather \u00b6 <pre><code>all_gather(input_: Tensor, dim: int = -1) -&gt; torch.Tensor\n</code></pre> <p>Performs an all_gather operation with gradient support.</p> Source code in <code>fastvideo/distributed/device_communicators/base_device_communicator.py</code> <pre><code>def all_gather(self, input_: torch.Tensor, dim: int = -1) -&gt; torch.Tensor:\n    \"\"\"Performs an all_gather operation with gradient support.\"\"\"\n    if dim &lt; 0:\n        dim += input_.dim()\n    return DistributedAutograd.AllGather.apply(self.device_group, input_,\n                                               self.world_size, dim)\n</code></pre> fastvideo.distributed.device_communicators.base_device_communicator.DeviceCommunicatorBase.all_reduce \u00b6 <pre><code>all_reduce(\n    input_: Tensor, op: ReduceOp | None = ReduceOp.SUM\n) -&gt; torch.Tensor\n</code></pre> <p>Performs an all_reduce operation with gradient support.</p> Source code in <code>fastvideo/distributed/device_communicators/base_device_communicator.py</code> <pre><code>def all_reduce(self,\n               input_: torch.Tensor,\n               op: dist.ReduceOp | None = ReduceOp.SUM) -&gt; torch.Tensor:\n    \"\"\"Performs an all_reduce operation with gradient support.\"\"\"\n    return DistributedAutograd.AllReduce.apply(self.device_group, input_,\n                                               op)\n</code></pre> fastvideo.distributed.device_communicators.base_device_communicator.DeviceCommunicatorBase.all_to_all_4D \u00b6 <pre><code>all_to_all_4D(\n    input_: Tensor,\n    scatter_dim: int = 2,\n    gather_dim: int = 1,\n) -&gt; torch.Tensor\n</code></pre> <p>Performs a 4D all-to-all operation with gradient support.</p> Source code in <code>fastvideo/distributed/device_communicators/base_device_communicator.py</code> <pre><code>def all_to_all_4D(self,\n                  input_: torch.Tensor,\n                  scatter_dim: int = 2,\n                  gather_dim: int = 1) -&gt; torch.Tensor:\n    \"\"\"Performs a 4D all-to-all operation with gradient support.\"\"\"\n    return DistributedAutograd.AllToAll4D.apply(self.device_group, input_,\n                                                self.world_size,\n                                                scatter_dim, gather_dim)\n</code></pre> fastvideo.distributed.device_communicators.base_device_communicator.DeviceCommunicatorBase.gather \u00b6 <pre><code>gather(\n    input_: Tensor, dst: int = 0, dim: int = -1\n) -&gt; torch.Tensor | None\n</code></pre> <p>NOTE: We assume that the input tensor is on the same device across all the ranks. NOTE: <code>dst</code> is the local rank of the destination rank.</p> Source code in <code>fastvideo/distributed/device_communicators/base_device_communicator.py</code> <pre><code>def gather(self,\n           input_: torch.Tensor,\n           dst: int = 0,\n           dim: int = -1) -&gt; torch.Tensor | None:\n    \"\"\"\n    NOTE: We assume that the input tensor is on the same device across\n    all the ranks.\n    NOTE: `dst` is the local rank of the destination rank.\n    \"\"\"\n    world_size = self.world_size\n    assert -input_.dim() &lt;= dim &lt; input_.dim(), (\n        f\"Invalid dim ({dim}) for input tensor with shape {input_.size()}\")\n    if dim &lt; 0:\n        # Convert negative dim to positive.\n        dim += input_.dim()\n\n    # Allocate output tensor.\n    if self.rank_in_group == dst:\n        gather_list = [torch.empty_like(input_) for _ in range(world_size)]\n    else:\n        gather_list = None\n    # Gather.\n    torch.distributed.gather(input_,\n                             gather_list,\n                             dst=self.ranks[dst],\n                             group=self.device_group)\n    if self.rank_in_group == dst:\n        output_tensor = torch.cat(gather_list, dim=dim)\n    else:\n        output_tensor = None\n    return output_tensor\n</code></pre> fastvideo.distributed.device_communicators.base_device_communicator.DeviceCommunicatorBase.recv \u00b6 <pre><code>recv(\n    size: Size, dtype: dtype, src: int | None = None\n) -&gt; torch.Tensor\n</code></pre> <p>Receives a tensor from the source rank.</p> Source code in <code>fastvideo/distributed/device_communicators/base_device_communicator.py</code> <pre><code>def recv(self,\n         size: torch.Size,\n         dtype: torch.dtype,\n         src: int | None = None) -&gt; torch.Tensor:\n    \"\"\"Receives a tensor from the source rank.\"\"\"\n    \"\"\"NOTE: `src` is the local rank of the source rank.\"\"\"\n    if src is None:\n        src = (self.rank_in_group - 1) % self.world_size\n\n    tensor = torch.empty(size, dtype=dtype, device=self.device)\n    torch.distributed.recv(tensor, self.ranks[src], self.device_group)\n    return tensor\n</code></pre> fastvideo.distributed.device_communicators.base_device_communicator.DeviceCommunicatorBase.send \u00b6 <pre><code>send(tensor: Tensor, dst: int | None = None) -&gt; None\n</code></pre> <p>Sends a tensor to the destination rank in a non-blocking way</p> Source code in <code>fastvideo/distributed/device_communicators/base_device_communicator.py</code> <pre><code>def send(self, tensor: torch.Tensor, dst: int | None = None) -&gt; None:\n    \"\"\"Sends a tensor to the destination rank in a non-blocking way\"\"\"\n    \"\"\"NOTE: `dst` is the local rank of the destination rank.\"\"\"\n    if dst is None:\n        dst = (self.rank_in_group + 1) % self.world_size\n    torch.distributed.send(tensor, self.ranks[dst], self.device_group)\n</code></pre> fastvideo.distributed.device_communicators.base_device_communicator.DistributedAutograd \u00b6 <p>Collection of autograd functions for distributed operations.</p> <p>This class provides custom autograd functions for distributed operations like all_reduce, all_gather, and all_to_all. Each operation is implemented as a static inner class with proper forward and backward implementations.</p> Classes\u00b6 fastvideo.distributed.device_communicators.base_device_communicator.DistributedAutograd.AllGather \u00b6 <p>               Bases: <code>Function</code></p> <p>Differentiable all_gather operation.</p> <p>The operation gathers tensors from all ranks and concatenates them along a specified dimension. The backward pass uses reduce_scatter to efficiently distribute gradients back to source ranks.</p> fastvideo.distributed.device_communicators.base_device_communicator.DistributedAutograd.AllReduce \u00b6 <p>               Bases: <code>Function</code></p> <p>Differentiable all_reduce operation.</p> <p>The gradient of all_reduce is another all_reduce operation since the operation combines values from all ranks equally.</p> fastvideo.distributed.device_communicators.base_device_communicator.DistributedAutograd.AllToAll4D \u00b6 <p>               Bases: <code>Function</code></p> <p>Differentiable all_to_all operation specialized for 4D tensors.</p> <p>This operation is particularly useful for attention operations where we need to redistribute data across ranks for efficient parallel processing.</p> <p>The operation supports two modes: 1. scatter_dim=2, gather_dim=1: Used for redistributing attention heads 2. scatter_dim=1, gather_dim=2: Used for redistributing sequence dimensions</p>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.device_communicators.cpu_communicator","title":"fastvideo.distributed.device_communicators.cpu_communicator","text":"Classes\u00b6 fastvideo.distributed.device_communicators.cpu_communicator.CpuCommunicator \u00b6 <pre><code>CpuCommunicator(\n    cpu_group: ProcessGroup,\n    device: device | None = None,\n    device_group: ProcessGroup | None = None,\n    unique_name: str = \"\",\n)\n</code></pre> <p>               Bases: <code>DeviceCommunicatorBase</code></p> Source code in <code>fastvideo/distributed/device_communicators/cpu_communicator.py</code> <pre><code>def __init__(self,\n             cpu_group: ProcessGroup,\n             device: torch.device | None = None,\n             device_group: ProcessGroup | None = None,\n             unique_name: str = \"\"):\n    super().__init__(cpu_group, device, device_group, unique_name)\n    self.dist_module = torch.distributed\n\n    from fastvideo.platforms import current_platform\n\n    if (current_platform.get_cpu_architecture()\n            == CpuArchEnum.X86) and hasattr(\n                torch.ops._C,\n                \"init_shm_manager\") and unique_name.startswith(\"tp\"):\n        self.dist_module = _CPUSHMDistributed(self)\n</code></pre> Functions\u00b6 fastvideo.distributed.device_communicators.cpu_communicator.CpuCommunicator.gather \u00b6 <pre><code>gather(\n    input_: Tensor, dst: int = 0, dim: int = -1\n) -&gt; torch.Tensor | None\n</code></pre> <p>NOTE: We assume that the input tensor is on the same device across all the ranks. NOTE: <code>dst</code> is the local rank of the destination rank.</p> Source code in <code>fastvideo/distributed/device_communicators/cpu_communicator.py</code> <pre><code>def gather(self,\n           input_: torch.Tensor,\n           dst: int = 0,\n           dim: int = -1) -&gt; torch.Tensor | None:\n    \"\"\"\n    NOTE: We assume that the input tensor is on the same device across\n    all the ranks.\n    NOTE: `dst` is the local rank of the destination rank.\n    \"\"\"\n    world_size = self.world_size\n    assert -input_.dim() &lt;= dim &lt; input_.dim(), (\n        f\"Invalid dim ({dim}) for input tensor with shape {input_.size()}\")\n    if dim &lt; 0:\n        # Convert negative dim to positive.\n        dim += input_.dim()\n\n    # Allocate output tensor.\n    if self.rank_in_group == dst:\n        gather_list = [torch.empty_like(input_) for _ in range(world_size)]\n    else:\n        gather_list = None\n\n    # Gather.\n    self.dist_module.gather(input_,\n                            gather_list,\n                            dst=self.ranks[dst],\n                            group=self.device_group)\n\n    if self.rank_in_group == dst:\n        output_tensor = torch.cat(gather_list, dim=dim)\n    else:\n        output_tensor = None\n    return output_tensor\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.device_communicators.cuda_communicator","title":"fastvideo.distributed.device_communicators.cuda_communicator","text":"Classes\u00b6 fastvideo.distributed.device_communicators.cuda_communicator.CudaCommunicator \u00b6 <pre><code>CudaCommunicator(\n    cpu_group: ProcessGroup,\n    device: device | None = None,\n    device_group: ProcessGroup | None = None,\n    unique_name: str = \"\",\n)\n</code></pre> <p>               Bases: <code>DeviceCommunicatorBase</code></p> Source code in <code>fastvideo/distributed/device_communicators/cuda_communicator.py</code> <pre><code>def __init__(self,\n             cpu_group: ProcessGroup,\n             device: torch.device | None = None,\n             device_group: ProcessGroup | None = None,\n             unique_name: str = \"\"):\n    super().__init__(cpu_group, device, device_group, unique_name)\n\n    from fastvideo.distributed.device_communicators.pynccl import (\n        PyNcclCommunicator)\n\n    self.pynccl_comm: PyNcclCommunicator | None = None\n    if self.world_size &gt; 1:\n        self.pynccl_comm = PyNcclCommunicator(\n            group=self.cpu_group,\n            device=self.device,\n        )\n</code></pre> Functions\u00b6 fastvideo.distributed.device_communicators.cuda_communicator.CudaCommunicator.recv \u00b6 <pre><code>recv(\n    size: Size, dtype: dtype, src: int | None = None\n) -&gt; torch.Tensor\n</code></pre> <p>Receives a tensor from the source rank.</p> Source code in <code>fastvideo/distributed/device_communicators/cuda_communicator.py</code> <pre><code>def recv(self,\n         size: torch.Size,\n         dtype: torch.dtype,\n         src: int | None = None) -&gt; torch.Tensor:\n    \"\"\"Receives a tensor from the source rank.\"\"\"\n    \"\"\"NOTE: `src` is the local rank of the source rank.\"\"\"\n    if src is None:\n        src = (self.rank_in_group - 1) % self.world_size\n\n    tensor = torch.empty(size, dtype=dtype, device=self.device)\n    pynccl_comm = self.pynccl_comm\n    if pynccl_comm is not None and not pynccl_comm.disabled:\n        pynccl_comm.recv(tensor, src)\n    else:\n        torch.distributed.recv(tensor, self.ranks[src], self.device_group)\n    return tensor\n</code></pre> fastvideo.distributed.device_communicators.cuda_communicator.CudaCommunicator.send \u00b6 <pre><code>send(tensor: Tensor, dst: int | None = None) -&gt; None\n</code></pre> <p>Sends a tensor to the destination rank in a non-blocking way</p> Source code in <code>fastvideo/distributed/device_communicators/cuda_communicator.py</code> <pre><code>def send(self, tensor: torch.Tensor, dst: int | None = None) -&gt; None:\n    \"\"\"Sends a tensor to the destination rank in a non-blocking way\"\"\"\n    \"\"\"NOTE: `dst` is the local rank of the destination rank.\"\"\"\n    if dst is None:\n        dst = (self.rank_in_group + 1) % self.world_size\n\n    pynccl_comm = self.pynccl_comm\n    if pynccl_comm is not None and not pynccl_comm.disabled:\n        pynccl_comm.send(tensor, dst)\n    else:\n        torch.distributed.send(tensor, self.ranks[dst], self.device_group)\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.device_communicators.npu_communicator","title":"fastvideo.distributed.device_communicators.npu_communicator","text":"Classes\u00b6 fastvideo.distributed.device_communicators.npu_communicator.NpuCommunicator \u00b6 <pre><code>NpuCommunicator(\n    cpu_group: ProcessGroup,\n    device: device | None = None,\n    device_group: ProcessGroup | None = None,\n    unique_name: str = \"\",\n)\n</code></pre> <p>               Bases: <code>DeviceCommunicatorBase</code></p> Source code in <code>fastvideo/distributed/device_communicators/npu_communicator.py</code> <pre><code>def __init__(self,\n             cpu_group: ProcessGroup,\n             device: torch.device | None = None,\n             device_group: ProcessGroup | None = None,\n             unique_name: str = \"\"):\n    super().__init__(cpu_group, device, device_group, unique_name)\n\n    from fastvideo.distributed.device_communicators.pyhccl import (\n        PyHcclCommunicator)\n\n    self.pyhccl_comm: PyHcclCommunicator | None = None\n    if self.world_size &gt; 1:\n        self.pyhccl_comm = PyHcclCommunicator(\n            group=self.cpu_group,\n            device=self.device,\n        )\n</code></pre> Functions\u00b6 fastvideo.distributed.device_communicators.npu_communicator.NpuCommunicator.recv \u00b6 <pre><code>recv(\n    size: Size, dtype: dtype, src: int | None = None\n) -&gt; torch.Tensor\n</code></pre> <p>Receives a tensor from the source rank.</p> Source code in <code>fastvideo/distributed/device_communicators/npu_communicator.py</code> <pre><code>def recv(self,\n         size: torch.Size,\n         dtype: torch.dtype,\n         src: int | None = None) -&gt; torch.Tensor:\n    \"\"\"Receives a tensor from the source rank.\"\"\"\n    \"\"\"NOTE: `src` is the local rank of the source rank.\"\"\"\n    if src is None:\n        src = (self.rank_in_group - 1) % self.world_size\n\n    tensor = torch.empty(size, dtype=dtype, device=self.device)\n    pyhccl_comm = self.pyhccl_comm\n    if pyhccl_comm is not None and not pyhccl_comm.disabled:\n        pyhccl_comm.recv(tensor, src)\n    else:\n        torch.distributed.recv(tensor, self.ranks[src], self.device_group)\n    return tensor\n</code></pre> fastvideo.distributed.device_communicators.npu_communicator.NpuCommunicator.send \u00b6 <pre><code>send(tensor: Tensor, dst: int | None = None) -&gt; None\n</code></pre> <p>Sends a tensor to the destination rank in a non-blocking way</p> Source code in <code>fastvideo/distributed/device_communicators/npu_communicator.py</code> <pre><code>def send(self, tensor: torch.Tensor, dst: int | None = None) -&gt; None:\n    \"\"\"Sends a tensor to the destination rank in a non-blocking way\"\"\"\n    \"\"\"NOTE: `dst` is the local rank of the destination rank.\"\"\"\n    if dst is None:\n        dst = (self.rank_in_group + 1) % self.world_size\n\n    pyhccl_comm = self.pyhccl_comm\n    if pyhccl_comm is not None and not pyhccl_comm.disabled:\n        pyhccl_comm.send(tensor, dst)\n    else:\n        torch.distributed.send(tensor, self.ranks[dst], self.device_group)\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.device_communicators.pyhccl","title":"fastvideo.distributed.device_communicators.pyhccl","text":"Classes\u00b6 fastvideo.distributed.device_communicators.pyhccl.PyHcclCommunicator \u00b6 <pre><code>PyHcclCommunicator(\n    group: ProcessGroup | StatelessProcessGroup,\n    device: int | str | device,\n    library_path: str | None = None,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>ProcessGroup | StatelessProcessGroup</code> <p>the process group to work on. If None, it will use the default process group.</p> required <code>device</code> <code>int | str | device</code> <p>the device to bind the PyHcclCommunicator to. If None, it will be bind to f\"npu:{local_rank}\".</p> required <code>library_path</code> <code>str | None</code> <p>the path to the HCCL library. If None, it will use the default library path.</p> <code>None</code> <p>It is the caller's responsibility to make sure each communicator is bind to a unique device.</p> Source code in <code>fastvideo/distributed/device_communicators/pyhccl.py</code> <pre><code>def __init__(\n    self,\n    group: ProcessGroup | StatelessProcessGroup,\n    device: int | str | torch.device,\n    library_path: str | None = None,\n):\n    \"\"\"\n    Args:\n        group: the process group to work on. If None, it will use the\n            default process group.\n        device: the device to bind the PyHcclCommunicator to. If None,\n            it will be bind to f\"npu:{local_rank}\".\n        library_path: the path to the HCCL library. If None, it will\n            use the default library path.\n    It is the caller's responsibility to make sure each communicator\n    is bind to a unique device.\n    \"\"\"\n\n    if not isinstance(group, StatelessProcessGroup):\n        assert dist.is_initialized()\n        assert dist.get_backend(group) != dist.Backend.HCCL, (\n            \"PyHcclCommunicator should be attached to a non-HCCL group.\")\n        # note: this rank is the rank in the group\n        self.rank = dist.get_rank(group)\n        self.world_size = dist.get_world_size(group)\n    else:\n        self.rank = group.rank\n        self.world_size = group.world_size\n\n    self.group = group\n\n    # if world_size == 1, no need to create communicator\n    if self.world_size == 1:\n        self.available = False\n        self.disabled = True\n        return\n\n    try:\n        self.hccl = HCCLLibrary(library_path)\n    except Exception:\n        logger.warning(\"disable hccl because of missing HCCL library\")\n        # disable because of missing HCCL library\n        # e.g. in a non-NPU environment\n        self.available = False\n        self.disabled = True\n        return\n\n    self.available = True\n    self.disabled = False\n\n    logger.info(\"FastVideo is using pyhccl\")\n\n    if isinstance(device, int):\n        device = torch.device(f\"npu:{device}\")\n    elif isinstance(device, str):\n        device = torch.device(device)\n    # now `device` is a `torch.device` object\n    assert isinstance(device, torch.device)\n    self.device = device\n\n    if self.rank == 0:\n        # get the unique id from HCCL\n        with torch.npu.device(device):\n            self.unique_id = self.hccl.hcclGetUniqueId()\n    else:\n        # construct an empty unique id\n        self.unique_id = hcclUniqueId()\n\n    if not isinstance(group, StatelessProcessGroup):\n        tensor = torch.ByteTensor(list(self.unique_id.internal))\n        ranks = dist.get_process_group_ranks(group)\n        # arg `src` in `broadcast` is the global rank\n        dist.broadcast(tensor, src=ranks[0], group=group)\n        byte_list = tensor.tolist()\n        for i, byte in enumerate(byte_list):\n            self.unique_id.internal[i] = byte\n    else:\n        self.unique_id = group.broadcast_obj(self.unique_id, src=0)\n\n    # hccl communicator and stream will use this device\n    # `torch.npu.device` is a context manager that changes the\n    # current npu device to the specified one\n    with torch.npu.device(device):\n        self.comm: hcclComm_t = self.hccl.hcclCommInitRank(\n            self.world_size, self.unique_id, self.rank)\n\n        stream = current_stream()\n        # A small all_reduce for warmup.\n        data = torch.zeros(1, device=device)\n        self.all_reduce(data)\n        stream.synchronize()\n        del data\n</code></pre> Functions\u00b6 Functions\u00b6"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.device_communicators.pynccl","title":"fastvideo.distributed.device_communicators.pynccl","text":"Classes\u00b6 fastvideo.distributed.device_communicators.pynccl.PyNcclCommunicator \u00b6 <pre><code>PyNcclCommunicator(\n    group: ProcessGroup | StatelessProcessGroup,\n    device: int | str | device,\n    library_path: str | None = None,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>ProcessGroup | StatelessProcessGroup</code> <p>the process group to work on. If None, it will use the default process group.</p> required <code>device</code> <code>int | str | device</code> <p>the device to bind the PyNcclCommunicator to. If None, it will be bind to f\"cuda:{local_rank}\".</p> required <code>library_path</code> <code>str | None</code> <p>the path to the NCCL library. If None, it will use the default library path.</p> <code>None</code> <p>It is the caller's responsibility to make sure each communicator is bind to a unique device.</p> Source code in <code>fastvideo/distributed/device_communicators/pynccl.py</code> <pre><code>def __init__(\n    self,\n    group: ProcessGroup | StatelessProcessGroup,\n    device: int | str | torch.device,\n    library_path: str | None = None,\n):\n    \"\"\"\n    Args:\n        group: the process group to work on. If None, it will use the\n            default process group.\n        device: the device to bind the PyNcclCommunicator to. If None,\n            it will be bind to f\"cuda:{local_rank}\".\n        library_path: the path to the NCCL library. If None, it will\n            use the default library path.\n    It is the caller's responsibility to make sure each communicator\n    is bind to a unique device.\n    \"\"\"\n    if not isinstance(group, StatelessProcessGroup):\n        assert dist.is_initialized()\n        assert dist.get_backend(group) != dist.Backend.NCCL, (\n            \"PyNcclCommunicator should be attached to a non-NCCL group.\")\n        # note: this rank is the rank in the group\n        self.rank = dist.get_rank(group)\n        self.world_size = dist.get_world_size(group)\n    else:\n        self.rank = group.rank\n        self.world_size = group.world_size\n\n    self.group = group\n\n    # if world_size == 1, no need to create communicator\n    if self.world_size == 1:\n        self.available = False\n        self.disabled = True\n        return\n    try:\n        self.nccl = NCCLLibrary(library_path)\n    except Exception:\n        # disable because of missing NCCL library\n        # e.g. in a non-GPU environment\n        self.available = False\n        self.disabled = True\n        return\n\n    self.available = True\n    self.disabled = False\n\n    logger.info(\"FastVideo is using nccl==%s\", self.nccl.ncclGetVersion())\n\n    if self.rank == 0:\n        # get the unique id from NCCL\n        self.unique_id = self.nccl.ncclGetUniqueId()\n    else:\n        # construct an empty unique id\n        self.unique_id = ncclUniqueId()\n\n    if not isinstance(group, StatelessProcessGroup):\n        tensor = torch.ByteTensor(list(self.unique_id.internal))\n        ranks = dist.get_process_group_ranks(group)\n        # arg `src` in `broadcast` is the global rank\n        dist.broadcast(tensor, src=ranks[0], group=group)\n        byte_list = tensor.tolist()\n        for i, byte in enumerate(byte_list):\n            self.unique_id.internal[i] = byte\n    else:\n        self.unique_id = group.broadcast_obj(self.unique_id, src=0)\n    if isinstance(device, int):\n        device = torch.device(f\"cuda:{device}\")\n    elif isinstance(device, str):\n        device = torch.device(device)\n    # now `device` is a `torch.device` object\n    assert isinstance(device, torch.device)\n    self.device = device\n    # nccl communicator and stream will use this device\n    # `torch.cuda.device` is a context manager that changes the\n    # current cuda device to the specified one\n    with torch.cuda.device(device):\n        self.comm: ncclComm_t = self.nccl.ncclCommInitRank(\n            self.world_size, self.unique_id, self.rank)\n\n        stream = current_stream()\n        # A small all_reduce for warmup.\n        data = torch.zeros(1, device=device)\n        self.all_reduce(data)\n        if stream is not None:\n            stream.synchronize()\n        del data\n</code></pre> Functions\u00b6 Functions\u00b6"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.parallel_state","title":"fastvideo.distributed.parallel_state","text":"<p>FastVideo distributed state. It takes over the control of the distributed environment from PyTorch. The typical workflow is:</p> <ul> <li>call <code>init_distributed_environment</code> to initialize the distributed environment.</li> <li> <p>call <code>initialize_model_parallel</code> or <code>ensure_model_parallel_initialized</code> to  initialize the model parallel groups.</p> </li> <li> <p>any code dealing with the distributed stuff</p> </li> <li> <p>call <code>destroy_model_parallel</code> to destroy the model parallel groups.</p> </li> <li>call <code>destroy_distributed_environment</code> to destroy the distributed environment.</li> </ul> <p>If you only need to use the distributed environment without model parallelism,  you can skip the model parallel initialization and destruction steps.</p>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.parallel_state-classes","title":"Classes","text":""},{"location":"api/fastvideo/distributed/#fastvideo.distributed.parallel_state.GroupCoordinator","title":"fastvideo.distributed.parallel_state.GroupCoordinator","text":"<pre><code>GroupCoordinator(\n    group_ranks: list[list[int]],\n    local_rank: int,\n    torch_distributed_backend: str | Backend,\n    use_device_communicator: bool,\n    use_message_queue_broadcaster: bool = False,\n    group_name: str | None = None,\n)\n</code></pre> <p>PyTorch ProcessGroup wrapper for a group of processes. PyTorch ProcessGroup is bound to one specific communication backend,     e.g. NCCL, Gloo, MPI, etc. GroupCoordinator takes charge of all the communication operations among     the processes in the group. It manages both CPU and device     communication.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def __init__(\n    self,\n    group_ranks: list[list[int]],\n    local_rank: int,\n    torch_distributed_backend: str | Backend,\n    use_device_communicator: bool,\n    use_message_queue_broadcaster: bool = False,\n    group_name: str | None = None,\n):\n    group_name = group_name or \"anonymous\"\n    self.unique_name = _get_unique_name(group_name)\n    _register_group(self)\n\n    self.rank = torch.distributed.get_rank()\n    self.local_rank = local_rank\n    self.device_group = None\n    self.cpu_group = None\n\n    for ranks in group_ranks:\n        device_group = torch.distributed.new_group(\n            ranks, backend=torch_distributed_backend)\n        # a group with `gloo` backend, to allow direct coordination between\n        # processes through the CPU.\n        cpu_group = torch.distributed.new_group(ranks, backend=\"gloo\")\n        if self.rank in ranks:\n            self.ranks = ranks\n            self.world_size = len(ranks)\n            self.rank_in_group = ranks.index(self.rank)\n            self.device_group = device_group\n            self.cpu_group = cpu_group\n    try:\n        assert self.cpu_group is not None\n        assert self.device_group is not None\n    except Exception as e:\n        print(f\"rank: {self.rank} group not found\")\n        raise e\n\n    from fastvideo.platforms import current_platform\n\n    # TODO: fix it for other platforms\n    self.device = get_local_torch_device()\n\n    self.use_device_communicator = use_device_communicator\n    self.device_communicator: DeviceCommunicatorBase = None  # type: ignore\n    if use_device_communicator and self.world_size &gt; 1:\n        # Platform-aware device communicator selection\n        if current_platform.is_cuda_alike():\n            from fastvideo.distributed.device_communicators.cuda_communicator import (\n                CudaCommunicator)\n            self.device_communicator = CudaCommunicator(\n                cpu_group=self.cpu_group,\n                device=self.device,\n                device_group=self.device_group,\n                unique_name=self.unique_name,\n            )\n        elif current_platform.is_npu():\n            from fastvideo.distributed.device_communicators.npu_communicator import (\n                NpuCommunicator)\n            self.device_communicator = NpuCommunicator(\n                cpu_group=self.cpu_group,\n                device=self.device,\n                device_group=self.device_group,\n                unique_name=self.unique_name,\n            )\n        else:\n            # For MPS and CPU, use the CPU communicator\n            self.device_communicator = CpuCommunicator(\n                cpu_group=self.cpu_group,\n                device=self.device,\n                device_group=self.device_group,\n                unique_name=self.unique_name,\n            )\n\n    self.mq_broadcaster = None\n\n    from fastvideo.platforms import current_platform\n\n    # TODO(will): check if this is needed\n    # self.use_custom_op_call = current_platform.is_cuda_alike()\n    self.use_custom_op_call = False\n</code></pre> Attributes\u00b6 fastvideo.distributed.parallel_state.GroupCoordinator.first_rank <code>property</code> \u00b6 <pre><code>first_rank\n</code></pre> <p>Return the global rank of the first process in the group</p> fastvideo.distributed.parallel_state.GroupCoordinator.is_first_rank <code>property</code> \u00b6 <pre><code>is_first_rank\n</code></pre> <p>Return whether the caller is the first process in the group</p> fastvideo.distributed.parallel_state.GroupCoordinator.is_last_rank <code>property</code> \u00b6 <pre><code>is_last_rank\n</code></pre> <p>Return whether the caller is the last process in the group</p> fastvideo.distributed.parallel_state.GroupCoordinator.last_rank <code>property</code> \u00b6 <pre><code>last_rank\n</code></pre> <p>Return the global rank of the last process in the group</p> fastvideo.distributed.parallel_state.GroupCoordinator.next_rank <code>property</code> \u00b6 <pre><code>next_rank\n</code></pre> <p>Return the global rank of the process that follows the caller</p> fastvideo.distributed.parallel_state.GroupCoordinator.prev_rank <code>property</code> \u00b6 <pre><code>prev_rank\n</code></pre> <p>Return the global rank of the process that precedes the caller</p> Functions\u00b6 fastvideo.distributed.parallel_state.GroupCoordinator.all_reduce \u00b6 <pre><code>all_reduce(\n    input_: Tensor, op: ReduceOp | None = ReduceOp.SUM\n) -&gt; torch.Tensor\n</code></pre> <p>User-facing all-reduce function before we actually call the all-reduce operation.</p> <p>We need this because Dynamo does not support passing an arbitrary object (<code>self</code> in this case) to a custom op. We need to pass the  group name as a string, and then look up the group coordinator from  the group name, dispatch the all-reduce operation to the group  coordinator.</p> <p>In addition, PyTorch custom ops do not support mutation or returning a new tensor in the same op. So we always make the all-reduce operation out-of-place.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def all_reduce(\n        self,\n        input_: torch.Tensor,\n        op: torch.distributed.ReduceOp | None = ReduceOp.SUM\n) -&gt; torch.Tensor:\n    \"\"\"\n    User-facing all-reduce function before we actually call the\n    all-reduce operation.\n\n    We need this because Dynamo does not support passing an arbitrary\n    object (`self` in this case) to a custom op. We need to pass the\n     group name as a string, and then look up the group coordinator from\n     the group name, dispatch the all-reduce operation to the group\n     coordinator.\n\n    In addition, PyTorch custom ops do not support mutation or returning\n    a new tensor in the same op. So we always make the all-reduce operation\n    out-of-place.\n    \"\"\"\n    # Bypass the function if we are using only 1 GPU.\n    if self.world_size == 1:\n        return input_\n\n    if self.use_custom_op_call:\n        return torch.ops.vllm.all_reduce(input_,\n                                         group_name=self.unique_name)\n    else:\n        return self._all_reduce_out_place(input_, op=op)\n</code></pre> fastvideo.distributed.parallel_state.GroupCoordinator.barrier \u00b6 <pre><code>barrier() -&gt; None\n</code></pre> <p>Barrier synchronization among the group. NOTE: don't use <code>device_group</code> here! <code>barrier</code> in NCCL is terrible because it is internally a broadcast operation with secretly created GPU tensors. It is easy to mess up the current device. Use the CPU group instead.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def barrier(self) -&gt; None:\n    \"\"\"Barrier synchronization among the group.\n    NOTE: don't use `device_group` here! `barrier` in NCCL is\n    terrible because it is internally a broadcast operation with\n    secretly created GPU tensors. It is easy to mess up the current\n    device. Use the CPU group instead.\n    \"\"\"\n    torch.distributed.barrier(group=self.cpu_group)\n</code></pre> fastvideo.distributed.parallel_state.GroupCoordinator.broadcast \u00b6 <pre><code>broadcast(input_: Tensor, src: int = 0)\n</code></pre> <p>Broadcast the input tensor. NOTE: <code>src</code> is the local rank of the source rank.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def broadcast(self, input_: torch.Tensor, src: int = 0):\n    \"\"\"Broadcast the input tensor.\n    NOTE: `src` is the local rank of the source rank.\n    \"\"\"\n    assert src &lt; self.world_size, f\"Invalid src rank ({src})\"\n\n    # Bypass the function if we are using only 1 GPU.\n    if self.world_size == 1:\n        return input_\n    # Broadcast.\n    torch.distributed.broadcast(input_,\n                                src=self.ranks[src],\n                                group=self.device_group)\n    return input_\n</code></pre> fastvideo.distributed.parallel_state.GroupCoordinator.broadcast_object \u00b6 <pre><code>broadcast_object(obj: Any | None = None, src: int = 0)\n</code></pre> <p>Broadcast the input object. NOTE: <code>src</code> is the local rank of the source rank.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def broadcast_object(self, obj: Any | None = None, src: int = 0):\n    \"\"\"Broadcast the input object.\n    NOTE: `src` is the local rank of the source rank.\n    \"\"\"\n    assert src &lt; self.world_size, f\"Invalid src rank ({src})\"\n\n    # Bypass the function if we are using only 1 GPU.\n    if self.world_size == 1:\n        return obj\n    if self.mq_broadcaster is not None:\n        assert src == 0, \"Message queue broadcaster only supports src=0\"\n        return self.mq_broadcaster.broadcast_object(obj)\n    if self.rank_in_group == src:\n        torch.distributed.broadcast_object_list([obj],\n                                                src=self.ranks[src],\n                                                group=self.cpu_group)\n        return obj\n    else:\n        recv = [None]\n        torch.distributed.broadcast_object_list(recv,\n                                                src=self.ranks[src],\n                                                group=self.cpu_group)\n        return recv[0]\n</code></pre> fastvideo.distributed.parallel_state.GroupCoordinator.broadcast_object_list \u00b6 <pre><code>broadcast_object_list(\n    obj_list: list[Any],\n    src: int = 0,\n    group: ProcessGroup | None = None,\n)\n</code></pre> <p>Broadcast the input object list. NOTE: <code>src</code> is the local rank of the source rank.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def broadcast_object_list(self,\n                          obj_list: list[Any],\n                          src: int = 0,\n                          group: ProcessGroup | None = None):\n    \"\"\"Broadcast the input object list.\n    NOTE: `src` is the local rank of the source rank.\n    \"\"\"\n    assert src &lt; self.world_size, f\"Invalid src rank ({src})\"\n\n    # Bypass the function if we are using only 1 GPU.\n    if self.world_size == 1:\n        return obj_list\n    # Broadcast.\n    torch.distributed.broadcast_object_list(obj_list,\n                                            src=self.ranks[src],\n                                            group=self.device_group)\n    return obj_list\n</code></pre> fastvideo.distributed.parallel_state.GroupCoordinator.broadcast_tensor_dict \u00b6 <pre><code>broadcast_tensor_dict(\n    tensor_dict: dict[str, Tensor | Any] | None = None,\n    src: int = 0,\n    group: ProcessGroup | None = None,\n    metadata_group: ProcessGroup | None = None,\n) -&gt; dict[str, torch.Tensor | Any] | None\n</code></pre> <p>Broadcast the input tensor dictionary. NOTE: <code>src</code> is the local rank of the source rank.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def broadcast_tensor_dict(\n    self,\n    tensor_dict: dict[str, torch.Tensor | Any] | None = None,\n    src: int = 0,\n    group: ProcessGroup | None = None,\n    metadata_group: ProcessGroup | None = None\n) -&gt; dict[str, torch.Tensor | Any] | None:\n    \"\"\"Broadcast the input tensor dictionary.\n    NOTE: `src` is the local rank of the source rank.\n    \"\"\"\n    # Bypass the function if we are using only 1 GPU.\n    if (not torch.distributed.is_initialized() or self.world_size == 1):\n        return tensor_dict\n\n    group = self.device_group\n    metadata_group = self.cpu_group\n    assert src &lt; self.world_size, f\"Invalid src rank ({src})\"\n\n    rank_in_group = self.rank_in_group\n    if rank_in_group == src:\n        metadata_list: list[tuple[Any, Any]] = []\n        assert isinstance(\n            tensor_dict,\n            dict), (f\"Expecting a dictionary, got {type(tensor_dict)}\")\n        metadata_list, tensor_list = _split_tensor_dict(tensor_dict)\n        # `metadata_list` lives in CPU memory.\n        # `broadcast_object_list` has serialization &amp; deserialization,\n        # all happening on CPU. Therefore, we can use the CPU group.\n        self.broadcast_object(metadata_list, src=src)\n        async_handles = []\n        for tensor in tensor_list:\n            if tensor.numel() == 0:\n                # Skip broadcasting empty tensors.\n                continue\n            if tensor.is_cpu:\n                # use metadata_group for CPU tensors\n                handle = torch.distributed.broadcast(tensor,\n                                                     src=self.ranks[src],\n                                                     group=metadata_group,\n                                                     async_op=True)\n            else:\n                # use group for GPU tensors\n                handle = torch.distributed.broadcast(tensor,\n                                                     src=self.ranks[src],\n                                                     group=group,\n                                                     async_op=True)\n            async_handles.append(handle)\n        for async_handle in async_handles:\n            async_handle.wait()\n\n    else:\n        metadata_list = self.broadcast_object(None, src=src)\n        tensor_dict = {}\n        async_handles = []\n        for key, value in metadata_list:\n            if isinstance(value, TensorMetadata):\n                tensor = torch.empty(value.size,\n                                     dtype=value.dtype,\n                                     device=value.device)\n                if tensor.numel() == 0:\n                    # Skip broadcasting empty tensors.\n                    tensor_dict[key] = tensor\n                    continue\n                if tensor.is_cpu:\n                    # use metadata_group for CPU tensors\n                    handle = torch.distributed.broadcast(\n                        tensor,\n                        src=self.ranks[src],\n                        group=metadata_group,\n                        async_op=True)\n                else:\n                    # use group for GPU tensors\n                    handle = torch.distributed.broadcast(\n                        tensor,\n                        src=self.ranks[src],\n                        group=group,\n                        async_op=True)\n                async_handles.append(handle)\n                tensor_dict[key] = tensor\n            else:\n                tensor_dict[key] = value\n        for async_handle in async_handles:\n            async_handle.wait()\n    return tensor_dict\n</code></pre> fastvideo.distributed.parallel_state.GroupCoordinator.gather \u00b6 <pre><code>gather(\n    input_: Tensor, dst: int = 0, dim: int = -1\n) -&gt; torch.Tensor | None\n</code></pre> <p>NOTE: We assume that the input tensor is on the same device across all the ranks. NOTE: <code>dst</code> is the local rank of the destination rank.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def gather(self,\n           input_: torch.Tensor,\n           dst: int = 0,\n           dim: int = -1) -&gt; torch.Tensor | None:\n    \"\"\"\n    NOTE: We assume that the input tensor is on the same device across\n    all the ranks.\n    NOTE: `dst` is the local rank of the destination rank.\n    \"\"\"\n    world_size = self.world_size\n    # Bypass the function if we are using only 1 GPU.\n    if world_size == 1:\n        return input_\n    return self.device_communicator.gather(input_, dst, dim)\n</code></pre> fastvideo.distributed.parallel_state.GroupCoordinator.recv \u00b6 <pre><code>recv(\n    size: Size, dtype: dtype, src: int | None = None\n) -&gt; torch.Tensor\n</code></pre> <p>Receives a tensor from the source rank.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def recv(self,\n         size: torch.Size,\n         dtype: torch.dtype,\n         src: int | None = None) -&gt; torch.Tensor:\n    \"\"\"Receives a tensor from the source rank.\"\"\"\n    \"\"\"NOTE: `src` is the local rank of the source rank.\"\"\"\n    return self.device_communicator.recv(size, dtype, src)\n</code></pre> fastvideo.distributed.parallel_state.GroupCoordinator.recv_object \u00b6 <pre><code>recv_object(src: int) -&gt; Any\n</code></pre> <p>Receive the input object list from the source rank.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def recv_object(self, src: int) -&gt; Any:\n    \"\"\"Receive the input object list from the source rank.\"\"\"\n    \"\"\"NOTE: `src` is the local rank of the source rank.\"\"\"\n\n    assert src &lt; self.world_size, f\"Invalid src rank ({src})\"\n\n    assert src != self.rank_in_group, (\n        \"Invalid source rank. Source rank is the same as the current rank.\")\n\n    size_tensor = torch.empty(1, dtype=torch.long, device=\"cpu\")\n\n    # Receive object size\n    rank_size = torch.distributed.recv(size_tensor,\n                                       src=self.ranks[src],\n                                       group=self.cpu_group)\n\n    # Tensor to receive serialized objects into.\n    object_tensor = torch.empty(  # type: ignore[call-overload]\n        size_tensor.item(),  # type: ignore[arg-type]\n        dtype=torch.uint8,\n        device=\"cpu\")\n\n    rank_object = torch.distributed.recv(object_tensor,\n                                         src=self.ranks[src],\n                                         group=self.cpu_group)\n\n    assert rank_object == rank_size, (\n        \"Received object sender rank does not match the size sender rank.\")\n\n    obj = pickle.loads(object_tensor.numpy().tobytes())\n\n    return obj\n</code></pre> fastvideo.distributed.parallel_state.GroupCoordinator.recv_tensor_dict \u00b6 <pre><code>recv_tensor_dict(\n    src: int | None = None,\n    all_gather_group: Optional[GroupCoordinator] = None,\n) -&gt; dict[str, torch.Tensor | Any] | None\n</code></pre> <p>Recv the input tensor dictionary. NOTE: <code>src</code> is the local rank of the source rank.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def recv_tensor_dict(\n    self,\n    src: int | None = None,\n    all_gather_group: Optional[\"GroupCoordinator\"] = None,\n) -&gt; dict[str, torch.Tensor | Any] | None:\n    \"\"\"Recv the input tensor dictionary.\n    NOTE: `src` is the local rank of the source rank.\n    \"\"\"\n    # Bypass the function if we are using only 1 GPU.\n    if not torch.distributed.is_initialized() or self.world_size == 1:\n        return None\n\n    all_gather_size = (1 if all_gather_group is None else\n                       all_gather_group.world_size)\n    all_gather_rank = (0 if all_gather_group is None else\n                       all_gather_group.rank_in_group)\n\n    group = self.device_group\n    metadata_group = self.cpu_group\n\n    if src is None:\n        src = (self.rank_in_group - 1) % self.world_size\n    assert src &lt; self.world_size, f\"Invalid src rank ({src})\"\n\n    recv_metadata_list = self.recv_object(src=src)\n    tensor_dict: dict[str, Any] = {}\n    for key, value in recv_metadata_list:\n        if isinstance(value, TensorMetadata):\n            tensor = torch.empty(value.size,\n                                 dtype=value.dtype,\n                                 device=value.device)\n            if tensor.numel() == 0:\n                # Skip broadcasting empty tensors.\n                tensor_dict[key] = tensor\n                continue\n\n            # send-allgather: send only a slice, then do allgather.\n            use_all_gather = (all_gather_group is not None\n                              and tensor.numel() % all_gather_size == 0)\n\n            if use_all_gather:\n                orig_shape = tensor.shape\n                tensor = tensor.reshape(all_gather_size,\n                                        -1)[all_gather_rank]\n\n            if tensor.is_cpu:\n                # use metadata_group for CPU tensors\n                torch.distributed.recv(tensor,\n                                       src=self.ranks[src],\n                                       group=metadata_group)\n            else:\n                # use group for GPU tensors\n                torch.distributed.recv(tensor,\n                                       src=self.ranks[src],\n                                       group=group)\n            if use_all_gather:\n                # do the allgather\n                tensor = all_gather_group.all_gather(  # type: ignore\n                    tensor, dim=0)\n                tensor = tensor.reshape(orig_shape)\n\n            tensor_dict[key] = tensor\n        else:\n            tensor_dict[key] = value\n    return tensor_dict\n</code></pre> fastvideo.distributed.parallel_state.GroupCoordinator.send \u00b6 <pre><code>send(tensor: Tensor, dst: int | None = None) -&gt; None\n</code></pre> <p>Sends a tensor to the destination rank in a non-blocking way</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def send(self, tensor: torch.Tensor, dst: int | None = None) -&gt; None:\n    \"\"\"Sends a tensor to the destination rank in a non-blocking way\"\"\"\n    \"\"\"NOTE: `dst` is the local rank of the destination rank.\"\"\"\n    self.device_communicator.send(tensor, dst)\n</code></pre> fastvideo.distributed.parallel_state.GroupCoordinator.send_object \u00b6 <pre><code>send_object(obj: Any, dst: int) -&gt; None\n</code></pre> <p>Send the input object list to the destination rank.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def send_object(self, obj: Any, dst: int) -&gt; None:\n    \"\"\"Send the input object list to the destination rank.\"\"\"\n    \"\"\"NOTE: `dst` is the local rank of the destination rank.\"\"\"\n\n    assert dst &lt; self.world_size, f\"Invalid dst rank ({dst})\"\n\n    assert dst != self.rank_in_group, (\n        \"Invalid destination rank. Destination rank is the same \"\n        \"as the current rank.\")\n\n    # Serialize object to tensor and get the size as well\n    object_tensor = torch.frombuffer(pickle.dumps(obj), dtype=torch.uint8)\n\n    size_tensor = torch.tensor([object_tensor.numel()],\n                               dtype=torch.long,\n                               device=\"cpu\")\n\n    # Send object size\n\n    torch.distributed.send(size_tensor,\n                           dst=self.ranks[dst],\n                           group=self.cpu_group)\n\n    # Send object\n    torch.distributed.send(object_tensor,\n                           dst=self.ranks[dst],\n                           group=self.cpu_group)\n\n    return None\n</code></pre> fastvideo.distributed.parallel_state.GroupCoordinator.send_tensor_dict \u00b6 <pre><code>send_tensor_dict(\n    tensor_dict: dict[str, Tensor | Any],\n    dst: int | None = None,\n    all_gather_group: Optional[GroupCoordinator] = None,\n) -&gt; dict[str, torch.Tensor | Any] | None\n</code></pre> <p>Send the input tensor dictionary. NOTE: <code>dst</code> is the local rank of the source rank.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def send_tensor_dict(\n    self,\n    tensor_dict: dict[str, torch.Tensor | Any],\n    dst: int | None = None,\n    all_gather_group: Optional[\"GroupCoordinator\"] = None,\n) -&gt; dict[str, torch.Tensor | Any] | None:\n    \"\"\"Send the input tensor dictionary.\n    NOTE: `dst` is the local rank of the source rank.\n    \"\"\"\n    # Bypass the function if we are using only 1 GPU.\n    if not torch.distributed.is_initialized() or self.world_size == 1:\n        return tensor_dict\n\n    all_gather_size = (1 if all_gather_group is None else\n                       all_gather_group.world_size)\n    all_gather_rank = (0 if all_gather_group is None else\n                       all_gather_group.rank_in_group)\n\n    group = self.device_group\n    metadata_group = self.cpu_group\n\n    if dst is None:\n        dst = (self.rank_in_group + 1) % self.world_size\n    assert dst &lt; self.world_size, f\"Invalid dst rank ({dst})\"\n\n    metadata_list: list[tuple[Any, Any]] = []\n    assert isinstance(\n        tensor_dict,\n        dict), f\"Expecting a dictionary, got {type(tensor_dict)}\"\n    metadata_list, tensor_list = _split_tensor_dict(tensor_dict)\n    # `metadata_list` lives in CPU memory.\n    # `send_object_list` has serialization &amp; deserialization,\n    # all happening on CPU. Therefore, we can use the CPU group.\n    self.send_object(metadata_list, dst=dst)\n    for tensor in tensor_list:\n        if tensor.numel() == 0:\n            # Skip sending empty tensors.\n            continue\n\n        # send-allgather: send only a slice, then do allgather.\n        if (all_gather_group is not None\n                and tensor.numel() % all_gather_size == 0):\n            tensor = tensor.reshape(all_gather_size, -1)[all_gather_rank]\n\n        if tensor.is_cpu:\n            # use metadata_group for CPU tensors\n            torch.distributed.send(tensor,\n                                   dst=self.ranks[dst],\n                                   group=metadata_group)\n        else:\n            # use group for GPU tensors\n            torch.distributed.send(tensor, dst=self.ranks[dst], group=group)\n    return None\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.parallel_state-functions","title":"Functions","text":""},{"location":"api/fastvideo/distributed/#fastvideo.distributed.parallel_state.destroy_model_parallel","title":"fastvideo.distributed.parallel_state.destroy_model_parallel","text":"<pre><code>destroy_model_parallel() -&gt; None\n</code></pre> <p>Set the groups to none and destroy them.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def destroy_model_parallel() -&gt; None:\n    \"\"\"Set the groups to none and destroy them.\"\"\"\n    global _TP\n    if _TP:\n        _TP.destroy()\n    _TP = None\n\n    global _SP\n    if _SP:\n        _SP.destroy()\n    _SP = None\n\n    global _DP\n    if _DP:\n        _DP.destroy()\n    _DP = None\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.parallel_state.get_dp_rank","title":"fastvideo.distributed.parallel_state.get_dp_rank","text":"<pre><code>get_dp_rank() -&gt; int\n</code></pre> <p>Return my rank for the data parallel group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_dp_rank() -&gt; int:\n    \"\"\"Return my rank for the data parallel group.\"\"\"\n    return get_dp_group().rank_in_group\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.parallel_state.get_dp_world_size","title":"fastvideo.distributed.parallel_state.get_dp_world_size","text":"<pre><code>get_dp_world_size() -&gt; int\n</code></pre> <p>Return world size for the data parallel group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_dp_world_size() -&gt; int:\n    \"\"\"Return world size for the data parallel group.\"\"\"\n    return get_dp_group().world_size\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.parallel_state.get_local_torch_device","title":"fastvideo.distributed.parallel_state.get_local_torch_device","text":"<pre><code>get_local_torch_device() -&gt; torch.device\n</code></pre> <p>Return the torch device for the current rank.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_local_torch_device() -&gt; torch.device:\n    \"\"\"Return the torch device for the current rank.\"\"\"\n    from fastvideo.platforms import current_platform\n    if current_platform.is_npu():\n        device = torch.device(f\"npu:{envs.LOCAL_RANK}\")\n    elif current_platform.is_cuda_alike() or current_platform.is_cuda():\n        device = torch.device(f\"cuda:{envs.LOCAL_RANK}\")\n    else:\n        device = torch.device(\"mps\")\n    return device\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.parallel_state.get_sp_parallel_rank","title":"fastvideo.distributed.parallel_state.get_sp_parallel_rank","text":"<pre><code>get_sp_parallel_rank() -&gt; int\n</code></pre> <p>Return my rank for the sequence model parallel group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_sp_parallel_rank() -&gt; int:\n    \"\"\"Return my rank for the sequence model parallel group.\"\"\"\n    return get_sp_group().rank_in_group\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.parallel_state.get_sp_world_size","title":"fastvideo.distributed.parallel_state.get_sp_world_size","text":"<pre><code>get_sp_world_size() -&gt; int\n</code></pre> <p>Return world size for the sequence model parallel group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_sp_world_size() -&gt; int:\n    \"\"\"Return world size for the sequence model parallel group.\"\"\"\n    return get_sp_group().world_size\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.parallel_state.get_tp_rank","title":"fastvideo.distributed.parallel_state.get_tp_rank","text":"<pre><code>get_tp_rank() -&gt; int\n</code></pre> <p>Return my rank for the tensor model parallel group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_tp_rank() -&gt; int:\n    \"\"\"Return my rank for the tensor model parallel group.\"\"\"\n    return get_tp_group().rank_in_group\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.parallel_state.get_tp_world_size","title":"fastvideo.distributed.parallel_state.get_tp_world_size","text":"<pre><code>get_tp_world_size() -&gt; int\n</code></pre> <p>Return world size for the tensor model parallel group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_tp_world_size() -&gt; int:\n    \"\"\"Return world size for the tensor model parallel group.\"\"\"\n    return get_tp_group().world_size\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.parallel_state.get_world_rank","title":"fastvideo.distributed.parallel_state.get_world_rank","text":"<pre><code>get_world_rank() -&gt; int\n</code></pre> <p>Return my rank for the world group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_world_rank() -&gt; int:\n    \"\"\"Return my rank for the world group.\"\"\"\n    return get_world_group().rank\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.parallel_state.get_world_size","title":"fastvideo.distributed.parallel_state.get_world_size","text":"<pre><code>get_world_size() -&gt; int\n</code></pre> <p>Return world size for the world group.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def get_world_size() -&gt; int:\n    \"\"\"Return world size for the world group.\"\"\"\n    return get_world_group().world_size\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.parallel_state.initialize_model_parallel","title":"fastvideo.distributed.parallel_state.initialize_model_parallel","text":"<pre><code>initialize_model_parallel(\n    tensor_model_parallel_size: int = 1,\n    sequence_model_parallel_size: int = 1,\n    data_parallel_size: int = 1,\n    backend: str | None = None,\n) -&gt; None\n</code></pre> <p>Initialize model parallel groups.</p> <p>Parameters:</p> Name Type Description Default <code>tensor_model_parallel_size</code> <code>int</code> <p>number of GPUs used for tensor model parallelism (used for language encoder).</p> <code>1</code> <code>sequence_model_parallel_size</code> <code>int</code> <p>number of GPUs used for sequence model parallelism (used for DiT).</p> <code>1</code> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def initialize_model_parallel(\n    tensor_model_parallel_size: int = 1,\n    sequence_model_parallel_size: int = 1,\n    data_parallel_size: int = 1,\n    backend: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Initialize model parallel groups.\n\n    Arguments:\n        tensor_model_parallel_size: number of GPUs used for tensor model\n            parallelism (used for language encoder).\n        sequence_model_parallel_size: number of GPUs used for sequence model\n            parallelism (used for DiT).\n    \"\"\"\n    # Get world size and rank. Ensure some consistencies.\n    assert _WORLD is not None, \"world group is not initialized, please call init_distributed_environment first\"\n    world_size: int = get_world_size()\n    backend = backend or torch.distributed.get_backend(\n        get_world_group().device_group)\n    assert world_size &gt;= tensor_model_parallel_size, f\"world_size({world_size}) must be greater than or equal to tensor_model_parallel_size({tensor_model_parallel_size})\"\n    num_tensor_model_parallel_groups: int = (world_size //\n                                             tensor_model_parallel_size)\n    global _TP\n    assert _TP is None, (\"tensor model parallel group is already initialized\")\n    group_ranks = []\n    for i in range(num_tensor_model_parallel_groups):\n        ranks = list(\n            range(i * tensor_model_parallel_size,\n                  (i + 1) * tensor_model_parallel_size))\n        group_ranks.append(ranks)\n\n    # message queue broadcaster is only used in tensor model parallel group\n    _TP = init_model_parallel_group(group_ranks,\n                                    get_world_group().local_rank,\n                                    backend,\n                                    use_message_queue_broadcaster=True,\n                                    group_name=\"tp\")\n\n    # Build the sequence model-parallel groups.\n    num_sequence_model_parallel_groups: int = (world_size //\n                                               sequence_model_parallel_size)\n    global _SP\n    assert _SP is None, (\"sequence model parallel group is already initialized\")\n    group_ranks = []\n\n    # Since SP is incompatible with TP and PP, we can use a simpler group creation logic\n    for i in range(num_sequence_model_parallel_groups):\n        # Create groups of consecutive ranks\n        ranks = list(\n            range(i * sequence_model_parallel_size,\n                  (i + 1) * sequence_model_parallel_size))\n        group_ranks.append(ranks)\n\n    _SP = init_model_parallel_group(group_ranks,\n                                    get_world_group().local_rank,\n                                    backend,\n                                    group_name=\"sp\")\n\n    # Build the data parallel groups.\n    num_data_parallel_groups: int = sequence_model_parallel_size\n    global _DP\n    assert _DP is None, (\"data parallel group is already initialized\")\n    group_ranks = []\n\n    for i in range(num_data_parallel_groups):\n        ranks = list(range(i, world_size, num_data_parallel_groups))\n        group_ranks.append(ranks)\n\n    _DP = init_model_parallel_group(group_ranks,\n                                    get_world_group().local_rank,\n                                    backend,\n                                    group_name=\"dp\")\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.parallel_state.initialize_sequence_parallel_group","title":"fastvideo.distributed.parallel_state.initialize_sequence_parallel_group","text":"<pre><code>initialize_sequence_parallel_group(\n    sequence_model_parallel_size: int = 1,\n    backend: str | None = None,\n    group_name_suffix: str = \"\",\n) -&gt; GroupCoordinator\n</code></pre> <p>Initialize a sequence parallel group for a specific model.</p> <p>This function creates a sequence parallel group that can be used with the patch_sequence_parallel_group context manager. It allows different models to use different sequence parallelism configurations.</p> <p>Parameters:</p> Name Type Description Default <code>sequence_model_parallel_size</code> <code>int</code> <p>number of GPUs used for sequence model parallelism.</p> <code>1</code> <code>backend</code> <code>str | None</code> <p>communication backend to use.</p> <code>None</code> <code>group_name_suffix</code> <code>str</code> <p>optional suffix to make the group name unique.</p> <code>''</code> <p>Returns:</p> Type Description <code>GroupCoordinator</code> <p>A GroupCoordinator for sequence parallelism that can be used with</p> <code>GroupCoordinator</code> <p>the patch_sequence_parallel_group context manager.</p> Example usage <pre><code># Initialize sequence parallel group for model2\nsp_group_model2 = initialize_sequence_parallel_group(\n    sequence_model_parallel_size=2,\n    group_name_suffix=\"model2\"\n)\n\n# Use sequence parallelism for model2\nwith patch_sequence_parallel_group(sp_group_model2):\n    # Run model2 with sequence parallelism\n    output2 = model2(input2)\n</code></pre> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def initialize_sequence_parallel_group(\n        sequence_model_parallel_size: int = 1,\n        backend: str | None = None,\n        group_name_suffix: str = \"\") -&gt; GroupCoordinator:\n    \"\"\"Initialize a sequence parallel group for a specific model.\n\n    This function creates a sequence parallel group that can be used with the\n    patch_sequence_parallel_group context manager. It allows different models\n    to use different sequence parallelism configurations.\n\n    Arguments:\n        sequence_model_parallel_size: number of GPUs used for sequence model parallelism.\n        backend: communication backend to use.\n        group_name_suffix: optional suffix to make the group name unique.\n\n    Returns:\n        A GroupCoordinator for sequence parallelism that can be used with\n        the patch_sequence_parallel_group context manager.\n\n    Example usage:\n        ```python\n        # Initialize sequence parallel group for model2\n        sp_group_model2 = initialize_sequence_parallel_group(\n            sequence_model_parallel_size=2,\n            group_name_suffix=\"model2\"\n        )\n\n        # Use sequence parallelism for model2\n        with patch_sequence_parallel_group(sp_group_model2):\n            # Run model2 with sequence parallelism\n            output2 = model2(input2)\n        ```\n    \"\"\"\n    # Get world size and rank. Ensure some consistencies.\n    assert torch.distributed.is_initialized()\n    world_size: int = torch.distributed.get_world_size()\n    backend = backend or torch.distributed.get_backend(\n        get_world_group().device_group)\n\n    # Ensure the world size is compatible with the parallelism configuration\n    assert world_size % sequence_model_parallel_size == 0, \\\n        f\"World size ({world_size}) must be divisible by sequence_model_parallel_size ({sequence_model_parallel_size})\"\n\n    # Build the sequence model-parallel groups.\n    num_sequence_model_parallel_groups: int = (world_size //\n                                               sequence_model_parallel_size)\n    sp_group_ranks = []\n\n    for i in range(num_sequence_model_parallel_groups):\n        # Create groups of consecutive ranks\n        ranks = list(\n            range(i * sequence_model_parallel_size,\n                  (i + 1) * sequence_model_parallel_size))\n        sp_group_ranks.append(ranks)\n\n    # Create SP group coordinator with a unique name\n    group_name = f\"sp_{group_name_suffix}\" if group_name_suffix else \"sp\"\n    sp_group = init_model_parallel_group(sp_group_ranks,\n                                         get_world_group().local_rank,\n                                         backend,\n                                         group_name=group_name)\n\n    return sp_group\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.parallel_state.initialize_tensor_parallel_group","title":"fastvideo.distributed.parallel_state.initialize_tensor_parallel_group","text":"<pre><code>initialize_tensor_parallel_group(\n    tensor_model_parallel_size: int = 1,\n    backend: str | None = None,\n    group_name_suffix: str = \"\",\n) -&gt; GroupCoordinator\n</code></pre> <p>Initialize a tensor parallel group for a specific model.</p> <p>This function creates a tensor parallel group that can be used with the patch_tensor_parallel_group context manager. It allows different models to use different tensor parallelism configurations.</p> <p>Parameters:</p> Name Type Description Default <code>tensor_model_parallel_size</code> <code>int</code> <p>number of GPUs used for tensor model parallelism.</p> <code>1</code> <code>backend</code> <code>str | None</code> <p>communication backend to use.</p> <code>None</code> <code>group_name_suffix</code> <code>str</code> <p>optional suffix to make the group name unique.</p> <code>''</code> <p>Returns:</p> Type Description <code>GroupCoordinator</code> <p>A GroupCoordinator for tensor parallelism that can be used with</p> <code>GroupCoordinator</code> <p>the patch_tensor_parallel_group context manager.</p> Example usage <pre><code># Initialize tensor parallel group for model1\ntp_group_model1 = initialize_tensor_parallel_group(\n    tensor_model_parallel_size=4,\n    group_name_suffix=\"model1\"\n)\n\n# Use tensor parallelism for model1\nwith patch_tensor_parallel_group(tp_group_model1):\n    # Run model1 with tensor parallelism\n    output1 = model1(input1)\n</code></pre> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def initialize_tensor_parallel_group(\n        tensor_model_parallel_size: int = 1,\n        backend: str | None = None,\n        group_name_suffix: str = \"\") -&gt; GroupCoordinator:\n    \"\"\"Initialize a tensor parallel group for a specific model.\n\n    This function creates a tensor parallel group that can be used with the\n    patch_tensor_parallel_group context manager. It allows different models\n    to use different tensor parallelism configurations.\n\n    Arguments:\n        tensor_model_parallel_size: number of GPUs used for tensor model parallelism.\n        backend: communication backend to use.\n        group_name_suffix: optional suffix to make the group name unique.\n\n    Returns:\n        A GroupCoordinator for tensor parallelism that can be used with\n        the patch_tensor_parallel_group context manager.\n\n    Example usage:\n        ```python\n        # Initialize tensor parallel group for model1\n        tp_group_model1 = initialize_tensor_parallel_group(\n            tensor_model_parallel_size=4,\n            group_name_suffix=\"model1\"\n        )\n\n        # Use tensor parallelism for model1\n        with patch_tensor_parallel_group(tp_group_model1):\n            # Run model1 with tensor parallelism\n            output1 = model1(input1)\n        ```\n    \"\"\"\n    # Get world size and rank. Ensure some consistencies.\n    assert torch.distributed.is_initialized()\n    world_size: int = torch.distributed.get_world_size()\n    backend = backend or torch.distributed.get_backend(\n        get_world_group().device_group)\n\n    # Ensure the world size is compatible with the parallelism configuration\n    assert world_size % tensor_model_parallel_size == 0, \\\n        f\"World size ({world_size}) must be divisible by tensor_model_parallel_size ({tensor_model_parallel_size})\"\n\n    # Build the tensor model-parallel groups.\n    num_tensor_model_parallel_groups: int = (world_size //\n                                             tensor_model_parallel_size)\n    tp_group_ranks = []\n    for i in range(num_tensor_model_parallel_groups):\n        ranks = list(\n            range(i * tensor_model_parallel_size,\n                  (i + 1) * tensor_model_parallel_size))\n        tp_group_ranks.append(ranks)\n\n    # Create TP group coordinator with a unique name\n    group_name = f\"tp_{group_name_suffix}\" if group_name_suffix else \"tp\"\n    tp_group = init_model_parallel_group(tp_group_ranks,\n                                         get_world_group().local_rank,\n                                         backend,\n                                         use_message_queue_broadcaster=True,\n                                         group_name=group_name)\n\n    return tp_group\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.parallel_state.is_the_same_node_as","title":"fastvideo.distributed.parallel_state.is_the_same_node_as","text":"<pre><code>is_the_same_node_as(\n    pg: ProcessGroup | StatelessProcessGroup,\n    source_rank: int = 0,\n) -&gt; list[int]\n</code></pre> <p>This is a collective operation that returns if each rank is in the same node as the source rank. It tests if processes are attached to the same memory system (shared access to shared memory).</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def is_the_same_node_as(pg: ProcessGroup | StatelessProcessGroup,\n                        source_rank: int = 0) -&gt; list[int]:\n    \"\"\"\n    This is a collective operation that returns if each rank is in the same node\n    as the source rank. It tests if processes are attached to the same\n    memory system (shared access to shared memory).\n    \"\"\"\n    if isinstance(pg, ProcessGroup):\n        assert torch.distributed.get_backend(\n            pg) != torch.distributed.Backend.NCCL, (\n                \"in_the_same_node_as should be tested with a non-NCCL group.\")\n        # local rank inside the group\n        rank = torch.distributed.get_rank(group=pg)\n        world_size = torch.distributed.get_world_size(group=pg)\n\n        # global ranks of the processes in the group\n        ranks = torch.distributed.get_process_group_ranks(pg)\n    else:\n        rank = pg.rank\n        world_size = pg.world_size\n        ranks = list(range(world_size))\n\n    # local tensor in each process to store the result\n    is_in_the_same_node = torch.tensor([0] * world_size, dtype=torch.int32)\n\n    magic_message = b\"magic_message\"\n    shm = None\n\n    try:\n        with contextlib.suppress(OSError):\n            if rank == source_rank:\n                # create a shared memory segment\n                shm = shared_memory.SharedMemory(create=True, size=128)\n                shm.buf[:len(magic_message)] = magic_message\n                if isinstance(pg, ProcessGroup):\n                    torch.distributed.broadcast_object_list(\n                        [shm.name], src=ranks[source_rank], group=pg)\n                else:\n                    pg.broadcast_obj(shm.name, src=source_rank)\n                is_in_the_same_node[rank] = 1\n            else:\n                # try to open the shared memory segment\n                if isinstance(pg, ProcessGroup):\n                    recv = [None]\n                    torch.distributed.broadcast_object_list(\n                        recv, src=ranks[source_rank], group=pg)\n                    name = recv[0]\n                else:\n                    name = pg.broadcast_obj(None, src=source_rank)\n                # fix to https://stackoverflow.com/q/62748654/9191338\n                # Python incorrectly tracks shared memory even if it is not\n                # created by the process. The following patch is a workaround.\n                with patch(\"multiprocessing.resource_tracker.register\",\n                           lambda *args, **kwargs: None):\n                    shm = shared_memory.SharedMemory(name=name)\n                if shm.buf[:len(magic_message)] == magic_message:\n                    is_in_the_same_node[rank] = 1\n    except Exception as e:\n        logger.error(\"Error ignored in is_in_the_same_node: %s\", e)\n    finally:\n        if shm:\n            shm.close()\n\n    if isinstance(pg, ProcessGroup):\n        torch.distributed.barrier(group=pg)\n    else:\n        pg.barrier()\n\n    # clean up the shared memory segment\n    with contextlib.suppress(OSError):\n        if rank == source_rank and shm:\n            shm.unlink()\n\n    if isinstance(pg, ProcessGroup):\n        torch.distributed.all_reduce(is_in_the_same_node, group=pg)\n        aggregated_data = is_in_the_same_node\n    else:\n        aggregated_data = torch.zeros_like(is_in_the_same_node)\n        for i in range(world_size):\n            rank_data = pg.broadcast_obj(is_in_the_same_node, src=i)\n            aggregated_data += rank_data\n\n    return [x == 1 for x in aggregated_data.tolist()]\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.parallel_state.model_parallel_is_initialized","title":"fastvideo.distributed.parallel_state.model_parallel_is_initialized","text":"<pre><code>model_parallel_is_initialized() -&gt; bool\n</code></pre> <p>Check if tensor, sequence parallel groups are initialized.</p> Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>def model_parallel_is_initialized() -&gt; bool:\n    \"\"\"Check if tensor, sequence parallel groups are initialized.\"\"\"\n    return _TP is not None and _SP is not None and _DP is not None\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.parallel_state.patch_tensor_parallel_group","title":"fastvideo.distributed.parallel_state.patch_tensor_parallel_group","text":"<pre><code>patch_tensor_parallel_group(tp_group: GroupCoordinator)\n</code></pre> <p>Patch the tp group temporarily until this function ends.</p> <p>This method is for draft workers of speculative decoding to run draft model with different tp degree from that of target model workers.</p> <p>Parameters:</p> Name Type Description Default <code>tp_group</code> <code>GroupCoordinator</code> <p>the tp group coordinator</p> required Source code in <code>fastvideo/distributed/parallel_state.py</code> <pre><code>@contextmanager\ndef patch_tensor_parallel_group(tp_group: GroupCoordinator):\n    \"\"\"Patch the tp group temporarily until this function ends.\n\n    This method is for draft workers of speculative decoding to run draft model\n    with different tp degree from that of target model workers.\n\n    Args:\n        tp_group (GroupCoordinator): the tp group coordinator\n    \"\"\"\n    global _TP_STATE_PATCHED\n    assert not _TP_STATE_PATCHED, \"Should not call when it's already patched\"\n\n    _TP_STATE_PATCHED = True\n    old_tp_group = get_tp_group()\n    global _TP\n    _TP = tp_group\n    try:\n        yield\n    finally:\n        # restore the original state\n        _TP_STATE_PATCHED = False\n        _TP = old_tp_group\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.utils","title":"fastvideo.distributed.utils","text":""},{"location":"api/fastvideo/distributed/#fastvideo.distributed.utils-classes","title":"Classes","text":""},{"location":"api/fastvideo/distributed/#fastvideo.distributed.utils.StatelessProcessGroup","title":"fastvideo.distributed.utils.StatelessProcessGroup  <code>dataclass</code>","text":"<pre><code>StatelessProcessGroup(\n    rank: int,\n    world_size: int,\n    store: Store,\n    data_expiration_seconds: int = 3600,\n    send_dst_counter: dict[int, int] = dict(),\n    recv_src_counter: dict[int, int] = dict(),\n    broadcast_send_counter: int = 0,\n    broadcast_recv_src_counter: dict[int, int] = dict(),\n    entries: deque[tuple[str, float]] = deque(),\n)\n</code></pre> <p>A dataclass to hold a metadata store, and the rank, world_size of the group. Only use it to communicate metadata between processes. For data-plane communication, create NCCL-related objects.</p> Functions\u00b6 fastvideo.distributed.utils.StatelessProcessGroup.all_gather_obj \u00b6 <pre><code>all_gather_obj(obj: Any) -&gt; list[Any]\n</code></pre> <p>All gather an object from all ranks.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def all_gather_obj(self, obj: Any) -&gt; list[Any]:\n    \"\"\"All gather an object from all ranks.\"\"\"\n    gathered_objs = []\n    for i in range(self.world_size):\n        if i == self.rank:\n            gathered_objs.append(obj)\n            self.broadcast_obj(obj, src=self.rank)\n        else:\n            recv_obj = self.broadcast_obj(None, src=i)\n            gathered_objs.append(recv_obj)\n    return gathered_objs\n</code></pre> fastvideo.distributed.utils.StatelessProcessGroup.barrier \u00b6 <pre><code>barrier()\n</code></pre> <p>A barrier to synchronize all ranks.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def barrier(self):\n    \"\"\"A barrier to synchronize all ranks.\"\"\"\n    for i in range(self.world_size):\n        if i == self.rank:\n            self.broadcast_obj(None, src=self.rank)\n        else:\n            self.broadcast_obj(None, src=i)\n</code></pre> fastvideo.distributed.utils.StatelessProcessGroup.broadcast_obj \u00b6 <pre><code>broadcast_obj(obj: Any | None, src: int) -&gt; Any\n</code></pre> <p>Broadcast an object from a source rank to all other ranks. It does not clean up after all ranks have received the object. Use it for limited times, e.g., for initialization.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def broadcast_obj(self, obj: Any | None, src: int) -&gt; Any:\n    \"\"\"Broadcast an object from a source rank to all other ranks.\n    It does not clean up after all ranks have received the object.\n    Use it for limited times, e.g., for initialization.\n    \"\"\"\n    if self.rank == src:\n        self.expire_data()\n        key = (f\"broadcast_from/{src}/\"\n               f\"{self.broadcast_send_counter}\")\n        self.store.set(key, pickle.dumps(obj))\n        self.broadcast_send_counter += 1\n        self.entries.append((key, time.perf_counter()))\n        return obj\n    else:\n        key = (f\"broadcast_from/{src}/\"\n               f\"{self.broadcast_recv_src_counter[src]}\")\n        recv_obj = pickle.loads(self.store.get(key))\n        self.broadcast_recv_src_counter[src] += 1\n        return recv_obj\n</code></pre> fastvideo.distributed.utils.StatelessProcessGroup.create <code>staticmethod</code> \u00b6 <pre><code>create(\n    host: str,\n    port: int,\n    rank: int,\n    world_size: int,\n    data_expiration_seconds: int = 3600,\n) -&gt; StatelessProcessGroup\n</code></pre> <p>A replacement for <code>torch.distributed.init_process_group</code> that does not pollute the global state.</p> <p>If we have process A and process B called <code>torch.distributed.init_process_group</code> to form a group, and then we want to form another group with process A, B, C, D, it is not possible in PyTorch, because process A and process B have already formed a group, and process C and process D cannot join that group. This function is a workaround for this issue.</p> <p><code>torch.distributed.init_process_group</code> is a global call, while this function is a stateless call. It will return a <code>StatelessProcessGroup</code> object that can be used for exchanging metadata. With this function, process A and process B can call <code>StatelessProcessGroup.create</code> to form a group, and then process A, B, C, and D can call <code>StatelessProcessGroup.create</code> to form another group.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>@staticmethod\ndef create(\n    host: str,\n    port: int,\n    rank: int,\n    world_size: int,\n    data_expiration_seconds: int = 3600,\n) -&gt; \"StatelessProcessGroup\":\n    \"\"\"A replacement for `torch.distributed.init_process_group` that does not\n    pollute the global state.\n\n    If we have process A and process B called `torch.distributed.init_process_group`\n    to form a group, and then we want to form another group with process A, B, C,\n    D, it is not possible in PyTorch, because process A and process B have already\n    formed a group, and process C and process D cannot join that group. This\n    function is a workaround for this issue.\n\n    `torch.distributed.init_process_group` is a global call, while this function\n    is a stateless call. It will return a `StatelessProcessGroup` object that can be\n    used for exchanging metadata. With this function, process A and process B\n    can call `StatelessProcessGroup.create` to form a group, and then process A, B,\n    C, and D can call `StatelessProcessGroup.create` to form another group.\n    \"\"\" # noqa\n    store = TCPStore(\n        host_name=host,\n        port=port,\n        world_size=world_size,\n        is_master=(rank == 0),\n    )\n\n    return StatelessProcessGroup(\n        rank=rank,\n        world_size=world_size,\n        store=store,\n        data_expiration_seconds=data_expiration_seconds)\n</code></pre> fastvideo.distributed.utils.StatelessProcessGroup.expire_data \u00b6 <pre><code>expire_data() -&gt; None\n</code></pre> <p>Expire data that is older than <code>data_expiration_seconds</code> seconds.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def expire_data(self) -&gt; None:\n    \"\"\"Expire data that is older than `data_expiration_seconds` seconds.\"\"\"\n    while self.entries:\n        # check the oldest entry\n        key, timestamp = self.entries[0]\n        if time.perf_counter() - timestamp &gt; self.data_expiration_seconds:\n            self.store.delete_key(key)\n            self.entries.popleft()\n        else:\n            break\n</code></pre> fastvideo.distributed.utils.StatelessProcessGroup.recv_obj \u00b6 <pre><code>recv_obj(src: int) -&gt; Any\n</code></pre> <p>Receive an object from a source rank.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def recv_obj(self, src: int) -&gt; Any:\n    \"\"\"Receive an object from a source rank.\"\"\"\n    obj = pickle.loads(\n        self.store.get(f\"send_to/{self.rank}/{self.recv_src_counter[src]}\"))\n    self.recv_src_counter[src] += 1\n    return obj\n</code></pre> fastvideo.distributed.utils.StatelessProcessGroup.send_obj \u00b6 <pre><code>send_obj(obj: Any, dst: int)\n</code></pre> <p>Send an object to a destination rank.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def send_obj(self, obj: Any, dst: int):\n    \"\"\"Send an object to a destination rank.\"\"\"\n    self.expire_data()\n    key = f\"send_to/{dst}/{self.send_dst_counter[dst]}\"\n    self.store.set(key, pickle.dumps(obj))\n    self.send_dst_counter[dst] += 1\n    self.entries.append((key, time.perf_counter()))\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.utils-functions","title":"Functions","text":""},{"location":"api/fastvideo/distributed/#fastvideo.distributed.utils.divide","title":"fastvideo.distributed.utils.divide","text":"<pre><code>divide(numerator: int, denominator: int) -&gt; int\n</code></pre> <p>Ensure that numerator is divisible by the denominator and return the division value.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def divide(numerator: int, denominator: int) -&gt; int:\n    \"\"\"Ensure that numerator is divisible by the denominator and return\n    the division value.\"\"\"\n    ensure_divisibility(numerator, denominator)\n    return numerator // denominator\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.utils.ensure_divisibility","title":"fastvideo.distributed.utils.ensure_divisibility","text":"<pre><code>ensure_divisibility(numerator, denominator) -&gt; None\n</code></pre> <p>Ensure that numerator is divisible by the denominator.</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def ensure_divisibility(numerator, denominator) -&gt; None:\n    \"\"\"Ensure that numerator is divisible by the denominator.\"\"\"\n    assert numerator % denominator == 0, \"{} is not divisible by {}\".format(\n        numerator, denominator)\n</code></pre>"},{"location":"api/fastvideo/distributed/#fastvideo.distributed.utils.split_tensor_along_last_dim","title":"fastvideo.distributed.utils.split_tensor_along_last_dim","text":"<pre><code>split_tensor_along_last_dim(\n    tensor: Tensor,\n    num_partitions: int,\n    contiguous_split_chunks: bool = False,\n) -&gt; Sequence[torch.Tensor]\n</code></pre> <p>Split a tensor along its last dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>input tensor.</p> required <code>num_partitions</code> <code>int</code> <p>number of partitions to split the tensor</p> required <code>contiguous_split_chunks</code> <code>bool</code> <p>If True, make each chunk contiguous                      in memory.</p> <code>False</code> <p>Returns:</p> Type Description <code>Sequence[Tensor]</code> <p>A list of Tensors</p> Source code in <code>fastvideo/distributed/utils.py</code> <pre><code>def split_tensor_along_last_dim(\n    tensor: torch.Tensor,\n    num_partitions: int,\n    contiguous_split_chunks: bool = False,\n) -&gt; Sequence[torch.Tensor]:\n    \"\"\" Split a tensor along its last dimension.\n\n        Arguments:\n            tensor: input tensor.\n            num_partitions: number of partitions to split the tensor\n            contiguous_split_chunks: If True, make each chunk contiguous\n                                     in memory.\n\n        Returns:\n            A list of Tensors\n    \"\"\"\n    # Get the size and dimension.\n    last_dim = tensor.dim() - 1\n    last_dim_size = divide(tensor.size()[last_dim], num_partitions)\n    # Split.\n    tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)\n    # NOTE: torch.split does not create contiguous tensors by default.\n    if contiguous_split_chunks:\n        return tuple(chunk.contiguous() for chunk in tensor_list)\n\n    return tuple(tensor_list)\n</code></pre>"},{"location":"api/fastvideo/entrypoints/","title":"entrypoints","text":""},{"location":"api/fastvideo/entrypoints/#fastvideo.entrypoints","title":"entrypoints","text":""},{"location":"api/fastvideo/entrypoints/#fastvideo.entrypoints-modules","title":"Modules","text":""},{"location":"api/fastvideo/entrypoints/#fastvideo.entrypoints.cli","title":"fastvideo.entrypoints.cli","text":""},{"location":"api/fastvideo/entrypoints/#fastvideo.entrypoints.cli-modules","title":"Modules","text":""},{"location":"api/fastvideo/entrypoints/#fastvideo.entrypoints.cli.cli_types","title":"fastvideo.entrypoints.cli.cli_types","text":"Classes\u00b6 fastvideo.entrypoints.cli.cli_types.CLISubcommand \u00b6 <p>Base class for CLI subcommands</p> Functions\u00b6 fastvideo.entrypoints.cli.cli_types.CLISubcommand.cmd \u00b6 <pre><code>cmd(args: Namespace) -&gt; None\n</code></pre> <p>Execute the command with the given arguments</p> Source code in <code>fastvideo/entrypoints/cli/cli_types.py</code> <pre><code>def cmd(self, args: argparse.Namespace) -&gt; None:\n    \"\"\"Execute the command with the given arguments\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.entrypoints.cli.cli_types.CLISubcommand.subparser_init \u00b6 <pre><code>subparser_init(\n    subparsers: _SubParsersAction,\n) -&gt; FlexibleArgumentParser\n</code></pre> <p>Initialize the subparser for this command</p> Source code in <code>fastvideo/entrypoints/cli/cli_types.py</code> <pre><code>def subparser_init(\n        self,\n        subparsers: argparse._SubParsersAction) -&gt; FlexibleArgumentParser:\n    \"\"\"Initialize the subparser for this command\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.entrypoints.cli.cli_types.CLISubcommand.validate \u00b6 <pre><code>validate(args: Namespace) -&gt; None\n</code></pre> <p>Validate the arguments for this command</p> Source code in <code>fastvideo/entrypoints/cli/cli_types.py</code> <pre><code>def validate(self, args: argparse.Namespace) -&gt; None:\n    \"\"\"Validate the arguments for this command\"\"\"\n    pass\n</code></pre>"},{"location":"api/fastvideo/entrypoints/#fastvideo.entrypoints.cli.generate","title":"fastvideo.entrypoints.cli.generate","text":"Classes\u00b6 fastvideo.entrypoints.cli.generate.GenerateSubcommand \u00b6 <pre><code>GenerateSubcommand()\n</code></pre> <p>               Bases: <code>CLISubcommand</code></p> <p>The <code>generate</code> subcommand for the FastVideo CLI</p> Source code in <code>fastvideo/entrypoints/cli/generate.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.name = \"generate\"\n    super().__init__()\n    self.init_arg_names = self._get_init_arg_names()\n    self.generation_arg_names = self._get_generation_arg_names()\n</code></pre> Functions\u00b6 fastvideo.entrypoints.cli.generate.GenerateSubcommand.validate \u00b6 <pre><code>validate(args: Namespace) -&gt; None\n</code></pre> <p>Validate the arguments for this command</p> Source code in <code>fastvideo/entrypoints/cli/generate.py</code> <pre><code>def validate(self, args: argparse.Namespace) -&gt; None:\n    \"\"\"Validate the arguments for this command\"\"\"\n    if args.num_gpus is not None and args.num_gpus &lt;= 0:\n        raise ValueError(\"Number of gpus must be positive\")\n\n    if args.config and not os.path.exists(args.config):\n        raise ValueError(f\"Config file not found: {args.config}\")\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/entrypoints/#fastvideo.entrypoints.cli.main","title":"fastvideo.entrypoints.cli.main","text":"Classes\u00b6 Functions\u00b6 fastvideo.entrypoints.cli.main.cmd_init \u00b6 <pre><code>cmd_init() -&gt; list[CLISubcommand]\n</code></pre> <p>Initialize all commands from separate modules</p> Source code in <code>fastvideo/entrypoints/cli/main.py</code> <pre><code>def cmd_init() -&gt; list[CLISubcommand]:\n    \"\"\"Initialize all commands from separate modules\"\"\"\n    commands = []\n    commands.extend(generate_cmd_init())\n    return commands\n</code></pre>"},{"location":"api/fastvideo/entrypoints/#fastvideo.entrypoints.cli.utils","title":"fastvideo.entrypoints.cli.utils","text":"Functions\u00b6 fastvideo.entrypoints.cli.utils.launch_distributed \u00b6 <pre><code>launch_distributed(\n    num_gpus: int,\n    args: list[str],\n    master_port: int | None = None,\n) -&gt; int\n</code></pre> <p>Launch a distributed job with the given arguments</p> <p>Parameters:</p> Name Type Description Default <code>num_gpus</code> <code>int</code> <p>Number of GPUs to use</p> required <code>args</code> <code>list[str]</code> <p>Arguments to pass to v1_fastvideo_inference.py (defaults to sys.argv[1:])</p> required <code>master_port</code> <code>int | None</code> <p>Port for the master process (default: random)</p> <code>None</code> Source code in <code>fastvideo/entrypoints/cli/utils.py</code> <pre><code>def launch_distributed(num_gpus: int,\n                       args: list[str],\n                       master_port: int | None = None) -&gt; int:\n    \"\"\"\n    Launch a distributed job with the given arguments\n\n    Args:\n        num_gpus: Number of GPUs to use\n        args: Arguments to pass to v1_fastvideo_inference.py (defaults to sys.argv[1:])\n        master_port: Port for the master process (default: random)\n    \"\"\"\n\n    current_env = os.environ.copy()\n    python_executable = sys.executable\n    project_root = os.path.abspath(\n        os.path.join(os.path.dirname(__file__), \"../../../..\"))\n    main_script = os.path.join(project_root,\n                               \"fastvideo/sample/v1_fastvideo_inference.py\")\n\n    cmd = [\n        python_executable, \"-m\", \"torch.distributed.run\",\n        f\"--nproc_per_node={num_gpus}\"\n    ]\n\n    if master_port is not None:\n        cmd.append(f\"--master_port={master_port}\")\n\n    cmd.append(main_script)\n    cmd.extend(args)\n\n    logger.info(\"Running inference with %d GPU(s)\", num_gpus)\n    logger.info(\"Launching command: %s\", \" \".join(cmd))\n\n    current_env[\"PYTHONIOENCODING\"] = \"utf-8\"\n    process = subprocess.Popen(cmd,\n                               env=current_env,\n                               stdout=subprocess.PIPE,\n                               stderr=subprocess.STDOUT,\n                               universal_newlines=True,\n                               bufsize=1,\n                               encoding='utf-8',\n                               errors='replace')\n\n    if process.stdout:\n        for line in iter(process.stdout.readline, ''):\n            print(line.strip())\n\n    return process.wait()\n</code></pre>"},{"location":"api/fastvideo/entrypoints/#fastvideo.entrypoints.video_generator","title":"fastvideo.entrypoints.video_generator","text":"<p>VideoGenerator module for FastVideo.</p> <p>This module provides a consolidated interface for generating videos using diffusion models.</p>"},{"location":"api/fastvideo/entrypoints/#fastvideo.entrypoints.video_generator-classes","title":"Classes","text":""},{"location":"api/fastvideo/entrypoints/#fastvideo.entrypoints.video_generator.VideoGenerator","title":"fastvideo.entrypoints.video_generator.VideoGenerator","text":"<pre><code>VideoGenerator(\n    fastvideo_args: FastVideoArgs,\n    executor_class: type[Executor],\n    log_stats: bool,\n)\n</code></pre> <p>A unified class for generating videos using diffusion models.</p> <p>This class provides a simple interface for video generation with rich customization options, similar to popular frameworks like HF Diffusers.</p> <p>Initialize the video generator.</p> <p>Parameters:</p> Name Type Description Default <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments</p> required <code>executor_class</code> <code>type[Executor]</code> <p>The executor class to use for inference</p> required Source code in <code>fastvideo/entrypoints/video_generator.py</code> <pre><code>def __init__(self, fastvideo_args: FastVideoArgs,\n             executor_class: type[Executor], log_stats: bool):\n    \"\"\"\n    Initialize the video generator.\n\n    Args:\n        fastvideo_args: The inference arguments\n        executor_class: The executor class to use for inference\n    \"\"\"\n    self.fastvideo_args = fastvideo_args\n    self.executor = executor_class(fastvideo_args)\n</code></pre> Functions\u00b6 fastvideo.entrypoints.video_generator.VideoGenerator.from_fastvideo_args <code>classmethod</code> \u00b6 <pre><code>from_fastvideo_args(\n    fastvideo_args: FastVideoArgs,\n) -&gt; VideoGenerator\n</code></pre> <p>Create a video generator with the specified arguments.</p> <p>Parameters:</p> Name Type Description Default <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments</p> required <p>Returns:</p> Type Description <code>VideoGenerator</code> <p>The created video generator</p> Source code in <code>fastvideo/entrypoints/video_generator.py</code> <pre><code>@classmethod\ndef from_fastvideo_args(cls,\n                        fastvideo_args: FastVideoArgs) -&gt; \"VideoGenerator\":\n    \"\"\"\n    Create a video generator with the specified arguments.\n\n    Args:\n        fastvideo_args: The inference arguments\n\n    Returns:\n        The created video generator\n    \"\"\"\n    # Initialize distributed environment if needed\n    # initialize_distributed_and_parallelism(fastvideo_args)\n\n    executor_class = Executor.get_class(fastvideo_args)\n    return cls(\n        fastvideo_args=fastvideo_args,\n        executor_class=executor_class,\n        log_stats=False,  # TODO: implement\n    )\n</code></pre> fastvideo.entrypoints.video_generator.VideoGenerator.from_pretrained <code>classmethod</code> \u00b6 <pre><code>from_pretrained(\n    model_path: str,\n    device: str | None = None,\n    torch_dtype: dtype | None = None,\n    **kwargs\n) -&gt; VideoGenerator\n</code></pre> <p>Create a video generator from a pretrained model.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path or identifier for the pretrained model</p> required <code>device</code> <code>str | None</code> <p>Device to load the model on (e.g., \"cuda\", \"cuda:0\", \"cpu\")</p> <code>None</code> <code>torch_dtype</code> <code>dtype | None</code> <p>Data type for model weights (e.g., torch.float16)</p> <code>None</code> <code>pipeline_config</code> <p>Pipeline config to use for inference</p> required <code>**kwargs</code> <p>Additional arguments to customize model loading, set any FastVideoArgs or PipelineConfig attributes here.</p> <code>{}</code> <p>Returns:</p> Type Description <code>VideoGenerator</code> <p>The created video generator</p> <p>Priority level: Default pipeline config &lt; User's pipeline config &lt; User's kwargs</p> Source code in <code>fastvideo/entrypoints/video_generator.py</code> <pre><code>@classmethod\ndef from_pretrained(cls,\n                    model_path: str,\n                    device: str | None = None,\n                    torch_dtype: torch.dtype | None = None,\n                    **kwargs) -&gt; \"VideoGenerator\":\n    \"\"\"\n    Create a video generator from a pretrained model.\n\n    Args:\n        model_path: Path or identifier for the pretrained model\n        device: Device to load the model on (e.g., \"cuda\", \"cuda:0\", \"cpu\")\n        torch_dtype: Data type for model weights (e.g., torch.float16)\n        pipeline_config: Pipeline config to use for inference\n        **kwargs: Additional arguments to customize model loading, set any FastVideoArgs or PipelineConfig attributes here.\n\n    Returns:\n        The created video generator\n\n    Priority level: Default pipeline config &lt; User's pipeline config &lt; User's kwargs\n    \"\"\"\n    # If users also provide some kwargs, it will override the FastVideoArgs and PipelineConfig.\n    kwargs['model_path'] = model_path\n    fastvideo_args = FastVideoArgs.from_kwargs(**kwargs)\n\n    return cls.from_fastvideo_args(fastvideo_args)\n</code></pre> fastvideo.entrypoints.video_generator.VideoGenerator.generate_video \u00b6 <pre><code>generate_video(\n    prompt: str | None = None,\n    sampling_param: SamplingParam | None = None,\n    **kwargs\n) -&gt; dict[str, Any] | list[np.ndarray] | list[\n    dict[str, Any]\n]\n</code></pre> <p>Generate a video based on the given prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str | None</code> <p>The prompt to use for generation (optional if prompt_txt is provided)</p> <code>None</code> <code>negative_prompt</code> <p>The negative prompt to use (overrides the one in fastvideo_args)</p> required <code>output_path</code> <p>Path to save the video (overrides the one in fastvideo_args)</p> required <code>output_video_name</code> <p>Name of the video file to save. Default is the first 100 characters of the prompt.</p> required <code>save_video</code> <p>Whether to save the video to disk</p> required <code>return_frames</code> <p>Whether to return the raw frames</p> required <code>num_inference_steps</code> <p>Number of denoising steps (overrides fastvideo_args)</p> required <code>guidance_scale</code> <p>Classifier-free guidance scale (overrides fastvideo_args)</p> required <code>num_frames</code> <p>Number of frames to generate (overrides fastvideo_args)</p> required <code>height</code> <p>Height of generated video (overrides fastvideo_args)</p> required <code>width</code> <p>Width of generated video (overrides fastvideo_args)</p> required <code>fps</code> <p>Frames per second for saved video (overrides fastvideo_args)</p> required <code>seed</code> <p>Random seed for generation (overrides fastvideo_args)</p> required <code>callback</code> <p>Callback function called after each step</p> required <code>callback_steps</code> <p>Number of steps between each callback</p> required <p>Returns:</p> Type Description <code>dict[str, Any] | list[ndarray] | list[dict[str, Any]]</code> <p>Either the output dictionary, list of frames, or list of results for batch processing</p> Source code in <code>fastvideo/entrypoints/video_generator.py</code> <pre><code>def generate_video(\n    self,\n    prompt: str | None = None,\n    sampling_param: SamplingParam | None = None,\n    **kwargs,\n) -&gt; dict[str, Any] | list[np.ndarray] | list[dict[str, Any]]:\n    \"\"\"\n    Generate a video based on the given prompt.\n\n    Args:\n        prompt: The prompt to use for generation (optional if prompt_txt is provided)\n        negative_prompt: The negative prompt to use (overrides the one in fastvideo_args)\n        output_path: Path to save the video (overrides the one in fastvideo_args)\n        output_video_name: Name of the video file to save. Default is the first 100 characters of the prompt.\n        save_video: Whether to save the video to disk\n        return_frames: Whether to return the raw frames\n        num_inference_steps: Number of denoising steps (overrides fastvideo_args)\n        guidance_scale: Classifier-free guidance scale (overrides fastvideo_args)\n        num_frames: Number of frames to generate (overrides fastvideo_args)\n        height: Height of generated video (overrides fastvideo_args)\n        width: Width of generated video (overrides fastvideo_args)\n        fps: Frames per second for saved video (overrides fastvideo_args)\n        seed: Random seed for generation (overrides fastvideo_args)\n        callback: Callback function called after each step\n        callback_steps: Number of steps between each callback\n\n    Returns:\n        Either the output dictionary, list of frames, or list of results for batch processing\n    \"\"\"\n    # Handle batch processing from text file\n    if self.fastvideo_args.prompt_txt is not None:\n        prompt_txt_path = self.fastvideo_args.prompt_txt\n        if not os.path.exists(prompt_txt_path):\n            raise FileNotFoundError(\n                f\"Prompt text file not found: {prompt_txt_path}\")\n\n        # Read prompts from file\n        with open(prompt_txt_path, encoding='utf-8') as f:\n            prompts = [line.strip() for line in f if line.strip()]\n\n        if not prompts:\n            raise ValueError(f\"No prompts found in file: {prompt_txt_path}\")\n\n        logger.info(\"Found %d prompts in %s\", len(prompts), prompt_txt_path)\n\n        if sampling_param is not None:\n            original_output_video_name = sampling_param.output_video_name\n        else:\n            original_output_video_name = None\n\n        results = []\n        for i, batch_prompt in enumerate(prompts):\n            logger.info(\"Processing prompt %d/%d: %s...\", i + 1,\n                        len(prompts), batch_prompt[:100])\n\n            try:\n                # Generate video for this prompt using the same logic below\n                if sampling_param is not None and original_output_video_name is not None:\n                    sampling_param.output_video_name = original_output_video_name + f\"_{i}\"\n                result = self._generate_single_video(\n                    batch_prompt, sampling_param, **kwargs)\n\n                # Add prompt info to result\n                if isinstance(result, dict):\n                    result[\"prompt_index\"] = i\n                    result[\"prompt\"] = batch_prompt\n\n                results.append(result)\n                logger.info(\"Successfully generated video for prompt %d\",\n                            i + 1)\n\n            except Exception as e:\n                logger.error(\"Failed to generate video for prompt %d: %s\",\n                             i + 1, e)\n                continue\n\n        logger.info(\n            \"Completed batch processing. Generated %d videos successfully.\",\n            len(results))\n        return results\n\n    # Single prompt generation (original behavior)\n    if prompt is None:\n        raise ValueError(\"Either prompt or prompt_txt must be provided\")\n\n    return self._generate_single_video(prompt, sampling_param, **kwargs)\n</code></pre> fastvideo.entrypoints.video_generator.VideoGenerator.shutdown \u00b6 <pre><code>shutdown()\n</code></pre> <p>Shutdown the video generator.</p> Source code in <code>fastvideo/entrypoints/video_generator.py</code> <pre><code>def shutdown(self):\n    \"\"\"\n    Shutdown the video generator.\n    \"\"\"\n    self.executor.shutdown()\n    del self.executor\n</code></pre> fastvideo.entrypoints.video_generator.VideoGenerator.unmerge_lora_weights \u00b6 <pre><code>unmerge_lora_weights() -&gt; None\n</code></pre> <p>Use unmerged weights for inference to produce videos that align with  validation videos generated during training.</p> Source code in <code>fastvideo/entrypoints/video_generator.py</code> <pre><code>def unmerge_lora_weights(self) -&gt; None:\n    \"\"\"\n    Use unmerged weights for inference to produce videos that align with \n    validation videos generated during training.\n    \"\"\"\n    self.executor.unmerge_lora_weights()\n</code></pre>"},{"location":"api/fastvideo/entrypoints/#fastvideo.entrypoints.video_generator-functions","title":"Functions","text":""},{"location":"api/fastvideo/envs/","title":"envs","text":""},{"location":"api/fastvideo/envs/#fastvideo.envs","title":"envs","text":""},{"location":"api/fastvideo/fastvideo_args/","title":"fastvideo_args","text":""},{"location":"api/fastvideo/fastvideo_args/#fastvideo.fastvideo_args","title":"fastvideo_args","text":"<p>The arguments of FastVideo Inference.</p>"},{"location":"api/fastvideo/fastvideo_args/#fastvideo.fastvideo_args-classes","title":"Classes","text":""},{"location":"api/fastvideo/fastvideo_args/#fastvideo.fastvideo_args.ExecutionMode","title":"fastvideo.fastvideo_args.ExecutionMode","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration for different pipeline modes.</p> <p>Inherits from str to allow string comparison for backward compatibility.</p>"},{"location":"api/fastvideo/fastvideo_args/#fastvideo.fastvideo_args.ExecutionMode-functions","title":"Functions","text":""},{"location":"api/fastvideo/fastvideo_args/#fastvideo.fastvideo_args.ExecutionMode.choices","title":"fastvideo.fastvideo_args.ExecutionMode.choices  <code>classmethod</code>","text":"<pre><code>choices() -&gt; list[str]\n</code></pre> <p>Get all available choices as strings for argparse.</p> Source code in <code>fastvideo/fastvideo_args.py</code> <pre><code>@classmethod\ndef choices(cls) -&gt; list[str]:\n    \"\"\"Get all available choices as strings for argparse.\"\"\"\n    return [mode.value for mode in cls]\n</code></pre>"},{"location":"api/fastvideo/fastvideo_args/#fastvideo.fastvideo_args.ExecutionMode.from_string","title":"fastvideo.fastvideo_args.ExecutionMode.from_string  <code>classmethod</code>","text":"<pre><code>from_string(value: str) -&gt; ExecutionMode\n</code></pre> <p>Convert string to ExecutionMode enum.</p> Source code in <code>fastvideo/fastvideo_args.py</code> <pre><code>@classmethod\ndef from_string(cls, value: str) -&gt; \"ExecutionMode\":\n    \"\"\"Convert string to ExecutionMode enum.\"\"\"\n    try:\n        return cls(value.lower())\n    except ValueError:\n        raise ValueError(\n            f\"Invalid mode: {value}. Must be one of: {', '.join([m.value for m in cls])}\"\n        ) from None\n</code></pre>"},{"location":"api/fastvideo/fastvideo_args/#fastvideo.fastvideo_args.FastVideoArgs","title":"fastvideo.fastvideo_args.FastVideoArgs  <code>dataclass</code>","text":"<pre><code>FastVideoArgs(\n    model_path: str,\n    mode: ExecutionMode = ExecutionMode.INFERENCE,\n    workload_type: WorkloadType = WorkloadType.T2V,\n    cache_strategy: str = \"none\",\n    distributed_executor_backend: str = \"mp\",\n    ray_placement_group: PlacementGroup | None = None,\n    ray_runtime_env: RuntimeEnv | None = None,\n    inference_mode: bool = True,\n    trust_remote_code: bool = False,\n    revision: str | None = None,\n    num_gpus: int = 1,\n    tp_size: int = -1,\n    sp_size: int = -1,\n    hsdp_replicate_dim: int = 1,\n    hsdp_shard_dim: int = -1,\n    dist_timeout: int | None = None,\n    pipeline_config: PipelineConfig = PipelineConfig(),\n    preprocess_config: PreprocessConfig | None = None,\n    lora_path: str | None = None,\n    lora_nickname: str = \"default\",\n    lora_target_modules: list[str] | None = None,\n    output_type: str = \"pil\",\n    dit_cpu_offload: bool = True,\n    use_fsdp_inference: bool = True,\n    text_encoder_cpu_offload: bool = True,\n    image_encoder_cpu_offload: bool = True,\n    vae_cpu_offload: bool = True,\n    pin_cpu_memory: bool = True,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    enable_torch_compile: bool = False,\n    torch_compile_kwargs: dict[str, Any] = dict(),\n    disable_autocast: bool = False,\n    VSA_sparsity: float = 0.0,\n    moba_config_path: str | None = None,\n    moba_config: dict[str, Any] = dict(),\n    master_port: int | None = None,\n    enable_stage_verification: bool = True,\n    prompt_txt: str | None = None,\n    model_paths: dict[str, str] = dict(),\n    model_loaded: dict[str, bool] = (\n        lambda: {\"transformer\": True, \"vae\": True}\n    )(),\n    override_transformer_cls_name: str | None = None,\n    init_weights_from_safetensors: str = \"\",\n    init_weights_from_safetensors_2: str = \"\",\n    boundary_ratio: float | None = 0.875,\n)\n</code></pre>"},{"location":"api/fastvideo/fastvideo_args/#fastvideo.fastvideo_args.FastVideoArgs-functions","title":"Functions","text":""},{"location":"api/fastvideo/fastvideo_args/#fastvideo.fastvideo_args.FastVideoArgs.check_fastvideo_args","title":"fastvideo.fastvideo_args.FastVideoArgs.check_fastvideo_args","text":"<pre><code>check_fastvideo_args() -&gt; None\n</code></pre> <p>Validate inference arguments for consistency</p> Source code in <code>fastvideo/fastvideo_args.py</code> <pre><code>def check_fastvideo_args(self) -&gt; None:\n    \"\"\"Validate inference arguments for consistency\"\"\"\n    from fastvideo.platforms import current_platform\n\n    if current_platform.is_mps():\n        self.use_fsdp_inference = False\n\n    # Validate mode and inference_mode consistency\n    assert isinstance(\n        self.mode, ExecutionMode\n    ), f\"Mode must be an ExecutionMode enum, got {type(self.mode)}\"\n    assert self.mode in ExecutionMode.choices(\n    ), f\"Invalid execution mode: {self.mode}\"\n\n    # Validate workload type\n    assert isinstance(\n        self.workload_type, WorkloadType\n    ), f\"Workload type must be a WorkloadType enum, got {type(self.workload_type)}\"\n    assert self.workload_type in WorkloadType.choices(\n    ), f\"Invalid workload type: {self.workload_type}\"\n\n    if self.mode in [ExecutionMode.DISTILLATION, ExecutionMode.FINETUNING\n                     ] and self.inference_mode:\n        logger.warning(\n            \"Mode is 'training' but inference_mode is True. Setting inference_mode to False.\"\n        )\n        self.inference_mode = False\n    elif self.mode in [ExecutionMode.INFERENCE, ExecutionMode.PREPROCESS\n                       ] and not self.inference_mode:\n        logger.warning(\n            \"Mode is '%s' but inference_mode is False. Setting inference_mode to True.\",\n            self.mode)\n        self.inference_mode = True\n\n    if not self.inference_mode:\n        assert self.hsdp_replicate_dim != -1, \"hsdp_replicate_dim must be set for training\"\n        assert self.hsdp_shard_dim != -1, \"hsdp_shard_dim must be set for training\"\n        assert self.sp_size != -1, \"sp_size must be set for training\"\n\n    if self.tp_size == -1:\n        self.tp_size = 1\n    if self.sp_size == -1:\n        self.sp_size = self.num_gpus\n    if self.hsdp_shard_dim == -1:\n        self.hsdp_shard_dim = self.num_gpus\n\n    assert self.sp_size &lt;= self.num_gpus and self.num_gpus % self.sp_size == 0, \"num_gpus must &gt;= and be divisible by sp_size\"\n    assert self.hsdp_replicate_dim &lt;= self.num_gpus and self.num_gpus % self.hsdp_replicate_dim == 0, \"num_gpus must &gt;= and be divisible by hsdp_replicate_dim\"\n    assert self.hsdp_shard_dim &lt;= self.num_gpus and self.num_gpus % self.hsdp_shard_dim == 0, \"num_gpus must &gt;= and be divisible by hsdp_shard_dim\"\n\n    if self.num_gpus &lt; max(self.tp_size, self.sp_size):\n        self.num_gpus = max(self.tp_size, self.sp_size)\n\n    if self.pipeline_config is None:\n        raise ValueError(\"pipeline_config is not set in FastVideoArgs\")\n\n    self.pipeline_config.check_pipeline_config()\n\n    # Add preprocessing config validation if needed\n    if self.mode == ExecutionMode.PREPROCESS:\n        if self.preprocess_config is None:\n            raise ValueError(\n                \"preprocess_config is not set in FastVideoArgs when mode is PREPROCESS\"\n            )\n        if self.preprocess_config.model_path == \"\":\n            self.preprocess_config.model_path = self.model_path\n        if not self.pipeline_config.vae_config.load_encoder:\n            self.pipeline_config.vae_config.load_encoder = True\n        self.preprocess_config.check_preprocess_config()\n</code></pre>"},{"location":"api/fastvideo/fastvideo_args/#fastvideo.fastvideo_args.TrainingArgs","title":"fastvideo.fastvideo_args.TrainingArgs  <code>dataclass</code>","text":"<pre><code>TrainingArgs(\n    model_path: str,\n    mode: ExecutionMode = ExecutionMode.INFERENCE,\n    workload_type: WorkloadType = WorkloadType.T2V,\n    cache_strategy: str = \"none\",\n    distributed_executor_backend: str = \"mp\",\n    ray_placement_group: PlacementGroup | None = None,\n    ray_runtime_env: RuntimeEnv | None = None,\n    inference_mode: bool = True,\n    trust_remote_code: bool = False,\n    revision: str | None = None,\n    num_gpus: int = 1,\n    tp_size: int = -1,\n    sp_size: int = -1,\n    hsdp_replicate_dim: int = 1,\n    hsdp_shard_dim: int = -1,\n    dist_timeout: int | None = None,\n    pipeline_config: PipelineConfig = PipelineConfig(),\n    preprocess_config: PreprocessConfig | None = None,\n    lora_path: str | None = None,\n    lora_nickname: str = \"default\",\n    lora_target_modules: list[str] | None = None,\n    output_type: str = \"pil\",\n    dit_cpu_offload: bool = True,\n    use_fsdp_inference: bool = True,\n    text_encoder_cpu_offload: bool = True,\n    image_encoder_cpu_offload: bool = True,\n    vae_cpu_offload: bool = True,\n    pin_cpu_memory: bool = True,\n    mask_strategy_file_path: str | None = None,\n    STA_mode: STA_Mode = STA_Mode.STA_INFERENCE,\n    skip_time_steps: int = 15,\n    enable_torch_compile: bool = False,\n    torch_compile_kwargs: dict[str, Any] = dict(),\n    disable_autocast: bool = False,\n    VSA_sparsity: float = 0.0,\n    moba_config_path: str | None = None,\n    moba_config: dict[str, Any] = dict(),\n    master_port: int | None = None,\n    enable_stage_verification: bool = True,\n    prompt_txt: str | None = None,\n    model_paths: dict[str, str] = dict(),\n    model_loaded: dict[str, bool] = (\n        lambda: {\"transformer\": True, \"vae\": True}\n    )(),\n    override_transformer_cls_name: str | None = None,\n    init_weights_from_safetensors: str = \"\",\n    init_weights_from_safetensors_2: str = \"\",\n    boundary_ratio: float | None = 0.875,\n    data_path: str = \"\",\n    dataloader_num_workers: int = 0,\n    num_height: int = 0,\n    num_width: int = 0,\n    num_frames: int = 0,\n    train_batch_size: int = 0,\n    num_latent_t: int = 0,\n    group_frame: bool = False,\n    group_resolution: bool = False,\n    pretrained_model_name_or_path: str = \"\",\n    real_score_model_path: str = \"\",\n    fake_score_model_path: str = \"\",\n    ema_decay: float = 0.0,\n    ema_start_step: int = 0,\n    training_cfg_rate: float = 0.0,\n    precondition_outputs: bool = False,\n    validation_dataset_file: str = \"\",\n    validation_preprocessed_path: str = \"\",\n    validation_sampling_steps: str = \"\",\n    validation_guidance_scale: str = \"\",\n    validation_steps: float = 0.0,\n    log_validation: bool = False,\n    trackers: list[str] = list(),\n    tracker_project_name: str = \"\",\n    wandb_run_name: str = \"\",\n    seed: int | None = None,\n    output_dir: str = \"\",\n    checkpoints_total_limit: int = 0,\n    resume_from_checkpoint: str = \"\",\n    num_train_epochs: int = 0,\n    max_train_steps: int = 0,\n    gradient_accumulation_steps: int = 0,\n    learning_rate: float = 0.0,\n    scale_lr: bool = False,\n    lr_scheduler: str = \"constant\",\n    lr_warmup_steps: int = 0,\n    max_grad_norm: float = 0.0,\n    enable_gradient_checkpointing_type: str | None = None,\n    selective_checkpointing: float = 0.0,\n    mixed_precision: str = \"\",\n    train_sp_batch_size: int = 0,\n    fsdp_sharding_startegy: str = \"\",\n    weighting_scheme: str = \"\",\n    logit_mean: float = 0.0,\n    logit_std: float = 1.0,\n    mode_scale: float = 0.0,\n    num_euler_timesteps: int = 0,\n    lr_num_cycles: int = 0,\n    lr_power: float = 0.0,\n    min_lr_ratio: float = 0.5,\n    not_apply_cfg_solver: bool = False,\n    distill_cfg: float = 0.0,\n    scheduler_type: str = \"\",\n    linear_quadratic_threshold: float = 0.0,\n    linear_range: float = 0.0,\n    weight_decay: float = 0.0,\n    betas: str = \"0.9,0.999\",\n    use_ema: bool = False,\n    multi_phased_distill_schedule: str = \"\",\n    pred_decay_weight: float = 0.0,\n    pred_decay_type: str = \"\",\n    hunyuan_teacher_disable_cfg: bool = False,\n    master_weight_type: str = \"\",\n    VSA_decay_rate: float = 0.01,\n    VSA_decay_interval_steps: int = 1,\n    lora_rank: int | None = None,\n    lora_alpha: int | None = None,\n    lora_training: bool = False,\n    generator_update_interval: int = 5,\n    dfake_gen_update_ratio: int = 5,\n    min_timestep_ratio: float = 0.2,\n    max_timestep_ratio: float = 0.98,\n    real_score_guidance_scale: float = 3.5,\n    fake_score_learning_rate: float = 0.0,\n    fake_score_lr_scheduler: str = \"constant\",\n    fake_score_betas: str = \"0.9,0.999\",\n    training_state_checkpointing_steps: int = 0,\n    weight_only_checkpointing_steps: int = 0,\n    log_visualization: bool = False,\n    simulate_generator_forward: bool = False,\n    warp_denoising_step: bool = False,\n    num_frame_per_block: int = 3,\n    independent_first_frame: bool = False,\n    enable_gradient_masking: bool = True,\n    gradient_mask_last_n_frames: int = 21,\n    same_step_across_blocks: bool = False,\n    last_step_only: bool = False,\n    context_noise: int = 0,\n)\n</code></pre> <p>               Bases: <code>FastVideoArgs</code></p> <p>Training arguments. Inherits from FastVideoArgs and adds training-specific arguments. If there are any conflicts, the training arguments will take precedence.</p>"},{"location":"api/fastvideo/fastvideo_args/#fastvideo.fastvideo_args.WorkloadType","title":"fastvideo.fastvideo_args.WorkloadType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration for different workload types.</p> <p>Inherits from str to allow string comparison for backward compatibility.</p>"},{"location":"api/fastvideo/fastvideo_args/#fastvideo.fastvideo_args.WorkloadType-functions","title":"Functions","text":""},{"location":"api/fastvideo/fastvideo_args/#fastvideo.fastvideo_args.WorkloadType.choices","title":"fastvideo.fastvideo_args.WorkloadType.choices  <code>classmethod</code>","text":"<pre><code>choices() -&gt; list[str]\n</code></pre> <p>Get all available choices as strings for argparse.</p> Source code in <code>fastvideo/fastvideo_args.py</code> <pre><code>@classmethod\ndef choices(cls) -&gt; list[str]:\n    \"\"\"Get all available choices as strings for argparse.\"\"\"\n    return [workload.value for workload in cls]\n</code></pre>"},{"location":"api/fastvideo/fastvideo_args/#fastvideo.fastvideo_args.WorkloadType.from_string","title":"fastvideo.fastvideo_args.WorkloadType.from_string  <code>classmethod</code>","text":"<pre><code>from_string(value: str) -&gt; WorkloadType\n</code></pre> <p>Convert string to WorkloadType enum.</p> Source code in <code>fastvideo/fastvideo_args.py</code> <pre><code>@classmethod\ndef from_string(cls, value: str) -&gt; \"WorkloadType\":\n    \"\"\"Convert string to WorkloadType enum.\"\"\"\n    try:\n        return cls(value.lower())\n    except ValueError:\n        raise ValueError(\n            f\"Invalid workload type: {value}. Must be one of: {', '.join([m.value for m in cls])}\"\n        ) from None\n</code></pre>"},{"location":"api/fastvideo/fastvideo_args/#fastvideo.fastvideo_args-functions","title":"Functions","text":""},{"location":"api/fastvideo/fastvideo_args/#fastvideo.fastvideo_args.prepare_fastvideo_args","title":"fastvideo.fastvideo_args.prepare_fastvideo_args","text":"<pre><code>prepare_fastvideo_args(argv: list[str]) -&gt; FastVideoArgs\n</code></pre> <p>Prepare the inference arguments from the command line arguments.</p> <p>Parameters:</p> Name Type Description Default <code>argv</code> <code>list[str]</code> <p>The command line arguments. Typically, it should be <code>sys.argv[1:]</code> to ensure compatibility with <code>parse_args</code> when no arguments are passed.</p> required <p>Returns:</p> Type Description <code>FastVideoArgs</code> <p>The inference arguments.</p> Source code in <code>fastvideo/fastvideo_args.py</code> <pre><code>def prepare_fastvideo_args(argv: list[str]) -&gt; FastVideoArgs:\n    \"\"\"\n    Prepare the inference arguments from the command line arguments.\n\n    Args:\n        argv: The command line arguments. Typically, it should be `sys.argv[1:]`\n            to ensure compatibility with `parse_args` when no arguments are passed.\n\n    Returns:\n        The inference arguments.\n    \"\"\"\n    parser = FlexibleArgumentParser()\n    FastVideoArgs.add_cli_args(parser)\n    raw_args = parser.parse_args(argv)\n    fastvideo_args = FastVideoArgs.from_cli_args(raw_args)\n    global _current_fastvideo_args\n    _current_fastvideo_args = fastvideo_args\n    return fastvideo_args\n</code></pre>"},{"location":"api/fastvideo/fastvideo_args/#fastvideo.fastvideo_args.set_current_fastvideo_args","title":"fastvideo.fastvideo_args.set_current_fastvideo_args","text":"<pre><code>set_current_fastvideo_args(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Temporarily set the current fastvideo config. Used during model initialization. We save the current fastvideo config in a global variable, so that all modules can access it, e.g. custom ops can access the fastvideo config to determine how to dispatch.</p> Source code in <code>fastvideo/fastvideo_args.py</code> <pre><code>@contextmanager\ndef set_current_fastvideo_args(fastvideo_args: FastVideoArgs):\n    \"\"\"\n    Temporarily set the current fastvideo config.\n    Used during model initialization.\n    We save the current fastvideo config in a global variable,\n    so that all modules can access it, e.g. custom ops\n    can access the fastvideo config to determine how to dispatch.\n    \"\"\"\n    global _current_fastvideo_args\n    old_fastvideo_args = _current_fastvideo_args\n    try:\n        _current_fastvideo_args = fastvideo_args\n        yield\n    finally:\n        _current_fastvideo_args = old_fastvideo_args\n</code></pre>"},{"location":"api/fastvideo/layers/","title":"layers","text":""},{"location":"api/fastvideo/layers/#fastvideo.layers","title":"layers","text":""},{"location":"api/fastvideo/layers/#fastvideo.layers-modules","title":"Modules","text":""},{"location":"api/fastvideo/layers/#fastvideo.layers.activation","title":"fastvideo.layers.activation","text":"<p>Custom activation functions.</p>"},{"location":"api/fastvideo/layers/#fastvideo.layers.activation-classes","title":"Classes","text":""},{"location":"api/fastvideo/layers/#fastvideo.layers.activation.GeluAndMul","title":"fastvideo.layers.activation.GeluAndMul","text":"<pre><code>GeluAndMul(approximate: str = 'none')\n</code></pre> <p>               Bases: <code>CustomOp</code></p> <p>An activation function for GeGLU.</p> <p>The function computes x -&gt; GELU(x[:d]) * x[d:] where d = x.shape[-1] // 2.</p> Shapes <p>x: (batch_size, seq_len, 2 * d) or (num_tokens, 2 * d) return: (batch_size, seq_len, d) or (num_tokens, d)</p> Source code in <code>fastvideo/layers/activation.py</code> <pre><code>def __init__(self, approximate: str = \"none\"):\n    super().__init__()\n    self.approximate = approximate\n    if approximate not in (\"none\", \"tanh\"):\n        raise ValueError(f\"Unknown approximate mode: {approximate}\")\n</code></pre> Functions\u00b6 fastvideo.layers.activation.GeluAndMul.forward_native \u00b6 <pre><code>forward_native(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>PyTorch-native implementation equivalent to forward().</p> Source code in <code>fastvideo/layers/activation.py</code> <pre><code>def forward_native(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"PyTorch-native implementation equivalent to forward().\"\"\"\n    d = x.shape[-1] // 2\n    return F.gelu(x[..., :d], approximate=self.approximate) * x[..., d:]\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.activation.NewGELU","title":"fastvideo.layers.activation.NewGELU","text":"<pre><code>NewGELU()\n</code></pre> <p>               Bases: <code>CustomOp</code></p> Source code in <code>fastvideo/layers/activation.py</code> <pre><code>def __init__(self):\n    super().__init__()\n</code></pre> Functions\u00b6 fastvideo.layers.activation.NewGELU.forward_native \u00b6 <pre><code>forward_native(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>PyTorch-native implementation equivalent to forward().</p> Source code in <code>fastvideo/layers/activation.py</code> <pre><code>def forward_native(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"PyTorch-native implementation equivalent to forward().\"\"\"\n    c = math.sqrt(2.0 / math.pi)\n    return 0.5 * x * (1.0 + torch.tanh(c *\n                                       (x + 0.044715 * torch.pow(x, 3.0))))\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.activation.QuickGELU","title":"fastvideo.layers.activation.QuickGELU","text":"<pre><code>QuickGELU()\n</code></pre> <p>               Bases: <code>CustomOp</code></p> Source code in <code>fastvideo/layers/activation.py</code> <pre><code>def __init__(self):\n    super().__init__()\n</code></pre> Functions\u00b6 fastvideo.layers.activation.QuickGELU.forward_native \u00b6 <pre><code>forward_native(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>PyTorch-native implementation equivalent to forward().</p> Source code in <code>fastvideo/layers/activation.py</code> <pre><code>def forward_native(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"PyTorch-native implementation equivalent to forward().\"\"\"\n    return x * torch.sigmoid(1.702 * x)\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.activation.SiluAndMul","title":"fastvideo.layers.activation.SiluAndMul","text":"<pre><code>SiluAndMul()\n</code></pre> <p>               Bases: <code>CustomOp</code></p> <p>An activation function for SwiGLU.</p> <p>The function computes x -&gt; silu(x[:d]) * x[d:] where d = x.shape[-1] // 2.</p> Shapes <p>x: (num_tokens, 2 * d) or (batch_size, seq_len, 2 * d) return: (num_tokens, d) or (batch_size, seq_len, d)</p> Source code in <code>fastvideo/layers/activation.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__()\n</code></pre> Functions\u00b6 fastvideo.layers.activation.SiluAndMul.forward_native \u00b6 <pre><code>forward_native(x: Tensor) -&gt; torch.Tensor\n</code></pre> <p>PyTorch-native implementation equivalent to forward().</p> Source code in <code>fastvideo/layers/activation.py</code> <pre><code>def forward_native(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"PyTorch-native implementation equivalent to forward().\"\"\"\n    d = x.shape[-1] // 2\n    return F.silu(x[..., :d]) * x[..., d:]\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.activation-functions","title":"Functions","text":""},{"location":"api/fastvideo/layers/#fastvideo.layers.activation.get_act_and_mul_fn","title":"fastvideo.layers.activation.get_act_and_mul_fn","text":"<pre><code>get_act_and_mul_fn(act_fn_name: str) -&gt; nn.Module\n</code></pre> <p>Get an activation-and-mul (i.e. SiluAndMul) function by name.</p> Source code in <code>fastvideo/layers/activation.py</code> <pre><code>def get_act_and_mul_fn(act_fn_name: str) -&gt; nn.Module:\n    \"\"\"Get an activation-and-mul (i.e. SiluAndMul) function by name.\"\"\"\n    act_fn_name = act_fn_name.lower()\n    if act_fn_name not in _ACTIVATION_AND_MUL_REGISTRY:\n        raise ValueError(\n            f\"Activation function {act_fn_name!r} is not supported.\")\n\n    return _ACTIVATION_AND_MUL_REGISTRY[act_fn_name]()\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.activation.get_act_fn","title":"fastvideo.layers.activation.get_act_fn","text":"<pre><code>get_act_fn(act_fn_name: str) -&gt; nn.Module\n</code></pre> <p>Get an activation function by name.</p> Source code in <code>fastvideo/layers/activation.py</code> <pre><code>def get_act_fn(act_fn_name: str) -&gt; nn.Module:\n    \"\"\"Get an activation function by name.\"\"\"\n    act_fn_name = act_fn_name.lower()\n    if act_fn_name not in _ACTIVATION_REGISTRY:\n        raise ValueError(\n            f\"Activation function {act_fn_name!r} is not supported.\")\n\n    return _ACTIVATION_REGISTRY[act_fn_name]()\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.custom_op","title":"fastvideo.layers.custom_op","text":""},{"location":"api/fastvideo/layers/#fastvideo.layers.custom_op-classes","title":"Classes","text":""},{"location":"api/fastvideo/layers/#fastvideo.layers.custom_op.CustomOp","title":"fastvideo.layers.custom_op.CustomOp","text":"<pre><code>CustomOp()\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Base class for custom ops. Dispatches the forward method to the appropriate backend.</p> Source code in <code>fastvideo/layers/custom_op.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__()\n    self._forward_method = self.dispatch_forward()\n</code></pre> Functions\u00b6 fastvideo.layers.custom_op.CustomOp.default_on <code>staticmethod</code> \u00b6 <pre><code>default_on() -&gt; bool\n</code></pre> <p>On by default if level &lt; CompilationLevel.PIECEWISE Specifying 'all' or 'none' in custom_op takes precedence.</p> Source code in <code>fastvideo/layers/custom_op.py</code> <pre><code>@staticmethod\ndef default_on() -&gt; bool:\n    \"\"\"\n    On by default if level &lt; CompilationLevel.PIECEWISE\n    Specifying 'all' or 'none' in custom_op takes precedence.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.layers.custom_op.CustomOp.forward_native \u00b6 <pre><code>forward_native(*args, **kwargs) -&gt; Any\n</code></pre> <p>PyTorch-native implementation of the forward method. This method is optional. If implemented, it can be used with compilers such as torch.compile or PyTorch XLA. Also, it can be used for testing purposes.</p> Source code in <code>fastvideo/layers/custom_op.py</code> <pre><code>def forward_native(self, *args, **kwargs) -&gt; Any:\n    \"\"\"PyTorch-native implementation of the forward method.\n    This method is optional. If implemented, it can be used with compilers\n    such as torch.compile or PyTorch XLA. Also, it can be used for testing\n    purposes.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.custom_op-functions","title":"Functions","text":""},{"location":"api/fastvideo/layers/#fastvideo.layers.layernorm","title":"fastvideo.layers.layernorm","text":"<p>Custom normalization layers.</p>"},{"location":"api/fastvideo/layers/#fastvideo.layers.layernorm-classes","title":"Classes","text":""},{"location":"api/fastvideo/layers/#fastvideo.layers.layernorm.LayerNormScaleShift","title":"fastvideo.layers.layernorm.LayerNormScaleShift","text":"<pre><code>LayerNormScaleShift(\n    hidden_size: int,\n    norm_type: str = \"rms\",\n    eps: float = 1e-06,\n    elementwise_affine: bool = False,\n    dtype: dtype = torch.float32,\n    compute_dtype: dtype | None = None,\n    prefix: str = \"\",\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Fused operation that combines LayerNorm with scale and shift operations. This reduces memory bandwidth by combining memory-bound operations.</p> Source code in <code>fastvideo/layers/layernorm.py</code> <pre><code>def __init__(\n    self,\n    hidden_size: int,\n    norm_type: str = \"rms\",\n    eps: float = 1e-6,\n    elementwise_affine: bool = False,\n    dtype: torch.dtype = torch.float32,\n    compute_dtype: torch.dtype | None = None,\n    prefix: str = \"\",\n):\n    super().__init__()\n    self.compute_dtype = compute_dtype\n    if norm_type == \"rms\":\n        self.norm = RMSNorm(hidden_size,\n                            has_weight=elementwise_affine,\n                            eps=eps)\n    elif norm_type == \"layer\":\n        if self.compute_dtype == torch.float32:\n            self.norm = FP32LayerNorm(hidden_size,\n                                      elementwise_affine=elementwise_affine,\n                                      eps=eps)\n        else:\n            self.norm = nn.LayerNorm(hidden_size,\n                                     elementwise_affine=elementwise_affine,\n                                     eps=eps,\n                                     dtype=dtype)\n    else:\n        raise NotImplementedError(f\"Norm type {norm_type} not implemented\")\n</code></pre> Functions\u00b6 fastvideo.layers.layernorm.LayerNormScaleShift.forward \u00b6 <pre><code>forward(\n    x: Tensor, shift: Tensor, scale: Tensor\n) -&gt; torch.Tensor\n</code></pre> <p>Apply ln followed by scale and shift in a single fused operation.</p> Source code in <code>fastvideo/layers/layernorm.py</code> <pre><code>def forward(self, x: torch.Tensor, shift: torch.Tensor,\n            scale: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Apply ln followed by scale and shift in a single fused operation.\"\"\"\n    # x.shape: [batch_size, seq_len, inner_dim]\n    normalized = self.norm(x)\n    if self.compute_dtype == torch.float32:\n        normalized = normalized.float()\n\n    if scale.dim() == 4:\n        # scale.shape: [batch_size, num_frames, 1, inner_dim]\n        num_frames = scale.shape[1]\n        frame_seqlen = normalized.shape[1] // num_frames\n        output = (\n            normalized.unflatten(dim=1, sizes=(num_frames, frame_seqlen)) *\n            (1.0 + scale) + shift).flatten(1, 2)\n    else:\n        # scale.shape: [batch_size, 1, inner_dim]\n        # shift.shape: [batch_size, 1, inner_dim]\n        output = normalized * (1.0 + scale) + shift\n\n    if self.compute_dtype == torch.float32:\n        output = output.to(x.dtype)\n\n    return output\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.layernorm.RMSNorm","title":"fastvideo.layers.layernorm.RMSNorm","text":"<pre><code>RMSNorm(\n    hidden_size: int,\n    eps: float = 1e-06,\n    dtype: dtype = torch.float32,\n    var_hidden_size: int | None = None,\n    has_weight: bool = True,\n)\n</code></pre> <p>               Bases: <code>CustomOp</code></p> <p>Root mean square normalization.</p> <p>Computes x -&gt; w * x / sqrt(E[x^2] + eps) where w is the learned weight. Refer to https://arxiv.org/abs/1910.07467</p> Source code in <code>fastvideo/layers/layernorm.py</code> <pre><code>def __init__(\n    self,\n    hidden_size: int,\n    eps: float = 1e-6,\n    dtype: torch.dtype = torch.float32,\n    var_hidden_size: int | None = None,\n    has_weight: bool = True,\n) -&gt; None:\n    super().__init__()\n\n    self.hidden_size = hidden_size\n    self.variance_epsilon = eps\n    self.variance_size_override = (None if var_hidden_size == hidden_size\n                                   else var_hidden_size)\n    self.has_weight = has_weight\n\n    from fastvideo.platforms import current_platform\n\n    self.weight = torch.ones(hidden_size) if current_platform.is_cuda_alike(\n    ) else torch.ones(hidden_size, dtype=dtype)\n    if self.has_weight:\n        self.weight = nn.Parameter(self.weight)\n</code></pre> Functions\u00b6 fastvideo.layers.layernorm.RMSNorm.forward_native \u00b6 <pre><code>forward_native(\n    x: Tensor, residual: Tensor | None = None\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>PyTorch-native implementation equivalent to forward().</p> Source code in <code>fastvideo/layers/layernorm.py</code> <pre><code>def forward_native(\n    self,\n    x: torch.Tensor,\n    residual: torch.Tensor | None = None,\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"PyTorch-native implementation equivalent to forward().\"\"\"\n    orig_dtype = x.dtype\n    x = x.to(torch.float32)\n    if residual is not None:\n        x = x + residual.to(torch.float32)\n        residual = x.to(orig_dtype)\n\n    hidden_size = x.shape[-1]\n    if hidden_size != self.hidden_size:\n        raise ValueError(\"Expected hidden_size to be \"\n                         f\"{self.hidden_size}, but found: {hidden_size}\")\n\n    if self.variance_size_override is None:\n        x_var = x\n    else:\n        if hidden_size &lt; self.variance_size_override:\n            raise ValueError(\n                \"Expected hidden_size to be at least \"\n                f\"{self.variance_size_override}, but found: {hidden_size}\")\n\n        x_var = x[:, :, :self.variance_size_override]\n\n    variance = x_var.pow(2).mean(dim=-1, keepdim=True)\n\n    x = x * torch.rsqrt(variance + self.variance_epsilon)\n    x = x.to(orig_dtype)\n    if self.has_weight:\n        x = x * self.weight\n    if residual is None:\n        return x\n    else:\n        return x, residual\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.layernorm.ScaleResidual","title":"fastvideo.layers.layernorm.ScaleResidual","text":"<pre><code>ScaleResidual(prefix: str = '')\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Applies gated residual connection.</p> Source code in <code>fastvideo/layers/layernorm.py</code> <pre><code>def __init__(self, prefix: str = \"\"):\n    super().__init__()\n</code></pre> Functions\u00b6 fastvideo.layers.layernorm.ScaleResidual.forward \u00b6 <pre><code>forward(\n    residual: Tensor, x: Tensor, gate: Tensor\n) -&gt; torch.Tensor\n</code></pre> <p>Apply gated residual connection.</p> Source code in <code>fastvideo/layers/layernorm.py</code> <pre><code>def forward(self, residual: torch.Tensor, x: torch.Tensor,\n            gate: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Apply gated residual connection.\"\"\"\n    # x.shape: [batch_size, seq_len, inner_dim]\n    if gate.dim() == 4:\n        # gate.shape: [batch_size, num_frames, 1, inner_dim]\n        num_frames = gate.shape[1]\n        frame_seqlen = x.shape[1] // num_frames\n        return residual + (x.unflatten(\n            dim=1, sizes=(num_frames, frame_seqlen)) * gate).flatten(1, 2)\n    else:\n        # gate.shape: [batch_size, 1, inner_dim]\n        return residual + x * gate\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.layernorm.ScaleResidualLayerNormScaleShift","title":"fastvideo.layers.layernorm.ScaleResidualLayerNormScaleShift","text":"<pre><code>ScaleResidualLayerNormScaleShift(\n    hidden_size: int,\n    norm_type: str = \"rms\",\n    eps: float = 1e-06,\n    elementwise_affine: bool = False,\n    dtype: dtype = torch.float32,\n    compute_dtype: dtype | None = None,\n    prefix: str = \"\",\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Fused operation that combines: 1. Gated residual connection 2. LayerNorm 3. Scale and shift operations</p> <p>This reduces memory bandwidth by combining memory-bound operations.</p> Source code in <code>fastvideo/layers/layernorm.py</code> <pre><code>def __init__(\n    self,\n    hidden_size: int,\n    norm_type: str = \"rms\",\n    eps: float = 1e-6,\n    elementwise_affine: bool = False,\n    dtype: torch.dtype = torch.float32,\n    compute_dtype: torch.dtype | None = None,\n    prefix: str = \"\",\n):\n    super().__init__()\n    if norm_type == \"rms\":\n        self.norm = RMSNorm(hidden_size,\n                            has_weight=elementwise_affine,\n                            eps=eps,\n                            dtype=dtype)\n    elif norm_type == \"layer\":\n        if compute_dtype == torch.float32:\n            self.norm = FP32LayerNorm(hidden_size,\n                                      elementwise_affine=elementwise_affine,\n                                      eps=eps)\n        else:\n            self.norm = nn.LayerNorm(hidden_size,\n                                     elementwise_affine=elementwise_affine,\n                                     eps=eps,\n                                     dtype=dtype)\n    else:\n        raise NotImplementedError(f\"Norm type {norm_type} not implemented\")\n</code></pre> Functions\u00b6 fastvideo.layers.layernorm.ScaleResidualLayerNormScaleShift.forward \u00b6 <pre><code>forward(\n    residual: Tensor,\n    x: Tensor,\n    gate: Tensor | int,\n    shift: Tensor,\n    scale: Tensor,\n) -&gt; tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Apply gated residual connection, followed by layernorm and  scale/shift in a single fused operation.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tuple containing:</p> <code>Tensor</code> <ul> <li>normalized and modulated output</li> </ul> <code>tuple[Tensor, Tensor]</code> <ul> <li>residual value (value after residual connection  but before normalization)</li> </ul> Source code in <code>fastvideo/layers/layernorm.py</code> <pre><code>def forward(self, residual: torch.Tensor, x: torch.Tensor,\n            gate: torch.Tensor | int, shift: torch.Tensor,\n            scale: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Apply gated residual connection, followed by layernorm and \n    scale/shift in a single fused operation.\n\n    Returns:\n        Tuple containing:\n        - normalized and modulated output\n        - residual value (value after residual connection \n          but before normalization)\n    \"\"\"\n    # x.shape: [batch_size, seq_len, inner_dim]\n    # Apply residual connection with gating\n    if isinstance(gate, int):\n        # used by cross-attention, should be 1\n        assert gate == 1\n        residual_output = residual + x\n    elif isinstance(gate, torch.Tensor):\n        if gate.dim() == 4:\n            # gate.shape: [batch_size, num_frames, 1, inner_dim]\n            num_frames = gate.shape[1]\n            frame_seqlen = x.shape[1] // num_frames\n            residual_output = residual + (\n                x.unflatten(dim=1, sizes=(num_frames, frame_seqlen)) *\n                gate).flatten(1, 2)\n        else:\n            # used by bidirectional self attention\n            # gate.shape: [batch_size, 1, inner_dim]\n            residual_output = residual + x * gate\n    else:\n        raise ValueError(f\"Gate type {type(gate)} not supported\")\n    # residual_output.shape: [batch_size, seq_len, inner_dim]\n\n    # Apply normalization\n    normalized = self.norm(residual_output)\n    # Apply scale and shift\n    if isinstance(scale, torch.Tensor) and scale.dim() == 4:\n        # scale.shape: [batch_size, num_frames, 1, inner_dim]\n        # shift.shape: [batch_size, num_frames, 1, inner_dim]\n        num_frames = scale.shape[1]\n        frame_seqlen = normalized.shape[1] // num_frames\n        modulated = (\n            normalized.unflatten(dim=1, sizes=(num_frames, frame_seqlen)) *\n            (1.0 + scale) + shift).flatten(1, 2)\n    else:\n        modulated = normalized * (1.0 + scale) + shift\n    return modulated, residual_output\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.linear","title":"fastvideo.layers.linear","text":""},{"location":"api/fastvideo/layers/#fastvideo.layers.linear-classes","title":"Classes","text":""},{"location":"api/fastvideo/layers/#fastvideo.layers.linear.ColumnParallelLinear","title":"fastvideo.layers.linear.ColumnParallelLinear","text":"<pre><code>ColumnParallelLinear(\n    input_size: int,\n    output_size: int,\n    bias: bool = True,\n    gather_output: bool = False,\n    skip_bias_add: bool = False,\n    params_dtype: dtype | None = None,\n    quant_config: QuantizationConfig | None = None,\n    output_sizes: list[int] | None = None,\n    prefix: str = \"\",\n)\n</code></pre> <p>               Bases: <code>LinearBase</code></p> <p>Linear layer with column parallelism.</p> <p>The linear layer is defined as Y = XA + b. A is parallelized along its second dimension as A = [A_1, ..., A_p].</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>first dimension of matrix A.</p> required <code>output_size</code> <code>int</code> <p>second dimension of matrix A.</p> required <code>bias</code> <code>bool</code> <p>If true, add bias.</p> <code>True</code> <code>gather_output</code> <code>bool</code> <p>If true, call all-gather on output and make Y available            to all GPUs, otherwise, every GPU will have its output            which is Y_i = XA_i</p> <code>False</code> <code>skip_bias_add</code> <code>bool</code> <p>This was added to enable performance optimizations where            bias can be fused with other element-wise operations. we            skip adding bias but instead return it.</p> <code>False</code> <code>params_dtype</code> <code>dtype | None</code> <p>Data type for the parameters.</p> <code>None</code> <code>quant_config</code> <code>QuantizationConfig | None</code> <p>Quantization configure.</p> <code>None</code> <code>output_sizes</code> <code>list[int] | None</code> <p>list of output sizes packed into one output, like for QKV            the list would be size 3.</p> <code>None</code> <code>prefix</code> <code>str</code> <p>The name of the layer in the state dict, including all parents             (e.g. model.layers.0.qkv_proj)</p> <code>''</code> Source code in <code>fastvideo/layers/linear.py</code> <pre><code>def __init__(self,\n             input_size: int,\n             output_size: int,\n             bias: bool = True,\n             gather_output: bool = False,\n             skip_bias_add: bool = False,\n             params_dtype: torch.dtype | None = None,\n             quant_config: QuantizationConfig | None = None,\n             output_sizes: list[int] | None = None,\n             prefix: str = \"\"):\n    # Divide the weight matrix along the last dimension.\n    self.tp_size = get_tp_world_size()\n    self.input_size_per_partition = input_size\n    self.output_size_per_partition = divide(output_size, self.tp_size)\n    self.output_partition_sizes = [self.output_size_per_partition]\n    # If QKV or MergedColumn, use output size of each partition.\n    if hasattr(self, \"output_sizes\"):\n        self.output_partition_sizes = [\n            divide(output_size, self.tp_size)\n            for output_size in self.output_sizes\n        ]\n\n    super().__init__(input_size, output_size, skip_bias_add, params_dtype,\n                     quant_config, prefix)\n\n    self.gather_output = gather_output\n\n    if output_sizes is None:\n        output_sizes = [output_size]\n\n    assert self.quant_method is not None\n    self.quant_method.create_weights(\n        layer=self,\n        input_size_per_partition=self.input_size_per_partition,\n        output_partition_sizes=self.output_partition_sizes,\n        input_size=self.input_size,\n        output_size=self.output_size,\n        params_dtype=self.params_dtype,\n        weight_loader=(\n            self.weight_loader_v2 if self.quant_method.__class__.__name__\n            in WEIGHT_LOADER_V2_SUPPORTED else self.weight_loader))\n    if bias:\n        self.bias = Parameter(\n            torch.empty(\n                self.output_size_per_partition,\n                dtype=params_dtype,\n            ))\n        set_weight_attrs(self.bias, {\n            \"output_dim\": 0,\n            \"weight_loader\": self.weight_loader,\n        })\n    else:\n        self.register_parameter(\"bias\", None)\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.linear.LinearBase","title":"fastvideo.layers.linear.LinearBase","text":"<pre><code>LinearBase(\n    input_size: int,\n    output_size: int,\n    skip_bias_add: bool = False,\n    params_dtype: dtype | None = None,\n    quant_config: QuantizationConfig | None = None,\n    prefix: str = \"\",\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Base linear layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>input dimension of the linear layer.</p> required <code>output_size</code> <code>int</code> <p>output dimension of the linear layer.</p> required <code>bias</code> <p>If true, add bias.</p> required <code>skip_bias_add</code> <code>bool</code> <p>If true, skip adding bias but instead return it.</p> <code>False</code> <code>params_dtype</code> <code>dtype | None</code> <p>Data type for the parameters.</p> <code>None</code> <code>quant_config</code> <code>QuantizationConfig | None</code> <p>Quantization configure.</p> <code>None</code> Source code in <code>fastvideo/layers/linear.py</code> <pre><code>def __init__(\n    self,\n    input_size: int,\n    output_size: int,\n    skip_bias_add: bool = False,\n    params_dtype: torch.dtype | None = None,\n    quant_config: QuantizationConfig | None = None,\n    prefix: str = \"\",\n):\n    super().__init__()\n\n    # Keep input parameters\n    self.input_size = input_size\n    self.output_size = output_size\n    self.skip_bias_add = skip_bias_add\n    if params_dtype is None:\n        params_dtype = torch.get_default_dtype()\n    self.params_dtype = params_dtype\n    self.quant_config = quant_config\n    self.prefix = prefix\n    if quant_config is None:\n        self.quant_method: QuantizeMethodBase | None = UnquantizedLinearMethod(\n        )\n    else:\n        self.quant_method = quant_config.get_quant_method(self,\n                                                          prefix=prefix)\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.linear.LinearMethodBase","title":"fastvideo.layers.linear.LinearMethodBase","text":"<p>               Bases: <code>QuantizeMethodBase</code></p> <p>Base class for different (maybe quantized) linear methods.</p> Functions\u00b6 fastvideo.layers.linear.LinearMethodBase.apply <code>abstractmethod</code> \u00b6 <pre><code>apply(\n    layer: Module, x: Tensor, bias: Tensor | None = None\n) -&gt; torch.Tensor\n</code></pre> <p>Apply the weights in layer to the input tensor. Expects create_weights to have been called before on the layer.</p> Source code in <code>fastvideo/layers/linear.py</code> <pre><code>@abstractmethod\ndef apply(self,\n          layer: torch.nn.Module,\n          x: torch.Tensor,\n          bias: torch.Tensor | None = None) -&gt; torch.Tensor:\n    \"\"\"Apply the weights in layer to the input tensor.\n    Expects create_weights to have been called before on the layer.\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.layers.linear.LinearMethodBase.create_weights <code>abstractmethod</code> \u00b6 <pre><code>create_weights(\n    layer: Module,\n    input_size_per_partition: int,\n    output_partition_sizes: list[int],\n    input_size: int,\n    output_size: int,\n    params_dtype: dtype,\n    **extra_weight_attrs\n) -&gt; None\n</code></pre> <p>Create weights for a linear layer.     The weights will be set as attributes of the layer.</p> <p>Parameters:</p> Name Type Description Default <code>layer</code> <code>Module</code> <p>The layer that is using the LinearMethodBase factory.</p> required <code>input_size_per_partition</code> <code>int</code> <p>Size of the weight input dim on rank X.</p> required <code>output_partition_sizes</code> <code>list[int]</code> <p>Sizes of the output dim of each logical  weight on rank X. E.g., output_partition_sizes for QKVLinear is a list contains the width of Wq, Wk, Wv on rank X.</p> required <code>input_size</code> <code>int</code> <p>Size of the input dim of the weight across all ranks.</p> required <code>output_size</code> <code>int</code> <p>Size of the output dim of the weight across all ranks.</p> required <code>params_dtype</code> <code>dtype</code> <p>Datatype of the parameters.</p> required Source code in <code>fastvideo/layers/linear.py</code> <pre><code>@abstractmethod\ndef create_weights(self, layer: torch.nn.Module,\n                   input_size_per_partition: int,\n                   output_partition_sizes: list[int], input_size: int,\n                   output_size: int, params_dtype: torch.dtype,\n                   **extra_weight_attrs) -&gt; None:\n    \"\"\"Create weights for a linear layer. \n       The weights will be set as attributes of the layer.\n\n    Args:\n        layer: The layer that is using the LinearMethodBase factory.\n        input_size_per_partition: Size of the weight input dim on rank X.\n        output_partition_sizes: Sizes of the output dim of each logical \n            weight on rank X. E.g., output_partition_sizes for QKVLinear\n            is a list contains the width of Wq, Wk, Wv on rank X.\n        input_size: Size of the input dim of the weight across all ranks.\n        output_size: Size of the output dim of the weight across all ranks.\n        params_dtype: Datatype of the parameters.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.linear.MergedColumnParallelLinear","title":"fastvideo.layers.linear.MergedColumnParallelLinear","text":"<pre><code>MergedColumnParallelLinear(\n    input_size: int,\n    output_sizes: list[int],\n    bias: bool = True,\n    gather_output: bool = False,\n    skip_bias_add: bool = False,\n    params_dtype: dtype | None = None,\n    quant_config: QuantizationConfig | None = None,\n    prefix: str = \"\",\n)\n</code></pre> <p>               Bases: <code>ColumnParallelLinear</code></p> <p>Packed linear layers with column parallelism.</p> <p>Similar to ColumnParallelLinear, but the weight matrix is concatenated along the output dimension. When the weight matrix is loaded, the different partitions are sharded separately.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>input dimension of the linear layer.</p> required <code>output_sizes</code> <code>list[int]</code> <p>list of output dimensions of the linear layer.</p> required <code>bias</code> <code>bool</code> <p>If true, add bias.</p> <code>True</code> <code>gather_output</code> <code>bool</code> <p>If true, call all-gather on output and make the output            available to all GPUs, otherwise, every GPU will have            its own output.</p> <code>False</code> <code>skip_bias_add</code> <code>bool</code> <p>This was added to enable performance optimizations where            bias can be fused with other element-wise operations. we            skip adding bias but instead return it.</p> <code>False</code> <code>params_dtype</code> <code>dtype | None</code> <p>Data type for the parameters.</p> <code>None</code> <code>quant_config</code> <code>QuantizationConfig | None</code> <p>Quantization configure.</p> <code>None</code> <code>prefix</code> <code>str</code> <p>The name of the layer in the state dict, including all parents             (e.g. model.layers.0.qkv_proj)</p> <code>''</code> Source code in <code>fastvideo/layers/linear.py</code> <pre><code>def __init__(self,\n             input_size: int,\n             output_sizes: list[int],\n             bias: bool = True,\n             gather_output: bool = False,\n             skip_bias_add: bool = False,\n             params_dtype: torch.dtype | None = None,\n             quant_config: QuantizationConfig | None = None,\n             prefix: str = \"\"):\n    self.output_sizes = output_sizes\n    tp_size = get_tp_world_size()\n    assert all(output_size % tp_size == 0 for output_size in output_sizes)\n    super().__init__(input_size=input_size,\n                     output_size=sum(output_sizes),\n                     bias=bias,\n                     gather_output=gather_output,\n                     skip_bias_add=skip_bias_add,\n                     params_dtype=params_dtype,\n                     quant_config=quant_config,\n                     prefix=prefix)\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.linear.QKVParallelLinear","title":"fastvideo.layers.linear.QKVParallelLinear","text":"<pre><code>QKVParallelLinear(\n    hidden_size: int,\n    head_size: int,\n    total_num_heads: int,\n    total_num_kv_heads: int | None = None,\n    bias: bool = True,\n    skip_bias_add: bool = False,\n    params_dtype: dtype | None = None,\n    quant_config: QuantizationConfig | None = None,\n    prefix: str = \"\",\n)\n</code></pre> <p>               Bases: <code>ColumnParallelLinear</code></p> <p>Linear layers for the attention's QKV transformation.</p> <p>Linear layers for the linear transformation of the query, key, and value vectors in the attention layer. The weight matrix is concatenated along the output dimension. The layer is parallelized along the head dimension. When the number of key/value heads is smaller than the number of query heads (e.g., multi-query/grouped-query attention), the key/value head may be replicated while the query heads are partitioned.</p> <p>Parameters:</p> Name Type Description Default <code>hidden_size</code> <code>int</code> <p>input hidden state size of the transformer.</p> required <code>head_size</code> <code>int</code> <p>size of each attention head.</p> required <code>total_num_heads</code> <code>int</code> <p>total number of attention query heads.</p> required <code>total_num_kv_heads</code> <code>int | None</code> <p>total number of attention key/value heads. If                 None, assume total_num_kv_heads = total_num_heads.</p> <code>None</code> <code>bias</code> <code>bool</code> <p>If true, add bias.</p> <code>True</code> <code>skip_bias_add</code> <code>bool</code> <p>This was added to enable performance optimizations where            bias can be fused with other element-wise operations. we            skip adding bias but instead return it.</p> <code>False</code> <code>params_dtype</code> <code>dtype | None</code> <p>Data type for the parameters.</p> <code>None</code> <code>quant_config</code> <code>QuantizationConfig | None</code> <p>Quantization configure.</p> <code>None</code> <code>prefix</code> <code>str</code> <p>The name of the layer in the state dict, including all parents             (e.g. model.layers.0.qkv_proj)</p> <code>''</code> Source code in <code>fastvideo/layers/linear.py</code> <pre><code>def __init__(self,\n             hidden_size: int,\n             head_size: int,\n             total_num_heads: int,\n             total_num_kv_heads: int | None = None,\n             bias: bool = True,\n             skip_bias_add: bool = False,\n             params_dtype: torch.dtype | None = None,\n             quant_config: QuantizationConfig | None = None,\n             prefix: str = \"\"):\n    self.hidden_size = hidden_size\n    self.head_size = head_size\n    self.total_num_heads = total_num_heads\n    if total_num_kv_heads is None:\n        total_num_kv_heads = total_num_heads\n    self.total_num_kv_heads = total_num_kv_heads\n    # Divide the weight matrix along the last dimension.\n    tp_size = get_tp_world_size()\n    self.num_heads = divide(self.total_num_heads, tp_size)\n    if tp_size &gt;= self.total_num_kv_heads:\n        self.num_kv_heads = 1\n        self.num_kv_head_replicas = divide(tp_size, self.total_num_kv_heads)\n    else:\n        self.num_kv_heads = divide(self.total_num_kv_heads, tp_size)\n        self.num_kv_head_replicas = 1\n    input_size = self.hidden_size\n    output_size = (self.num_heads +\n                   2 * self.num_kv_heads) * tp_size * self.head_size\n    self.output_sizes = [\n        self.num_heads * self.head_size * tp_size,  # q_proj\n        self.num_kv_heads * self.head_size * tp_size,  # k_proj\n        self.num_kv_heads * self.head_size * tp_size,  # v_proj \n    ]\n\n    super().__init__(input_size=input_size,\n                     output_size=output_size,\n                     bias=bias,\n                     gather_output=False,\n                     skip_bias_add=skip_bias_add,\n                     params_dtype=params_dtype,\n                     quant_config=quant_config,\n                     prefix=prefix)\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.linear.ReplicatedLinear","title":"fastvideo.layers.linear.ReplicatedLinear","text":"<pre><code>ReplicatedLinear(\n    input_size: int,\n    output_size: int,\n    bias: bool = True,\n    skip_bias_add: bool = False,\n    params_dtype: dtype | None = None,\n    quant_config: QuantizationConfig | None = None,\n    prefix: str = \"\",\n)\n</code></pre> <p>               Bases: <code>LinearBase</code></p> <p>Replicated linear layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>input dimension of the linear layer.</p> required <code>output_size</code> <code>int</code> <p>output dimension of the linear layer.</p> required <code>bias</code> <code>bool</code> <p>If true, add bias.</p> <code>True</code> <code>skip_bias_add</code> <code>bool</code> <p>If true, skip adding bias but instead return it.</p> <code>False</code> <code>params_dtype</code> <code>dtype | None</code> <p>Data type for the parameters.</p> <code>None</code> <code>quant_config</code> <code>QuantizationConfig | None</code> <p>Quantization configure.</p> <code>None</code> <code>prefix</code> <code>str</code> <p>The name of the layer in the state dict, including all parents             (e.g. model.layers.0.qkv_proj)</p> <code>''</code> Source code in <code>fastvideo/layers/linear.py</code> <pre><code>def __init__(self,\n             input_size: int,\n             output_size: int,\n             bias: bool = True,\n             skip_bias_add: bool = False,\n             params_dtype: torch.dtype | None = None,\n             quant_config: QuantizationConfig | None = None,\n             prefix: str = \"\"):\n    super().__init__(input_size,\n                     output_size,\n                     skip_bias_add,\n                     params_dtype,\n                     quant_config,\n                     prefix=prefix)\n\n    # All the linear layer supports quant method.\n    assert self.quant_method is not None\n    self.quant_method.create_weights(self,\n                                     self.input_size, [self.output_size],\n                                     self.input_size,\n                                     self.output_size,\n                                     self.params_dtype,\n                                     weight_loader=self.weight_loader)\n\n    if bias:\n        self.bias = Parameter(\n            torch.empty(\n                self.output_size,\n                dtype=self.params_dtype,\n            ))\n        set_weight_attrs(self.bias, {\n            \"output_dim\": 0,\n            \"weight_loader\": self.weight_loader,\n        })\n    else:\n        self.register_parameter(\"bias\", None)\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.linear.RowParallelLinear","title":"fastvideo.layers.linear.RowParallelLinear","text":"<pre><code>RowParallelLinear(\n    input_size: int,\n    output_size: int,\n    bias: bool = True,\n    input_is_parallel: bool = True,\n    skip_bias_add: bool = False,\n    params_dtype: dtype | None = None,\n    reduce_results: bool = True,\n    quant_config: QuantizationConfig | None = None,\n    prefix: str = \"\",\n)\n</code></pre> <p>               Bases: <code>LinearBase</code></p> <p>Linear layer with row parallelism.</p> <p>The linear layer is defined as Y = XA + b. A is parallelized along its first dimension and X along its second dimension as:            -   -           | A_1 |           | .   |       A = | .   |        X = [X_1, ..., X_p]           | .   |           | A_p |            -   - Arguments:     input_size: first dimension of matrix A.     output_size: second dimension of matrix A.     bias: If true, add bias. Note that bias is not parallelized.     input_is_parallel: If true, we assume that the input is already                        split across the GPUs and we do not split                        again.     skip_bias_add: This was added to enable performance optimization where                    bias can be fused with other element-wise operations.                    We skip adding bias but instead return it.     params_dtype: Data type for the parameters.     quant_config: Quantization configure.</p> Source code in <code>fastvideo/layers/linear.py</code> <pre><code>def __init__(self,\n             input_size: int,\n             output_size: int,\n             bias: bool = True,\n             input_is_parallel: bool = True,\n             skip_bias_add: bool = False,\n             params_dtype: torch.dtype | None = None,\n             reduce_results: bool = True,\n             quant_config: QuantizationConfig | None = None,\n             prefix: str = \"\"):\n    # Divide the weight matrix along the first dimension.\n    self.tp_rank = get_tp_rank()\n    self.tp_size = get_tp_world_size()\n    self.input_size_per_partition = divide(input_size, self.tp_size)\n    self.output_size_per_partition = output_size\n    self.output_partition_sizes = [output_size]\n\n    super().__init__(input_size, output_size, skip_bias_add, params_dtype,\n                     quant_config, prefix)\n\n    self.input_is_parallel = input_is_parallel\n    self.reduce_results = reduce_results\n\n    assert self.quant_method is not None\n    self.quant_method.create_weights(\n        layer=self,\n        input_size_per_partition=self.input_size_per_partition,\n        output_partition_sizes=self.output_partition_sizes,\n        input_size=self.input_size,\n        output_size=self.output_size,\n        params_dtype=self.params_dtype,\n        weight_loader=(\n            self.weight_loader_v2 if self.quant_method.__class__.__name__\n            in WEIGHT_LOADER_V2_SUPPORTED else self.weight_loader))\n    if not reduce_results and (bias and not skip_bias_add):\n        raise ValueError(\"When not reduce the results, adding bias to the \"\n                         \"results can lead to incorrect results\")\n\n    if bias:\n        self.bias = Parameter(\n            torch.empty(self.output_size, dtype=params_dtype))\n        set_weight_attrs(self.bias, {\n            \"output_dim\": 0,\n            \"weight_loader\": self.weight_loader,\n        })\n    else:\n        self.register_parameter(\"bias\", None)\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.linear.UnquantizedLinearMethod","title":"fastvideo.layers.linear.UnquantizedLinearMethod","text":"<p>               Bases: <code>LinearMethodBase</code></p> <p>Linear method without quantization.</p>"},{"location":"api/fastvideo/layers/#fastvideo.layers.linear-functions","title":"Functions","text":""},{"location":"api/fastvideo/layers/#fastvideo.layers.linear.adjust_scalar_to_fused_array","title":"fastvideo.layers.linear.adjust_scalar_to_fused_array","text":"<pre><code>adjust_scalar_to_fused_array(\n    param: Tensor,\n    loaded_weight: Tensor,\n    shard_id: str | int,\n) -&gt; tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>For fused modules (QKV and MLP) we have an array of length N that holds 1 scale for each \"logical\" matrix. So the param is an array of length N. The loaded_weight corresponds to  one of the shards on disk. Here, we slice the param based on  the shard_id for loading.</p> Source code in <code>fastvideo/layers/linear.py</code> <pre><code>def adjust_scalar_to_fused_array(\n        param: torch.Tensor, loaded_weight: torch.Tensor,\n        shard_id: str | int) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"For fused modules (QKV and MLP) we have an array of length\n    N that holds 1 scale for each \"logical\" matrix. So the param\n    is an array of length N. The loaded_weight corresponds to \n    one of the shards on disk. Here, we slice the param based on \n    the shard_id for loading.\n    \"\"\"\n    qkv_idxs = {\"q\": 0, \"k\": 1, \"v\": 2}\n\n    if isinstance(shard_id, str):\n        shard_id = qkv_idxs[shard_id]\n    elif not isinstance(shard_id, int):\n        raise ValueError(f\"Unknown Shard Id {shard_id}\")\n\n    # AutoFP8 scales do not have a shape\n    # compressed-tensors scales do have a shape\n    if len(loaded_weight.shape) != 0:\n        assert loaded_weight.shape[0] == 1\n        loaded_weight = loaded_weight[0]\n\n    return param[shard_id], loaded_weight\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.mlp","title":"fastvideo.layers.mlp","text":""},{"location":"api/fastvideo/layers/#fastvideo.layers.mlp-classes","title":"Classes","text":""},{"location":"api/fastvideo/layers/#fastvideo.layers.mlp.MLP","title":"fastvideo.layers.mlp.MLP","text":"<pre><code>MLP(\n    input_dim: int,\n    mlp_hidden_dim: int,\n    output_dim: int | None = None,\n    bias: bool = True,\n    act_type: str = \"gelu_pytorch_tanh\",\n    dtype: dtype | None = None,\n    prefix: str = \"\",\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>MLP for DiT blocks, NO gated linear units</p> Source code in <code>fastvideo/layers/mlp.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    mlp_hidden_dim: int,\n    output_dim: int | None = None,\n    bias: bool = True,\n    act_type: str = \"gelu_pytorch_tanh\",\n    dtype: torch.dtype | None = None,\n    prefix: str = \"\",\n):\n    super().__init__()\n    self.fc_in = ReplicatedLinear(\n        input_dim,\n        mlp_hidden_dim,  # For activation func like SiLU that need 2x width\n        bias=bias,\n        params_dtype=dtype)\n\n    self.act = get_act_fn(act_type)\n    if output_dim is None:\n        output_dim = input_dim\n    self.fc_out = ReplicatedLinear(mlp_hidden_dim,\n                                   output_dim,\n                                   bias=bias,\n                                   params_dtype=dtype)\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.mlp-functions","title":"Functions","text":""},{"location":"api/fastvideo/layers/#fastvideo.layers.quantization","title":"fastvideo.layers.quantization","text":""},{"location":"api/fastvideo/layers/#fastvideo.layers.quantization-classes","title":"Classes","text":""},{"location":"api/fastvideo/layers/#fastvideo.layers.quantization-functions","title":"Functions","text":""},{"location":"api/fastvideo/layers/#fastvideo.layers.quantization.register_quantization_config","title":"fastvideo.layers.quantization.register_quantization_config","text":"<pre><code>register_quantization_config(quantization: str)\n</code></pre> <p>Register a customized vllm quantization config.</p> <p>When a quantization method is not supported by vllm, you can register a customized quantization config to support it.</p> <p>Parameters:</p> Name Type Description Default <code>quantization</code> <code>str</code> <p>The quantization method name.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from fastvideo.layers.quantization import register_quantization_config\n&gt;&gt;&gt; from fastvideo.layers.quantization import get_quantization_config\n&gt;&gt;&gt; from fastvideo.layers.quantization.base_config import QuantizationConfig\n&gt;&gt;&gt;\n&gt;&gt;&gt; @register_quantization_config(\"my_quant\")\n... class MyQuantConfig(QuantizationConfig):\n...     pass\n&gt;&gt;&gt;\n&gt;&gt;&gt; get_quantization_config(\"my_quant\")\n&lt;class 'MyQuantConfig'&gt;\n</code></pre> Source code in <code>fastvideo/layers/quantization/__init__.py</code> <pre><code>def register_quantization_config(quantization: str):\n    \"\"\"Register a customized vllm quantization config.\n\n    When a quantization method is not supported by vllm, you can register a customized\n    quantization config to support it.\n\n    Args:\n        quantization (str): The quantization method name.\n\n    Examples:\n        &gt;&gt;&gt; from fastvideo.layers.quantization import register_quantization_config\n        &gt;&gt;&gt; from fastvideo.layers.quantization import get_quantization_config\n        &gt;&gt;&gt; from fastvideo.layers.quantization.base_config import QuantizationConfig\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; @register_quantization_config(\"my_quant\")\n        ... class MyQuantConfig(QuantizationConfig):\n        ...     pass\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; get_quantization_config(\"my_quant\")\n        &lt;class 'MyQuantConfig'&gt;\n    \"\"\"  # noqa: E501\n\n    def _wrapper(quant_config_cls):\n        if quantization in QUANTIZATION_METHODS:\n            raise ValueError(\n                f\"The quantization method `{quantization}` is already exists.\")\n        if not issubclass(quant_config_cls, QuantizationConfig):\n            raise ValueError(\"The quantization config must be a subclass of \"\n                             \"`QuantizationConfig`.\")\n        _CUSTOMIZED_METHOD_TO_QUANT_CONFIG[quantization] = quant_config_cls\n        QUANTIZATION_METHODS.append(quantization)\n        return quant_config_cls\n\n    return _wrapper\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.quantization-modules","title":"Modules","text":""},{"location":"api/fastvideo/layers/#fastvideo.layers.quantization.base_config","title":"fastvideo.layers.quantization.base_config","text":"Classes\u00b6 fastvideo.layers.quantization.base_config.QuantizationConfig \u00b6 <pre><code>QuantizationConfig()\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for quantization configs.</p> Source code in <code>fastvideo/layers/quantization/base_config.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    # mapping is updated by models as they initialize\n    self.packed_modules_mapping: dict[str, list[str]] = dict()\n</code></pre> Functions\u00b6 fastvideo.layers.quantization.base_config.QuantizationConfig.from_config <code>abstractmethod</code> <code>classmethod</code> \u00b6 <pre><code>from_config(config: dict[str, Any]) -&gt; QuantizationConfig\n</code></pre> <p>Create a config class from the model's quantization config.</p> Source code in <code>fastvideo/layers/quantization/base_config.py</code> <pre><code>@classmethod\n@abstractmethod\ndef from_config(cls, config: dict[str, Any]) -&gt; \"QuantizationConfig\":\n    \"\"\"Create a config class from the model's quantization config.\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.layers.quantization.base_config.QuantizationConfig.get_config_filenames <code>abstractmethod</code> <code>staticmethod</code> \u00b6 <pre><code>get_config_filenames() -&gt; list[str]\n</code></pre> <p>List of filenames to search for in the model directory.</p> Source code in <code>fastvideo/layers/quantization/base_config.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef get_config_filenames() -&gt; list[str]:\n    \"\"\"List of filenames to search for in the model directory.\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.layers.quantization.base_config.QuantizationConfig.get_from_keys <code>staticmethod</code> \u00b6 <pre><code>get_from_keys(\n    config: dict[str, Any], keys: list[str]\n) -&gt; Any\n</code></pre> <p>Get a value from the model's quantization config.</p> Source code in <code>fastvideo/layers/quantization/base_config.py</code> <pre><code>@staticmethod\ndef get_from_keys(config: dict[str, Any], keys: list[str]) -&gt; Any:\n    \"\"\"Get a value from the model's quantization config.\"\"\"\n    for key in keys:\n        if key in config:\n            return config[key]\n    raise ValueError(f\"Cannot find any of {keys} in the model's \"\n                     \"quantization config.\")\n</code></pre> fastvideo.layers.quantization.base_config.QuantizationConfig.get_from_keys_or <code>staticmethod</code> \u00b6 <pre><code>get_from_keys_or(\n    config: dict[str, Any], keys: list[str], default: Any\n) -&gt; Any\n</code></pre> <p>Get a optional value from the model's quantization config.</p> Source code in <code>fastvideo/layers/quantization/base_config.py</code> <pre><code>@staticmethod\ndef get_from_keys_or(config: dict[str, Any], keys: list[str],\n                     default: Any) -&gt; Any:\n    \"\"\"Get a optional value from the model's quantization config.\"\"\"\n    try:\n        return QuantizationConfig.get_from_keys(config, keys)\n    except ValueError:\n        return default\n</code></pre> fastvideo.layers.quantization.base_config.QuantizationConfig.get_min_capability <code>abstractmethod</code> <code>classmethod</code> \u00b6 <pre><code>get_min_capability() -&gt; int\n</code></pre> <p>Minimum GPU capability to support the quantization method.</p> <p>E.g., 70 for Volta, 75 for Turing, 80 for Ampere. This requirement is due to the custom CUDA kernels used by the quantization method.</p> Source code in <code>fastvideo/layers/quantization/base_config.py</code> <pre><code>@classmethod\n@abstractmethod\ndef get_min_capability(cls) -&gt; int:\n    \"\"\"Minimum GPU capability to support the quantization method.\n\n    E.g., 70 for Volta, 75 for Turing, 80 for Ampere.\n    This requirement is due to the custom CUDA kernels used by the\n    quantization method.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.layers.quantization.base_config.QuantizationConfig.get_name <code>abstractmethod</code> \u00b6 <pre><code>get_name() -&gt; QuantizationMethods\n</code></pre> <p>Name of the quantization method.</p> Source code in <code>fastvideo/layers/quantization/base_config.py</code> <pre><code>@abstractmethod\ndef get_name(self) -&gt; QuantizationMethods:\n    \"\"\"Name of the quantization method.\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.layers.quantization.base_config.QuantizationConfig.get_quant_method <code>abstractmethod</code> \u00b6 <pre><code>get_quant_method(\n    layer: Module, prefix: str\n) -&gt; QuantizeMethodBase | None\n</code></pre> <p>Get the quantize method to use for the quantized layer.</p> <p>Parameters:</p> Name Type Description Default <code>layer</code> <code>Module</code> <p>The layer for the quant method.</p> required <code>prefix</code> <code>str</code> <p>The full name of the layer in the state dict</p> required <p>Returns:     The quantize method. None if the given layer doesn't support quant     method.</p> Source code in <code>fastvideo/layers/quantization/base_config.py</code> <pre><code>@abstractmethod\ndef get_quant_method(self, layer: torch.nn.Module,\n                     prefix: str) -&gt; QuantizeMethodBase | None:\n    \"\"\"Get the quantize method to use for the quantized layer.\n\n    Args:\n        layer: The layer for the quant method.\n        prefix: The full name of the layer in the state dict\n    Returns:\n        The quantize method. None if the given layer doesn't support quant\n        method.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.layers.quantization.base_config.QuantizationConfig.get_supported_act_dtypes <code>abstractmethod</code> \u00b6 <pre><code>get_supported_act_dtypes() -&gt; list[torch.dtype]\n</code></pre> <p>List of supported activation dtypes.</p> Source code in <code>fastvideo/layers/quantization/base_config.py</code> <pre><code>@abstractmethod\ndef get_supported_act_dtypes(self) -&gt; list[torch.dtype]:\n    \"\"\"List of supported activation dtypes.\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.layers.quantization.base_config.QuantizationConfig.override_quantization_method <code>classmethod</code> \u00b6 <pre><code>override_quantization_method(\n    hf_quant_cfg, user_quant\n) -&gt; QuantizationMethods | None\n</code></pre> <p>Detects if this quantization method can support a given checkpoint format by overriding the user specified quantization method --  this method should only be overwritten by subclasses in exceptional  circumstances</p> Source code in <code>fastvideo/layers/quantization/base_config.py</code> <pre><code>@classmethod\ndef override_quantization_method(cls, hf_quant_cfg,\n                                 user_quant) -&gt; QuantizationMethods | None:\n    \"\"\"\n       Detects if this quantization method can support a given checkpoint\n       format by overriding the user specified quantization method -- \n       this method should only be overwritten by subclasses in exceptional \n       circumstances\n    \"\"\"\n    return None\n</code></pre> fastvideo.layers.quantization.base_config.QuantizeMethodBase \u00b6 <p>               Bases: <code>ABC</code></p> <p>Base class for different quantized methods.</p> Functions\u00b6 fastvideo.layers.quantization.base_config.QuantizeMethodBase.apply <code>abstractmethod</code> \u00b6 <pre><code>apply(layer: Module, *args, **kwargs) -&gt; torch.Tensor\n</code></pre> <p>Apply the weights in layer to the input tensor.</p> <p>Expects create_weights to have been called before on the layer.</p> Source code in <code>fastvideo/layers/quantization/base_config.py</code> <pre><code>@abstractmethod\ndef apply(self, layer: torch.nn.Module, *args, **kwargs) -&gt; torch.Tensor:\n    \"\"\"Apply the weights in layer to the input tensor.\n\n    Expects create_weights to have been called before on the layer.\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.layers.quantization.base_config.QuantizeMethodBase.create_weights <code>abstractmethod</code> \u00b6 <pre><code>create_weights(\n    layer: Module, *weight_args, **extra_weight_attrs\n)\n</code></pre> <p>Create weights for a layer.</p> <p>The weights will be set as attributes of the layer.</p> Source code in <code>fastvideo/layers/quantization/base_config.py</code> <pre><code>@abstractmethod\ndef create_weights(self, layer: torch.nn.Module, *weight_args,\n                   **extra_weight_attrs):\n    \"\"\"Create weights for a layer.\n\n    The weights will be set as attributes of the layer.\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.layers.quantization.base_config.QuantizeMethodBase.embedding \u00b6 <pre><code>embedding(layer: Module, *args, **kwargs) -&gt; torch.Tensor\n</code></pre> <p>Gather embeddings in the layer based on indices in the input tensor.</p> <p>Expects create_weights to have been called before on the layer.</p> Source code in <code>fastvideo/layers/quantization/base_config.py</code> <pre><code>def embedding(self, layer: torch.nn.Module, *args,\n              **kwargs) -&gt; torch.Tensor:\n    \"\"\"Gather embeddings in the layer based on indices in the input tensor.\n\n    Expects create_weights to have been called before on the layer.\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.layers.quantization.base_config.QuantizeMethodBase.process_weights_after_loading \u00b6 <pre><code>process_weights_after_loading(layer: Module) -&gt; None\n</code></pre> <p>Process the weight after loading.</p> <p>This can be used for example, to transpose weights for computation.</p> Source code in <code>fastvideo/layers/quantization/base_config.py</code> <pre><code>def process_weights_after_loading(self, layer: nn.Module) -&gt; None:\n    \"\"\"Process the weight after loading.\n\n    This can be used for example, to transpose weights for computation.\n    \"\"\"\n    return\n</code></pre> Functions\u00b6 fastvideo.layers.quantization.base_config.method_has_implemented_embedding \u00b6 <pre><code>method_has_implemented_embedding(\n    method_class: type[QuantizeMethodBase],\n) -&gt; bool\n</code></pre> <p>Not all quant methods have embedding implemented, so we need to check that it exists for our given method. We check this by making sure the function has been changed from the base implementation.</p> Source code in <code>fastvideo/layers/quantization/base_config.py</code> <pre><code>def method_has_implemented_embedding(\n        method_class: type[QuantizeMethodBase]) -&gt; bool:\n    \"\"\"\n    Not all quant methods have embedding implemented, so we need to check that\n    it exists for our given method. We check this by making sure the function\n    has been changed from the base implementation.\n    \"\"\"\n    base_embedding = inspect.getattr_static(QuantizeMethodBase, \"embedding\",\n                                            None)\n    class_embedding = inspect.getattr_static(method_class, \"embedding\", None)\n\n    return (class_embedding is not None\n            and class_embedding is not base_embedding)\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.rotary_embedding","title":"fastvideo.layers.rotary_embedding","text":"<p>Rotary Positional Embeddings.</p>"},{"location":"api/fastvideo/layers/#fastvideo.layers.rotary_embedding-classes","title":"Classes","text":""},{"location":"api/fastvideo/layers/#fastvideo.layers.rotary_embedding.RotaryEmbedding","title":"fastvideo.layers.rotary_embedding.RotaryEmbedding","text":"<pre><code>RotaryEmbedding(\n    head_size: int,\n    rotary_dim: int,\n    max_position_embeddings: int,\n    base: int | float,\n    is_neox_style: bool,\n    dtype: dtype,\n)\n</code></pre> <p>               Bases: <code>CustomOp</code></p> <p>Original rotary positional embedding.</p> Source code in <code>fastvideo/layers/rotary_embedding.py</code> <pre><code>def __init__(\n    self,\n    head_size: int,\n    rotary_dim: int,\n    max_position_embeddings: int,\n    base: int | float,\n    is_neox_style: bool,\n    dtype: torch.dtype,\n) -&gt; None:\n    super().__init__()\n    self.head_size = head_size\n    self.rotary_dim = rotary_dim\n    self.max_position_embeddings = max_position_embeddings\n    self.base = base\n    self.is_neox_style = is_neox_style\n    self.dtype = dtype\n\n    cache = self._compute_cos_sin_cache()\n    cache = cache.to(dtype)\n    self.cos_sin_cache: torch.Tensor\n    self.register_buffer(\"cos_sin_cache\", cache, persistent=False)\n</code></pre> Functions\u00b6 fastvideo.layers.rotary_embedding.RotaryEmbedding.forward_native \u00b6 <pre><code>forward_native(\n    positions: Tensor,\n    query: Tensor,\n    key: Tensor,\n    offsets: Tensor | None = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>A PyTorch-native implementation of forward().</p> Source code in <code>fastvideo/layers/rotary_embedding.py</code> <pre><code>def forward_native(\n    self,\n    positions: torch.Tensor,\n    query: torch.Tensor,\n    key: torch.Tensor,\n    offsets: torch.Tensor | None = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"A PyTorch-native implementation of forward().\"\"\"\n    if offsets is not None:\n        positions = positions + offsets\n    positions = positions.flatten()\n    num_tokens = positions.shape[0]\n    cos_sin = self.cos_sin_cache.index_select(0, positions)\n    cos, sin = cos_sin.chunk(2, dim=-1)\n\n    query_shape = query.shape\n    query = query.view(num_tokens, -1, self.head_size)\n    query_rot = query[..., :self.rotary_dim]\n    query_pass = query[..., self.rotary_dim:]\n    query_rot = _apply_rotary_emb(query_rot, cos, sin, self.is_neox_style)\n    query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)\n\n    key_shape = key.shape\n    key = key.view(num_tokens, -1, self.head_size)\n    key_rot = key[..., :self.rotary_dim]\n    key_pass = key[..., self.rotary_dim:]\n    key_rot = _apply_rotary_emb(key_rot, cos, sin, self.is_neox_style)\n    key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)\n    return query, key\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.rotary_embedding-functions","title":"Functions","text":""},{"location":"api/fastvideo/layers/#fastvideo.layers.rotary_embedding.get_1d_rotary_pos_embed","title":"fastvideo.layers.rotary_embedding.get_1d_rotary_pos_embed","text":"<pre><code>get_1d_rotary_pos_embed(\n    dim: int,\n    pos: FloatTensor | int,\n    theta: float = 10000.0,\n    theta_rescale_factor: float = 1.0,\n    interpolation_factor: float = 1.0,\n    dtype: dtype = torch.float32,\n) -&gt; tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Precompute the frequency tensor for complex exponential (cis) with given dimensions. (Note: <code>cis</code> means <code>cos + i * sin</code>, where i is the imaginary unit.)</p> <p>This function calculates a frequency tensor with complex exponential using the given dimension 'dim' and the end index 'end'. The 'theta' parameter scales the frequencies.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Dimension of the frequency tensor.</p> required <code>pos</code> <code>int or FloatTensor</code> <p>Position indices for the frequency tensor. [S] or scalar</p> required <code>theta</code> <code>float</code> <p>Scaling factor for frequency computation. Defaults to 10000.0.</p> <code>10000.0</code> <code>theta_rescale_factor</code> <code>float</code> <p>Rescale factor for theta. Defaults to 1.0.</p> <code>1.0</code> <code>interpolation_factor</code> <code>float</code> <p>Factor to scale positions. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>freqs_cos, freqs_sin: Precomputed frequency tensor with real and imaginary parts separately. [S, D]</p> Source code in <code>fastvideo/layers/rotary_embedding.py</code> <pre><code>def get_1d_rotary_pos_embed(\n    dim: int,\n    pos: torch.FloatTensor | int,\n    theta: float = 10000.0,\n    theta_rescale_factor: float = 1.0,\n    interpolation_factor: float = 1.0,\n    dtype: torch.dtype = torch.float32,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Precompute the frequency tensor for complex exponential (cis) with given dimensions.\n    (Note: `cis` means `cos + i * sin`, where i is the imaginary unit.)\n\n    This function calculates a frequency tensor with complex exponential using the given dimension 'dim'\n    and the end index 'end'. The 'theta' parameter scales the frequencies.\n\n    Args:\n        dim (int): Dimension of the frequency tensor.\n        pos (int or torch.FloatTensor): Position indices for the frequency tensor. [S] or scalar\n        theta (float, optional): Scaling factor for frequency computation. Defaults to 10000.0.\n        theta_rescale_factor (float, optional): Rescale factor for theta. Defaults to 1.0.\n        interpolation_factor (float, optional): Factor to scale positions. Defaults to 1.0.\n\n    Returns:\n        freqs_cos, freqs_sin: Precomputed frequency tensor with real and imaginary parts separately. [S, D]\n    \"\"\"\n    if isinstance(pos, int):\n        pos = torch.arange(pos).float()\n\n    # proposed by reddit user bloc97, to rescale rotary embeddings to longer sequence length without fine-tuning\n    # has some connection to NTK literature\n    if theta_rescale_factor != 1.0:\n        theta *= theta_rescale_factor**(dim / (dim - 2))\n\n    freqs = 1.0 / (theta**(torch.arange(0, dim, 2)[:(dim // 2)].to(dtype) / dim)\n                   )  # [D/2]\n    freqs = torch.outer(pos * interpolation_factor, freqs)  # [S, D/2]\n    freqs_cos = freqs.cos()  # [S, D/2]\n    freqs_sin = freqs.sin()  # [S, D/2]\n    return freqs_cos, freqs_sin\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.rotary_embedding.get_meshgrid_nd","title":"fastvideo.layers.rotary_embedding.get_meshgrid_nd","text":"<pre><code>get_meshgrid_nd(\n    start: int | tuple[int, ...],\n    *args: int | tuple[int, ...],\n    dim: int = 2\n) -&gt; torch.Tensor\n</code></pre> <p>Get n-D meshgrid with start, stop and num.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int or tuple</code> <p>If len(args) == 0, start is num; If len(args) == 1, start is start, args[0] is stop, step is 1; If len(args) == 2, start is start, args[0] is stop, args[1] is num. For n-dim, start/stop/num should be int or n-tuple. If n-tuple is provided, the meshgrid will be stacked following the dim order in n-tuples.</p> required <code>*args</code> <code>int | tuple[int, ...]</code> <p>See above.</p> <code>()</code> <code>dim</code> <code>int</code> <p>Dimension of the meshgrid. Defaults to 2.</p> <code>2</code> <p>Returns:</p> Name Type Description <code>grid</code> <code>ndarray</code> <p>[dim, ...]</p> Source code in <code>fastvideo/layers/rotary_embedding.py</code> <pre><code>def get_meshgrid_nd(start: int | tuple[int, ...],\n                    *args: int | tuple[int, ...],\n                    dim: int = 2) -&gt; torch.Tensor:\n    \"\"\"\n    Get n-D meshgrid with start, stop and num.\n\n    Args:\n        start (int or tuple): If len(args) == 0, start is num; If len(args) == 1, start is start, args[0] is stop,\n            step is 1; If len(args) == 2, start is start, args[0] is stop, args[1] is num. For n-dim, start/stop/num\n            should be int or n-tuple. If n-tuple is provided, the meshgrid will be stacked following the dim order in\n            n-tuples.\n        *args: See above.\n        dim (int): Dimension of the meshgrid. Defaults to 2.\n\n    Returns:\n        grid (np.ndarray): [dim, ...]\n    \"\"\"\n    if len(args) == 0:\n        # start is grid_size\n        num = _to_tuple(start, dim=dim)\n        start = (0, ) * dim\n        stop = num\n    elif len(args) == 1:\n        # start is start, args[0] is stop, step is 1\n        start = _to_tuple(start, dim=dim)\n        stop = _to_tuple(args[0], dim=dim)\n        num = tuple(stop[i] - start[i] for i in range(dim))\n    elif len(args) == 2:\n        # start is start, args[0] is stop, args[1] is num\n        start = _to_tuple(start, dim=dim)  # Left-Top       eg: 12,0\n        stop = _to_tuple(args[0], dim=dim)  # Right-Bottom   eg: 20,32\n        num = _to_tuple(args[1], dim=dim)  # Target Size    eg: 32,124\n    else:\n        raise ValueError(f\"len(args) should be 0, 1 or 2, but got {len(args)}\")\n\n    # PyTorch implement of np.linspace(start[i], stop[i], num[i], endpoint=False)\n    axis_grid = []\n    for i in range(dim):\n        a, b, n = start[i], stop[i], num[i]\n        g = torch.linspace(a, b, n + 1, dtype=torch.float32)[:n]\n        axis_grid.append(g)\n    grid = torch.meshgrid(*axis_grid, indexing=\"ij\")  # dim x [W, H, D]\n    grid = torch.stack(grid, dim=0)  # [dim, W, H, D]\n\n    return grid\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.rotary_embedding.get_nd_rotary_pos_embed","title":"fastvideo.layers.rotary_embedding.get_nd_rotary_pos_embed","text":"<pre><code>get_nd_rotary_pos_embed(\n    rope_dim_list,\n    start,\n    *args,\n    theta=10000.0,\n    theta_rescale_factor: float | list[float] = 1.0,\n    interpolation_factor: float | list[float] = 1.0,\n    shard_dim: int = 0,\n    sp_rank: int = 0,\n    sp_world_size: int = 1,\n    dtype: dtype = torch.float32,\n    start_frame: int = 0\n) -&gt; tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>This is a n-d version of precompute_freqs_cis, which is a RoPE for tokens with n-d structure. Supports sequence parallelism by allowing sharding of a specific dimension.</p> <p>Parameters:</p> Name Type Description Default <code>rope_dim_list</code> <code>list of int</code> <p>Dimension of each rope. len(rope_dim_list) should equal to n. sum(rope_dim_list) should equal to head_dim of attention layer.</p> required <code>start</code> <code>int | tuple of int | list of int</code> <p>If len(args) == 0, start is num; If len(args) == 1, start is start, args[0] is stop, step is 1; If len(args) == 2, start is start, args[0] is stop, args[1] is num.</p> required <code>*args</code> <p>See above.</p> <code>()</code> <code>theta</code> <code>float</code> <p>Scaling factor for frequency computation. Defaults to 10000.0.</p> <code>10000.0</code> <code>theta_rescale_factor</code> <code>float</code> <p>Rescale factor for theta. Defaults to 1.0.</p> <code>1.0</code> <code>interpolation_factor</code> <code>float</code> <p>Factor to scale positions. Defaults to 1.0.</p> <code>1.0</code> <code>shard_dim</code> <code>int</code> <p>Which dimension to shard for sequence parallelism. Defaults to 0.</p> <code>0</code> <code>sp_rank</code> <code>int</code> <p>Rank in the sequence parallel group. Defaults to 0.</p> <code>0</code> <code>sp_world_size</code> <code>int</code> <p>World size of the sequence parallel group. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>Tuple[torch.Tensor, torch.Tensor]: (cos, sin) tensors of shape [HW, D/2]</p> Source code in <code>fastvideo/layers/rotary_embedding.py</code> <pre><code>def get_nd_rotary_pos_embed(\n    rope_dim_list,\n    start,\n    *args,\n    theta=10000.0,\n    theta_rescale_factor: float | list[float] = 1.0,\n    interpolation_factor: float | list[float] = 1.0,\n    shard_dim: int = 0,\n    sp_rank: int = 0,\n    sp_world_size: int = 1,\n    dtype: torch.dtype = torch.float32,\n    start_frame: int = 0,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    This is a n-d version of precompute_freqs_cis, which is a RoPE for tokens with n-d structure.\n    Supports sequence parallelism by allowing sharding of a specific dimension.\n\n    Args:\n        rope_dim_list (list of int): Dimension of each rope. len(rope_dim_list) should equal to n.\n            sum(rope_dim_list) should equal to head_dim of attention layer.\n        start (int | tuple of int | list of int): If len(args) == 0, start is num; If len(args) == 1, start is start,\n            args[0] is stop, step is 1; If len(args) == 2, start is start, args[0] is stop, args[1] is num.\n        *args: See above.\n        theta (float): Scaling factor for frequency computation. Defaults to 10000.0.\n        theta_rescale_factor (float): Rescale factor for theta. Defaults to 1.0.\n        interpolation_factor (float): Factor to scale positions. Defaults to 1.0.\n        shard_dim (int): Which dimension to shard for sequence parallelism. Defaults to 0.\n        sp_rank (int): Rank in the sequence parallel group. Defaults to 0.\n        sp_world_size (int): World size of the sequence parallel group. Defaults to 1.\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor]: (cos, sin) tensors of shape [HW, D/2]\n    \"\"\"\n    # Get the full grid\n    full_grid = get_meshgrid_nd(\n        start, *args, dim=len(rope_dim_list))  # [3, W, H, D] / [2, W, H]\n\n    if start_frame &gt; 0:\n        full_grid[0] += start_frame\n\n    # Shard the grid if using sequence parallelism (sp_world_size &gt; 1)\n    assert shard_dim &lt; len(\n        rope_dim_list\n    ), f\"shard_dim {shard_dim} must be less than number of dimensions {len(rope_dim_list)}\"\n    if sp_world_size &gt; 1:\n        # Get the shape of the full grid\n        grid_shape = list(full_grid.shape[1:])\n\n        # Ensure the dimension to shard is divisible by sp_world_size\n        assert grid_shape[shard_dim] % sp_world_size == 0, (\n            f\"Dimension {shard_dim} with size {grid_shape[shard_dim]} is not divisible \"\n            f\"by sequence parallel world size {sp_world_size}\")\n\n        # Compute the start and end indices for this rank's shard\n        shard_size = grid_shape[shard_dim] // sp_world_size\n        start_idx = sp_rank * shard_size\n        end_idx = (sp_rank + 1) * shard_size\n\n        # Create slicing indices for each dimension\n        slice_indices = [slice(None) for _ in range(len(grid_shape))]\n        slice_indices[shard_dim] = slice(start_idx, end_idx)\n\n        # Shard the grid\n        # Update grid shape for the sharded dimension\n        grid_shape[shard_dim] = grid_shape[shard_dim] // sp_world_size\n        grid = torch.empty((len(rope_dim_list), ) + tuple(grid_shape),\n                           dtype=full_grid.dtype)\n        for i in range(len(rope_dim_list)):\n            grid[i] = full_grid[i][tuple(slice_indices)]\n    else:\n        grid = full_grid\n\n    if isinstance(theta_rescale_factor, int | float):\n        theta_rescale_factor = [theta_rescale_factor] * len(rope_dim_list)\n    elif isinstance(theta_rescale_factor,\n                    list) and len(theta_rescale_factor) == 1:\n        theta_rescale_factor = [theta_rescale_factor[0]] * len(rope_dim_list)\n    assert len(theta_rescale_factor) == len(\n        rope_dim_list\n    ), \"len(theta_rescale_factor) should equal to len(rope_dim_list)\"\n\n    if isinstance(interpolation_factor, int | float):\n        interpolation_factor = [interpolation_factor] * len(rope_dim_list)\n    elif isinstance(interpolation_factor,\n                    list) and len(interpolation_factor) == 1:\n        interpolation_factor = [interpolation_factor[0]] * len(rope_dim_list)\n    assert len(interpolation_factor) == len(\n        rope_dim_list\n    ), \"len(interpolation_factor) should equal to len(rope_dim_list)\"\n\n    # use 1/ndim of dimensions to encode grid_axis\n    embs = []\n    for i in range(len(rope_dim_list)):\n        emb = get_1d_rotary_pos_embed(\n            rope_dim_list[i],\n            grid[i].reshape(-1),\n            theta,\n            theta_rescale_factor=theta_rescale_factor[i],\n            interpolation_factor=interpolation_factor[i],\n            dtype=dtype,\n        )  # 2 x [WHD, rope_dim_list[i]]\n        embs.append(emb)\n\n    cos = torch.cat([emb[0] for emb in embs], dim=1)  # (WHD, D/2)\n    sin = torch.cat([emb[1] for emb in embs], dim=1)  # (WHD, D/2)\n    return cos, sin\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.rotary_embedding.get_rotary_pos_embed","title":"fastvideo.layers.rotary_embedding.get_rotary_pos_embed","text":"<pre><code>get_rotary_pos_embed(\n    rope_sizes,\n    hidden_size,\n    heads_num,\n    rope_dim_list,\n    rope_theta,\n    theta_rescale_factor=1.0,\n    interpolation_factor=1.0,\n    shard_dim: int = 0,\n    dtype: dtype = torch.float32,\n    start_frame: int = 0,\n) -&gt; tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Generate rotary positional embeddings for the given sizes.</p> <p>Parameters:</p> Name Type Description Default <code>rope_sizes</code> <p>Tuple of dimensions (t, h, w)</p> required <code>hidden_size</code> <p>Hidden dimension size</p> required <code>heads_num</code> <p>Number of attention heads</p> required <code>rope_dim_list</code> <p>List of dimensions for each axis, or None</p> required <code>rope_theta</code> <p>Base for frequency calculations</p> required <code>theta_rescale_factor</code> <p>Rescale factor for theta. Defaults to 1.0</p> <code>1.0</code> <code>interpolation_factor</code> <p>Factor to scale positions. Defaults to 1.0</p> <code>1.0</code> <code>shard_dim</code> <code>int</code> <p>Which dimension to shard for sequence parallelism. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>Tuple of (cos, sin) tensors for rotary embeddings</p> Source code in <code>fastvideo/layers/rotary_embedding.py</code> <pre><code>def get_rotary_pos_embed(\n    rope_sizes,\n    hidden_size,\n    heads_num,\n    rope_dim_list,\n    rope_theta,\n    theta_rescale_factor=1.0,\n    interpolation_factor=1.0,\n    shard_dim: int = 0,\n    dtype: torch.dtype = torch.float32,\n    start_frame: int = 0,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Generate rotary positional embeddings for the given sizes.\n\n    Args:\n        rope_sizes: Tuple of dimensions (t, h, w)\n        hidden_size: Hidden dimension size\n        heads_num: Number of attention heads\n        rope_dim_list: List of dimensions for each axis, or None\n        rope_theta: Base for frequency calculations\n        theta_rescale_factor: Rescale factor for theta. Defaults to 1.0\n        interpolation_factor: Factor to scale positions. Defaults to 1.0\n        shard_dim: Which dimension to shard for sequence parallelism. Defaults to 0.\n\n    Returns:\n        Tuple of (cos, sin) tensors for rotary embeddings\n    \"\"\"\n\n    target_ndim = 3\n    head_dim = hidden_size // heads_num\n\n    if rope_dim_list is None:\n        rope_dim_list = [head_dim // target_ndim for _ in range(target_ndim)]\n\n    assert sum(\n        rope_dim_list\n    ) == head_dim, \"sum(rope_dim_list) should equal to head_dim of attention layer\"\n\n    # Get SP info\n    sp_group = get_sp_group()\n    sp_rank = sp_group.rank_in_group\n    sp_world_size = sp_group.world_size\n\n    freqs_cos, freqs_sin = get_nd_rotary_pos_embed(\n        rope_dim_list,\n        rope_sizes,\n        theta=rope_theta,\n        theta_rescale_factor=theta_rescale_factor,\n        interpolation_factor=interpolation_factor,\n        shard_dim=shard_dim,\n        sp_rank=sp_rank,\n        sp_world_size=sp_world_size,\n        dtype=dtype,\n        start_frame=start_frame,\n    )\n    return freqs_cos, freqs_sin\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.utils","title":"fastvideo.layers.utils","text":"<p>Utility methods for model layers.</p>"},{"location":"api/fastvideo/layers/#fastvideo.layers.visual_embedding","title":"fastvideo.layers.visual_embedding","text":""},{"location":"api/fastvideo/layers/#fastvideo.layers.visual_embedding-classes","title":"Classes","text":""},{"location":"api/fastvideo/layers/#fastvideo.layers.visual_embedding.ModulateProjection","title":"fastvideo.layers.visual_embedding.ModulateProjection","text":"<pre><code>ModulateProjection(\n    hidden_size: int,\n    factor: int = 2,\n    act_layer: str = \"silu\",\n    dtype: dtype | None = None,\n    prefix: str = \"\",\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Modulation layer for DiT blocks.</p> Source code in <code>fastvideo/layers/visual_embedding.py</code> <pre><code>def __init__(\n    self,\n    hidden_size: int,\n    factor: int = 2,\n    act_layer: str = \"silu\",\n    dtype: torch.dtype | None = None,\n    prefix: str = \"\",\n):\n    super().__init__()\n    self.factor = factor\n    self.hidden_size = hidden_size\n    self.linear = ReplicatedLinear(hidden_size,\n                                   hidden_size * factor,\n                                   bias=True,\n                                   params_dtype=dtype)\n    self.act = get_act_fn(act_layer)\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.visual_embedding.PatchEmbed","title":"fastvideo.layers.visual_embedding.PatchEmbed","text":"<pre><code>PatchEmbed(\n    patch_size=16,\n    in_chans=3,\n    embed_dim=768,\n    norm_layer=None,\n    flatten=True,\n    bias=True,\n    dtype=None,\n    prefix: str = \"\",\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>2D Image to Patch Embedding</p> <p>Image to Patch Embedding using Conv2d</p> <p>A convolution based approach to patchifying a 2D image w/ embedding projection.</p> <p>Based on the impl in https://github.com/google-research/vision_transformer</p> <p>Hacked together by / Copyright 2020 Ross Wightman</p> <p>Remove the _assert function in forward function to be compatible with multi-resolution images.</p> Source code in <code>fastvideo/layers/visual_embedding.py</code> <pre><code>def __init__(self,\n             patch_size=16,\n             in_chans=3,\n             embed_dim=768,\n             norm_layer=None,\n             flatten=True,\n             bias=True,\n             dtype=None,\n             prefix: str = \"\"):\n    super().__init__()\n    # Convert patch_size to 2-tuple\n    if isinstance(patch_size, list | tuple):\n        if len(patch_size) == 1:\n            patch_size = (patch_size[0], patch_size[0])\n    else:\n        patch_size = (patch_size, patch_size)\n\n    self.patch_size = patch_size\n    self.flatten = flatten\n\n    self.proj = nn.Conv3d(in_chans,\n                          embed_dim,\n                          kernel_size=patch_size,\n                          stride=patch_size,\n                          bias=bias,\n                          dtype=dtype)\n    self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.visual_embedding.TimestepEmbedder","title":"fastvideo.layers.visual_embedding.TimestepEmbedder","text":"<pre><code>TimestepEmbedder(\n    hidden_size,\n    act_layer=\"silu\",\n    frequency_embedding_size=256,\n    max_period=10000,\n    dtype=None,\n    freq_dtype=torch.float32,\n    prefix: str = \"\",\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Embeds scalar timesteps into vector representations.</p> Source code in <code>fastvideo/layers/visual_embedding.py</code> <pre><code>def __init__(\n    self,\n    hidden_size,\n    act_layer=\"silu\",\n    frequency_embedding_size=256,\n    max_period=10000,\n    dtype=None,\n    freq_dtype=torch.float32,\n    prefix: str = \"\",\n):\n    super().__init__()\n    self.frequency_embedding_size = frequency_embedding_size\n    self.max_period = max_period\n\n    self.mlp = MLP(frequency_embedding_size,\n                   hidden_size,\n                   hidden_size,\n                   act_type=act_layer,\n                   dtype=dtype)\n    self.freq_dtype = freq_dtype\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.visual_embedding-functions","title":"Functions","text":""},{"location":"api/fastvideo/layers/#fastvideo.layers.visual_embedding.timestep_embedding","title":"fastvideo.layers.visual_embedding.timestep_embedding","text":"<pre><code>timestep_embedding(\n    t: Tensor,\n    dim: int,\n    max_period: int = 10000,\n    dtype: dtype = torch.float32,\n) -&gt; torch.Tensor\n</code></pre> <p>Create sinusoidal timestep embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>Tensor of shape [B] with timesteps</p> required <code>dim</code> <code>int</code> <p>Embedding dimension</p> required <code>max_period</code> <code>int</code> <p>Controls the minimum frequency of the embeddings</p> <code>10000</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape [B, dim] with embeddings</p> Source code in <code>fastvideo/layers/visual_embedding.py</code> <pre><code>def timestep_embedding(t: torch.Tensor,\n                       dim: int,\n                       max_period: int = 10000,\n                       dtype: torch.dtype = torch.float32) -&gt; torch.Tensor:\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n\n    Args:\n        t: Tensor of shape [B] with timesteps\n        dim: Embedding dimension\n        max_period: Controls the minimum frequency of the embeddings\n\n    Returns:\n        Tensor of shape [B, dim] with embeddings\n    \"\"\"\n    half = dim // 2\n    freqs = torch.exp(-math.log(max_period) *\n                      torch.arange(start=0, end=half, dtype=dtype) /\n                      half).to(device=t.device)\n    args = t[:, None].float() * freqs[None]\n    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    if dim % 2:\n        embedding = torch.cat(\n            [embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n    return embedding\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.visual_embedding.unpatchify","title":"fastvideo.layers.visual_embedding.unpatchify","text":"<pre><code>unpatchify(\n    x, t, h, w, patch_size, channels\n) -&gt; torch.Tensor\n</code></pre> <p>Convert patched representation back to image space.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>Tensor of shape [B, THW, CP_tP_h*P_w]</p> required <code>t, h, w</code> <p>Temporal and spatial dimensions</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Unpatchified tensor of shape [B, C, TP_t, HP_h, W*P_w]</p> Source code in <code>fastvideo/layers/visual_embedding.py</code> <pre><code>def unpatchify(x, t, h, w, patch_size, channels) -&gt; torch.Tensor:\n    \"\"\"\n    Convert patched representation back to image space.\n\n    Args:\n        x: Tensor of shape [B, T*H*W, C*P_t*P_h*P_w]\n        t, h, w: Temporal and spatial dimensions\n\n    Returns:\n        Unpatchified tensor of shape [B, C, T*P_t, H*P_h, W*P_w]\n    \"\"\"\n    assert x.ndim == 3, f\"x.ndim: {x.ndim}\"\n    assert len(patch_size) == 3, f\"patch_size: {patch_size}\"\n    assert t * h * w == x.shape[\n        1], f\"t * h * w: {t * h * w}, x.shape[1]: {x.shape[1]}\"\n    c = channels\n    pt, ph, pw = patch_size\n\n    x = x.reshape(shape=(x.shape[0], t, h, w, c, pt, ph, pw))\n    x = torch.einsum(\"nthwcopq-&gt;nctohpwq\", x)\n    imgs = x.reshape(shape=(x.shape[0], c, t * pt, h * ph, w * pw))\n\n    return imgs\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.vocab_parallel_embedding","title":"fastvideo.layers.vocab_parallel_embedding","text":""},{"location":"api/fastvideo/layers/#fastvideo.layers.vocab_parallel_embedding-classes","title":"Classes","text":""},{"location":"api/fastvideo/layers/#fastvideo.layers.vocab_parallel_embedding.UnquantizedEmbeddingMethod","title":"fastvideo.layers.vocab_parallel_embedding.UnquantizedEmbeddingMethod","text":"<p>               Bases: <code>QuantizeMethodBase</code></p> <p>Unquantized method for embeddings.</p> Functions\u00b6 fastvideo.layers.vocab_parallel_embedding.UnquantizedEmbeddingMethod.create_weights \u00b6 <pre><code>create_weights(\n    layer: Module,\n    input_size_per_partition: int,\n    output_partition_sizes: list[int],\n    input_size: int,\n    output_size: int,\n    params_dtype: dtype,\n    **extra_weight_attrs\n)\n</code></pre> <p>Create weights for embedding layer.</p> Source code in <code>fastvideo/layers/vocab_parallel_embedding.py</code> <pre><code>def create_weights(self, layer: torch.nn.Module,\n                   input_size_per_partition: int,\n                   output_partition_sizes: list[int], input_size: int,\n                   output_size: int, params_dtype: torch.dtype,\n                   **extra_weight_attrs):\n    \"\"\"Create weights for embedding layer.\"\"\"\n\n    weight = Parameter(torch.empty(\n        sum(output_partition_sizes),\n        input_size_per_partition,\n        dtype=params_dtype,\n    ),\n                       requires_grad=False)\n    set_weight_attrs(weight, {\"input_dim\": 1, \"output_dim\": 0})\n    layer.register_parameter(\"weight\", weight)\n    set_weight_attrs(weight, extra_weight_attrs)\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.vocab_parallel_embedding.VocabParallelEmbedding","title":"fastvideo.layers.vocab_parallel_embedding.VocabParallelEmbedding","text":"<pre><code>VocabParallelEmbedding(\n    num_embeddings: int,\n    embedding_dim: int,\n    params_dtype: dtype | None = None,\n    org_num_embeddings: int | None = None,\n    padding_size: int = DEFAULT_VOCAB_PADDING_SIZE,\n    quant_config: QuantizationConfig | None = None,\n    prefix: str = \"\",\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Embedding parallelized in the vocabulary dimension.</p> <p>Adapted from torch.nn.Embedding, note that we pad the vocabulary size to make sure it is divisible by the number of model parallel GPUs.</p> <p>In order to support various loading methods, we ensure that LoRA-added embeddings are always at the end of TP-sharded tensors. In other words, we shard base embeddings and LoRA embeddings separately (both padded), and place them in the same tensor. In this example, we will have the original vocab size = 1010, added vocab size = 16 and padding to 64. Therefore, the total vocab size with padding will be 1088 (because we first pad 1010 to 1024, add 16, and then pad to 1088). Therefore, the tensor format looks like the following: TP1, rank 0 (no sharding):                         |&lt; --------BASE-------- &gt;|&lt; -BASE PADDING-- &gt;|&lt; -----LORA------ &gt;|&lt; -LORA PADDING-- &gt;| corresponding token_id: |  0  |  1  | ... | 1009 |  -1  | ... |  -1  | 1010 | ... | 1015 |  -1  | ... |  -1  |                  index: |  0  |  1  | ... | 1009 | 1010 | ... | 1023 | 1024 | ... | 1039 | 1040 | ... | 1087 |</p> <p>TP2, rank 0:                         |&lt; --------------------BASE--------------------- &gt;|&lt; -----LORA------ &gt;|&lt; -LORA PADDING- &gt;| corresponding token_id: |  0  |  1  |  2  | ... | 497  | 498 | ...  | 511 | 1000 | ... | 1015 |  -1  | ... |  -1 |                  index: |  0  |  1  |  2  | ... | 497  | 498 | ...  | 511 | 512  | ... | 527  |  520 | ... | 543 | TP2, rank 1:                         |&lt; -----------BASE----------- &gt;|&lt; -BASE PADDING- &gt;|&lt; -----------LORA PADDING----------- &gt;| corresponding token_id: | 512 | 513 | 514 | ... | 1009 | -1  | ...  | -1  |  -1  | ... |  -1  | -1  | ... |   -1 |                  index: |  0  |  1  |  2  | ... | 497  | 498 | ...  | 511 | 512  | ... | 519  | 520 | ... |  543 |</p> <p>Parameters:</p> Name Type Description Default <code>num_embeddings</code> <code>int</code> <p>vocabulary size.</p> required <code>embedding_dim</code> <code>int</code> <p>size of hidden state.</p> required <code>params_dtype</code> <code>dtype | None</code> <p>type of the parameters.</p> <code>None</code> <code>org_num_embeddings</code> <code>int | None</code> <p>original vocabulary size (without LoRA).</p> <code>None</code> <code>padding_size</code> <code>int</code> <p>padding size for the vocabulary.</p> <code>DEFAULT_VOCAB_PADDING_SIZE</code> <code>quant_config</code> <code>QuantizationConfig | None</code> <p>quant config for the layer</p> <code>None</code> <code>prefix</code> <code>str</code> <p>full name of the layer in the state dict</p> <code>''</code> Source code in <code>fastvideo/layers/vocab_parallel_embedding.py</code> <pre><code>def __init__(self,\n             num_embeddings: int,\n             embedding_dim: int,\n             params_dtype: torch.dtype | None = None,\n             org_num_embeddings: int | None = None,\n             padding_size: int = DEFAULT_VOCAB_PADDING_SIZE,\n             quant_config: QuantizationConfig | None = None,\n             prefix: str = \"\"):\n    super().__init__()\n\n    # Keep the input dimensions.\n    tp_rank = get_tp_rank()\n    self.tp_size = get_tp_world_size()\n    self.num_embeddings = num_embeddings\n    self.padding_size = padding_size\n    self.org_vocab_size = org_num_embeddings or num_embeddings\n    num_added_embeddings = num_embeddings - self.org_vocab_size\n    self.org_vocab_size_padded = pad_vocab_size(self.org_vocab_size,\n                                                self.padding_size)\n    self.num_embeddings_padded = pad_vocab_size(\n        self.org_vocab_size_padded + num_added_embeddings,\n        self.padding_size)\n    assert self.org_vocab_size_padded &lt;= self.num_embeddings_padded\n\n    self.shard_indices = self._get_indices(self.num_embeddings_padded,\n                                           self.org_vocab_size_padded,\n                                           self.num_embeddings,\n                                           self.org_vocab_size, tp_rank,\n                                           self.tp_size)\n    self.embedding_dim = embedding_dim\n\n    quant_method = None\n    if quant_config is not None:\n        quant_method = quant_config.get_quant_method(self, prefix=prefix)\n    if quant_method is None:\n        quant_method = UnquantizedEmbeddingMethod()\n\n    # If we are making an embedding layer, then our quantization linear\n    # method must implement the embedding operation. If we are another\n    # layer type like ParallelLMHead, this is not important.\n    is_embedding_layer = type(self.__class__) is VocabParallelEmbedding\n    quant_method_implements_embedding = method_has_implemented_embedding(\n        type(quant_method))\n    if is_embedding_layer and not quant_method_implements_embedding:\n        raise NotImplementedError(\n            f\"The class {type(quant_method).__name__} must implement \"\n            \"the 'embedding' method, see UnquantizedEmbeddingMethod.\")\n\n    self.quant_method: QuantizeMethodBase = quant_method\n\n    if params_dtype is None:\n        params_dtype = torch.get_default_dtype()\n    # Divide the weight matrix along the vocaburaly dimension.\n    self.num_added_embeddings = self.num_embeddings - self.org_vocab_size\n    self.num_embeddings_per_partition = divide(self.num_embeddings_padded,\n                                               self.tp_size)\n    assert (self.shard_indices.num_elements_padded ==\n            self.num_embeddings_per_partition)\n    self.num_org_embeddings_per_partition = (\n        self.shard_indices.org_vocab_end_index -\n        self.shard_indices.org_vocab_start_index)\n    self.num_added_embeddings_per_partition = (\n        self.shard_indices.added_vocab_end_index -\n        self.shard_indices.added_vocab_start_index)\n\n    self.quant_method.create_weights(self,\n                                     self.embedding_dim,\n                                     [self.num_embeddings_per_partition],\n                                     self.embedding_dim,\n                                     self.num_embeddings_padded,\n                                     params_dtype=params_dtype,\n                                     weight_loader=self.weight_loader)\n</code></pre> Functions\u00b6 fastvideo.layers.vocab_parallel_embedding.VocabParallelEmbedding.get_sharded_to_full_mapping \u00b6 <pre><code>get_sharded_to_full_mapping() -&gt; list[int] | None\n</code></pre> <p>Get a mapping that can be used to reindex the gathered logits for sampling.</p> <p>During sampling, we gather logits from all ranks. The relationship of index-&gt;token_id will follow the same format as outlined in the class docstring. However, after the gather, we want to reindex the final logits tensor to map index-&gt;token_id one-to-one (the index is always equal the token_id it corresponds to). The indices returned by this method allow us to do that.</p> Source code in <code>fastvideo/layers/vocab_parallel_embedding.py</code> <pre><code>def get_sharded_to_full_mapping(self) -&gt; list[int] | None:\n    \"\"\"Get a mapping that can be used to reindex the gathered\n    logits for sampling.\n\n    During sampling, we gather logits from all ranks. The relationship\n    of index-&gt;token_id will follow the same format as outlined in the class\n    docstring. However, after the gather, we want to reindex the final\n    logits tensor to map index-&gt;token_id one-to-one (the index is always\n    equal the token_id it corresponds to). The indices returned by this\n    method allow us to do that.\n    \"\"\"\n    if self.tp_size &lt; 2:\n        return None\n\n    base_embeddings: list[int] = []\n    added_embeddings: list[int] = []\n    padding: list[int] = []\n    for tp_rank in range(self.tp_size):\n        shard_indices = self._get_indices(self.num_embeddings_padded,\n                                          self.org_vocab_size_padded,\n                                          self.num_embeddings,\n                                          self.org_vocab_size, tp_rank,\n                                          self.tp_size)\n        range_start = self.num_embeddings_per_partition * tp_rank\n        range_end = self.num_embeddings_per_partition * (tp_rank + 1)\n        base_embeddings.extend(\n            range(range_start,\n                  range_start + shard_indices.num_org_elements))\n        padding.extend(\n            range(range_start + shard_indices.num_org_elements,\n                  range_start + shard_indices.num_org_elements_padded))\n        added_embeddings.extend(\n            range(\n                range_start + shard_indices.num_org_elements_padded,\n                range_start + shard_indices.num_org_elements_padded +\n                shard_indices.num_added_elements))\n        padding.extend(\n            range(\n                range_start + shard_indices.num_org_elements_padded +\n                shard_indices.num_added_elements,\n                range_start + shard_indices.num_org_elements_padded +\n                shard_indices.num_added_elements_padded))\n        assert (range_start + shard_indices.num_org_elements_padded +\n                shard_indices.num_added_elements_padded == range_end)\n    ret = base_embeddings + added_embeddings + padding\n    assert len(ret) == self.num_embeddings_padded\n    return ret\n</code></pre>"},{"location":"api/fastvideo/layers/#fastvideo.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices","title":"fastvideo.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices  <code>dataclass</code>","text":"<pre><code>VocabParallelEmbeddingShardIndices(\n    padded_org_vocab_start_index: int,\n    padded_org_vocab_end_index: int,\n    padded_added_vocab_start_index: int,\n    padded_added_vocab_end_index: int,\n    org_vocab_start_index: int,\n    org_vocab_end_index: int,\n    added_vocab_start_index: int,\n    added_vocab_end_index: int,\n)\n</code></pre> <p>Indices for a shard of a vocab parallel embedding.</p>"},{"location":"api/fastvideo/layers/#fastvideo.layers.vocab_parallel_embedding-functions","title":"Functions","text":""},{"location":"api/fastvideo/layers/#fastvideo.layers.vocab_parallel_embedding.pad_vocab_size","title":"fastvideo.layers.vocab_parallel_embedding.pad_vocab_size","text":"<pre><code>pad_vocab_size(\n    vocab_size: int,\n    pad_to: int = DEFAULT_VOCAB_PADDING_SIZE,\n) -&gt; int\n</code></pre> <p>Pad the vocab size to the given value.</p> Source code in <code>fastvideo/layers/vocab_parallel_embedding.py</code> <pre><code>def pad_vocab_size(vocab_size: int,\n                   pad_to: int = DEFAULT_VOCAB_PADDING_SIZE) -&gt; int:\n    \"\"\"Pad the vocab size to the given value.\"\"\"\n    return ((vocab_size + pad_to - 1) // pad_to) * pad_to\n</code></pre>"},{"location":"api/fastvideo/logger/","title":"logger","text":""},{"location":"api/fastvideo/logger/#fastvideo.logger","title":"logger","text":"<p>Logging configuration for fastvideo.</p>"},{"location":"api/fastvideo/logger/#fastvideo.logger-functions","title":"Functions","text":""},{"location":"api/fastvideo/logger/#fastvideo.logger.enable_trace_function_call","title":"fastvideo.logger.enable_trace_function_call","text":"<pre><code>enable_trace_function_call(\n    log_file_path: str, root_dir: str | None = None\n)\n</code></pre> <p>Enable tracing of every function call in code under <code>root_dir</code>. This is useful for debugging hangs or crashes. <code>log_file_path</code> is the path to the log file. <code>root_dir</code> is the root directory of the code to trace. If None, it is the fastvideo root directory.</p> <p>Note that this call is thread-level, any threads calling this function will have the trace enabled. Other threads will not be affected.</p> Source code in <code>fastvideo/logger.py</code> <pre><code>def enable_trace_function_call(log_file_path: str, root_dir: str | None = None):\n    \"\"\"\n    Enable tracing of every function call in code under `root_dir`.\n    This is useful for debugging hangs or crashes.\n    `log_file_path` is the path to the log file.\n    `root_dir` is the root directory of the code to trace. If None, it is the\n    fastvideo root directory.\n\n    Note that this call is thread-level, any threads calling this function\n    will have the trace enabled. Other threads will not be affected.\n    \"\"\"\n    logger.warning(\n        \"FASTVIDEO_TRACE_FUNCTION is enabled. It will record every\"\n        \" function executed by Python. This will slow down the code. It \"\n        \"is suggested to be used for debugging hang or crashes only.\")\n    logger.info(\"Trace frame log is saved to %s\", log_file_path)\n    if root_dir is None:\n        # by default, this is the fastvideo root directory\n        root_dir = os.path.dirname(os.path.dirname(__file__))\n    sys.settrace(partial(_trace_calls, log_file_path, root_dir))\n</code></pre>"},{"location":"api/fastvideo/logger/#fastvideo.logger.init_logger","title":"fastvideo.logger.init_logger","text":"<pre><code>init_logger(name: str) -&gt; _FastvideoLogger\n</code></pre> <p>The main purpose of this function is to ensure that loggers are retrieved in such a way that we can be sure the root fastvideo logger has already been configured.</p> Source code in <code>fastvideo/logger.py</code> <pre><code>def init_logger(name: str) -&gt; _FastvideoLogger:\n    \"\"\"The main purpose of this function is to ensure that loggers are\n    retrieved in such a way that we can be sure the root fastvideo logger has\n    already been configured.\"\"\"\n\n    logger = logging.getLogger(name)\n\n    methods_to_patch = {\n        \"info_once\": _print_info_once,\n        \"warning_once\": _print_warning_once,\n        \"info\": _info,\n    }\n\n    for method_name, method in methods_to_patch.items():\n        setattr(logger, method_name,\n                MethodType(method, logger))  # type: ignore[arg-type]\n\n    return cast(_FastvideoLogger, logger)\n</code></pre>"},{"location":"api/fastvideo/logging_utils/","title":"logging_utils","text":""},{"location":"api/fastvideo/logging_utils/#fastvideo.logging_utils","title":"logging_utils","text":""},{"location":"api/fastvideo/logging_utils/#fastvideo.logging_utils-classes","title":"Classes","text":""},{"location":"api/fastvideo/logging_utils/#fastvideo.logging_utils.NewLineFormatter","title":"fastvideo.logging_utils.NewLineFormatter","text":"<pre><code>NewLineFormatter(fmt, datefmt=None, style='%')\n</code></pre> <p>               Bases: <code>Formatter</code></p> <p>Adds logging prefix to newlines to align multi-line messages.</p> Source code in <code>fastvideo/logging_utils/formatter.py</code> <pre><code>def __init__(self, fmt, datefmt=None, style=\"%\"):\n    logging.Formatter.__init__(self, fmt, datefmt, style)\n</code></pre>"},{"location":"api/fastvideo/logging_utils/#fastvideo.logging_utils-modules","title":"Modules","text":""},{"location":"api/fastvideo/logging_utils/#fastvideo.logging_utils.formatter","title":"fastvideo.logging_utils.formatter","text":""},{"location":"api/fastvideo/logging_utils/#fastvideo.logging_utils.formatter-classes","title":"Classes","text":""},{"location":"api/fastvideo/logging_utils/#fastvideo.logging_utils.formatter.NewLineFormatter","title":"fastvideo.logging_utils.formatter.NewLineFormatter","text":"<pre><code>NewLineFormatter(fmt, datefmt=None, style='%')\n</code></pre> <p>               Bases: <code>Formatter</code></p> <p>Adds logging prefix to newlines to align multi-line messages.</p> Source code in <code>fastvideo/logging_utils/formatter.py</code> <pre><code>def __init__(self, fmt, datefmt=None, style=\"%\"):\n    logging.Formatter.__init__(self, fmt, datefmt, style)\n</code></pre>"},{"location":"api/fastvideo/models/","title":"models","text":""},{"location":"api/fastvideo/models/#fastvideo.models","title":"models","text":""},{"location":"api/fastvideo/models/#fastvideo.models-modules","title":"Modules","text":""},{"location":"api/fastvideo/models/#fastvideo.models.hf_transformer_utils","title":"fastvideo.models.hf_transformer_utils","text":"<p>Utilities for Huggingface Transformers.</p>"},{"location":"api/fastvideo/models/#fastvideo.models.hf_transformer_utils-functions","title":"Functions","text":""},{"location":"api/fastvideo/models/#fastvideo.models.hf_transformer_utils.check_gguf_file","title":"fastvideo.models.hf_transformer_utils.check_gguf_file","text":"<pre><code>check_gguf_file(model: str | PathLike) -&gt; bool\n</code></pre> <p>Check if the file is a GGUF model.</p> Source code in <code>fastvideo/models/hf_transformer_utils.py</code> <pre><code>def check_gguf_file(model: str | os.PathLike) -&gt; bool:\n    \"\"\"Check if the file is a GGUF model.\"\"\"\n    model = Path(model)\n    if not model.is_file():\n        return False\n    elif model.suffix == \".gguf\":\n        return True\n\n    with open(model, \"rb\") as f:\n        header = f.read(4)\n    return header == b\"GGUF\"\n</code></pre>"},{"location":"api/fastvideo/models/#fastvideo.models.hf_transformer_utils.get_diffusers_config","title":"fastvideo.models.hf_transformer_utils.get_diffusers_config","text":"<pre><code>get_diffusers_config(model: str) -&gt; dict[str, Any]\n</code></pre> <p>Gets a configuration for the given diffusers model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The model name or path.</p> required <code>fastvideo_args</code> <p>Optional inference arguments to override in the config.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The loaded configuration.</p> Source code in <code>fastvideo/models/hf_transformer_utils.py</code> <pre><code>def get_diffusers_config(model: str, ) -&gt; dict[str, Any]:\n    \"\"\"Gets a configuration for the given diffusers model.\n\n    Args:\n        model: The model name or path.\n        fastvideo_args: Optional inference arguments to override in the config.\n\n    Returns:\n        The loaded configuration.\n    \"\"\"\n    config_name = \"config.json\"\n    if \"scheduler\" in model:\n        config_name = \"scheduler_config.json\"\n    # Check if the model path exists\n    if os.path.exists(model):\n        config_file = os.path.join(model, config_name)\n        if os.path.exists(config_file):\n            try:\n                # Load the config directly from the file\n                with open(config_file) as f:\n                    config_dict: dict[str, Any] = json.load(f)\n                if \"_diffusers_version\" in config_dict:\n                    config_dict.pop(\"_diffusers_version\")\n                # TODO(will): apply any overrides from inference args\n                return config_dict\n            except Exception as e:\n                raise RuntimeError(\n                    f\"Failed to load diffusers config from {config_file}: {e}\"\n                ) from e\n        raise RuntimeError(f\"Config file not found at {config_file}\")\n    else:\n        raise RuntimeError(f\"Diffusers config file not found at {model}\")\n</code></pre>"},{"location":"api/fastvideo/models/#fastvideo.models.loader","title":"fastvideo.models.loader","text":""},{"location":"api/fastvideo/models/#fastvideo.models.loader-modules","title":"Modules","text":""},{"location":"api/fastvideo/models/#fastvideo.models.loader.component_loader","title":"fastvideo.models.loader.component_loader","text":"Classes\u00b6 fastvideo.models.loader.component_loader.ComponentLoader \u00b6 <pre><code>ComponentLoader(device=None)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for loading a specific type of model component.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def __init__(self, device=None) -&gt; None:\n    self.device = device\n</code></pre> Functions\u00b6 fastvideo.models.loader.component_loader.ComponentLoader.for_module_type <code>classmethod</code> \u00b6 <pre><code>for_module_type(\n    module_type: str, transformers_or_diffusers: str\n) -&gt; ComponentLoader\n</code></pre> <p>Factory method to create a component loader for a specific module type.</p> <p>Parameters:</p> Name Type Description Default <code>module_type</code> <code>str</code> <p>Type of module (e.g., \"vae\", \"text_encoder\", \"transformer\", \"scheduler\")</p> required <code>transformers_or_diffusers</code> <code>str</code> <p>Whether the module is from transformers or diffusers</p> required <p>Returns:</p> Type Description <code>ComponentLoader</code> <p>A component loader for the specified module type</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>@classmethod\ndef for_module_type(cls, module_type: str,\n                    transformers_or_diffusers: str) -&gt; 'ComponentLoader':\n    \"\"\"\n    Factory method to create a component loader for a specific module type.\n\n    Args:\n        module_type: Type of module (e.g., \"vae\", \"text_encoder\", \"transformer\", \"scheduler\")\n        transformers_or_diffusers: Whether the module is from transformers or diffusers\n\n    Returns:\n        A component loader for the specified module type\n    \"\"\"\n    # Map of module types to their loader classes and expected library\n    module_loaders = {\n        \"scheduler\": (SchedulerLoader, \"diffusers\"),\n        \"transformer\": (TransformerLoader, \"diffusers\"),\n        \"transformer_2\": (TransformerLoader, \"diffusers\"),\n        \"vae\": (VAELoader, \"diffusers\"),\n        \"text_encoder\": (TextEncoderLoader, \"transformers\"),\n        \"text_encoder_2\": (TextEncoderLoader, \"transformers\"),\n        \"tokenizer\": (TokenizerLoader, \"transformers\"),\n        \"tokenizer_2\": (TokenizerLoader, \"transformers\"),\n        \"image_processor\": (ImageProcessorLoader, \"transformers\"),\n        \"image_encoder\": (ImageEncoderLoader, \"transformers\"),\n    }\n\n    if module_type in module_loaders:\n        loader_cls, expected_library = module_loaders[module_type]\n        # Assert that the library matches what's expected for this module type\n        assert transformers_or_diffusers == expected_library, f\"{module_type} must be loaded from {expected_library}, got {transformers_or_diffusers}\"\n        return loader_cls()\n\n    # For unknown module types, use a generic loader\n    logger.warning(\n        \"No specific loader found for module type: %s. Using generic loader.\",\n        module_type)\n    return GenericComponentLoader(transformers_or_diffusers)\n</code></pre> fastvideo.models.loader.component_loader.ComponentLoader.load <code>abstractmethod</code> \u00b6 <pre><code>load(model_path: str, fastvideo_args: FastVideoArgs)\n</code></pre> <p>Load the component based on the model path, architecture, and inference args.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the component model</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>FastVideoArgs</p> required <p>Returns:</p> Type Description <p>The loaded component</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>@abstractmethod\ndef load(self, model_path: str, fastvideo_args: FastVideoArgs):\n    \"\"\"\n    Load the component based on the model path, architecture, and inference args.\n\n    Args:\n        model_path: Path to the component model\n        fastvideo_args: FastVideoArgs\n\n    Returns:\n        The loaded component\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.models.loader.component_loader.GenericComponentLoader \u00b6 <pre><code>GenericComponentLoader(library='transformers')\n</code></pre> <p>               Bases: <code>ComponentLoader</code></p> <p>Generic loader for components that don't have a specific loader.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def __init__(self, library=\"transformers\") -&gt; None:\n    super().__init__()\n    self.library = library\n</code></pre> Functions\u00b6 fastvideo.models.loader.component_loader.GenericComponentLoader.load \u00b6 <pre><code>load(model_path: str, fastvideo_args: FastVideoArgs)\n</code></pre> <p>Load a generic component based on the model path, and inference args.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def load(self, model_path: str, fastvideo_args: FastVideoArgs):\n    \"\"\"Load a generic component based on the model path, and inference args.\"\"\"\n    logger.warning(\"Using generic loader for %s with library %s\",\n                   model_path, self.library)\n\n    if self.library == \"transformers\":\n        from transformers import AutoModel\n\n        model = AutoModel.from_pretrained(\n            model_path,\n            trust_remote_code=fastvideo_args.trust_remote_code,\n            revision=fastvideo_args.revision,\n        )\n        logger.info(\"Loaded generic transformers model: %s\",\n                    model.__class__.__name__)\n        return model\n    elif self.library == \"diffusers\":\n        logger.warning(\n            \"Generic loading for diffusers components is not fully implemented\"\n        )\n\n        model_config = get_diffusers_config(model=model_path)\n        logger.info(\"Diffusers Model config: %s\", model_config)\n        # This is a placeholder - in a real implementation, you'd need to handle this properly\n        return None\n    else:\n        raise ValueError(f\"Unsupported library: {self.library}\")\n</code></pre> fastvideo.models.loader.component_loader.ImageEncoderLoader \u00b6 <pre><code>ImageEncoderLoader(device=None)\n</code></pre> <p>               Bases: <code>TextEncoderLoader</code></p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def __init__(self, device=None) -&gt; None:\n    self.device = device\n</code></pre> Functions\u00b6 fastvideo.models.loader.component_loader.ImageEncoderLoader.load \u00b6 <pre><code>load(model_path: str, fastvideo_args: FastVideoArgs)\n</code></pre> <p>Load the text encoders based on the model path, and inference args.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def load(self, model_path: str, fastvideo_args: FastVideoArgs):\n    \"\"\"Load the text encoders based on the model path, and inference args.\"\"\"\n    # model_config: PretrainedConfig = get_hf_config(\n    #     model=model_path,\n    #     trust_remote_code=fastvideo_args.trust_remote_code,\n    #     revision=fastvideo_args.revision,\n    #     model_override_args=None,\n    # )\n    with open(os.path.join(model_path, \"config.json\")) as f:\n        model_config = json.load(f)\n    model_config.pop(\"_name_or_path\", None)\n    model_config.pop(\"transformers_version\", None)\n    model_config.pop(\"torch_dtype\", None)\n    model_config.pop(\"model_type\", None)\n    logger.info(\"HF Model config: %s\", model_config)\n\n    encoder_config = fastvideo_args.pipeline_config.image_encoder_config\n    encoder_config.update_model_arch(model_config)\n\n    from fastvideo.platforms import current_platform\n\n    if fastvideo_args.image_encoder_cpu_offload:\n        target_device = torch.device(\"mps\") if current_platform.is_mps() else torch.device(\"cpu\")\n    else:\n        target_device = get_local_torch_device()\n    # TODO(will): add support for other dtypes\n    return self.load_model(\n        model_path, encoder_config, target_device, fastvideo_args,\n        fastvideo_args.pipeline_config.image_encoder_precision)\n</code></pre> fastvideo.models.loader.component_loader.ImageProcessorLoader \u00b6 <pre><code>ImageProcessorLoader(device=None)\n</code></pre> <p>               Bases: <code>ComponentLoader</code></p> <p>Loader for image processor.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def __init__(self, device=None) -&gt; None:\n    self.device = device\n</code></pre> Functions\u00b6 fastvideo.models.loader.component_loader.ImageProcessorLoader.load \u00b6 <pre><code>load(model_path: str, fastvideo_args: FastVideoArgs)\n</code></pre> <p>Load the image processor based on the model path, and inference args.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def load(self, model_path: str, fastvideo_args: FastVideoArgs):\n    \"\"\"Load the image processor based on the model path, and inference args.\"\"\"\n    logger.info(\"Loading image processor from %s\", model_path)\n\n    image_processor = AutoImageProcessor.from_pretrained(model_path, )\n    logger.info(\"Loaded image processor: %s\",\n                image_processor.__class__.__name__)\n    return image_processor\n</code></pre> fastvideo.models.loader.component_loader.PipelineComponentLoader \u00b6 <p>Utility class for loading pipeline components. This replaces the chain of if-else statements in load_pipeline_module.</p> Functions\u00b6 fastvideo.models.loader.component_loader.PipelineComponentLoader.load_module <code>staticmethod</code> \u00b6 <pre><code>load_module(\n    module_name: str,\n    component_model_path: str,\n    transformers_or_diffusers: str,\n    fastvideo_args: FastVideoArgs,\n)\n</code></pre> <p>Load a pipeline module.</p> <p>Parameters:</p> Name Type Description Default <code>module_name</code> <code>str</code> <p>Name of the module (e.g., \"vae\", \"text_encoder\", \"transformer\", \"scheduler\")</p> required <code>component_model_path</code> <code>str</code> <p>Path to the component model</p> required <code>transformers_or_diffusers</code> <code>str</code> <p>Whether the module is from transformers or diffusers</p> required <code>pipeline_args</code> <p>Inference arguments</p> required <p>Returns:</p> Type Description <p>The loaded module</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>@staticmethod\ndef load_module(module_name: str, component_model_path: str,\n                transformers_or_diffusers: str,\n                fastvideo_args: FastVideoArgs):\n    \"\"\"\n    Load a pipeline module.\n\n    Args:\n        module_name: Name of the module (e.g., \"vae\", \"text_encoder\", \"transformer\", \"scheduler\")\n        component_model_path: Path to the component model\n        transformers_or_diffusers: Whether the module is from transformers or diffusers\n        pipeline_args: Inference arguments\n\n    Returns:\n        The loaded module\n    \"\"\"\n    logger.info(\n        \"Loading %s using %s from %s\",\n        module_name,\n        transformers_or_diffusers,\n        component_model_path,\n    )\n\n    # Get the appropriate loader for this module type\n    loader = ComponentLoader.for_module_type(module_name,\n                                             transformers_or_diffusers)\n\n    # Load the module\n    return loader.load(component_model_path, fastvideo_args)\n</code></pre> fastvideo.models.loader.component_loader.SchedulerLoader \u00b6 <pre><code>SchedulerLoader(device=None)\n</code></pre> <p>               Bases: <code>ComponentLoader</code></p> <p>Loader for scheduler.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def __init__(self, device=None) -&gt; None:\n    self.device = device\n</code></pre> Functions\u00b6 fastvideo.models.loader.component_loader.SchedulerLoader.load \u00b6 <pre><code>load(model_path: str, fastvideo_args: FastVideoArgs)\n</code></pre> <p>Load the scheduler based on the model path, and inference args.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def load(self, model_path: str, fastvideo_args: FastVideoArgs):\n    \"\"\"Load the scheduler based on the model path, and inference args.\"\"\"\n    config = get_diffusers_config(model=model_path)\n\n    class_name = config.pop(\"_class_name\")\n    assert class_name is not None, \"Model config does not contain a _class_name attribute. Only diffusers format is supported.\"\n\n    scheduler_cls, _ = ModelRegistry.resolve_model_cls(class_name)\n\n    scheduler = scheduler_cls(**config)\n    if fastvideo_args.pipeline_config.flow_shift is not None:\n        scheduler.set_shift(fastvideo_args.pipeline_config.flow_shift)\n    if fastvideo_args.pipeline_config.timesteps_scale is not None:\n        scheduler.set_timesteps_scale(\n            fastvideo_args.pipeline_config.timesteps_scale)\n    return scheduler\n</code></pre> fastvideo.models.loader.component_loader.TextEncoderLoader \u00b6 <pre><code>TextEncoderLoader(device=None)\n</code></pre> <p>               Bases: <code>ComponentLoader</code></p> <p>Loader for text encoders.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def __init__(self, device=None) -&gt; None:\n    self.device = device\n</code></pre> Classes\u00b6 fastvideo.models.loader.component_loader.TextEncoderLoader.Source <code>dataclass</code> \u00b6 <pre><code>Source(\n    model_or_path: str,\n    prefix: str = \"\",\n    fall_back_to_pt: bool = True,\n    allow_patterns_overrides: list[str] | None = None,\n)\n</code></pre> <p>A source for weights.</p> Attributes\u00b6 fastvideo.models.loader.component_loader.TextEncoderLoader.Source.allow_patterns_overrides <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>allow_patterns_overrides: list[str] | None = None\n</code></pre> <p>If defined, weights will load exclusively using these patterns.</p> fastvideo.models.loader.component_loader.TextEncoderLoader.Source.fall_back_to_pt <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>fall_back_to_pt: bool = True\n</code></pre> <p>Whether .pt weights can be used.</p> fastvideo.models.loader.component_loader.TextEncoderLoader.Source.model_or_path <code>instance-attribute</code> \u00b6 <pre><code>model_or_path: str\n</code></pre> <p>The model ID or path.</p> fastvideo.models.loader.component_loader.TextEncoderLoader.Source.prefix <code>class-attribute</code> <code>instance-attribute</code> \u00b6 <pre><code>prefix: str = ''\n</code></pre> <p>A prefix to prepend to all weights.</p> Functions\u00b6 fastvideo.models.loader.component_loader.TextEncoderLoader.load \u00b6 <pre><code>load(model_path: str, fastvideo_args: FastVideoArgs)\n</code></pre> <p>Load the text encoders based on the model path, and inference args.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def load(self, model_path: str, fastvideo_args: FastVideoArgs):\n    \"\"\"Load the text encoders based on the model path, and inference args.\"\"\"\n    # model_config: PretrainedConfig = get_hf_config(\n    #     model=model_path,\n    #     trust_remote_code=fastvideo_args.trust_remote_code,\n    #     revision=fastvideo_args.revision,\n    #     model_override_args=None,\n    # )\n    model_config = get_diffusers_config(model=model_path)\n    model_config.pop(\"_name_or_path\", None)\n    model_config.pop(\"transformers_version\", None)\n    model_config.pop(\"model_type\", None)\n    model_config.pop(\"tokenizer_class\", None)\n    model_config.pop(\"torch_dtype\", None)\n    logger.info(\"HF Model config: %s\", model_config)\n\n    # @TODO(Wei): Better way to handle this?\n    try:\n        encoder_config = fastvideo_args.pipeline_config.text_encoder_configs[\n            0]\n        encoder_config.update_model_arch(model_config)\n        encoder_precision = fastvideo_args.pipeline_config.text_encoder_precisions[\n            0]\n    except Exception:\n        encoder_config = fastvideo_args.pipeline_config.text_encoder_configs[\n            1]\n        encoder_config.update_model_arch(model_config)\n        encoder_precision = fastvideo_args.pipeline_config.text_encoder_precisions[\n            1]\n\n    target_device = get_local_torch_device()\n    # TODO(will): add support for other dtypes\n    return self.load_model(model_path, encoder_config, target_device,\n                           fastvideo_args, encoder_precision)\n</code></pre> fastvideo.models.loader.component_loader.TokenizerLoader \u00b6 <pre><code>TokenizerLoader(device=None)\n</code></pre> <p>               Bases: <code>ComponentLoader</code></p> <p>Loader for tokenizers.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def __init__(self, device=None) -&gt; None:\n    self.device = device\n</code></pre> Functions\u00b6 fastvideo.models.loader.component_loader.TokenizerLoader.load \u00b6 <pre><code>load(model_path: str, fastvideo_args: FastVideoArgs)\n</code></pre> <p>Load the tokenizer based on the model path, and inference args.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def load(self, model_path: str, fastvideo_args: FastVideoArgs):\n    \"\"\"Load the tokenizer based on the model path, and inference args.\"\"\"\n    logger.info(\"Loading tokenizer from %s\", model_path)\n\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_path,  # \"&lt;path to model&gt;/tokenizer\"\n        # in v0, this was same string as encoder_name \"ClipTextModel\"\n        # TODO(will): pass these tokenizer kwargs from inference args? Maybe\n        # other method of config?\n        padding_size='right',\n    )\n    logger.info(\"Loaded tokenizer: %s\", tokenizer.__class__.__name__)\n    return tokenizer\n</code></pre> fastvideo.models.loader.component_loader.TransformerLoader \u00b6 <pre><code>TransformerLoader(device=None)\n</code></pre> <p>               Bases: <code>ComponentLoader</code></p> <p>Loader for transformer.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def __init__(self, device=None) -&gt; None:\n    self.device = device\n</code></pre> Functions\u00b6 fastvideo.models.loader.component_loader.TransformerLoader.load \u00b6 <pre><code>load(model_path: str, fastvideo_args: FastVideoArgs)\n</code></pre> <p>Load the transformer based on the model path, and inference args.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def load(self, model_path: str, fastvideo_args: FastVideoArgs):\n    \"\"\"Load the transformer based on the model path, and inference args.\"\"\"\n    config = get_diffusers_config(model=model_path)\n    hf_config = deepcopy(config)\n    cls_name = config.pop(\"_class_name\")\n    if cls_name is None:\n        raise ValueError(\n            \"Model config does not contain a _class_name attribute. \"\n            \"Only diffusers format is supported.\")\n\n    logger.info(\"transformer cls_name: %s\", cls_name)\n    if fastvideo_args.override_transformer_cls_name is not None:\n        cls_name = fastvideo_args.override_transformer_cls_name\n        logger.info(\"Overriding transformer cls_name to %s\", cls_name)\n\n    fastvideo_args.model_paths[\"transformer\"] = model_path\n\n    # Config from Diffusers supersedes fastvideo's model config\n    dit_config = fastvideo_args.pipeline_config.dit_config\n    dit_config.update_model_arch(config)\n\n    model_cls, _ = ModelRegistry.resolve_model_cls(cls_name)\n\n    # Find all safetensors files\n    safetensors_list = glob.glob(\n        os.path.join(str(model_path), \"*.safetensors\"))\n    if not safetensors_list:\n        raise ValueError(f\"No safetensors files found in {model_path}\")\n\n    # Check if we should use custom initialization weights\n    custom_weights_path = getattr(fastvideo_args, 'init_weights_from_safetensors', None)\n    use_custom_weights = (custom_weights_path and os.path.exists(custom_weights_path) and \n                        not hasattr(fastvideo_args, '_loading_teacher_critic_model'))\n\n    if use_custom_weights:\n        if 'transformer_2' in model_path:\n            custom_weights_path = getattr(fastvideo_args, 'init_weights_from_safetensors_2', None)\n        assert custom_weights_path is not None, \"Custom initialization weights must be provided\"\n        if os.path.isdir(custom_weights_path):\n            safetensors_list = glob.glob(\n                os.path.join(str(custom_weights_path), \"*.safetensors\"))\n        else:\n            assert custom_weights_path.endswith(\".safetensors\"), \"Custom initialization weights must be a safetensors file\"\n            safetensors_list = [custom_weights_path]\n\n    logger.info(\"Loading model from %s safetensors files: %s\",\n                len(safetensors_list), safetensors_list)\n\n    default_dtype = PRECISION_TO_TYPE[\n        fastvideo_args.pipeline_config.dit_precision]\n\n    # Load the model using FSDP loader\n    logger.info(\"Loading model from %s, default_dtype: %s\", cls_name,\n                default_dtype)\n    assert fastvideo_args.hsdp_shard_dim is not None\n    model = maybe_load_fsdp_model(\n        model_cls=model_cls,\n        init_params={\n            \"config\": dit_config,\n            \"hf_config\": hf_config\n        },\n        weight_dir_list=safetensors_list,\n        device=get_local_torch_device(),\n        hsdp_replicate_dim=fastvideo_args.hsdp_replicate_dim,\n        hsdp_shard_dim=fastvideo_args.hsdp_shard_dim,\n        cpu_offload=fastvideo_args.dit_cpu_offload,\n        pin_cpu_memory=fastvideo_args.pin_cpu_memory,\n        fsdp_inference=fastvideo_args.use_fsdp_inference,\n        # TODO(will): make these configurable\n        default_dtype=default_dtype,\n        param_dtype=torch.bfloat16,\n        reduce_dtype=torch.float32,\n        output_dtype=None,\n        training_mode=fastvideo_args.training_mode,\n        enable_torch_compile=fastvideo_args.enable_torch_compile,\n        torch_compile_kwargs=fastvideo_args.torch_compile_kwargs)\n\n\n    total_params = sum(p.numel() for p in model.parameters())\n    logger.info(\"Loaded model with %.2fB parameters\", total_params / 1e9)\n\n    assert next(model.parameters()).dtype == default_dtype, \"Model dtype does not match default dtype\"\n\n    model = model.eval()\n    return model\n</code></pre> fastvideo.models.loader.component_loader.VAELoader \u00b6 <pre><code>VAELoader(device=None)\n</code></pre> <p>               Bases: <code>ComponentLoader</code></p> <p>Loader for VAE.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def __init__(self, device=None) -&gt; None:\n    self.device = device\n</code></pre> Functions\u00b6 fastvideo.models.loader.component_loader.VAELoader.load \u00b6 <pre><code>load(model_path: str, fastvideo_args: FastVideoArgs)\n</code></pre> <p>Load the VAE based on the model path, and inference args.</p> Source code in <code>fastvideo/models/loader/component_loader.py</code> <pre><code>def load(self, model_path: str, fastvideo_args: FastVideoArgs):\n    \"\"\"Load the VAE based on the model path, and inference args.\"\"\"\n    config = get_diffusers_config(model=model_path)\n    class_name = config.pop(\"_class_name\")\n    assert class_name is not None, \"Model config does not contain a _class_name attribute. Only diffusers format is supported.\"\n    fastvideo_args.model_paths[\"vae\"] = model_path\n\n    vae_config = fastvideo_args.pipeline_config.vae_config\n    vae_config.update_model_arch(config)\n\n    from fastvideo.platforms import current_platform\n\n    if fastvideo_args.vae_cpu_offload:\n        target_device = torch.device(\"mps\") if current_platform.is_mps() else torch.device(\"cpu\")\n    else:\n        target_device = get_local_torch_device()\n\n    with set_default_torch_dtype(PRECISION_TO_TYPE[\n            fastvideo_args.pipeline_config.vae_precision]):\n        vae_cls, _ = ModelRegistry.resolve_model_cls(class_name)\n        vae = vae_cls(vae_config).to(target_device)\n\n    # Find all safetensors files\n    safetensors_list = glob.glob(\n        os.path.join(str(model_path), \"*.safetensors\"))\n    # TODO(PY)\n    assert len(\n        safetensors_list\n    ) == 1, f\"Found {len(safetensors_list)} safetensors files in {model_path}\"\n    loaded = safetensors_load_file(safetensors_list[0])\n    vae.load_state_dict(\n        loaded, strict=False)  # We might only load encoder or decoder\n\n    return vae.eval()\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/models/#fastvideo.models.loader.fsdp_load","title":"fastvideo.models.loader.fsdp_load","text":"Functions\u00b6 fastvideo.models.loader.fsdp_load.load_model_from_full_model_state_dict \u00b6 <pre><code>load_model_from_full_model_state_dict(\n    model: FSDPModule | Module,\n    full_sd_iterator: Generator[\n        tuple[str, Tensor], None, None\n    ],\n    device: device,\n    param_dtype: dtype,\n    strict: bool = False,\n    cpu_offload: bool = False,\n    param_names_mapping: Callable[\n        [str], tuple[str, Any, Any]\n    ]\n    | None = None,\n    training_mode: bool = True,\n) -&gt; _IncompatibleKeys\n</code></pre> <p>Converting full state dict into a sharded state dict and loading it into FSDP model (if training) or normal huggingface model Args:     model (Union[FSDPModule, torch.nn.Module]): Model to generate fully qualified names for cpu_state_dict     full_sd_iterator (Generator): an iterator yielding (param_name, tensor) pairs     device (torch.device): device used to move full state dict tensors     param_dtype (torch.dtype): dtype used to move full state dict tensors     strict (bool): flag to check if to load the model in strict mode     cpu_offload (bool): flag to check if FSDP offload is enabled     param_names_mapping (Optional[Callable[[str], str]]): a function that maps full param name to sharded param name     training_mode (bool): apply FSDP only for training Returns:     <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:         * missing_keys is a list of str containing the missing keys         * unexpected_keys is a list of str containing the unexpected keys</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If got FSDP with more than 1D.</p> Source code in <code>fastvideo/models/loader/fsdp_load.py</code> <pre><code>def load_model_from_full_model_state_dict(\n    model: FSDPModule | torch.nn.Module,\n    full_sd_iterator: Generator[tuple[str, torch.Tensor], None, None],\n    device: torch.device,\n    param_dtype: torch.dtype,\n    strict: bool = False,\n    cpu_offload: bool = False,\n    param_names_mapping: Callable[[str], tuple[str, Any, Any]] | None = None,\n    training_mode: bool = True,\n) -&gt; _IncompatibleKeys:\n    \"\"\"\n    Converting full state dict into a sharded state dict\n    and loading it into FSDP model (if training) or normal huggingface model\n    Args:\n        model (Union[FSDPModule, torch.nn.Module]): Model to generate fully qualified names for cpu_state_dict\n        full_sd_iterator (Generator): an iterator yielding (param_name, tensor) pairs\n        device (torch.device): device used to move full state dict tensors\n        param_dtype (torch.dtype): dtype used to move full state dict tensors\n        strict (bool): flag to check if to load the model in strict mode\n        cpu_offload (bool): flag to check if FSDP offload is enabled\n        param_names_mapping (Optional[Callable[[str], str]]): a function that maps full param name to sharded param name\n        training_mode (bool): apply FSDP only for training\n    Returns:\n        ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n            * **missing_keys** is a list of str containing the missing keys\n            * **unexpected_keys** is a list of str containing the unexpected keys\n\n    Raises:\n        NotImplementedError: If got FSDP with more than 1D.\n    \"\"\"\n    meta_sd = model.state_dict()\n    sharded_sd = {}\n    custom_param_sd, reverse_param_names_mapping = hf_to_custom_state_dict(\n        full_sd_iterator, param_names_mapping)  # type: ignore\n    for target_param_name, full_tensor in custom_param_sd.items():\n        meta_sharded_param = meta_sd.get(target_param_name)\n        if meta_sharded_param is None:\n            raise ValueError(\n                f\"Parameter {target_param_name} not found in custom model state dict. The hf to custom mapping may be incorrect.\"\n            )\n        if not hasattr(meta_sharded_param, \"device_mesh\"):\n            full_tensor = full_tensor.to(device=device, dtype=param_dtype)\n            # In cases where parts of the model aren't sharded, some parameters will be plain tensors\n            sharded_tensor = full_tensor\n        else:\n            full_tensor = full_tensor.to(device=device, dtype=param_dtype)\n            sharded_tensor = distribute_tensor(\n                full_tensor,\n                meta_sharded_param.device_mesh,\n                meta_sharded_param.placements,\n            )\n            if cpu_offload:\n                sharded_tensor = sharded_tensor.cpu()\n        sharded_sd[target_param_name] = nn.Parameter(sharded_tensor)\n\n    model.reverse_param_names_mapping = reverse_param_names_mapping\n    unused_keys = set(meta_sd.keys()) - set(sharded_sd.keys())\n    if unused_keys:\n        logger.warning(\"Found unloaded parameters in meta state dict: %s\",\n                       unused_keys)\n\n    # List of allowed parameter name patterns\n    ALLOWED_NEW_PARAM_PATTERNS = [\"gate_compress\"]  # Can be extended as needed\n    for new_param_name in unused_keys:\n        if not any(pattern in new_param_name\n                   for pattern in ALLOWED_NEW_PARAM_PATTERNS):\n            logger.error(\"Unsupported new parameter: %s. Allowed patterns: %s\",\n                         new_param_name, ALLOWED_NEW_PARAM_PATTERNS)\n            raise ValueError(\n                f\"New parameter '{new_param_name}' is not supported. \"\n                f\"Currently only parameters containing {ALLOWED_NEW_PARAM_PATTERNS} are allowed.\"\n            )\n        meta_sharded_param = meta_sd.get(new_param_name)\n        if not hasattr(meta_sharded_param, \"device_mesh\"):\n            # Initialize with zeros\n            sharded_tensor = torch.zeros_like(meta_sharded_param,\n                                              device=device,\n                                              dtype=param_dtype)\n        else:\n            # Initialize with zeros and distribute\n            full_tensor = torch.zeros_like(meta_sharded_param,\n                                           device=device,\n                                           dtype=param_dtype)\n            sharded_tensor = distribute_tensor(\n                full_tensor,\n                meta_sharded_param.device_mesh,\n                meta_sharded_param.placements,\n            )\n            if cpu_offload:\n                sharded_tensor = sharded_tensor.cpu()\n        sharded_sd[new_param_name] = nn.Parameter(sharded_tensor)\n\n    # choose `assign=True` since we cannot call `copy_` on meta tensor\n    return model.load_state_dict(sharded_sd, strict=strict, assign=True)\n</code></pre> fastvideo.models.loader.fsdp_load.maybe_load_fsdp_model \u00b6 <pre><code>maybe_load_fsdp_model(\n    model_cls: type[Module],\n    init_params: dict[str, Any],\n    weight_dir_list: list[str],\n    device: device,\n    hsdp_replicate_dim: int,\n    hsdp_shard_dim: int,\n    default_dtype: dtype,\n    param_dtype: dtype,\n    reduce_dtype: dtype,\n    cpu_offload: bool = False,\n    fsdp_inference: bool = False,\n    output_dtype: dtype | None = None,\n    training_mode: bool = True,\n    pin_cpu_memory: bool = True,\n    enable_torch_compile: bool = False,\n    torch_compile_kwargs: dict[str, Any] | None = None,\n) -&gt; torch.nn.Module\n</code></pre> <p>Load the model with FSDP if is training, else load the model without FSDP.</p> Source code in <code>fastvideo/models/loader/fsdp_load.py</code> <pre><code>def maybe_load_fsdp_model(\n    model_cls: type[nn.Module],\n    init_params: dict[str, Any],\n    weight_dir_list: list[str],\n    device: torch.device,\n    hsdp_replicate_dim: int,\n    hsdp_shard_dim: int,\n    default_dtype: torch.dtype,\n    param_dtype: torch.dtype,\n    reduce_dtype: torch.dtype,\n    cpu_offload: bool = False,\n    fsdp_inference: bool = False,\n    output_dtype: torch.dtype | None = None,\n    training_mode: bool = True,\n    pin_cpu_memory: bool = True,\n    enable_torch_compile: bool = False,\n    torch_compile_kwargs: dict[str, Any] | None = None,\n) -&gt; torch.nn.Module:\n    \"\"\"\n    Load the model with FSDP if is training, else load the model without FSDP.\n    \"\"\"\n    # NOTE(will): cast_forward_inputs=True shouldn't be needed as we are\n    # manually casting the inputs to the model\n    mp_policy = MixedPrecisionPolicy(param_dtype,\n                                     reduce_dtype,\n                                     output_dtype,\n                                     cast_forward_inputs=False)\n\n    set_mixed_precision_policy(\n        param_dtype=param_dtype,\n        reduce_dtype=reduce_dtype,\n        output_dtype=output_dtype,\n        mp_policy=mp_policy,\n    )\n\n    logger.info(\"Loading model with default_dtype: %s\", default_dtype)\n    with set_default_dtype(default_dtype), torch.device(\"meta\"):\n        model = model_cls(**init_params)\n\n    # Check if we should use FSDP\n    use_fsdp = training_mode or fsdp_inference\n\n    # Disable FSDP for MPS as it's not compatible\n    from fastvideo.platforms import current_platform\n    if current_platform.is_mps():\n        use_fsdp = False\n        logger.info(\"Disabling FSDP for MPS platform as it's not compatible\")\n\n    if use_fsdp:\n        world_size = hsdp_replicate_dim * hsdp_shard_dim\n        if not training_mode and not fsdp_inference:\n            hsdp_replicate_dim = world_size\n            hsdp_shard_dim = 1\n\n        if current_platform.is_npu():\n            with torch.device(\"cpu\"):\n                device_mesh = init_device_mesh(\n                    \"npu\",\n                    # (Replicate(), Shard(dim=0))\n                    mesh_shape=(hsdp_replicate_dim, hsdp_shard_dim),\n                    mesh_dim_names=(\"replicate\", \"shard\"),\n                )\n        else:\n            device_mesh = init_device_mesh(\n            \"cuda\",\n            # (Replicate(), Shard(dim=0))\n            mesh_shape=(hsdp_replicate_dim, hsdp_shard_dim),\n            mesh_dim_names=(\"replicate\", \"shard\"),\n        )\n        shard_model(model,\n                    cpu_offload=cpu_offload,\n                    reshard_after_forward=True,\n                    mp_policy=mp_policy,\n                    mesh=device_mesh,\n                    fsdp_shard_conditions=model._fsdp_shard_conditions,\n                    pin_cpu_memory=pin_cpu_memory)\n\n    weight_iterator = safetensors_weights_iterator(weight_dir_list)\n    param_names_mapping_fn = get_param_names_mapping(model.param_names_mapping)\n    load_model_from_full_model_state_dict(\n        model,\n        weight_iterator,\n        device,\n        default_dtype,\n        strict=True,\n        cpu_offload=cpu_offload,\n        param_names_mapping=param_names_mapping_fn,\n    )\n    for n, p in chain(model.named_parameters(), model.named_buffers()):\n        if p.is_meta:\n            raise RuntimeError(\n                f\"Unexpected param or buffer {n} on meta device.\")\n        # Avoid unintended computation graph accumulation during inference\n        if isinstance(p, torch.nn.Parameter):\n            p.requires_grad = False\n\n    compile_in_loader = enable_torch_compile and training_mode\n    if compile_in_loader:\n        compile_kwargs = torch_compile_kwargs or {}\n        logger.info(\"Enabling torch.compile for FSDP training module with kwargs=%s\",\n                    compile_kwargs)\n        model = torch.compile(model, **compile_kwargs)\n        logger.info(\"torch.compile enabled for %s\", type(model).__name__)\n    return model\n</code></pre> fastvideo.models.loader.fsdp_load.set_default_dtype \u00b6 <pre><code>set_default_dtype(\n    dtype: dtype,\n) -&gt; Generator[None, None, None]\n</code></pre> <p>Context manager to set torch's default dtype.</p> <p>Parameters:</p> Name Type Description Default <code>dtype</code> <code>dtype</code> <p>The desired default dtype inside the context manager.</p> required <p>Returns:</p> Name Type Description <code>ContextManager</code> <code>None</code> <p>context manager for setting default dtype.</p> Example <p>with set_default_dtype(torch.bfloat16):     x = torch.tensor([1, 2, 3])     x.dtype torch.bfloat16</p> Source code in <code>fastvideo/models/loader/fsdp_load.py</code> <pre><code>@contextlib.contextmanager\ndef set_default_dtype(dtype: torch.dtype) -&gt; Generator[None, None, None]:\n    \"\"\"\n    Context manager to set torch's default dtype.\n\n    Args:\n        dtype (torch.dtype): The desired default dtype inside the context manager.\n\n    Returns:\n        ContextManager: context manager for setting default dtype.\n\n    Example:\n        &gt;&gt;&gt; with set_default_dtype(torch.bfloat16):\n        &gt;&gt;&gt;     x = torch.tensor([1, 2, 3])\n        &gt;&gt;&gt;     x.dtype\n        torch.bfloat16\n\n\n    \"\"\"\n    old_dtype = torch.get_default_dtype()\n    torch.set_default_dtype(dtype)\n    try:\n        yield\n    finally:\n        torch.set_default_dtype(old_dtype)\n</code></pre> fastvideo.models.loader.fsdp_load.shard_model \u00b6 <pre><code>shard_model(\n    model,\n    *,\n    cpu_offload: bool,\n    reshard_after_forward: bool = True,\n    mp_policy: MixedPrecisionPolicy\n    | None = MixedPrecisionPolicy(),\n    mesh: DeviceMesh | None = None,\n    fsdp_shard_conditions: list[\n        Callable[[str, Module], bool]\n    ] = [],\n    pin_cpu_memory: bool = True\n) -&gt; None\n</code></pre> <p>Utility to shard a model with FSDP using the PyTorch Distributed fully_shard API.</p> <p>This method will over the model's named modules from the bottom-up and apply shard modules based on whether they meet any of the criteria from shard_conditions.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>TransformerDecoder</code> <p>Model to shard with FSDP.</p> required <code>shard_conditions</code> <code>List[Callable[[str, Module], bool]]</code> <p>A list of functions to determine which modules to shard with FSDP. Each function should take module name (relative to root) and the module itself, returning True if FSDP should shard the module and False otherwise. If any of shard_conditions return True for a given module, it will be sharded by FSDP.</p> required <code>cpu_offload</code> <code>bool</code> <p>If set to True, FSDP will offload parameters, gradients, and optimizer states to CPU.</p> required <code>reshard_after_forward</code> <code>bool</code> <p>Whether to reshard parameters and buffers after the forward pass. Setting this to True corresponds to the FULL_SHARD sharding strategy from FSDP1, while setting it to False corresponds to the SHARD_GRAD_OP sharding strategy.</p> <code>True</code> <code>mesh</code> <code>Optional[DeviceMesh]</code> <p>Device mesh to use for FSDP sharding under multiple parallelism. Default to None.</p> <code>None</code> <code>fsdp_shard_conditions</code> <code>List[Callable[[str, Module], bool]]</code> <p>A list of functions to determine which modules to shard with FSDP.</p> <code>[]</code> <code>pin_cpu_memory</code> <code>bool</code> <p>If set to True, FSDP will pin the CPU memory of the offloaded parameters.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no layer modules were sharded, indicating that no shard_condition was triggered.</p> Source code in <code>fastvideo/models/loader/fsdp_load.py</code> <pre><code>def shard_model(\n    model,\n    *,\n    cpu_offload: bool,\n    reshard_after_forward: bool = True,\n    mp_policy: MixedPrecisionPolicy | None = MixedPrecisionPolicy(),  # noqa\n    mesh: DeviceMesh | None = None,\n    fsdp_shard_conditions: list[Callable[[str, nn.Module], bool]] = [],  # noqa\n    pin_cpu_memory: bool = True,\n) -&gt; None:\n    \"\"\"\n    Utility to shard a model with FSDP using the PyTorch Distributed fully_shard API.\n\n    This method will over the model's named modules from the bottom-up and apply shard modules\n    based on whether they meet any of the criteria from shard_conditions.\n\n    Args:\n        model (TransformerDecoder): Model to shard with FSDP.\n        shard_conditions (List[Callable[[str, nn.Module], bool]]): A list of functions to determine\n            which modules to shard with FSDP. Each function should take module name (relative to root)\n            and the module itself, returning True if FSDP should shard the module and False otherwise.\n            If any of shard_conditions return True for a given module, it will be sharded by FSDP.\n        cpu_offload (bool): If set to True, FSDP will offload parameters, gradients, and optimizer\n            states to CPU.\n        reshard_after_forward (bool): Whether to reshard parameters and buffers after\n            the forward pass. Setting this to True corresponds to the FULL_SHARD sharding strategy\n            from FSDP1, while setting it to False corresponds to the SHARD_GRAD_OP sharding strategy.\n        mesh (Optional[DeviceMesh]): Device mesh to use for FSDP sharding under multiple parallelism.\n            Default to None.\n        fsdp_shard_conditions (List[Callable[[str, nn.Module], bool]]): A list of functions to determine\n            which modules to shard with FSDP.\n        pin_cpu_memory (bool): If set to True, FSDP will pin the CPU memory of the offloaded parameters.\n\n    Raises:\n        ValueError: If no layer modules were sharded, indicating that no shard_condition was triggered.\n    \"\"\"\n    if fsdp_shard_conditions is None or len(fsdp_shard_conditions) == 0:\n        logger.warning(\n            \"The FSDP shard condition list is empty or None. No modules will be sharded in %s\",\n            type(model).__name__)\n        return\n\n    fsdp_kwargs = {\n        \"reshard_after_forward\": reshard_after_forward,\n        \"mesh\": mesh,\n        \"mp_policy\": mp_policy,\n    }\n    if cpu_offload:\n        fsdp_kwargs[\"offload_policy\"] = CPUOffloadPolicy(\n            pin_memory=pin_cpu_memory)\n\n    # iterating in reverse to start with\n    # lowest-level modules first\n    num_layers_sharded = 0\n    # TODO(will): don't reshard after forward for the last layer to save on the\n    # all-gather that will immediately happen Shard the model with FSDP,\n    for n, m in reversed(list(model.named_modules())):\n        if any([\n                shard_condition(n, m)\n                for shard_condition in fsdp_shard_conditions\n        ]):\n            fully_shard(m, **fsdp_kwargs)\n            num_layers_sharded += 1\n\n    if num_layers_sharded == 0:\n        raise ValueError(\n            \"No layer modules were sharded. Please check if shard conditions are working as expected.\"\n        )\n\n    # Finally shard the entire model to account for any stragglers\n    fully_shard(model, **fsdp_kwargs)\n</code></pre>"},{"location":"api/fastvideo/models/#fastvideo.models.loader.utils","title":"fastvideo.models.loader.utils","text":"<p>Utilities for selecting and loading models.</p> Functions\u00b6 fastvideo.models.loader.utils.get_param_names_mapping \u00b6 <pre><code>get_param_names_mapping(\n    mapping_dict: dict[str, str]\n) -&gt; Callable[[str], tuple[str, Any, Any]]\n</code></pre> <p>Creates a mapping function that transforms parameter names using regex patterns.</p> <p>Parameters:</p> Name Type Description Default <code>mapping_dict</code> <code>Dict[str, str]</code> <p>Dictionary mapping regex patterns to replacement patterns</p> required <code>param_name</code> <code>str</code> <p>The parameter name to be transformed</p> required <p>Returns:</p> Type Description <code>Callable[[str], tuple[str, Any, Any]]</code> <p>Callable[[str], str]: A function that maps parameter names from source to target format</p> Source code in <code>fastvideo/models/loader/utils.py</code> <pre><code>def get_param_names_mapping(\n        mapping_dict: dict[str, str]) -&gt; Callable[[str], tuple[str, Any, Any]]:\n    \"\"\"\n    Creates a mapping function that transforms parameter names using regex patterns.\n\n    Args:\n        mapping_dict (Dict[str, str]): Dictionary mapping regex patterns to replacement patterns\n        param_name (str): The parameter name to be transformed\n\n    Returns:\n        Callable[[str], str]: A function that maps parameter names from source to target format\n    \"\"\"\n\n    def mapping_fn(name: str) -&gt; tuple[str, Any, Any]:\n        # Try to match and transform the name using the regex patterns in mapping_dict\n        for pattern, replacement in mapping_dict.items():\n            match = re.match(pattern, name)\n            if match:\n                merge_index = None\n                total_splitted_params = None\n                if isinstance(replacement, tuple):\n                    merge_index = replacement[1]\n                    total_splitted_params = replacement[2]\n                    replacement = replacement[0]\n                name = re.sub(pattern, replacement, name)\n                return name, merge_index, total_splitted_params\n\n        # If no pattern matches, return the original name\n        return name, None, None\n\n    return mapping_fn\n</code></pre> fastvideo.models.loader.utils.hf_to_custom_state_dict \u00b6 <pre><code>hf_to_custom_state_dict(\n    hf_param_sd: dict[str, Tensor]\n    | Iterator[tuple[str, Tensor]],\n    param_names_mapping: Callable[\n        [str], tuple[str, Any, Any]\n    ],\n) -&gt; tuple[\n    dict[str, torch.Tensor], dict[str, tuple[str, Any, Any]]\n]\n</code></pre> <p>Converts a Hugging Face parameter state dictionary to a custom parameter state dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>hf_param_sd</code> <code>Dict[str, Tensor]</code> <p>The Hugging Face parameter state dictionary</p> required <code>param_names_mapping</code> <code>Callable[[str], tuple[str, Any, Any]]</code> <p>A function that maps parameter names from source to target format</p> required <p>Returns:</p> Name Type Description <code>custom_param_sd</code> <code>Dict[str, Tensor]</code> <p>The custom formatted parameter state dict</p> <code>reverse_param_names_mapping</code> <code>Dict[str, Tuple[str, Any, Any]]</code> <p>Maps back from custom to hf</p> Source code in <code>fastvideo/models/loader/utils.py</code> <pre><code>def hf_to_custom_state_dict(\n    hf_param_sd: dict[str, torch.Tensor] | Iterator[tuple[str, torch.Tensor]],\n    param_names_mapping: Callable[[str], tuple[str, Any, Any]]\n) -&gt; tuple[dict[str, torch.Tensor], dict[str, tuple[str, Any, Any]]]:\n    \"\"\"\n    Converts a Hugging Face parameter state dictionary to a custom parameter state dictionary.\n\n    Args:\n        hf_param_sd (Dict[str, torch.Tensor]): The Hugging Face parameter state dictionary\n        param_names_mapping (Callable[[str], tuple[str, Any, Any]]): A function that maps parameter names from source to target format\n\n    Returns:\n        custom_param_sd (Dict[str, torch.Tensor]): The custom formatted parameter state dict\n        reverse_param_names_mapping (Dict[str, Tuple[str, Any, Any]]): Maps back from custom to hf\n    \"\"\"\n    custom_param_sd = {}\n    to_merge_params = defaultdict(dict)  # type: ignore\n    reverse_param_names_mapping = {}\n    if isinstance(hf_param_sd, dict):\n        hf_param_sd = hf_param_sd.items()  # type: ignore\n    for source_param_name, full_tensor in hf_param_sd:  # type: ignore\n        target_param_name, merge_index, num_params_to_merge = param_names_mapping(\n            source_param_name)\n        reverse_param_names_mapping[target_param_name] = (source_param_name,\n                                                          merge_index,\n                                                          num_params_to_merge)\n        if merge_index is not None:\n            to_merge_params[target_param_name][merge_index] = full_tensor\n            if len(to_merge_params[target_param_name]) == num_params_to_merge:\n                # cat at output dim according to the merge_index order\n                sorted_tensors = [\n                    to_merge_params[target_param_name][i]\n                    for i in range(num_params_to_merge)\n                ]\n                full_tensor = torch.cat(sorted_tensors, dim=0)\n                del to_merge_params[target_param_name]\n            else:\n                continue\n        custom_param_sd[target_param_name] = full_tensor\n    return custom_param_sd, reverse_param_names_mapping\n</code></pre> fastvideo.models.loader.utils.set_default_torch_dtype \u00b6 <pre><code>set_default_torch_dtype(dtype: dtype)\n</code></pre> <p>Sets the default torch dtype to the given dtype.</p> Source code in <code>fastvideo/models/loader/utils.py</code> <pre><code>@contextlib.contextmanager\ndef set_default_torch_dtype(dtype: torch.dtype):\n    \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n    old_dtype = torch.get_default_dtype()\n    torch.set_default_dtype(dtype)\n    yield\n    torch.set_default_dtype(old_dtype)\n</code></pre>"},{"location":"api/fastvideo/models/#fastvideo.models.loader.weight_utils","title":"fastvideo.models.loader.weight_utils","text":"<p>Utilities for downloading and initializing model weights.</p> Functions\u00b6 fastvideo.models.loader.weight_utils.default_weight_loader \u00b6 <pre><code>default_weight_loader(\n    param: Tensor, loaded_weight: Tensor\n) -&gt; None\n</code></pre> <p>Default weight loader.</p> Source code in <code>fastvideo/models/loader/weight_utils.py</code> <pre><code>def default_weight_loader(param: torch.Tensor,\n                          loaded_weight: torch.Tensor) -&gt; None:\n    \"\"\"Default weight loader.\"\"\"\n    try:\n        if param.numel() == 1 and loaded_weight.numel() == 1:\n            # Sometimes scalar values aren't considered tensors with shapes\n            # so if both param and loaded_weight are a scalar,\n            # \"broadcast\" instead of copy\n            param.data.fill_(loaded_weight.item())\n        else:\n            assert param.size() == loaded_weight.size(), (\n                f\"Attempted to load weight ({loaded_weight.size()}) \"\n                f\"into parameter ({param.size()})\")\n\n            param.data.copy_(loaded_weight)\n    except Exception:\n        # NOTE: This exception is added for the purpose of setting breakpoint to\n        # debug weight loading issues.\n        raise\n</code></pre> fastvideo.models.loader.weight_utils.enable_hf_transfer \u00b6 <pre><code>enable_hf_transfer() -&gt; None\n</code></pre> <p>automatically activates hf_transfer</p> Source code in <code>fastvideo/models/loader/weight_utils.py</code> <pre><code>def enable_hf_transfer() -&gt; None:\n    \"\"\"automatically activates hf_transfer\n    \"\"\"\n    if \"HF_HUB_ENABLE_HF_TRANSFER\" not in os.environ:\n        try:\n            # enable hf hub transfer if available\n            import hf_transfer  # type: ignore # noqa\n            huggingface_hub.constants.HF_HUB_ENABLE_HF_TRANSFER = True\n        except ImportError:\n            pass\n</code></pre> fastvideo.models.loader.weight_utils.filter_files_not_needed_for_inference \u00b6 <pre><code>filter_files_not_needed_for_inference(\n    hf_weights_files: list[str],\n) -&gt; list[str]\n</code></pre> <p>Exclude files that are not needed for inference.</p> <p>See https://github.com/huggingface/transformers/blob/v4.34.0/src/transformers/trainer.py#L227-L233</p> Source code in <code>fastvideo/models/loader/weight_utils.py</code> <pre><code>def filter_files_not_needed_for_inference(\n        hf_weights_files: list[str]) -&gt; list[str]:\n    \"\"\"\n    Exclude files that are not needed for inference.\n\n    See https://github.com/huggingface/transformers/blob/v4.34.0/src/transformers/trainer.py#L227-L233\n    \"\"\"\n    blacklist = [\n        \"training_args.bin\",\n        \"optimizer.bin\",\n        \"optimizer.pt\",\n        \"scheduler.pt\",\n        \"scaler.pt\",\n    ]\n    hf_weights_files = [\n        f for f in hf_weights_files\n        if not any(f.endswith(x) for x in blacklist)\n    ]\n    return hf_weights_files\n</code></pre> fastvideo.models.loader.weight_utils.maybe_remap_kv_scale_name \u00b6 <pre><code>maybe_remap_kv_scale_name(\n    name: str, params_dict: dict\n) -&gt; str | None\n</code></pre> <p>Remap the name of FP8 k/v_scale parameters.</p> <p>This function handles the remapping of FP8 k/v_scale parameter names. It detects if the given name ends with a suffix and attempts to remap it to the expected name format in the model. If the remapped name is not found in the params_dict, a warning is printed and None is returned.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The original loaded checkpoint parameter name.</p> required <code>params_dict</code> <code>dict</code> <p>Dictionary containing the model's named parameters.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str | None</code> <p>The remapped parameter name if successful, or the original name  if no remapping is needed.</p> <code>None</code> <code>str | None</code> <p>If the remapped name is not found in params_dict.</p> Source code in <code>fastvideo/models/loader/weight_utils.py</code> <pre><code>def maybe_remap_kv_scale_name(name: str, params_dict: dict) -&gt; str | None:\n    \"\"\"Remap the name of FP8 k/v_scale parameters.\n\n    This function handles the remapping of FP8 k/v_scale parameter names.\n    It detects if the given name ends with a suffix and attempts to remap\n    it to the expected name format in the model. If the remapped name is not\n    found in the params_dict, a warning is printed and None is returned.\n\n    Args:\n        name (str): The original loaded checkpoint parameter name.\n        params_dict (dict): Dictionary containing the model's named parameters.\n\n    Returns:\n        str: The remapped parameter name if successful, or the original name\n             if no remapping is needed.\n        None: If the remapped name is not found in params_dict.\n    \"\"\"\n    if name.endswith(\".kv_scale\"):\n        logger.warning_once(\n            \"DEPRECATED. Found kv_scale in the checkpoint. \"\n            \"This format is deprecated in favor of separate k_scale and \"\n            \"v_scale tensors and will be removed in a future release. \"\n            \"Functionally, we will remap kv_scale to k_scale and duplicate \"\n            \"k_scale to v_scale\")\n        # NOTE: we remap the deprecated kv_scale to k_scale\n        remapped_name = name.replace(\".kv_scale\", \".attn.k_scale\")\n        if remapped_name not in params_dict:\n            logger.warning_once(\n                f\"Found kv_scale in the checkpoint (e.g. {name}), \"\n                \"but not found the expected name in the model \"\n                f\"(e.g. {remapped_name}). kv_scale is \"\n                \"not loaded.\")\n            return None\n        return remapped_name\n\n    possible_scale_names = [\".k_scale\", \".v_scale\"]\n    modelopt_scale_names = [\n        \".self_attn.k_proj.k_scale\", \".self_attn.v_proj.v_scale\"\n    ]\n    for scale_name in possible_scale_names:\n        if name.endswith(scale_name):\n            if any(mo_scale_name in name\n                   for mo_scale_name in modelopt_scale_names):\n                remapped_name = name.replace(\n                    f\".self_attn.{scale_name[1]}_proj{scale_name}\",\n                    f\".self_attn.attn{scale_name}\")\n            else:\n                remapped_name = name.replace(scale_name, f\".attn{scale_name}\")\n            if remapped_name not in params_dict:\n                logger.warning_once(\n                    f\"Found {scale_name} in the checkpoint (e.g. {name}), \"\n                    \"but not found the expected name in the model \"\n                    f\"(e.g. {remapped_name}). {scale_name} is \"\n                    \"not loaded.\")\n                return None\n            return remapped_name\n\n    # If there were no matches, return the untouched param name\n    return name\n</code></pre> fastvideo.models.loader.weight_utils.pt_weights_iterator \u00b6 <pre><code>pt_weights_iterator(\n    hf_weights_files: list[str], to_cpu: bool = True\n) -&gt; Generator[tuple[str, torch.Tensor], None, None]\n</code></pre> <p>Iterate over the weights in the model bin/pt files.</p> Source code in <code>fastvideo/models/loader/weight_utils.py</code> <pre><code>def pt_weights_iterator(\n    hf_weights_files: list[str],\n    to_cpu: bool = True,\n) -&gt; Generator[tuple[str, torch.Tensor], None, None]:\n    \"\"\"Iterate over the weights in the model bin/pt files.\"\"\"\n    device = \"cpu\" if to_cpu else str(get_local_torch_device())\n    enable_tqdm = not torch.distributed.is_initialized(\n    ) or torch.distributed.get_rank() == 0\n    for bin_file in tqdm(\n            hf_weights_files,\n            desc=\"Loading pt checkpoint shards\",\n            disable=not enable_tqdm,\n            bar_format=_BAR_FORMAT,\n    ):\n        state = torch.load(bin_file, map_location=device, weights_only=True)\n        yield from state.items()\n        del state\n</code></pre> fastvideo.models.loader.weight_utils.safetensors_weights_iterator \u00b6 <pre><code>safetensors_weights_iterator(\n    hf_weights_files: list[str], to_cpu: bool = True\n) -&gt; Generator[tuple[str, torch.Tensor], None, None]\n</code></pre> <p>Iterate over the weights in the model safetensor files.</p> Source code in <code>fastvideo/models/loader/weight_utils.py</code> <pre><code>def safetensors_weights_iterator(\n    hf_weights_files: list[str],\n    to_cpu: bool = True,\n) -&gt; Generator[tuple[str, torch.Tensor], None, None]:\n    \"\"\"Iterate over the weights in the model safetensor files.\"\"\"\n    enable_tqdm = not torch.distributed.is_initialized(\n    ) or torch.distributed.get_rank() == 0\n    device = \"cpu\" if to_cpu else str(get_local_torch_device())\n    for st_file in tqdm(\n            hf_weights_files,\n            desc=\"Loading safetensors checkpoint shards\",\n            disable=not enable_tqdm,\n            bar_format=_BAR_FORMAT,\n    ):\n        with safe_open(st_file, framework=\"pt\", device=device) as f:\n            for name in f.keys():  # noqa: SIM118\n                param = f.get_tensor(name)\n                yield name, param\n</code></pre>"},{"location":"api/fastvideo/models/#fastvideo.models.parameter","title":"fastvideo.models.parameter","text":""},{"location":"api/fastvideo/models/#fastvideo.models.parameter-classes","title":"Classes","text":""},{"location":"api/fastvideo/models/#fastvideo.models.parameter.BasevLLMParameter","title":"fastvideo.models.parameter.BasevLLMParameter","text":"<pre><code>BasevLLMParameter(data: Tensor, weight_loader: Callable)\n</code></pre> <p>               Bases: <code>Parameter</code></p> <p>Base parameter for vLLM linear layers. Extends the torch.nn.parameter by taking in a linear weight loader. Will copy the loaded weight into the parameter when the provided weight loader is called.</p> <p>Initialize the BasevLLMParameter</p> <p>:param data: torch tensor with the parameter data :param weight_loader: weight loader callable</p> <p>:returns: a torch.nn.parameter</p> Source code in <code>fastvideo/models/parameter.py</code> <pre><code>def __init__(self, data: torch.Tensor, weight_loader: Callable):\n    \"\"\"\n    Initialize the BasevLLMParameter\n\n    :param data: torch tensor with the parameter data\n    :param weight_loader: weight loader callable\n\n    :returns: a torch.nn.parameter\n    \"\"\"\n\n    # During weight loading, we often do something like:\n    # narrowed_tensor = param.data.narrow(0, offset, len)\n    # narrowed_tensor.copy_(real_weight)\n    # expecting narrowed_tensor and param.data to share the same storage.\n    # However, on TPUs, narrowed_tensor will lazily propagate to the base\n    # tensor, which is param.data, leading to the redundant memory usage.\n    # This sometimes causes OOM errors during model loading. To avoid this,\n    # we sync the param tensor after its weight loader is called.\n    from fastvideo.platforms import current_platform\n    if current_platform.is_tpu():\n        weight_loader = _make_synced_weight_loader(weight_loader)\n\n    self._weight_loader = weight_loader\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/models/#fastvideo.models.parameter.BlockQuantScaleParameter","title":"fastvideo.models.parameter.BlockQuantScaleParameter","text":"<pre><code>BlockQuantScaleParameter(output_dim: int, **kwargs)\n</code></pre> <p>               Bases: <code>_ColumnvLLMParameter</code>, <code>RowvLLMParameter</code></p> <p>Parameter class for weight scales loaded for weights with block-wise quantization. Uses both column and row parallelism.</p> Source code in <code>fastvideo/models/parameter.py</code> <pre><code>def __init__(self, output_dim: int, **kwargs):\n    self._output_dim = output_dim\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/fastvideo/models/#fastvideo.models.parameter.ChannelQuantScaleParameter","title":"fastvideo.models.parameter.ChannelQuantScaleParameter","text":"<pre><code>ChannelQuantScaleParameter(output_dim: int, **kwargs)\n</code></pre> <p>               Bases: <code>_ColumnvLLMParameter</code></p> <p>Parameter class for weight scales loaded for weights with channel-wise quantization. Equivalent to _ColumnvLLMParameter.</p> Source code in <code>fastvideo/models/parameter.py</code> <pre><code>def __init__(self, output_dim: int, **kwargs):\n    self._output_dim = output_dim\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/fastvideo/models/#fastvideo.models.parameter.GroupQuantScaleParameter","title":"fastvideo.models.parameter.GroupQuantScaleParameter","text":"<pre><code>GroupQuantScaleParameter(output_dim: int, **kwargs)\n</code></pre> <p>               Bases: <code>_ColumnvLLMParameter</code>, <code>RowvLLMParameter</code></p> <p>Parameter class for weight scales loaded for weights with grouped quantization. Uses both column and row parallelism.</p> Source code in <code>fastvideo/models/parameter.py</code> <pre><code>def __init__(self, output_dim: int, **kwargs):\n    self._output_dim = output_dim\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/fastvideo/models/#fastvideo.models.parameter.ModelWeightParameter","title":"fastvideo.models.parameter.ModelWeightParameter","text":"<pre><code>ModelWeightParameter(output_dim: int, **kwargs)\n</code></pre> <p>               Bases: <code>_ColumnvLLMParameter</code>, <code>RowvLLMParameter</code></p> <p>Parameter class for linear layer weights. Uses both column and row parallelism.</p> Source code in <code>fastvideo/models/parameter.py</code> <pre><code>def __init__(self, output_dim: int, **kwargs):\n    self._output_dim = output_dim\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/fastvideo/models/#fastvideo.models.parameter.PackedColumnParameter","title":"fastvideo.models.parameter.PackedColumnParameter","text":"<pre><code>PackedColumnParameter(\n    packed_factor: int | Fraction, packed_dim: int, **kwargs\n)\n</code></pre> <p>               Bases: <code>_ColumnvLLMParameter</code></p> <p>Parameter for model parameters which are packed on disk and support column parallelism only. See PackedvLLMParameter for more details on the packed properties.</p> Source code in <code>fastvideo/models/parameter.py</code> <pre><code>def __init__(self, packed_factor: int | Fraction, packed_dim: int,\n             **kwargs):\n    self._packed_factor = packed_factor\n    self._packed_dim = packed_dim\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/fastvideo/models/#fastvideo.models.parameter.PackedvLLMParameter","title":"fastvideo.models.parameter.PackedvLLMParameter","text":"<pre><code>PackedvLLMParameter(\n    packed_factor: int | Fraction, packed_dim: int, **kwargs\n)\n</code></pre> <p>               Bases: <code>ModelWeightParameter</code></p> <p>Parameter for model weights which are packed on disk. Example: GPTQ Marlin weights are int4 or int8, packed into int32. Extends the ModelWeightParameter to take in the packed factor, the packed dimension, and optionally, marlin tile size for marlin kernels. Adjusts the shard_size and  shard_offset for fused linear layers model weight loading by accounting for packing and optionally, marlin tile size.</p> Source code in <code>fastvideo/models/parameter.py</code> <pre><code>def __init__(self, packed_factor: int | Fraction, packed_dim: int,\n             **kwargs):\n    self._packed_factor = packed_factor\n    self._packed_dim = packed_dim\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/fastvideo/models/#fastvideo.models.parameter.PerTensorScaleParameter","title":"fastvideo.models.parameter.PerTensorScaleParameter","text":"<pre><code>PerTensorScaleParameter(**kwargs)\n</code></pre> <p>               Bases: <code>BasevLLMParameter</code></p> <p>Parameter class for scales where the number of scales is equivalent to the number of logical matrices in fused linear layers (e.g. for QKV, there are 3 scales loaded from disk). This is relevant to weights with per-tensor quantization. Adds functionality to map the scalers to a shard during weight loading. </p> <p>Note: additional parameter manipulation may be handled  for each quantization config specifically, within  process_weights_after_loading</p> Source code in <code>fastvideo/models/parameter.py</code> <pre><code>def __init__(self, **kwargs):\n    self.qkv_idxs = {\"q\": 0, \"k\": 1, \"v\": 2}\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/fastvideo/models/#fastvideo.models.parameter.RowvLLMParameter","title":"fastvideo.models.parameter.RowvLLMParameter","text":"<pre><code>RowvLLMParameter(input_dim: int, **kwargs)\n</code></pre> <p>               Bases: <code>BasevLLMParameter</code></p> <p>Parameter class defining weight_loading functionality (load_row_parallel_weight) for parameters being loaded into linear layers with row parallel functionality. Requires an input_dim to be defined.</p> Source code in <code>fastvideo/models/parameter.py</code> <pre><code>def __init__(self, input_dim: int, **kwargs):\n    self._input_dim = input_dim\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/fastvideo/models/#fastvideo.models.parameter-functions","title":"Functions","text":""},{"location":"api/fastvideo/models/#fastvideo.models.parameter.permute_param_layout_","title":"fastvideo.models.parameter.permute_param_layout_","text":"<pre><code>permute_param_layout_(\n    param: BasevLLMParameter,\n    input_dim: int,\n    output_dim: int,\n    **kwargs\n) -&gt; BasevLLMParameter\n</code></pre> <p>Permute a parameter's layout to the specified input and output dimensions,  useful for forcing the parameter into a known layout, for example, if I need a packed (quantized) weight matrix to be in the layout      {input_dim = 0, output_dim = 1, packed_dim = 0} then I can call:     permute_param_layout_(x, input_dim=0, output_dim=1, packed_dim=0) to ensure x is in the correct layout (permuting it to the correct layout if  required, asserting if it cannot get it to the correct layout)</p> Source code in <code>fastvideo/models/parameter.py</code> <pre><code>def permute_param_layout_(param: BasevLLMParameter, input_dim: int,\n                          output_dim: int, **kwargs) -&gt; BasevLLMParameter:\n    \"\"\"\n    Permute a parameter's layout to the specified input and output dimensions, \n    useful for forcing the parameter into a known layout, for example, if I need\n    a packed (quantized) weight matrix to be in the layout \n        {input_dim = 0, output_dim = 1, packed_dim = 0}\n    then I can call:\n        permute_param_layout_(x, input_dim=0, output_dim=1, packed_dim=0)\n    to ensure x is in the correct layout (permuting it to the correct layout if \n    required, asserting if it cannot get it to the correct layout)\n    \"\"\"\n\n    curr_input_dim = getattr(param, \"input_dim\", None)\n    curr_output_dim = getattr(param, \"output_dim\", None)\n\n    if curr_input_dim is None or curr_output_dim is None:\n        assert param.data.dim() == 2,\\\n            \"permute_param_layout_ only supports 2D parameters when either \"\\\n            \"input_dim or output_dim is not set\"\n\n    # if one of the dimensions is not set, set it to the opposite of the other\n    #  we can only do this since we asserted the parameter is 2D above\n    if curr_input_dim is None:\n        assert curr_output_dim is not None,\\\n            \"either input or output dim must be set\"\n        curr_input_dim = (curr_output_dim + 1) % 2\n    if curr_output_dim is None:\n        assert curr_input_dim is not None,\\\n            \"either input or output dim must be set\"\n        curr_output_dim = (curr_input_dim + 1) % 2\n\n    # create permutation from the current layout to the layout with\n    # self.input_dim at input_dim and self.output_dim at output_dim preserving\n    # other dimensions\n    perm = [\n        i for i in range(param.data.dim())\n        if i not in [curr_input_dim, curr_output_dim]\n    ]\n    perm.insert(input_dim, curr_input_dim)\n    perm.insert(output_dim, curr_output_dim)\n\n    if \"packed_dim\" in kwargs:\n        assert hasattr(param, \"packed_dim\") and\\\n            param.packed_dim == perm[kwargs[\"packed_dim\"]],\\\n            \"permute_param_layout_ currently doesn't support repacking\"\n\n    param.data = param.data.permute(*perm)\n    if hasattr(param, \"_input_dim\"):\n        param._input_dim = input_dim\n    if hasattr(param, \"_output_dim\"):\n        param._output_dim = output_dim\n    if \"packed_dim\" in kwargs and hasattr(param, \"_packed_dim\"):\n        param._packed_dim = kwargs[\"packed_dim\"]\n\n    return param\n</code></pre>"},{"location":"api/fastvideo/models/#fastvideo.models.registry","title":"fastvideo.models.registry","text":""},{"location":"api/fastvideo/models/#fastvideo.models.registry-functions","title":"Functions","text":""},{"location":"api/fastvideo/models/#fastvideo.models.utils","title":"fastvideo.models.utils","text":"<p>Utils for model executor.</p>"},{"location":"api/fastvideo/models/#fastvideo.models.utils-functions","title":"Functions","text":""},{"location":"api/fastvideo/models/#fastvideo.models.utils.auto_attributes","title":"fastvideo.models.utils.auto_attributes","text":"<pre><code>auto_attributes(init_func)\n</code></pre> <p>Decorator that automatically adds all initialization arguments as object attributes.</p> Example <p>@auto_attributes def init(self, a=1, b=2):     pass</p> Source code in <code>fastvideo/models/utils.py</code> <pre><code>def auto_attributes(init_func):\n    \"\"\"\n    Decorator that automatically adds all initialization arguments as object attributes.\n\n    Example:\n        @auto_attributes\n        def __init__(self, a=1, b=2):\n            pass\n\n        # This will automatically set:\n        # - self.a = 1 and self.b = 2\n        # - self.config.a = 1 and self.config.b = 2\n    \"\"\"\n\n    def wrapper(self, *args, **kwargs):\n        # Get the function signature\n        import inspect\n        signature = inspect.signature(init_func)\n        parameters = signature.parameters\n\n        # Get parameter names (excluding 'self')\n        param_names = list(parameters.keys())[1:]\n\n        # Bind arguments to parameters\n        bound_args = signature.bind(self, *args, **kwargs)\n        bound_args.apply_defaults()\n\n        # Create config object if it doesn't exist\n        if not hasattr(self, 'config'):\n            self.config = type('Config', (), {})()\n\n        # Set attributes on self and self.config\n        for name in param_names:\n            if name in bound_args.arguments:\n                value = bound_args.arguments[name]\n                setattr(self, name, value)\n                setattr(self.config, name, value)\n\n        # Call the original __init__ function\n        return init_func(self, *args, **kwargs)\n\n    return wrapper\n</code></pre>"},{"location":"api/fastvideo/models/#fastvideo.models.utils.auto_attributes--this-will-automatically-set","title":"This will automatically set:","text":""},{"location":"api/fastvideo/models/#fastvideo.models.utils.auto_attributes---selfa-1-and-selfb-2","title":"- self.a = 1 and self.b = 2","text":""},{"location":"api/fastvideo/models/#fastvideo.models.utils.auto_attributes---selfconfiga-1-and-selfconfigb-2","title":"- self.config.a = 1 and self.config.b = 2","text":""},{"location":"api/fastvideo/models/#fastvideo.models.utils.extract_layer_index","title":"fastvideo.models.utils.extract_layer_index","text":"<pre><code>extract_layer_index(layer_name: str) -&gt; int\n</code></pre> <p>Extract the layer index from the module name. Examples: - \"encoder.layers.0\" -&gt; 0 - \"encoder.layers.1.self_attn\" -&gt; 1 - \"2.self_attn\" -&gt; 2 - \"model.encoder.layers.0.sub.1\" -&gt; ValueError</p> Source code in <code>fastvideo/models/utils.py</code> <pre><code>def extract_layer_index(layer_name: str) -&gt; int:\n    \"\"\"\n    Extract the layer index from the module name.\n    Examples:\n    - \"encoder.layers.0\" -&gt; 0\n    - \"encoder.layers.1.self_attn\" -&gt; 1\n    - \"2.self_attn\" -&gt; 2\n    - \"model.encoder.layers.0.sub.1\" -&gt; ValueError\n    \"\"\"\n    subnames = layer_name.split(\".\")\n    int_vals: list[int] = []\n    for subname in subnames:\n        try:\n            int_vals.append(int(subname))\n        except ValueError:\n            continue\n    assert len(int_vals) == 1, (f\"layer name {layer_name} should\"\n                                \" only contain one integer\")\n    return int_vals[0]\n</code></pre>"},{"location":"api/fastvideo/models/#fastvideo.models.utils.modulate","title":"fastvideo.models.utils.modulate","text":"<pre><code>modulate(\n    x: Tensor,\n    shift: Tensor | None = None,\n    scale: Tensor | None = None,\n) -&gt; torch.Tensor\n</code></pre> <p>modulate by shift and scale</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor.</p> required <code>shift</code> <code>Tensor</code> <p>shift tensor. Defaults to None.</p> <code>None</code> <code>scale</code> <code>Tensor</code> <p>scale tensor. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: the output tensor after modulate.</p> Source code in <code>fastvideo/models/utils.py</code> <pre><code>def modulate(x: torch.Tensor,\n             shift: torch.Tensor | None = None,\n             scale: torch.Tensor | None = None) -&gt; torch.Tensor:\n    \"\"\"modulate by shift and scale\n\n    Args:\n        x (torch.Tensor): input tensor.\n        shift (torch.Tensor, optional): shift tensor. Defaults to None.\n        scale (torch.Tensor, optional): scale tensor. Defaults to None.\n\n    Returns:\n        torch.Tensor: the output tensor after modulate.\n    \"\"\"\n    if scale is None and shift is None:\n        return x\n    elif shift is None:\n        return x * (1 + scale.unsqueeze(1))  # type: ignore[union-attr]\n    elif scale is None:\n        return x + shift.unsqueeze(1)  # type: ignore[union-attr]\n    else:\n        return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(\n            1)  # type: ignore[union-attr]\n</code></pre>"},{"location":"api/fastvideo/models/#fastvideo.models.utils.pred_noise_to_pred_video","title":"fastvideo.models.utils.pred_noise_to_pred_video","text":"<pre><code>pred_noise_to_pred_video(\n    pred_noise: Tensor,\n    noise_input_latent: Tensor,\n    timestep: Tensor,\n    scheduler: Any,\n) -&gt; torch.Tensor\n</code></pre> <p>Convert predicted noise to clean latent.</p> <p>pred_noise: the predicted noise with shape [B, C, H, W]     where B is batch_size or batch_size * num_frames noise_input_latent: the noisy latent with shape [B, C, H, W], timestep: the timestep with shape [1] or [bs * num_frames] or [bs, num_frames] scheduler: the scheduler</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>the predicted video with shape [B, C, H, W]</p> Source code in <code>fastvideo/models/utils.py</code> <pre><code>def pred_noise_to_pred_video(pred_noise: torch.Tensor,\n                             noise_input_latent: torch.Tensor,\n                             timestep: torch.Tensor,\n                             scheduler: Any) -&gt; torch.Tensor:\n    \"\"\"\n    Convert predicted noise to clean latent.\n\n    Args:\n    pred_noise: the predicted noise with shape [B, C, H, W]\n        where B is batch_size or batch_size * num_frames\n    noise_input_latent: the noisy latent with shape [B, C, H, W],\n    timestep: the timestep with shape [1] or [bs * num_frames] or [bs, num_frames]\n    scheduler: the scheduler\n\n    Returns:\n        the predicted video with shape [B, C, H, W]\n    \"\"\"\n    # If timestep is [bs, num_frames]\n    if timestep.ndim == 2:\n        timestep = timestep.flatten(0, 1)\n        assert timestep.numel() == noise_input_latent.shape[0]\n    elif timestep.ndim == 1:\n        # If timestep is [1]\n        if timestep.shape[0] == 1:\n            timestep = timestep.expand(noise_input_latent.shape[0])\n        else:\n            assert timestep.numel() == noise_input_latent.shape[0]\n    else:\n        raise ValueError(f\"[pred_noise_to_pred_video] Invalid timestep shape: {timestep.shape}\")\n    # timestep shape should be [B]\n    dtype = pred_noise.dtype\n    device = pred_noise.device\n    pred_noise = pred_noise.double().to(device)\n    noise_input_latent = noise_input_latent.double().to(device)\n    sigmas = scheduler.sigmas.double().to(device)\n    timesteps = scheduler.timesteps.double().to(device)\n    timestep_id = torch.argmin(\n        (timesteps.unsqueeze(0) - timestep.unsqueeze(1)).abs(), dim=1)\n    sigma_t = sigmas[timestep_id].reshape(-1, 1, 1, 1)\n    pred_video = noise_input_latent - sigma_t * pred_noise\n    return pred_video.to(dtype)\n</code></pre>"},{"location":"api/fastvideo/models/#fastvideo.models.utils.set_weight_attrs","title":"fastvideo.models.utils.set_weight_attrs","text":"<pre><code>set_weight_attrs(\n    weight: Tensor, weight_attrs: dict[str, Any] | None\n)\n</code></pre> <p>Set attributes on a weight tensor.</p> <p>This method is used to set attributes on a weight tensor. This method will not overwrite existing attributes.</p> <p>Parameters:</p> Name Type Description Default <code>weight</code> <code>Tensor</code> <p>The weight tensor.</p> required <code>weight_attrs</code> <code>dict[str, Any] | None</code> <p>A dictionary of attributes to set on the weight tensor.</p> required Source code in <code>fastvideo/models/utils.py</code> <pre><code>def set_weight_attrs(\n    weight: torch.Tensor,\n    weight_attrs: dict[str, Any] | None,\n):\n    \"\"\"Set attributes on a weight tensor.\n\n    This method is used to set attributes on a weight tensor. This method\n    will not overwrite existing attributes.\n\n    Args:\n        weight: The weight tensor.\n        weight_attrs: A dictionary of attributes to set on the weight tensor.\n    \"\"\"\n    if weight_attrs is None:\n        return\n    for key, value in weight_attrs.items():\n        assert not hasattr(\n            weight, key), (f\"Overwriting existing tensor attribute: {key}\")\n\n        # NOTE(woosuk): During weight loading, we often do something like:\n        # narrowed_tensor = param.data.narrow(0, offset, len)\n        # narrowed_tensor.copy_(real_weight)\n        # expecting narrowed_tensor and param.data to share the same storage.\n        # However, on TPUs, narrowed_tensor will lazily propagate to the base\n        # tensor, which is param.data, leading to the redundant memory usage.\n        # This sometimes causes OOM errors during model loading. To avoid this,\n        # we sync the param tensor after its weight loader is called.\n        # TODO(woosuk): Remove this hack once we have a better solution.\n        from fastvideo.platforms import current_platform\n        if current_platform.is_tpu() and key == \"weight_loader\":\n            value = _make_synced_weight_loader(value)\n        setattr(weight, key, value)\n</code></pre>"},{"location":"api/fastvideo/models/#fastvideo.models.vision_utils","title":"fastvideo.models.vision_utils","text":""},{"location":"api/fastvideo/models/#fastvideo.models.vision_utils-functions","title":"Functions","text":""},{"location":"api/fastvideo/models/#fastvideo.models.vision_utils.create_default_image","title":"fastvideo.models.vision_utils.create_default_image","text":"<pre><code>create_default_image(\n    width: int = 512,\n    height: int = 512,\n    color: tuple[int, int, int] = (0, 0, 0),\n) -&gt; PIL.Image.Image\n</code></pre> <p>Create a default black PIL image.</p> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>int</code> <p>Image width in pixels</p> <code>512</code> <code>height</code> <code>int</code> <p>Image height in pixels</p> <code>512</code> <code>color</code> <code>tuple[int, int, int]</code> <p>RGB color tuple</p> <code>(0, 0, 0)</code> <p>Returns:</p> Type Description <code>Image</code> <p>PIL.Image.Image: A new PIL image with specified dimensions and color</p> Source code in <code>fastvideo/models/vision_utils.py</code> <pre><code>def create_default_image(width: int = 512, height: int = 512, color: tuple[int, int, int] = (0, 0, 0)) -&gt; PIL.Image.Image:\n    \"\"\"\n    Create a default black PIL image.\n\n    Args:\n        width: Image width in pixels\n        height: Image height in pixels\n        color: RGB color tuple\n\n    Returns:\n        PIL.Image.Image: A new PIL image with specified dimensions and color\n    \"\"\"\n    return PIL.Image.new(\"RGB\", (width, height), color=color)\n</code></pre>"},{"location":"api/fastvideo/models/#fastvideo.models.vision_utils.get_default_height_width","title":"fastvideo.models.vision_utils.get_default_height_width","text":"<pre><code>get_default_height_width(\n    image: Image | ndarray | Tensor,\n    vae_scale_factor: int,\n    height: int | None = None,\n    width: int | None = None,\n) -&gt; tuple[int, int]\n</code></pre> <p>Returns the height and width of the image, downscaled to the next integer multiple of <code>vae_scale_factor</code>.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>`Union[PIL.Image.Image, np.ndarray, torch.Tensor]`</code> <p>The image input, which can be a PIL image, NumPy array, or PyTorch tensor. If it is a NumPy array, it should have shape <code>[batch, height, width]</code> or <code>[batch, height, width, channels]</code>. If it is a PyTorch tensor, it should have shape <code>[batch, channels, height, width]</code>.</p> required <code>height</code> <code>`Optional[int]`, *optional*, defaults to `None`</code> <p>The height of the preprocessed image. If <code>None</code>, the height of the <code>image</code> input will be used.</p> <code>None</code> <code>width</code> <code>`Optional[int]`, *optional*, defaults to `None`</code> <p>The width of the preprocessed image. If <code>None</code>, the width of the <code>image</code> input will be used.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p><code>Tuple[int, int]</code>: A tuple containing the height and width, both resized to the nearest integer multiple of <code>vae_scale_factor</code>.</p> Source code in <code>fastvideo/models/vision_utils.py</code> <pre><code>def get_default_height_width(\n    image: PIL.Image.Image | np.ndarray | torch.Tensor,\n    vae_scale_factor: int,\n    height: int | None = None,\n    width: int | None = None,\n) -&gt; tuple[int, int]:\n    r\"\"\"\n    Returns the height and width of the image, downscaled to the next integer multiple of `vae_scale_factor`.\n\n    Args:\n        image (`Union[PIL.Image.Image, np.ndarray, torch.Tensor]`):\n            The image input, which can be a PIL image, NumPy array, or PyTorch tensor. If it is a NumPy array, it\n            should have shape `[batch, height, width]` or `[batch, height, width, channels]`. If it is a PyTorch\n            tensor, it should have shape `[batch, channels, height, width]`.\n        height (`Optional[int]`, *optional*, defaults to `None`):\n            The height of the preprocessed image. If `None`, the height of the `image` input will be used.\n        width (`Optional[int]`, *optional*, defaults to `None`):\n            The width of the preprocessed image. If `None`, the width of the `image` input will be used.\n\n    Returns:\n        `Tuple[int, int]`:\n            A tuple containing the height and width, both resized to the nearest integer multiple of\n            `vae_scale_factor`.\n    \"\"\"\n\n    if height is None:\n        if isinstance(image, PIL.Image.Image):\n            height = image.height\n        elif isinstance(image, torch.Tensor):\n            height = image.shape[2]\n        else:\n            height = image.shape[1]\n\n    if width is None:\n        if isinstance(image, PIL.Image.Image):\n            width = image.width\n        elif isinstance(image, torch.Tensor):\n            width = image.shape[3]\n        else:\n            width = image.shape[2]\n\n    width, height = (x - x % vae_scale_factor for x in (width, height)\n                     )  # resize to integer multiple of vae_scale_factor\n\n    return height, width\n</code></pre>"},{"location":"api/fastvideo/models/#fastvideo.models.vision_utils.load_image","title":"fastvideo.models.vision_utils.load_image","text":"<pre><code>load_image(\n    image: str | Image,\n    convert_method: Callable[[Image], Image] | None = None,\n) -&gt; PIL.Image.Image\n</code></pre> <p>Loads <code>image</code> to a PIL Image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>`str` or `PIL.Image.Image`</code> <p>The image to convert to the PIL Image format.</p> required <code>convert_method</code> <code>Callable[[PIL.Image.Image], PIL.Image.Image], *optional*</code> <p>A conversion method to apply to the image after loading it. When set to <code>None</code> the image will be converted \"RGB\".</p> <code>None</code> <p>Returns:</p> Type Description <code>Image</code> <p><code>PIL.Image.Image</code>: A PIL Image.</p> Source code in <code>fastvideo/models/vision_utils.py</code> <pre><code>def load_image(\n    image: str | PIL.Image.Image,\n    convert_method: Callable[[PIL.Image.Image], PIL.Image.Image] | None = None\n) -&gt; PIL.Image.Image:\n    \"\"\"\n    Loads `image` to a PIL Image.\n\n    Args:\n        image (`str` or `PIL.Image.Image`):\n            The image to convert to the PIL Image format.\n        convert_method (Callable[[PIL.Image.Image], PIL.Image.Image], *optional*):\n            A conversion method to apply to the image after loading it. When set to `None` the image will be converted\n            \"RGB\".\n\n    Returns:\n        `PIL.Image.Image`:\n            A PIL Image.\n    \"\"\"\n    if isinstance(image, str):\n        if image.startswith(\"http://\") or image.startswith(\"https://\"):\n            image = PIL.Image.open(requests.get(image, stream=True).raw)\n        elif os.path.isfile(image):\n            image = PIL.Image.open(image)\n        else:\n            raise ValueError(\n                f\"Incorrect path or URL. URLs must start with `http://` or `https://`, and {image} is not a valid path.\"\n            )\n    elif isinstance(image, PIL.Image.Image):\n        image = image\n    else:\n        raise ValueError(\n            \"Incorrect format used for the image. Should be a URL linking to an image, a local path, or a PIL image.\"\n        )\n\n    image = PIL.ImageOps.exif_transpose(image)\n\n    if convert_method is not None:\n        image = convert_method(image)\n    else:\n        image = image.convert(\"RGB\")\n\n    return image\n</code></pre>"},{"location":"api/fastvideo/models/#fastvideo.models.vision_utils.load_video","title":"fastvideo.models.vision_utils.load_video","text":"<pre><code>load_video(\n    video: str,\n    convert_method: Callable[[list[Image]], list[Image]]\n    | None = None,\n    return_fps: bool = False,\n) -&gt; tuple[list[PIL.Image.Image], float | Any] | list[\n    PIL.Image.Image\n]\n</code></pre> <p>Loads <code>video</code> to a list of PIL Image. Args:     video (<code>str</code>):         A URL or Path to a video to convert to a list of PIL Image format.     convert_method (Callable[[List[PIL.Image.Image]], List[PIL.Image.Image]], optional):         A conversion method to apply to the video after loading it. When set to <code>None</code> the images will be converted         to \"RGB\".     return_fps (<code>bool</code>, optional, defaults to <code>False</code>):         Whether to return the FPS of the video. If <code>True</code>, returns a tuple of (images, fps).         If <code>False</code>, returns only the list of images. Returns:     <code>List[PIL.Image.Image]</code> or <code>Tuple[List[PIL.Image.Image], float | None]</code>:         The video as a list of PIL images. If <code>return_fps</code> is True, also returns the original FPS.</p> Source code in <code>fastvideo/models/vision_utils.py</code> <pre><code>def load_video(\n    video: str,\n    convert_method: Callable[[list[PIL.Image.Image]], list[PIL.Image.Image]]\n    | None = None,\n    return_fps: bool = False,\n) -&gt; tuple[list[PIL.Image.Image], float | Any] | list[PIL.Image.Image]:\n    \"\"\"\n    Loads `video` to a list of PIL Image.\n    Args:\n        video (`str`):\n            A URL or Path to a video to convert to a list of PIL Image format.\n        convert_method (Callable[[List[PIL.Image.Image]], List[PIL.Image.Image]], *optional*):\n            A conversion method to apply to the video after loading it. When set to `None` the images will be converted\n            to \"RGB\".\n        return_fps (`bool`, *optional*, defaults to `False`):\n            Whether to return the FPS of the video. If `True`, returns a tuple of (images, fps).\n            If `False`, returns only the list of images.\n    Returns:\n        `List[PIL.Image.Image]` or `Tuple[List[PIL.Image.Image], float | None]`:\n            The video as a list of PIL images. If `return_fps` is True, also returns the original FPS.\n    \"\"\"\n    is_url = video.startswith(\"http://\") or video.startswith(\"https://\")\n    is_file = os.path.isfile(video)\n    was_tempfile_created = False\n\n    if not (is_url or is_file):\n        raise ValueError(\n            f\"Incorrect path or URL. URLs must start with `http://` or `https://`, and {video} is not a valid path.\"\n        )\n\n    if is_url:\n        response = requests.get(video, stream=True)\n        if response.status_code != 200:\n            raise ValueError(\n                f\"Failed to download video. Status code: {response.status_code}\"\n            )\n\n        parsed_url = urlparse(video)\n        file_name = os.path.basename(unquote(parsed_url.path))\n\n        suffix = os.path.splitext(file_name)[1] or \".mp4\"\n        with tempfile.NamedTemporaryFile(suffix=suffix,\n                                         delete=False) as temp_file:\n            video_path = temp_file.name\n            video_data = response.iter_content(chunk_size=8192)\n            for chunk in video_data:\n                temp_file.write(chunk)\n        was_tempfile_created = True\n    else:\n        video_path = video\n\n    pil_images = []\n    original_fps = None\n\n    try:\n        if video_path.endswith(\".gif\"):\n            pil_images, original_fps = _load_gif(video_path)\n        else:\n            pil_images, original_fps = _load_video_with_ffmpeg(video_path)\n    finally:\n        # Clean up temporary file if it was created\n        if was_tempfile_created and os.path.exists(video_path):\n            os.remove(video_path)\n\n    if convert_method is not None:\n        pil_images = convert_method(pil_images)\n\n    return pil_images, original_fps if return_fps else pil_images\n</code></pre>"},{"location":"api/fastvideo/models/#fastvideo.models.vision_utils.normalize","title":"fastvideo.models.vision_utils.normalize","text":"<pre><code>normalize(\n    images: ndarray | Tensor,\n) -&gt; np.ndarray | torch.Tensor\n</code></pre> <p>Normalize an image array to [-1,1].</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>`np.ndarray` or `torch.Tensor`</code> <p>The image array to normalize.</p> required <p>Returns:</p> Type Description <code>ndarray | Tensor</code> <p><code>np.ndarray</code> or <code>torch.Tensor</code>: The normalized image array.</p> Source code in <code>fastvideo/models/vision_utils.py</code> <pre><code>def normalize(images: np.ndarray | torch.Tensor) -&gt; np.ndarray | torch.Tensor:\n    r\"\"\"\n    Normalize an image array to [-1,1].\n\n    Args:\n        images (`np.ndarray` or `torch.Tensor`):\n            The image array to normalize.\n\n    Returns:\n        `np.ndarray` or `torch.Tensor`:\n            The normalized image array.\n    \"\"\"\n    return 2.0 * images - 1.0\n</code></pre>"},{"location":"api/fastvideo/models/#fastvideo.models.vision_utils.numpy_to_pt","title":"fastvideo.models.vision_utils.numpy_to_pt","text":"<pre><code>numpy_to_pt(images: ndarray) -&gt; torch.Tensor\n</code></pre> <p>Convert a NumPy image to a PyTorch tensor.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>`np.ndarray`</code> <p>The NumPy image array to convert to PyTorch format.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p><code>torch.Tensor</code>: A PyTorch tensor representation of the images.</p> Source code in <code>fastvideo/models/vision_utils.py</code> <pre><code>def numpy_to_pt(images: np.ndarray) -&gt; torch.Tensor:\n    r\"\"\"\n    Convert a NumPy image to a PyTorch tensor.\n\n    Args:\n        images (`np.ndarray`):\n            The NumPy image array to convert to PyTorch format.\n\n    Returns:\n        `torch.Tensor`:\n            A PyTorch tensor representation of the images.\n    \"\"\"\n    if images.ndim == 3:\n        images = images[..., None]\n\n    images = torch.from_numpy(images.transpose(0, 3, 1, 2))\n    return images\n</code></pre>"},{"location":"api/fastvideo/models/#fastvideo.models.vision_utils.pil_to_numpy","title":"fastvideo.models.vision_utils.pil_to_numpy","text":"<pre><code>pil_to_numpy(images: list[Image] | Image) -&gt; np.ndarray\n</code></pre> <p>Convert a PIL image or a list of PIL images to NumPy arrays.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>`PIL.Image.Image` or `List[PIL.Image.Image]`</code> <p>The PIL image or list of images to convert to NumPy format.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p><code>np.ndarray</code>: A NumPy array representation of the images.</p> Source code in <code>fastvideo/models/vision_utils.py</code> <pre><code>def pil_to_numpy(images: list[PIL.Image.Image] | PIL.Image.Image) -&gt; np.ndarray:\n    r\"\"\"\n    Convert a PIL image or a list of PIL images to NumPy arrays.\n\n    Args:\n        images (`PIL.Image.Image` or `List[PIL.Image.Image]`):\n            The PIL image or list of images to convert to NumPy format.\n\n    Returns:\n        `np.ndarray`:\n            A NumPy array representation of the images.\n    \"\"\"\n    if not isinstance(images, list):\n        images = [images]\n    images = [np.array(image).astype(np.float32) / 255.0 for image in images]\n    images_arr: np.ndarray = np.stack(images, axis=0)\n\n    return images_arr\n</code></pre>"},{"location":"api/fastvideo/models/#fastvideo.models.vision_utils.preprocess_reference_image_for_clip","title":"fastvideo.models.vision_utils.preprocess_reference_image_for_clip","text":"<pre><code>preprocess_reference_image_for_clip(\n    image: Image, device: device\n) -&gt; PIL.Image.Image\n</code></pre> <p>Preprocess reference image to match CLIP encoder requirements.</p> <p>Applies normalization, resizing to 224x224, and denormalization to ensure the image is in the correct format for CLIP processing.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Input PIL image</p> required <code>device</code> <code>device</code> <p>Target device for tensor operations</p> required <p>Returns:</p> Type Description <code>Image</code> <p>Preprocessed PIL image ready for CLIP encoder</p> Source code in <code>fastvideo/models/vision_utils.py</code> <pre><code>def preprocess_reference_image_for_clip(image: PIL.Image.Image, device: torch.device) -&gt; PIL.Image.Image:\n    \"\"\"\n    Preprocess reference image to match CLIP encoder requirements.\n\n    Applies normalization, resizing to 224x224, and denormalization to ensure\n    the image is in the correct format for CLIP processing.\n\n    Args:\n        image: Input PIL image\n        device: Target device for tensor operations\n\n    Returns:\n        Preprocessed PIL image ready for CLIP encoder\n    \"\"\"\n    # Convert PIL to tensor and normalize to [-1, 1] range\n    image_tensor = TF.to_tensor(image).sub_(0.5).div_(0.5).to(device)\n\n    # Resize to CLIP's expected input size (224x224) using bicubic interpolation\n    resized_tensor = F.interpolate(\n        image_tensor.unsqueeze(0),\n        size=(224, 224),\n        mode='bicubic',\n        align_corners=False\n    ).squeeze(0)\n\n    # Denormalize back to [0, 1] range\n    denormalized_tensor = resized_tensor.mul_(0.5).add_(0.5)\n\n    return TF.to_pil_image(denormalized_tensor)\n</code></pre>"},{"location":"api/fastvideo/models/#fastvideo.models.vision_utils.resize","title":"fastvideo.models.vision_utils.resize","text":"<pre><code>resize(\n    image: Image | ndarray | Tensor,\n    height: int,\n    width: int,\n    resize_mode: str = \"default\",\n    resample: str = \"lanczos\",\n) -&gt; PIL.Image.Image | np.ndarray | torch.Tensor\n</code></pre> <p>Resize image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>`PIL.Image.Image`, `np.ndarray` or `torch.Tensor`</code> <p>The image input, can be a PIL image, numpy array or pytorch tensor.</p> required <code>height</code> <code>`int`</code> <p>The height to resize to.</p> required <code>width</code> <code>`int`</code> <p>The width to resize to.</p> required <code>resize_mode</code> <code>`str`, *optional*, defaults to `default`</code> <p>The resize mode to use, can be one of <code>default</code> or <code>fill</code>. If <code>default</code>, will resize the image to fit within the specified width and height, and it may not maintaining the original aspect ratio. If <code>fill</code>, will resize the image to fit within the specified width and height, maintaining the aspect ratio, and then center the image within the dimensions, filling empty with data from image. If <code>crop</code>, will resize the image to fit within the specified width and height, maintaining the aspect ratio, and then center the image within the dimensions, cropping the excess. Note that resize_mode <code>fill</code> and <code>crop</code> are only supported for PIL image input.</p> <code>'default'</code> <p>Returns:</p> Type Description <code>Image | ndarray | Tensor</code> <p><code>PIL.Image.Image</code>, <code>np.ndarray</code> or <code>torch.Tensor</code>: The resized image.</p> Source code in <code>fastvideo/models/vision_utils.py</code> <pre><code>def resize(\n    image: PIL.Image.Image | np.ndarray | torch.Tensor,\n    height: int,\n    width: int,\n    resize_mode: str = \"default\",  # \"default\", \"fill\", \"crop\"\n    resample: str = \"lanczos\",\n) -&gt; PIL.Image.Image | np.ndarray | torch.Tensor:\n    \"\"\"\n    Resize image.\n\n    Args:\n        image (`PIL.Image.Image`, `np.ndarray` or `torch.Tensor`):\n            The image input, can be a PIL image, numpy array or pytorch tensor.\n        height (`int`):\n            The height to resize to.\n        width (`int`):\n            The width to resize to.\n        resize_mode (`str`, *optional*, defaults to `default`):\n            The resize mode to use, can be one of `default` or `fill`. If `default`, will resize the image to fit\n            within the specified width and height, and it may not maintaining the original aspect ratio. If `fill`,\n            will resize the image to fit within the specified width and height, maintaining the aspect ratio, and\n            then center the image within the dimensions, filling empty with data from image. If `crop`, will resize\n            the image to fit within the specified width and height, maintaining the aspect ratio, and then center\n            the image within the dimensions, cropping the excess. Note that resize_mode `fill` and `crop` are only\n            supported for PIL image input.\n\n    Returns:\n        `PIL.Image.Image`, `np.ndarray` or `torch.Tensor`:\n            The resized image.\n    \"\"\"\n    if resize_mode != \"default\" and not isinstance(image, PIL.Image.Image):\n        raise ValueError(\n            f\"Only PIL image input is supported for resize_mode {resize_mode}\")\n    assert isinstance(image, PIL.Image.Image)\n    if resize_mode == \"default\":\n        image = image.resize((width, height),\n                             resample=PIL_INTERPOLATION[resample])\n    else:\n        raise ValueError(f\"resize_mode {resize_mode} is not supported\")\n    return image\n</code></pre>"},{"location":"api/fastvideo/pipelines/","title":"pipelines","text":""},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines","title":"pipelines","text":"<p>Diffusion pipelines for fastvideo.</p> <p>This package contains diffusion pipelines for generating videos and images.</p>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines-classes","title":"Classes","text":""},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.ComposedPipelineBase","title":"fastvideo.pipelines.ComposedPipelineBase","text":"<pre><code>ComposedPipelineBase(\n    model_path: str,\n    fastvideo_args: FastVideoArgs | TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for pipelines composed of multiple stages.</p> <p>This class provides the framework for creating pipelines by composing multiple stages together. Each stage is responsible for a specific part of the diffusion process, and the pipeline orchestrates the execution of these stages.</p> <p>Initialize the pipeline. After init, the pipeline should be ready to use. The pipeline should be stateless and not hold any batch state.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def __init__(self,\n             model_path: str,\n             fastvideo_args: FastVideoArgs | TrainingArgs,\n             required_config_modules: list[str] | None = None,\n             loaded_modules: dict[str, torch.nn.Module] | None = None):\n    \"\"\"\n    Initialize the pipeline. After __init__, the pipeline should be ready to\n    use. The pipeline should be stateless and not hold any batch state.\n    \"\"\"\n    self.fastvideo_args = fastvideo_args\n\n    self.model_path: str = model_path\n    self._stages: list[PipelineStage] = []\n    self._stage_name_mapping: dict[str, PipelineStage] = {}\n\n    if required_config_modules is not None:\n        self._required_config_modules = required_config_modules\n\n    if self._required_config_modules is None:\n        raise NotImplementedError(\n            \"Subclass must set _required_config_modules\")\n\n    maybe_init_distributed_environment_and_model_parallel(\n        fastvideo_args.tp_size, fastvideo_args.sp_size)\n\n    # Torch profiler. Enabled and configured through env vars:\n    # FASTVIDEO_TORCH_PROFILER_DIR=/path/to/save/trace\n    trace_dir = envs.FASTVIDEO_TORCH_PROFILER_DIR\n    self.profiler_controller = get_or_create_profiler(trace_dir)\n    self.profiler = self.profiler_controller.profiler\n\n    self.local_rank = get_world_group().local_rank\n\n    # Load modules directly in initialization\n    logger.info(\"Loading pipeline modules...\")\n    with self.profiler_controller.region(\"profiler_region_model_loading\"):\n        self.modules = self.load_modules(fastvideo_args, loaded_modules)\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.ComposedPipelineBase-attributes","title":"Attributes","text":""},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.ComposedPipelineBase.required_config_modules","title":"fastvideo.pipelines.ComposedPipelineBase.required_config_modules  <code>property</code>","text":"<pre><code>required_config_modules: list[str]\n</code></pre> <p>List of modules that are required by the pipeline. The names should match the diffusers directory and model_index.json file. These modules will be loaded using the PipelineComponentLoader and made available in the modules dictionary. Access these modules using the get_module method.</p> <p>class ConcretePipeline(ComposedPipelineBase):     _required_config_modules = [\"vae\", \"text_encoder\", \"transformer\", \"scheduler\", \"tokenizer\"]</p> <pre><code>@property\ndef required_config_modules(self):\n    return self._required_config_modules\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.ComposedPipelineBase.stages","title":"fastvideo.pipelines.ComposedPipelineBase.stages  <code>property</code>","text":"<pre><code>stages: list[PipelineStage]\n</code></pre> <p>List of stages in the pipeline.</p>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.ComposedPipelineBase-functions","title":"Functions","text":""},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.ComposedPipelineBase.create_pipeline_stages","title":"fastvideo.pipelines.ComposedPipelineBase.create_pipeline_stages  <code>abstractmethod</code>","text":"<pre><code>create_pipeline_stages(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Create the inference pipeline stages.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>@abstractmethod\ndef create_pipeline_stages(self, fastvideo_args: FastVideoArgs):\n    \"\"\"\n    Create the inference pipeline stages.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.ComposedPipelineBase.create_training_stages","title":"fastvideo.pipelines.ComposedPipelineBase.create_training_stages","text":"<pre><code>create_training_stages(training_args: TrainingArgs)\n</code></pre> <p>Create the training pipeline stages.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def create_training_stages(self, training_args: TrainingArgs):\n    \"\"\"\n    Create the training pipeline stages.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.ComposedPipelineBase.forward","title":"fastvideo.pipelines.ComposedPipelineBase.forward","text":"<pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Generate a video or image using the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The batch to generate from.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:     ForwardBatch: The batch with the generated video or image.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Generate a video or image using the pipeline.\n\n    Args:\n        batch: The batch to generate from.\n        fastvideo_args: The inference arguments.\n    Returns:\n        ForwardBatch: The batch with the generated video or image.\n    \"\"\"\n    if not self.post_init_called:\n        self.post_init()\n\n    # Execute each stage\n    logger.info(\"Running pipeline stages: %s\",\n                self._stage_name_mapping.keys())\n    # logger.info(\"Batch: %s\", batch)\n    for stage in self.stages:\n        batch = stage(batch, fastvideo_args)\n\n    # Return the output\n    return batch\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.ComposedPipelineBase.from_pretrained","title":"fastvideo.pipelines.ComposedPipelineBase.from_pretrained  <code>classmethod</code>","text":"<pre><code>from_pretrained(\n    model_path: str,\n    device: str | None = None,\n    torch_dtype: dtype | None = None,\n    pipeline_config: str | PipelineConfig | None = None,\n    args: Namespace | None = None,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n    **kwargs\n) -&gt; ComposedPipelineBase\n</code></pre> <p>Load a pipeline from a pretrained model. loaded_modules: Optional[Dict[str, torch.nn.Module]] = None, If provided, loaded_modules will be used instead of loading from config/pretrained weights.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>@classmethod\ndef from_pretrained(cls,\n                    model_path: str,\n                    device: str | None = None,\n                    torch_dtype: torch.dtype | None = None,\n                    pipeline_config: str | PipelineConfig | None = None,\n                    args: argparse.Namespace | None = None,\n                    required_config_modules: list[str] | None = None,\n                    loaded_modules: dict[str, torch.nn.Module]\n                    | None = None,\n                    **kwargs) -&gt; \"ComposedPipelineBase\":\n    \"\"\"\n    Load a pipeline from a pretrained model.\n    loaded_modules: Optional[Dict[str, torch.nn.Module]] = None,\n    If provided, loaded_modules will be used instead of loading from config/pretrained weights.\n    \"\"\"\n    if args is None or args.inference_mode:\n\n        kwargs['model_path'] = model_path\n        fastvideo_args = FastVideoArgs.from_kwargs(**kwargs)\n    else:\n        assert args is not None, \"args must be provided for training mode\"\n        fastvideo_args = TrainingArgs.from_cli_args(args)\n        # TODO(will): fix this so that its not so ugly\n        fastvideo_args.model_path = model_path\n        for key, value in kwargs.items():\n            setattr(fastvideo_args, key, value)\n\n        fastvideo_args.dit_cpu_offload = False\n        # we hijack the precision to be the master weight type so that the\n        # model is loaded with the correct precision. Subsequently we will\n        # use FSDP2's MixedPrecisionPolicy to set the precision for the\n        # fwd, bwd, and other operations' precision.\n        assert fastvideo_args.pipeline_config.dit_precision == 'fp32', 'only fp32 is supported for training'\n\n    logger.info(\"fastvideo_args in from_pretrained: %s\", fastvideo_args)\n\n    pipe = cls(model_path,\n               fastvideo_args,\n               required_config_modules=required_config_modules,\n               loaded_modules=loaded_modules)\n    pipe.post_init()\n    return pipe\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.ComposedPipelineBase.initialize_pipeline","title":"fastvideo.pipelines.ComposedPipelineBase.initialize_pipeline","text":"<pre><code>initialize_pipeline(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Initialize the pipeline.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def initialize_pipeline(self, fastvideo_args: FastVideoArgs):\n    \"\"\"\n    Initialize the pipeline.\n    \"\"\"\n    return\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.ComposedPipelineBase.load_modules","title":"fastvideo.pipelines.ComposedPipelineBase.load_modules","text":"<pre><code>load_modules(\n    fastvideo_args: FastVideoArgs,\n    loaded_modules: dict[str, Module] | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Load the modules from the config. loaded_modules: Optional[Dict[str, torch.nn.Module]] = None,  If provided, loaded_modules will be used instead of loading from config/pretrained weights.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def load_modules(\n    self,\n    fastvideo_args: FastVideoArgs,\n    loaded_modules: dict[str, torch.nn.Module] | None = None\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Load the modules from the config.\n    loaded_modules: Optional[Dict[str, torch.nn.Module]] = None, \n    If provided, loaded_modules will be used instead of loading from config/pretrained weights.\n    \"\"\"\n\n    model_index = self._load_config(self.model_path)\n    logger.info(\"Loading pipeline modules from config: %s\", model_index)\n\n    # remove keys that are not pipeline modules\n    model_index.pop(\"_class_name\")\n    model_index.pop(\"_diffusers_version\")\n    if \"boundary_ratio\" in model_index and model_index[\n            \"boundary_ratio\"] is not None:\n        logger.info(\n            \"MoE pipeline detected. Adding transformer_2 to self.required_config_modules...\"\n        )\n        self.required_config_modules.append(\"transformer_2\")\n        logger.info(\"MoE pipeline detected. Setting boundary ratio to %s\",\n                    model_index[\"boundary_ratio\"])\n        fastvideo_args.pipeline_config.dit_config.boundary_ratio = model_index[\n            \"boundary_ratio\"]\n\n    model_index.pop(\"boundary_ratio\", None)\n    # used by Wan2.2 ti2v\n    model_index.pop(\"expand_timesteps\", None)\n\n    # some sanity checks\n    assert len(\n        model_index\n    ) &gt; 1, \"model_index.json must contain at least one pipeline module\"\n\n    for module_name in self.required_config_modules:\n        if module_name not in model_index and module_name in self._extra_config_module_map:\n            extra_module_value = self._extra_config_module_map[module_name]\n            logger.warning(\n                \"model_index.json does not contain a %s module, but found {%s: %s} in _extra_config_module_map, adding to model_index.\",\n                module_name, module_name, extra_module_value)\n            if extra_module_value in model_index:\n                logger.info(\"Using module %s for %s\", extra_module_value,\n                            module_name)\n                model_index[module_name] = model_index[extra_module_value]\n                continue\n            else:\n                raise ValueError(\n                    f\"Required module key: {module_name} value: {model_index.get(module_name)} was not found in loaded modules {model_index.keys()}\"\n                )\n\n    # all the component models used by the pipeline\n    required_modules = self.required_config_modules\n    logger.info(\"Loading required modules: %s\", required_modules)\n\n    modules = {}\n    for module_name, (transformers_or_diffusers,\n                      architecture) in model_index.items():\n        if transformers_or_diffusers is None:\n            logger.warning(\n                \"Module %s in model_index.json has null value, removing from required_config_modules\",\n                module_name)\n            if module_name in self.required_config_modules:\n                self.required_config_modules.remove(module_name)\n            continue\n        if module_name not in required_modules:\n            logger.info(\"Skipping module %s\", module_name)\n            continue\n        if loaded_modules is not None and module_name in loaded_modules:\n            logger.info(\"Using module %s already provided\", module_name)\n            modules[module_name] = loaded_modules[module_name]\n            continue\n\n        # we load the module from the extra config module map if it exists\n        if module_name in self._extra_config_module_map:\n            load_module_name = self._extra_config_module_map[module_name]\n        else:\n            load_module_name = module_name\n\n        component_model_path = os.path.join(self.model_path,\n                                            load_module_name)\n        module = PipelineComponentLoader.load_module(\n            module_name=load_module_name,\n            component_model_path=component_model_path,\n            transformers_or_diffusers=transformers_or_diffusers,\n            fastvideo_args=fastvideo_args,\n        )\n        logger.info(\"Loaded module %s from %s\", module_name,\n                    component_model_path)\n\n        if module_name in modules:\n            logger.warning(\"Overwriting module %s\", module_name)\n        modules[module_name] = module\n\n    # Check if all required modules were loaded\n    for module_name in required_modules:\n        if module_name not in modules or modules[module_name] is None:\n            raise ValueError(\n                f\"Required module key: {module_name} value: {modules.get(module_name)} was not found in loaded modules {modules.keys()}\"\n            )\n\n    return modules\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.ForwardBatch","title":"fastvideo.pipelines.ForwardBatch  <code>dataclass</code>","text":"<pre><code>ForwardBatch(\n    data_type: str,\n    generator: Generator | list[Generator] | None = None,\n    image_path: str | None = None,\n    image_embeds: list[Tensor] = list(),\n    pil_image: Tensor | Image | None = None,\n    preprocessed_image: Tensor | None = None,\n    prompt: str | list[str] | None = None,\n    negative_prompt: str | list[str] | None = None,\n    prompt_path: str | None = None,\n    output_path: str = \"outputs/\",\n    output_video_name: str | None = None,\n    video_path: str | None = None,\n    video_latent: Tensor | None = None,\n    prompt_embeds: list[Tensor] = list(),\n    negative_prompt_embeds: list[Tensor] | None = None,\n    prompt_attention_mask: list[Tensor] | None = None,\n    negative_attention_mask: list[Tensor] | None = None,\n    clip_embedding_pos: list[Tensor] | None = None,\n    clip_embedding_neg: list[Tensor] | None = None,\n    max_sequence_length: int | None = None,\n    prompt_template: dict[str, Any] | None = None,\n    do_classifier_free_guidance: bool = False,\n    batch_size: int | None = None,\n    num_videos_per_prompt: int = 1,\n    seed: int | None = None,\n    seeds: list[int] | None = None,\n    is_prompt_processed: bool = False,\n    latents: Tensor | None = None,\n    raw_latent_shape: Tensor | None = None,\n    noise_pred: Tensor | None = None,\n    image_latent: Tensor | None = None,\n    height_latents: list[int] | int | None = None,\n    width_latents: list[int] | int | None = None,\n    num_frames: list[int] | int = 1,\n    num_frames_round_down: bool = False,\n    height: list[int] | int | None = None,\n    width: list[int] | int | None = None,\n    fps: list[int] | int | None = None,\n    timesteps: Tensor | None = None,\n    timestep: Tensor | float | int | None = None,\n    step_index: int | None = None,\n    boundary_ratio: float | None = None,\n    num_inference_steps: int = 50,\n    guidance_scale: float = 1.0,\n    guidance_scale_2: float | None = None,\n    guidance_rescale: float = 0.0,\n    eta: float = 0.0,\n    sigmas: list[float] | None = None,\n    n_tokens: int | None = None,\n    extra_step_kwargs: dict[str, Any] = dict(),\n    modules: dict[str, Any] = dict(),\n    output: Tensor | None = None,\n    return_trajectory_latents: bool = False,\n    return_trajectory_decoded: bool = False,\n    trajectory_timesteps: list[Tensor] | None = None,\n    trajectory_latents: Tensor | None = None,\n    trajectory_decoded: list[Tensor] | None = None,\n    extra: dict[str, Any] = dict(),\n    save_video: bool = True,\n    return_frames: bool = False,\n    enable_teacache: bool = False,\n    teacache_params: TeaCacheParams\n    | WanTeaCacheParams\n    | None = None,\n    STA_param: list | None = None,\n    is_cfg_negative: bool = False,\n    mask_search_final_result_pos: list[list] | None = None,\n    mask_search_final_result_neg: list[list] | None = None,\n    VSA_sparsity: float = 0.0,\n    logging_info: PipelineLoggingInfo = PipelineLoggingInfo(),\n)\n</code></pre> <p>Complete state passed through the pipeline execution.</p> <p>This dataclass contains all information needed during the diffusion pipeline execution, allowing methods to update specific components without needing to manage numerous individual parameters.</p>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.ForwardBatch-functions","title":"Functions","text":""},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.ForwardBatch.__post_init__","title":"fastvideo.pipelines.ForwardBatch.__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Initialize dependent fields after dataclass initialization.</p> Source code in <code>fastvideo/pipelines/pipeline_batch_info.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Initialize dependent fields after dataclass initialization.\"\"\"\n\n    # Set do_classifier_free_guidance based on guidance scale and negative prompt\n    if self.guidance_scale &gt; 1.0:\n        self.do_classifier_free_guidance = True\n    if self.negative_prompt_embeds is None:\n        self.negative_prompt_embeds = []\n    if self.guidance_scale_2 is None:\n        self.guidance_scale_2 = self.guidance_scale\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.LoRAPipeline","title":"fastvideo.pipelines.LoRAPipeline","text":"<pre><code>LoRAPipeline(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ComposedPipelineBase</code></p> <p>Pipeline that supports injecting LoRA adapters into the diffusion transformer. TODO: support training.</p> Source code in <code>fastvideo/pipelines/lora_pipeline.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.device = get_local_torch_device()\n    self.exclude_lora_layers = self.modules[\n        \"transformer\"].config.arch_config.exclude_lora_layers\n    self.lora_target_modules = self.fastvideo_args.lora_target_modules\n    self.lora_path = self.fastvideo_args.lora_path\n    self.lora_nickname = self.fastvideo_args.lora_nickname\n    self.training_mode = self.fastvideo_args.training_mode\n    if self.training_mode and getattr(self.fastvideo_args, \"lora_training\",\n                                      False):\n        assert isinstance(self.fastvideo_args, TrainingArgs)\n        if self.fastvideo_args.lora_alpha is None:\n            self.fastvideo_args.lora_alpha = self.fastvideo_args.lora_rank\n        self.lora_rank = self.fastvideo_args.lora_rank  # type: ignore\n        self.lora_alpha = self.fastvideo_args.lora_alpha  # type: ignore\n        logger.info(\"Using LoRA training with rank %d and alpha %d\",\n                    self.lora_rank, self.lora_alpha)\n        if self.lora_target_modules is None:\n            self.lora_target_modules = [\n                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"to_q\", \"to_k\",\n                \"to_v\", \"to_out\", \"to_qkv\"\n            ]\n        self.convert_to_lora_layers()\n    # Inference\n    elif not self.training_mode and self.lora_path is not None:\n        self.convert_to_lora_layers()\n        self.set_lora_adapter(\n            self.lora_nickname,  # type: ignore\n            self.lora_path)  # type: ignore\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.LoRAPipeline-functions","title":"Functions","text":""},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.LoRAPipeline.convert_to_lora_layers","title":"fastvideo.pipelines.LoRAPipeline.convert_to_lora_layers","text":"<pre><code>convert_to_lora_layers() -&gt; None\n</code></pre> <p>Unified method to convert the transformer to a LoRA transformer.</p> Source code in <code>fastvideo/pipelines/lora_pipeline.py</code> <pre><code>def convert_to_lora_layers(self) -&gt; None:\n    \"\"\"\n    Unified method to convert the transformer to a LoRA transformer.\n    \"\"\"\n    if self.lora_initialized:\n        return\n    self.lora_initialized = True\n    converted_count = 0\n    for name, layer in self.modules[\"transformer\"].named_modules():\n        if not self.is_target_layer(name):\n            continue\n\n        excluded = False\n        for exclude_layer in self.exclude_lora_layers:\n            if exclude_layer in name:\n                excluded = True\n                break\n        if excluded:\n            continue\n\n        layer = get_lora_layer(layer,\n                               lora_rank=self.lora_rank,\n                               lora_alpha=self.lora_alpha,\n                               training_mode=self.training_mode)\n        if layer is not None:\n            self.lora_layers[name] = layer\n            replace_submodule(self.modules[\"transformer\"], name, layer)\n            converted_count += 1\n    logger.info(\"Converted %d layers to LoRA layers\", converted_count)\n\n    if \"fake_score_transformer\" in self.modules:\n        for name, layer in self.modules[\n                \"fake_score_transformer\"].named_modules():\n            if not self.is_target_layer(name):\n                continue\n            layer = get_lora_layer(layer,\n                                   lora_rank=self.lora_rank,\n                                   lora_alpha=self.lora_alpha,\n                                   training_mode=self.training_mode)\n            if layer is not None:\n                self.lora_layers_critic[name] = layer\n                replace_submodule(self.modules[\"fake_score_transformer\"],\n                                  name, layer)\n                converted_count += 1\n        logger.info(\n            \"Converted %d layers to LoRA layers in the critic model\",\n            converted_count)\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.LoRAPipeline.set_lora_adapter","title":"fastvideo.pipelines.LoRAPipeline.set_lora_adapter","text":"<pre><code>set_lora_adapter(\n    lora_nickname: str, lora_path: str | None = None\n)\n</code></pre> <p>Load a LoRA adapter into the pipeline and merge it into the transformer. Args:     lora_nickname: The \"nick name\" of the adapter when referenced in the pipeline.     lora_path: The path to the adapter, either a local path or a Hugging Face repo id.</p> Source code in <code>fastvideo/pipelines/lora_pipeline.py</code> <pre><code>def set_lora_adapter(self,\n                     lora_nickname: str,\n                     lora_path: str | None = None):  # type: ignore\n    \"\"\"\n    Load a LoRA adapter into the pipeline and merge it into the transformer.\n    Args:\n        lora_nickname: The \"nick name\" of the adapter when referenced in the pipeline.\n        lora_path: The path to the adapter, either a local path or a Hugging Face repo id.\n    \"\"\"\n\n    if lora_nickname not in self.lora_adapters and lora_path is None:\n        raise ValueError(\n            f\"Adapter {lora_nickname} not found in the pipeline. Please provide lora_path to load it.\"\n        )\n    if not self.lora_initialized:\n        self.convert_to_lora_layers()\n    adapter_updated = False\n    rank = dist.get_rank()\n    if lora_path is not None and lora_path != self.cur_adapter_path:\n        lora_local_path = maybe_download_lora(lora_path)\n        lora_state_dict = load_file(lora_local_path)\n\n        # Map the hf layer names to our custom layer names\n        param_names_mapping_fn = get_param_names_mapping(\n            self.modules[\"transformer\"].param_names_mapping)\n        lora_param_names_mapping_fn = get_param_names_mapping(\n            self.modules[\"transformer\"].lora_param_names_mapping)\n\n        to_merge_params: defaultdict[Hashable,\n                                     dict[Any, Any]] = defaultdict(dict)\n        for name, weight in lora_state_dict.items():\n            name = name.replace(\"diffusion_model.\", \"\")\n            name = name.replace(\".weight\", \"\")\n            name, _, _ = lora_param_names_mapping_fn(name)\n            target_name, merge_index, num_params_to_merge = param_names_mapping_fn(\n                name)\n            # for (in_dim, r) @ (r, out_dim), we only merge (r, out_dim * n) where n is the number of linear layers to fuse\n            # see param mapping in HunyuanVideoArchConfig\n            if merge_index is not None and \"lora_B\" in name:\n                to_merge_params[target_name][merge_index] = weight\n                if len(to_merge_params[target_name]) == num_params_to_merge:\n                    # cat at output dim according to the merge_index order\n                    sorted_tensors = [\n                        to_merge_params[target_name][i]\n                        for i in range(num_params_to_merge)\n                    ]\n                    weight = torch.cat(sorted_tensors, dim=1)\n                    del to_merge_params[target_name]\n                else:\n                    continue\n\n            if target_name in self.lora_adapters[lora_nickname]:\n                raise ValueError(\n                    f\"Target name {target_name} already exists in lora_adapters[{lora_nickname}]\"\n                )\n            self.lora_adapters[lora_nickname][target_name] = weight.to(\n                self.device)\n        adapter_updated = True\n        self.cur_adapter_path = lora_path\n        logger.info(\"Rank %d: loaded LoRA adapter %s\", rank, lora_path)\n\n    if not adapter_updated and self.cur_adapter_name == lora_nickname:\n        return\n    self.cur_adapter_name = lora_nickname\n\n    # Merge the new adapter\n    adapted_count = 0\n    for name, layer in self.lora_layers.items():\n        lora_A_name = name + \".lora_A\"\n        lora_B_name = name + \".lora_B\"\n        if lora_A_name in self.lora_adapters[lora_nickname]\\\n            and lora_B_name in self.lora_adapters[lora_nickname]:\n            layer.set_lora_weights(\n                self.lora_adapters[lora_nickname][lora_A_name],\n                self.lora_adapters[lora_nickname][lora_B_name],\n                training_mode=self.fastvideo_args.training_mode,\n                lora_path=lora_path)\n            adapted_count += 1\n        else:\n            if rank == 0:\n                logger.warning(\n                    \"LoRA adapter %s does not contain the weights for layer %s. LoRA will not be applied to it.\",\n                    lora_path, name)\n            layer.disable_lora = True\n    logger.info(\"Rank %d: LoRA adapter %s applied to %d layers\", rank,\n                lora_path, adapted_count)\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.PipelineWithLoRA","title":"fastvideo.pipelines.PipelineWithLoRA","text":"<pre><code>PipelineWithLoRA(*args, **kwargs)\n</code></pre> <p>               Bases: <code>LoRAPipeline</code>, <code>ComposedPipelineBase</code></p> <p>Type for a pipeline that has both ComposedPipelineBase and LoRAPipeline functionality.</p> Source code in <code>fastvideo/pipelines/lora_pipeline.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.device = get_local_torch_device()\n    self.exclude_lora_layers = self.modules[\n        \"transformer\"].config.arch_config.exclude_lora_layers\n    self.lora_target_modules = self.fastvideo_args.lora_target_modules\n    self.lora_path = self.fastvideo_args.lora_path\n    self.lora_nickname = self.fastvideo_args.lora_nickname\n    self.training_mode = self.fastvideo_args.training_mode\n    if self.training_mode and getattr(self.fastvideo_args, \"lora_training\",\n                                      False):\n        assert isinstance(self.fastvideo_args, TrainingArgs)\n        if self.fastvideo_args.lora_alpha is None:\n            self.fastvideo_args.lora_alpha = self.fastvideo_args.lora_rank\n        self.lora_rank = self.fastvideo_args.lora_rank  # type: ignore\n        self.lora_alpha = self.fastvideo_args.lora_alpha  # type: ignore\n        logger.info(\"Using LoRA training with rank %d and alpha %d\",\n                    self.lora_rank, self.lora_alpha)\n        if self.lora_target_modules is None:\n            self.lora_target_modules = [\n                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"to_q\", \"to_k\",\n                \"to_v\", \"to_out\", \"to_qkv\"\n            ]\n        self.convert_to_lora_layers()\n    # Inference\n    elif not self.training_mode and self.lora_path is not None:\n        self.convert_to_lora_layers()\n        self.set_lora_adapter(\n            self.lora_nickname,  # type: ignore\n            self.lora_path)  # type: ignore\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines-functions","title":"Functions","text":""},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.build_pipeline","title":"fastvideo.pipelines.build_pipeline","text":"<pre><code>build_pipeline(\n    fastvideo_args: FastVideoArgs,\n    pipeline_type: PipelineType | str = PipelineType.BASIC,\n) -&gt; PipelineWithLoRA\n</code></pre> <p>Only works with valid hf diffusers configs. (model_index.json) We want to build a pipeline based on the inference args mode_path: 1. download the model from the hub if it's not already downloaded 2. verify the model config and directory 3. based on the config, determine the pipeline class</p> Source code in <code>fastvideo/pipelines/__init__.py</code> <pre><code>def build_pipeline(\n        fastvideo_args: FastVideoArgs,\n        pipeline_type: PipelineType | str = PipelineType.BASIC\n) -&gt; PipelineWithLoRA:\n    \"\"\"\n    Only works with valid hf diffusers configs. (model_index.json)\n    We want to build a pipeline based on the inference args mode_path:\n    1. download the model from the hub if it's not already downloaded\n    2. verify the model config and directory\n    3. based on the config, determine the pipeline class \n    \"\"\"\n    # Get pipeline type\n    model_path = fastvideo_args.model_path\n    model_path = maybe_download_model(model_path)\n    # fastvideo_args.downloaded_model_path = model_path\n    logger.info(\"Model path: %s\", model_path)\n\n    config = verify_model_config_and_directory(model_path)\n    pipeline_name = config.get(\"_class_name\")\n    if pipeline_name is None:\n        raise ValueError(\n            \"Model config does not contain a _class_name attribute. \"\n            \"Only diffusers format is supported.\")\n\n    # Get the appropriate pipeline registry based on pipeline_type\n    logger.info(\n        \"Building pipeline of type: %s\", pipeline_type.value if isinstance(\n            pipeline_type, PipelineType) else pipeline_type)\n    pipeline_registry = get_pipeline_registry(pipeline_type)\n\n    if isinstance(pipeline_type, str):\n        pipeline_type = PipelineType.from_string(pipeline_type)\n\n    pipeline_cls = pipeline_registry.resolve_pipeline_cls(\n        pipeline_name, pipeline_type, fastvideo_args.workload_type)\n\n    # instantiate the pipelines\n    pipeline = pipeline_cls(model_path, fastvideo_args)\n\n    logger.info(\"Pipelines instantiated\")\n\n    return cast(PipelineWithLoRA, pipeline)\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines-modules","title":"Modules","text":""},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.basic","title":"fastvideo.pipelines.basic","text":"<p>Basic inference pipelines for fastvideo.</p> <p>This package contains basic pipelines for video and image generation.</p>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.basic-modules","title":"Modules","text":""},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.basic.hunyuan","title":"fastvideo.pipelines.basic.hunyuan","text":"Modules\u00b6 fastvideo.pipelines.basic.hunyuan.hunyuan_pipeline \u00b6 <p>Hunyuan video diffusion pipeline implementation.</p> <p>This module contains an implementation of the Hunyuan video diffusion pipeline using the modular pipeline architecture.</p> Classes\u00b6 fastvideo.pipelines.basic.hunyuan.hunyuan_pipeline.HunyuanVideoPipeline \u00b6 <pre><code>HunyuanVideoPipeline(\n    model_path: str,\n    fastvideo_args: FastVideoArgs | TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>ComposedPipelineBase</code></p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def __init__(self,\n             model_path: str,\n             fastvideo_args: FastVideoArgs | TrainingArgs,\n             required_config_modules: list[str] | None = None,\n             loaded_modules: dict[str, torch.nn.Module] | None = None):\n    \"\"\"\n    Initialize the pipeline. After __init__, the pipeline should be ready to\n    use. The pipeline should be stateless and not hold any batch state.\n    \"\"\"\n    self.fastvideo_args = fastvideo_args\n\n    self.model_path: str = model_path\n    self._stages: list[PipelineStage] = []\n    self._stage_name_mapping: dict[str, PipelineStage] = {}\n\n    if required_config_modules is not None:\n        self._required_config_modules = required_config_modules\n\n    if self._required_config_modules is None:\n        raise NotImplementedError(\n            \"Subclass must set _required_config_modules\")\n\n    maybe_init_distributed_environment_and_model_parallel(\n        fastvideo_args.tp_size, fastvideo_args.sp_size)\n\n    # Torch profiler. Enabled and configured through env vars:\n    # FASTVIDEO_TORCH_PROFILER_DIR=/path/to/save/trace\n    trace_dir = envs.FASTVIDEO_TORCH_PROFILER_DIR\n    self.profiler_controller = get_or_create_profiler(trace_dir)\n    self.profiler = self.profiler_controller.profiler\n\n    self.local_rank = get_world_group().local_rank\n\n    # Load modules directly in initialization\n    logger.info(\"Loading pipeline modules...\")\n    with self.profiler_controller.region(\"profiler_region_model_loading\"):\n        self.modules = self.load_modules(fastvideo_args, loaded_modules)\n</code></pre> Functions\u00b6 fastvideo.pipelines.basic.hunyuan.hunyuan_pipeline.HunyuanVideoPipeline.create_pipeline_stages \u00b6 <pre><code>create_pipeline_stages(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Set up pipeline stages with proper dependency injection.</p> Source code in <code>fastvideo/pipelines/basic/hunyuan/hunyuan_pipeline.py</code> <pre><code>def create_pipeline_stages(self, fastvideo_args: FastVideoArgs):\n    \"\"\"Set up pipeline stages with proper dependency injection.\"\"\"\n\n    self.add_stage(stage_name=\"input_validation_stage\",\n                   stage=InputValidationStage())\n\n    self.add_stage(stage_name=\"prompt_encoding_stage_primary\",\n                   stage=TextEncodingStage(\n                       text_encoders=[\n                           self.get_module(\"text_encoder\"),\n                           self.get_module(\"text_encoder_2\")\n                       ],\n                       tokenizers=[\n                           self.get_module(\"tokenizer\"),\n                           self.get_module(\"tokenizer_2\")\n                       ],\n                   ))\n\n    self.add_stage(stage_name=\"conditioning_stage\",\n                   stage=ConditioningStage())\n\n    self.add_stage(stage_name=\"timestep_preparation_stage\",\n                   stage=TimestepPreparationStage(\n                       scheduler=self.get_module(\"scheduler\")))\n\n    self.add_stage(stage_name=\"latent_preparation_stage\",\n                   stage=LatentPreparationStage(\n                       scheduler=self.get_module(\"scheduler\"),\n                       transformer=self.get_module(\"transformer\")))\n\n    self.add_stage(stage_name=\"denoising_stage\",\n                   stage=DenoisingStage(\n                       transformer=self.get_module(\"transformer\"),\n                       scheduler=self.get_module(\"scheduler\")))\n\n    self.add_stage(stage_name=\"decoding_stage\",\n                   stage=DecodingStage(vae=self.get_module(\"vae\")))\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.basic.stepvideo","title":"fastvideo.pipelines.basic.stepvideo","text":"Modules\u00b6 fastvideo.pipelines.basic.stepvideo.stepvideo_pipeline \u00b6 <p>Hunyuan video diffusion pipeline implementation.</p> <p>This module contains an implementation of the Hunyuan video diffusion pipeline using the modular pipeline architecture.</p> Classes\u00b6 fastvideo.pipelines.basic.stepvideo.stepvideo_pipeline.StepVideoPipeline \u00b6 <pre><code>StepVideoPipeline(*args, **kwargs)\n</code></pre> <p>               Bases: <code>LoRAPipeline</code>, <code>ComposedPipelineBase</code></p> Source code in <code>fastvideo/pipelines/lora_pipeline.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.device = get_local_torch_device()\n    self.exclude_lora_layers = self.modules[\n        \"transformer\"].config.arch_config.exclude_lora_layers\n    self.lora_target_modules = self.fastvideo_args.lora_target_modules\n    self.lora_path = self.fastvideo_args.lora_path\n    self.lora_nickname = self.fastvideo_args.lora_nickname\n    self.training_mode = self.fastvideo_args.training_mode\n    if self.training_mode and getattr(self.fastvideo_args, \"lora_training\",\n                                      False):\n        assert isinstance(self.fastvideo_args, TrainingArgs)\n        if self.fastvideo_args.lora_alpha is None:\n            self.fastvideo_args.lora_alpha = self.fastvideo_args.lora_rank\n        self.lora_rank = self.fastvideo_args.lora_rank  # type: ignore\n        self.lora_alpha = self.fastvideo_args.lora_alpha  # type: ignore\n        logger.info(\"Using LoRA training with rank %d and alpha %d\",\n                    self.lora_rank, self.lora_alpha)\n        if self.lora_target_modules is None:\n            self.lora_target_modules = [\n                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"to_q\", \"to_k\",\n                \"to_v\", \"to_out\", \"to_qkv\"\n            ]\n        self.convert_to_lora_layers()\n    # Inference\n    elif not self.training_mode and self.lora_path is not None:\n        self.convert_to_lora_layers()\n        self.set_lora_adapter(\n            self.lora_nickname,  # type: ignore\n            self.lora_path)  # type: ignore\n</code></pre> Functions\u00b6 fastvideo.pipelines.basic.stepvideo.stepvideo_pipeline.StepVideoPipeline.create_pipeline_stages \u00b6 <pre><code>create_pipeline_stages(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Set up pipeline stages with proper dependency injection.</p> Source code in <code>fastvideo/pipelines/basic/stepvideo/stepvideo_pipeline.py</code> <pre><code>def create_pipeline_stages(self, fastvideo_args: FastVideoArgs):\n    \"\"\"Set up pipeline stages with proper dependency injection.\"\"\"\n\n    self.add_stage(stage_name=\"input_validation_stage\",\n                   stage=InputValidationStage())\n\n    self.add_stage(stage_name=\"prompt_encoding_stage\",\n                   stage=StepvideoPromptEncodingStage(\n                       stepllm=self.get_module(\"text_encoder\"),\n                       clip=self.get_module(\"text_encoder_2\"),\n                   ))\n\n    self.add_stage(stage_name=\"timestep_preparation_stage\",\n                   stage=TimestepPreparationStage(\n                       scheduler=self.get_module(\"scheduler\")))\n\n    self.add_stage(stage_name=\"latent_preparation_stage\",\n                   stage=LatentPreparationStage(\n                       scheduler=self.get_module(\"scheduler\"),\n                       transformer=self.get_module(\"transformer\"),\n                   ))\n\n    self.add_stage(stage_name=\"denoising_stage\",\n                   stage=DenoisingStage(\n                       transformer=self.get_module(\"transformer\"),\n                       scheduler=self.get_module(\"scheduler\")))\n\n    self.add_stage(stage_name=\"decoding_stage\",\n                   stage=DecodingStage(vae=self.get_module(\"vae\")))\n</code></pre> fastvideo.pipelines.basic.stepvideo.stepvideo_pipeline.StepVideoPipeline.initialize_pipeline \u00b6 <pre><code>initialize_pipeline(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Initialize the pipeline.</p> Source code in <code>fastvideo/pipelines/basic/stepvideo/stepvideo_pipeline.py</code> <pre><code>def initialize_pipeline(self, fastvideo_args: FastVideoArgs):\n    \"\"\"\n    Initialize the pipeline.\n    \"\"\"\n    target_device = get_local_torch_device()\n    llm_dir = os.path.join(self.model_path, \"step_llm\")\n    clip_dir = os.path.join(self.model_path, \"hunyuan_clip\")\n    text_enc = self.build_llm(llm_dir, target_device)\n    clip_enc = self.build_clip(clip_dir, target_device)\n    self.add_module(\"text_encoder\", text_enc)\n    self.add_module(\"text_encoder_2\", clip_enc)\n    lib_path = (\n        os.path.join(\n            fastvideo_args.model_path,\n            'lib/liboptimus_ths-torch2.5-cu124.cpython-310-x86_64-linux-gnu.so'\n        ) if os.path.isdir(fastvideo_args.model_path)  # local checkout\n        else hf_hub_download(\n            repo_id=fastvideo_args.model_path,\n            filename=\n            'lib/liboptimus_ths-torch2.5-cu124.cpython-310-x86_64-linux-gnu.so'\n        ))\n    torch.ops.load_library(lib_path)\n</code></pre> fastvideo.pipelines.basic.stepvideo.stepvideo_pipeline.StepVideoPipeline.load_modules \u00b6 <pre><code>load_modules(\n    fastvideo_args: FastVideoArgs,\n) -&gt; dict[str, Any]\n</code></pre> <p>Load the modules from the config.</p> Source code in <code>fastvideo/pipelines/basic/stepvideo/stepvideo_pipeline.py</code> <pre><code>def load_modules(self, fastvideo_args: FastVideoArgs) -&gt; dict[str, Any]:\n    \"\"\"\n    Load the modules from the config.\n    \"\"\"\n    model_index = self._load_config(self.model_path)\n    logger.info(\"Loading pipeline modules from config: %s\", model_index)\n\n    # remove keys that are not pipeline modules\n    model_index.pop(\"_class_name\")\n    model_index.pop(\"_diffusers_version\")\n\n    # some sanity checks\n    assert len(\n        model_index\n    ) &gt; 1, \"model_index.json must contain at least one pipeline module\"\n\n    required_modules = [\"transformer\", \"scheduler\", \"vae\"]\n    for module_name in required_modules:\n        if module_name not in model_index:\n            raise ValueError(\n                f\"model_index.json must contain a {module_name} module\")\n    logger.info(\"Diffusers config passed sanity checks\")\n\n    # all the component models used by the pipeline\n    modules = {}\n    for module_name, (transformers_or_diffusers,\n                      architecture) in model_index.items():\n        component_model_path = os.path.join(self.model_path, module_name)\n        module = PipelineComponentLoader.load_module(\n            module_name=module_name,\n            component_model_path=component_model_path,\n            transformers_or_diffusers=transformers_or_diffusers,\n            fastvideo_args=fastvideo_args,\n        )\n        logger.info(\"Loaded module %s from %s\", module_name,\n                    component_model_path)\n\n        if module_name in modules:\n            logger.warning(\"Overwriting module %s\", module_name)\n        modules[module_name] = module\n\n    required_modules = self.required_config_modules\n    # Check if all required modules were loaded\n    for module_name in required_modules:\n        if module_name not in modules or modules[module_name] is None:\n            raise ValueError(\n                f\"Required module {module_name} was not loaded properly\")\n\n    return modules\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.basic.wan","title":"fastvideo.pipelines.basic.wan","text":"Modules\u00b6 fastvideo.pipelines.basic.wan.wan_causal_dmd_pipeline \u00b6 <p>Wan causal DMD pipeline implementation.</p> <p>This module wires the causal DMD denoising stage into the modular pipeline.</p> Classes\u00b6 fastvideo.pipelines.basic.wan.wan_causal_dmd_pipeline.WanCausalDMDPipeline \u00b6 <pre><code>WanCausalDMDPipeline(*args, **kwargs)\n</code></pre> <p>               Bases: <code>LoRAPipeline</code>, <code>ComposedPipelineBase</code></p> Source code in <code>fastvideo/pipelines/lora_pipeline.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.device = get_local_torch_device()\n    self.exclude_lora_layers = self.modules[\n        \"transformer\"].config.arch_config.exclude_lora_layers\n    self.lora_target_modules = self.fastvideo_args.lora_target_modules\n    self.lora_path = self.fastvideo_args.lora_path\n    self.lora_nickname = self.fastvideo_args.lora_nickname\n    self.training_mode = self.fastvideo_args.training_mode\n    if self.training_mode and getattr(self.fastvideo_args, \"lora_training\",\n                                      False):\n        assert isinstance(self.fastvideo_args, TrainingArgs)\n        if self.fastvideo_args.lora_alpha is None:\n            self.fastvideo_args.lora_alpha = self.fastvideo_args.lora_rank\n        self.lora_rank = self.fastvideo_args.lora_rank  # type: ignore\n        self.lora_alpha = self.fastvideo_args.lora_alpha  # type: ignore\n        logger.info(\"Using LoRA training with rank %d and alpha %d\",\n                    self.lora_rank, self.lora_alpha)\n        if self.lora_target_modules is None:\n            self.lora_target_modules = [\n                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"to_q\", \"to_k\",\n                \"to_v\", \"to_out\", \"to_qkv\"\n            ]\n        self.convert_to_lora_layers()\n    # Inference\n    elif not self.training_mode and self.lora_path is not None:\n        self.convert_to_lora_layers()\n        self.set_lora_adapter(\n            self.lora_nickname,  # type: ignore\n            self.lora_path)  # type: ignore\n</code></pre> Functions\u00b6 fastvideo.pipelines.basic.wan.wan_causal_dmd_pipeline.WanCausalDMDPipeline.create_pipeline_stages \u00b6 <pre><code>create_pipeline_stages(\n    fastvideo_args: FastVideoArgs,\n) -&gt; None\n</code></pre> <p>Set up pipeline stages with proper dependency injection.</p> Source code in <code>fastvideo/pipelines/basic/wan/wan_causal_dmd_pipeline.py</code> <pre><code>def create_pipeline_stages(self, fastvideo_args: FastVideoArgs) -&gt; None:\n    \"\"\"Set up pipeline stages with proper dependency injection.\"\"\"\n\n    self.add_stage(stage_name=\"input_validation_stage\",\n                   stage=InputValidationStage())\n\n    self.add_stage(stage_name=\"prompt_encoding_stage\",\n                   stage=TextEncodingStage(\n                       text_encoders=[self.get_module(\"text_encoder\")],\n                       tokenizers=[self.get_module(\"tokenizer\")],\n                   ))\n\n    self.add_stage(stage_name=\"conditioning_stage\",\n                   stage=ConditioningStage())\n\n    self.add_stage(stage_name=\"latent_preparation_stage\",\n                   stage=LatentPreparationStage(\n                       scheduler=self.get_module(\"scheduler\"),\n                       transformer=self.get_module(\"transformer\", None)))\n\n    self.add_stage(stage_name=\"denoising_stage\",\n                   stage=CausalDMDDenosingStage(\n                       transformer=self.get_module(\"transformer\"),\n                       transformer_2=self.get_module(\"transformer_2\", None),\n                       scheduler=self.get_module(\"scheduler\")))\n\n    self.add_stage(stage_name=\"decoding_stage\",\n                   stage=DecodingStage(vae=self.get_module(\"vae\")))\n</code></pre> Functions\u00b6 fastvideo.pipelines.basic.wan.wan_dmd_pipeline \u00b6 <p>Wan video diffusion pipeline implementation.</p> <p>This module contains an implementation of the Wan video diffusion pipeline using the modular pipeline architecture.</p> Classes\u00b6 fastvideo.pipelines.basic.wan.wan_dmd_pipeline.WanDMDPipeline \u00b6 <pre><code>WanDMDPipeline(*args, **kwargs)\n</code></pre> <p>               Bases: <code>LoRAPipeline</code>, <code>ComposedPipelineBase</code></p> <p>Wan video diffusion pipeline with LoRA support.</p> Source code in <code>fastvideo/pipelines/lora_pipeline.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.device = get_local_torch_device()\n    self.exclude_lora_layers = self.modules[\n        \"transformer\"].config.arch_config.exclude_lora_layers\n    self.lora_target_modules = self.fastvideo_args.lora_target_modules\n    self.lora_path = self.fastvideo_args.lora_path\n    self.lora_nickname = self.fastvideo_args.lora_nickname\n    self.training_mode = self.fastvideo_args.training_mode\n    if self.training_mode and getattr(self.fastvideo_args, \"lora_training\",\n                                      False):\n        assert isinstance(self.fastvideo_args, TrainingArgs)\n        if self.fastvideo_args.lora_alpha is None:\n            self.fastvideo_args.lora_alpha = self.fastvideo_args.lora_rank\n        self.lora_rank = self.fastvideo_args.lora_rank  # type: ignore\n        self.lora_alpha = self.fastvideo_args.lora_alpha  # type: ignore\n        logger.info(\"Using LoRA training with rank %d and alpha %d\",\n                    self.lora_rank, self.lora_alpha)\n        if self.lora_target_modules is None:\n            self.lora_target_modules = [\n                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"to_q\", \"to_k\",\n                \"to_v\", \"to_out\", \"to_qkv\"\n            ]\n        self.convert_to_lora_layers()\n    # Inference\n    elif not self.training_mode and self.lora_path is not None:\n        self.convert_to_lora_layers()\n        self.set_lora_adapter(\n            self.lora_nickname,  # type: ignore\n            self.lora_path)  # type: ignore\n</code></pre> Functions\u00b6 fastvideo.pipelines.basic.wan.wan_dmd_pipeline.WanDMDPipeline.create_pipeline_stages \u00b6 <pre><code>create_pipeline_stages(\n    fastvideo_args: FastVideoArgs,\n) -&gt; None\n</code></pre> <p>Set up pipeline stages with proper dependency injection.</p> Source code in <code>fastvideo/pipelines/basic/wan/wan_dmd_pipeline.py</code> <pre><code>def create_pipeline_stages(self, fastvideo_args: FastVideoArgs) -&gt; None:\n    \"\"\"Set up pipeline stages with proper dependency injection.\"\"\"\n\n    self.add_stage(stage_name=\"input_validation_stage\",\n                   stage=InputValidationStage())\n\n    self.add_stage(stage_name=\"prompt_encoding_stage\",\n                   stage=TextEncodingStage(\n                       text_encoders=[self.get_module(\"text_encoder\")],\n                       tokenizers=[self.get_module(\"tokenizer\")],\n                   ))\n\n    self.add_stage(stage_name=\"conditioning_stage\",\n                   stage=ConditioningStage())\n\n    self.add_stage(stage_name=\"timestep_preparation_stage\",\n                   stage=TimestepPreparationStage(\n                       scheduler=self.get_module(\"scheduler\")))\n\n    self.add_stage(stage_name=\"latent_preparation_stage\",\n                   stage=LatentPreparationStage(\n                       scheduler=self.get_module(\"scheduler\"),\n                       transformer=self.get_module(\"transformer\", None)))\n\n    self.add_stage(stage_name=\"denoising_stage\",\n                   stage=DmdDenoisingStage(\n                       transformer=self.get_module(\"transformer\"),\n                       scheduler=self.get_module(\"scheduler\")))\n\n    self.add_stage(stage_name=\"decoding_stage\",\n                   stage=DecodingStage(vae=self.get_module(\"vae\")))\n</code></pre> Functions\u00b6 fastvideo.pipelines.basic.wan.wan_i2v_dmd_pipeline \u00b6 <p>Wan video diffusion pipeline implementation.</p> <p>This module contains an implementation of the Wan video diffusion pipeline using the modular pipeline architecture.</p> Classes\u00b6 fastvideo.pipelines.basic.wan.wan_i2v_dmd_pipeline.WanImageToVideoDmdPipeline \u00b6 <pre><code>WanImageToVideoDmdPipeline(*args, **kwargs)\n</code></pre> <p>               Bases: <code>LoRAPipeline</code>, <code>ComposedPipelineBase</code></p> Source code in <code>fastvideo/pipelines/lora_pipeline.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.device = get_local_torch_device()\n    self.exclude_lora_layers = self.modules[\n        \"transformer\"].config.arch_config.exclude_lora_layers\n    self.lora_target_modules = self.fastvideo_args.lora_target_modules\n    self.lora_path = self.fastvideo_args.lora_path\n    self.lora_nickname = self.fastvideo_args.lora_nickname\n    self.training_mode = self.fastvideo_args.training_mode\n    if self.training_mode and getattr(self.fastvideo_args, \"lora_training\",\n                                      False):\n        assert isinstance(self.fastvideo_args, TrainingArgs)\n        if self.fastvideo_args.lora_alpha is None:\n            self.fastvideo_args.lora_alpha = self.fastvideo_args.lora_rank\n        self.lora_rank = self.fastvideo_args.lora_rank  # type: ignore\n        self.lora_alpha = self.fastvideo_args.lora_alpha  # type: ignore\n        logger.info(\"Using LoRA training with rank %d and alpha %d\",\n                    self.lora_rank, self.lora_alpha)\n        if self.lora_target_modules is None:\n            self.lora_target_modules = [\n                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"to_q\", \"to_k\",\n                \"to_v\", \"to_out\", \"to_qkv\"\n            ]\n        self.convert_to_lora_layers()\n    # Inference\n    elif not self.training_mode and self.lora_path is not None:\n        self.convert_to_lora_layers()\n        self.set_lora_adapter(\n            self.lora_nickname,  # type: ignore\n            self.lora_path)  # type: ignore\n</code></pre> Functions\u00b6 fastvideo.pipelines.basic.wan.wan_i2v_dmd_pipeline.WanImageToVideoDmdPipeline.create_pipeline_stages \u00b6 <pre><code>create_pipeline_stages(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Set up pipeline stages with proper dependency injection.</p> Source code in <code>fastvideo/pipelines/basic/wan/wan_i2v_dmd_pipeline.py</code> <pre><code>def create_pipeline_stages(self, fastvideo_args: FastVideoArgs):\n    \"\"\"Set up pipeline stages with proper dependency injection.\"\"\"\n\n    self.add_stage(stage_name=\"input_validation_stage\",\n                   stage=InputValidationStage())\n\n    self.add_stage(stage_name=\"prompt_encoding_stage\",\n                   stage=TextEncodingStage(\n                       text_encoders=[self.get_module(\"text_encoder\")],\n                       tokenizers=[self.get_module(\"tokenizer\")],\n                   ))\n\n    self.add_stage(stage_name=\"image_encoding_stage\",\n                   stage=ImageEncodingStage(\n                       image_encoder=self.get_module(\"image_encoder\"),\n                       image_processor=self.get_module(\"image_processor\"),\n                   ))\n\n    self.add_stage(stage_name=\"conditioning_stage\",\n                   stage=ConditioningStage())\n\n    self.add_stage(stage_name=\"timestep_preparation_stage\",\n                   stage=TimestepPreparationStage(\n                       scheduler=self.get_module(\"scheduler\")))\n\n    self.add_stage(stage_name=\"latent_preparation_stage\",\n                   stage=LatentPreparationStage(\n                       scheduler=self.get_module(\"scheduler\"),\n                       transformer=self.get_module(\"transformer\")))\n\n    self.add_stage(stage_name=\"image_latent_preparation_stage\",\n                   stage=ImageVAEEncodingStage(vae=self.get_module(\"vae\")))\n\n    self.add_stage(stage_name=\"denoising_stage\",\n                   stage=DmdDenoisingStage(\n                       transformer=self.get_module(\"transformer\"),\n                       scheduler=self.get_module(\"scheduler\")))\n\n    self.add_stage(stage_name=\"decoding_stage\",\n                   stage=DecodingStage(vae=self.get_module(\"vae\")))\n</code></pre> Functions\u00b6 fastvideo.pipelines.basic.wan.wan_i2v_pipeline \u00b6 <p>Wan video diffusion pipeline implementation.</p> <p>This module contains an implementation of the Wan video diffusion pipeline using the modular pipeline architecture.</p> Classes\u00b6 fastvideo.pipelines.basic.wan.wan_i2v_pipeline.WanImageToVideoPipeline \u00b6 <pre><code>WanImageToVideoPipeline(*args, **kwargs)\n</code></pre> <p>               Bases: <code>LoRAPipeline</code>, <code>ComposedPipelineBase</code></p> Source code in <code>fastvideo/pipelines/lora_pipeline.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.device = get_local_torch_device()\n    self.exclude_lora_layers = self.modules[\n        \"transformer\"].config.arch_config.exclude_lora_layers\n    self.lora_target_modules = self.fastvideo_args.lora_target_modules\n    self.lora_path = self.fastvideo_args.lora_path\n    self.lora_nickname = self.fastvideo_args.lora_nickname\n    self.training_mode = self.fastvideo_args.training_mode\n    if self.training_mode and getattr(self.fastvideo_args, \"lora_training\",\n                                      False):\n        assert isinstance(self.fastvideo_args, TrainingArgs)\n        if self.fastvideo_args.lora_alpha is None:\n            self.fastvideo_args.lora_alpha = self.fastvideo_args.lora_rank\n        self.lora_rank = self.fastvideo_args.lora_rank  # type: ignore\n        self.lora_alpha = self.fastvideo_args.lora_alpha  # type: ignore\n        logger.info(\"Using LoRA training with rank %d and alpha %d\",\n                    self.lora_rank, self.lora_alpha)\n        if self.lora_target_modules is None:\n            self.lora_target_modules = [\n                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"to_q\", \"to_k\",\n                \"to_v\", \"to_out\", \"to_qkv\"\n            ]\n        self.convert_to_lora_layers()\n    # Inference\n    elif not self.training_mode and self.lora_path is not None:\n        self.convert_to_lora_layers()\n        self.set_lora_adapter(\n            self.lora_nickname,  # type: ignore\n            self.lora_path)  # type: ignore\n</code></pre> Functions\u00b6 fastvideo.pipelines.basic.wan.wan_i2v_pipeline.WanImageToVideoPipeline.create_pipeline_stages \u00b6 <pre><code>create_pipeline_stages(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Set up pipeline stages with proper dependency injection.</p> Source code in <code>fastvideo/pipelines/basic/wan/wan_i2v_pipeline.py</code> <pre><code>def create_pipeline_stages(self, fastvideo_args: FastVideoArgs):\n    \"\"\"Set up pipeline stages with proper dependency injection.\"\"\"\n\n    self.add_stage(stage_name=\"input_validation_stage\",\n                   stage=InputValidationStage())\n\n    self.add_stage(stage_name=\"prompt_encoding_stage\",\n                   stage=TextEncodingStage(\n                       text_encoders=[self.get_module(\"text_encoder\")],\n                       tokenizers=[self.get_module(\"tokenizer\")],\n                   ))\n\n    if (self.get_module(\"image_encoder\") is not None\n            and self.get_module(\"image_processor\") is not None):\n        self.add_stage(\n            stage_name=\"image_encoding_stage\",\n            stage=ImageEncodingStage(\n                image_encoder=self.get_module(\"image_encoder\"),\n                image_processor=self.get_module(\"image_processor\"),\n            ))\n\n    self.add_stage(stage_name=\"conditioning_stage\",\n                   stage=ConditioningStage())\n\n    self.add_stage(stage_name=\"timestep_preparation_stage\",\n                   stage=TimestepPreparationStage(\n                       scheduler=self.get_module(\"scheduler\")))\n\n    self.add_stage(stage_name=\"latent_preparation_stage\",\n                   stage=LatentPreparationStage(\n                       scheduler=self.get_module(\"scheduler\"),\n                       transformer=self.get_module(\"transformer\")))\n\n    self.add_stage(stage_name=\"image_latent_preparation_stage\",\n                   stage=ImageVAEEncodingStage(vae=self.get_module(\"vae\")))\n\n    self.add_stage(stage_name=\"denoising_stage\",\n                   stage=DenoisingStage(\n                       transformer=self.get_module(\"transformer\"),\n                       transformer_2=self.get_module(\"transformer_2\"),\n                       scheduler=self.get_module(\"scheduler\")))\n\n    self.add_stage(stage_name=\"decoding_stage\",\n                   stage=DecodingStage(vae=self.get_module(\"vae\")))\n</code></pre> Functions\u00b6 fastvideo.pipelines.basic.wan.wan_pipeline \u00b6 <p>Wan video diffusion pipeline implementation.</p> <p>This module contains an implementation of the Wan video diffusion pipeline using the modular pipeline architecture.</p> Classes\u00b6 fastvideo.pipelines.basic.wan.wan_pipeline.WanPipeline \u00b6 <pre><code>WanPipeline(*args, **kwargs)\n</code></pre> <p>               Bases: <code>LoRAPipeline</code>, <code>ComposedPipelineBase</code></p> <p>Wan video diffusion pipeline with LoRA support.</p> Source code in <code>fastvideo/pipelines/lora_pipeline.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.device = get_local_torch_device()\n    self.exclude_lora_layers = self.modules[\n        \"transformer\"].config.arch_config.exclude_lora_layers\n    self.lora_target_modules = self.fastvideo_args.lora_target_modules\n    self.lora_path = self.fastvideo_args.lora_path\n    self.lora_nickname = self.fastvideo_args.lora_nickname\n    self.training_mode = self.fastvideo_args.training_mode\n    if self.training_mode and getattr(self.fastvideo_args, \"lora_training\",\n                                      False):\n        assert isinstance(self.fastvideo_args, TrainingArgs)\n        if self.fastvideo_args.lora_alpha is None:\n            self.fastvideo_args.lora_alpha = self.fastvideo_args.lora_rank\n        self.lora_rank = self.fastvideo_args.lora_rank  # type: ignore\n        self.lora_alpha = self.fastvideo_args.lora_alpha  # type: ignore\n        logger.info(\"Using LoRA training with rank %d and alpha %d\",\n                    self.lora_rank, self.lora_alpha)\n        if self.lora_target_modules is None:\n            self.lora_target_modules = [\n                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"to_q\", \"to_k\",\n                \"to_v\", \"to_out\", \"to_qkv\"\n            ]\n        self.convert_to_lora_layers()\n    # Inference\n    elif not self.training_mode and self.lora_path is not None:\n        self.convert_to_lora_layers()\n        self.set_lora_adapter(\n            self.lora_nickname,  # type: ignore\n            self.lora_path)  # type: ignore\n</code></pre> Functions\u00b6 fastvideo.pipelines.basic.wan.wan_pipeline.WanPipeline.create_pipeline_stages \u00b6 <pre><code>create_pipeline_stages(\n    fastvideo_args: FastVideoArgs,\n) -&gt; None\n</code></pre> <p>Set up pipeline stages with proper dependency injection.</p> Source code in <code>fastvideo/pipelines/basic/wan/wan_pipeline.py</code> <pre><code>def create_pipeline_stages(self, fastvideo_args: FastVideoArgs) -&gt; None:\n    \"\"\"Set up pipeline stages with proper dependency injection.\"\"\"\n\n    self.add_stage(stage_name=\"input_validation_stage\",\n                   stage=InputValidationStage())\n\n    self.add_stage(stage_name=\"prompt_encoding_stage\",\n                   stage=TextEncodingStage(\n                       text_encoders=[self.get_module(\"text_encoder\")],\n                       tokenizers=[self.get_module(\"tokenizer\")],\n                   ))\n\n    self.add_stage(stage_name=\"conditioning_stage\",\n                   stage=ConditioningStage())\n\n    self.add_stage(stage_name=\"timestep_preparation_stage\",\n                   stage=TimestepPreparationStage(\n                       scheduler=self.get_module(\"scheduler\")))\n\n    self.add_stage(stage_name=\"latent_preparation_stage\",\n                   stage=LatentPreparationStage(\n                       scheduler=self.get_module(\"scheduler\"),\n                       transformer=self.get_module(\"transformer\", None)))\n\n    self.add_stage(stage_name=\"denoising_stage\",\n                   stage=DenoisingStage(\n                       transformer=self.get_module(\"transformer\"),\n                       transformer_2=self.get_module(\"transformer_2\", None),\n                       scheduler=self.get_module(\"scheduler\"),\n                       vae=self.get_module(\"vae\"),\n                       pipeline=self))\n\n    self.add_stage(stage_name=\"decoding_stage\",\n                   stage=DecodingStage(vae=self.get_module(\"vae\"),\n                                       pipeline=self))\n</code></pre> Functions\u00b6 fastvideo.pipelines.basic.wan.wan_v2v_pipeline \u00b6 <p>Wan video-to-video diffusion pipeline implementation.</p> <p>This module contains an implementation of the Wan video-to-video diffusion pipeline using the modular pipeline architecture.</p> Classes\u00b6 fastvideo.pipelines.basic.wan.wan_v2v_pipeline.WanVideoToVideoPipeline \u00b6 <pre><code>WanVideoToVideoPipeline(*args, **kwargs)\n</code></pre> <p>               Bases: <code>LoRAPipeline</code>, <code>ComposedPipelineBase</code></p> Source code in <code>fastvideo/pipelines/lora_pipeline.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.device = get_local_torch_device()\n    self.exclude_lora_layers = self.modules[\n        \"transformer\"].config.arch_config.exclude_lora_layers\n    self.lora_target_modules = self.fastvideo_args.lora_target_modules\n    self.lora_path = self.fastvideo_args.lora_path\n    self.lora_nickname = self.fastvideo_args.lora_nickname\n    self.training_mode = self.fastvideo_args.training_mode\n    if self.training_mode and getattr(self.fastvideo_args, \"lora_training\",\n                                      False):\n        assert isinstance(self.fastvideo_args, TrainingArgs)\n        if self.fastvideo_args.lora_alpha is None:\n            self.fastvideo_args.lora_alpha = self.fastvideo_args.lora_rank\n        self.lora_rank = self.fastvideo_args.lora_rank  # type: ignore\n        self.lora_alpha = self.fastvideo_args.lora_alpha  # type: ignore\n        logger.info(\"Using LoRA training with rank %d and alpha %d\",\n                    self.lora_rank, self.lora_alpha)\n        if self.lora_target_modules is None:\n            self.lora_target_modules = [\n                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"to_q\", \"to_k\",\n                \"to_v\", \"to_out\", \"to_qkv\"\n            ]\n        self.convert_to_lora_layers()\n    # Inference\n    elif not self.training_mode and self.lora_path is not None:\n        self.convert_to_lora_layers()\n        self.set_lora_adapter(\n            self.lora_nickname,  # type: ignore\n            self.lora_path)  # type: ignore\n</code></pre> Functions\u00b6 fastvideo.pipelines.basic.wan.wan_v2v_pipeline.WanVideoToVideoPipeline.create_pipeline_stages \u00b6 <pre><code>create_pipeline_stages(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Set up pipeline stages with proper dependency injection.</p> Source code in <code>fastvideo/pipelines/basic/wan/wan_v2v_pipeline.py</code> <pre><code>def create_pipeline_stages(self, fastvideo_args: FastVideoArgs):\n    \"\"\"Set up pipeline stages with proper dependency injection.\"\"\"\n\n    self.add_stage(stage_name=\"input_validation_stage\",\n                   stage=InputValidationStage())\n\n    self.add_stage(stage_name=\"prompt_encoding_stage\",\n                   stage=TextEncodingStage(\n                       text_encoders=[self.get_module(\"text_encoder\")],\n                       tokenizers=[self.get_module(\"tokenizer\")],\n                   ))\n\n    if (self.get_module(\"image_encoder\") is not None\n            and self.get_module(\"image_processor\") is not None):\n        self.add_stage(\n            stage_name=\"ref_image_encoding_stage\",\n            stage=RefImageEncodingStage(\n                image_encoder=self.get_module(\"image_encoder\"),\n                image_processor=self.get_module(\"image_processor\"),\n            ))\n\n    self.add_stage(stage_name=\"conditioning_stage\",\n                   stage=ConditioningStage())\n\n    self.add_stage(stage_name=\"timestep_preparation_stage\",\n                   stage=TimestepPreparationStage(\n                       scheduler=self.get_module(\"scheduler\")))\n\n    self.add_stage(stage_name=\"latent_preparation_stage\",\n                   stage=LatentPreparationStage(\n                       scheduler=self.get_module(\"scheduler\"),\n                       transformer=self.get_module(\"transformer\")))\n\n    self.add_stage(stage_name=\"video_latent_preparation_stage\",\n                   stage=VideoVAEEncodingStage(vae=self.get_module(\"vae\")))\n\n    self.add_stage(stage_name=\"denoising_stage\",\n                   stage=DenoisingStage(\n                       transformer=self.get_module(\"transformer\"),\n                       transformer_2=self.get_module(\"transformer_2\"),\n                       scheduler=self.get_module(\"scheduler\")))\n\n    self.add_stage(stage_name=\"decoding_stage\",\n                   stage=DecodingStage(vae=self.get_module(\"vae\")))\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.composed_pipeline_base","title":"fastvideo.pipelines.composed_pipeline_base","text":"<p>Base class for composed pipelines.</p> <p>This module defines the base class for pipelines that are composed of multiple stages.</p>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.composed_pipeline_base-classes","title":"Classes","text":""},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.composed_pipeline_base.ComposedPipelineBase","title":"fastvideo.pipelines.composed_pipeline_base.ComposedPipelineBase","text":"<pre><code>ComposedPipelineBase(\n    model_path: str,\n    fastvideo_args: FastVideoArgs | TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for pipelines composed of multiple stages.</p> <p>This class provides the framework for creating pipelines by composing multiple stages together. Each stage is responsible for a specific part of the diffusion process, and the pipeline orchestrates the execution of these stages.</p> <p>Initialize the pipeline. After init, the pipeline should be ready to use. The pipeline should be stateless and not hold any batch state.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def __init__(self,\n             model_path: str,\n             fastvideo_args: FastVideoArgs | TrainingArgs,\n             required_config_modules: list[str] | None = None,\n             loaded_modules: dict[str, torch.nn.Module] | None = None):\n    \"\"\"\n    Initialize the pipeline. After __init__, the pipeline should be ready to\n    use. The pipeline should be stateless and not hold any batch state.\n    \"\"\"\n    self.fastvideo_args = fastvideo_args\n\n    self.model_path: str = model_path\n    self._stages: list[PipelineStage] = []\n    self._stage_name_mapping: dict[str, PipelineStage] = {}\n\n    if required_config_modules is not None:\n        self._required_config_modules = required_config_modules\n\n    if self._required_config_modules is None:\n        raise NotImplementedError(\n            \"Subclass must set _required_config_modules\")\n\n    maybe_init_distributed_environment_and_model_parallel(\n        fastvideo_args.tp_size, fastvideo_args.sp_size)\n\n    # Torch profiler. Enabled and configured through env vars:\n    # FASTVIDEO_TORCH_PROFILER_DIR=/path/to/save/trace\n    trace_dir = envs.FASTVIDEO_TORCH_PROFILER_DIR\n    self.profiler_controller = get_or_create_profiler(trace_dir)\n    self.profiler = self.profiler_controller.profiler\n\n    self.local_rank = get_world_group().local_rank\n\n    # Load modules directly in initialization\n    logger.info(\"Loading pipeline modules...\")\n    with self.profiler_controller.region(\"profiler_region_model_loading\"):\n        self.modules = self.load_modules(fastvideo_args, loaded_modules)\n</code></pre> Attributes\u00b6 fastvideo.pipelines.composed_pipeline_base.ComposedPipelineBase.required_config_modules <code>property</code> \u00b6 <pre><code>required_config_modules: list[str]\n</code></pre> <p>List of modules that are required by the pipeline. The names should match the diffusers directory and model_index.json file. These modules will be loaded using the PipelineComponentLoader and made available in the modules dictionary. Access these modules using the get_module method.</p> <p>class ConcretePipeline(ComposedPipelineBase):     _required_config_modules = [\"vae\", \"text_encoder\", \"transformer\", \"scheduler\", \"tokenizer\"]</p> <pre><code>@property\ndef required_config_modules(self):\n    return self._required_config_modules\n</code></pre> fastvideo.pipelines.composed_pipeline_base.ComposedPipelineBase.stages <code>property</code> \u00b6 <pre><code>stages: list[PipelineStage]\n</code></pre> <p>List of stages in the pipeline.</p> Functions\u00b6 fastvideo.pipelines.composed_pipeline_base.ComposedPipelineBase.create_pipeline_stages <code>abstractmethod</code> \u00b6 <pre><code>create_pipeline_stages(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Create the inference pipeline stages.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>@abstractmethod\ndef create_pipeline_stages(self, fastvideo_args: FastVideoArgs):\n    \"\"\"\n    Create the inference pipeline stages.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.pipelines.composed_pipeline_base.ComposedPipelineBase.create_training_stages \u00b6 <pre><code>create_training_stages(training_args: TrainingArgs)\n</code></pre> <p>Create the training pipeline stages.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def create_training_stages(self, training_args: TrainingArgs):\n    \"\"\"\n    Create the training pipeline stages.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.pipelines.composed_pipeline_base.ComposedPipelineBase.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Generate a video or image using the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The batch to generate from.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:     ForwardBatch: The batch with the generated video or image.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Generate a video or image using the pipeline.\n\n    Args:\n        batch: The batch to generate from.\n        fastvideo_args: The inference arguments.\n    Returns:\n        ForwardBatch: The batch with the generated video or image.\n    \"\"\"\n    if not self.post_init_called:\n        self.post_init()\n\n    # Execute each stage\n    logger.info(\"Running pipeline stages: %s\",\n                self._stage_name_mapping.keys())\n    # logger.info(\"Batch: %s\", batch)\n    for stage in self.stages:\n        batch = stage(batch, fastvideo_args)\n\n    # Return the output\n    return batch\n</code></pre> fastvideo.pipelines.composed_pipeline_base.ComposedPipelineBase.from_pretrained <code>classmethod</code> \u00b6 <pre><code>from_pretrained(\n    model_path: str,\n    device: str | None = None,\n    torch_dtype: dtype | None = None,\n    pipeline_config: str | PipelineConfig | None = None,\n    args: Namespace | None = None,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n    **kwargs\n) -&gt; ComposedPipelineBase\n</code></pre> <p>Load a pipeline from a pretrained model. loaded_modules: Optional[Dict[str, torch.nn.Module]] = None, If provided, loaded_modules will be used instead of loading from config/pretrained weights.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>@classmethod\ndef from_pretrained(cls,\n                    model_path: str,\n                    device: str | None = None,\n                    torch_dtype: torch.dtype | None = None,\n                    pipeline_config: str | PipelineConfig | None = None,\n                    args: argparse.Namespace | None = None,\n                    required_config_modules: list[str] | None = None,\n                    loaded_modules: dict[str, torch.nn.Module]\n                    | None = None,\n                    **kwargs) -&gt; \"ComposedPipelineBase\":\n    \"\"\"\n    Load a pipeline from a pretrained model.\n    loaded_modules: Optional[Dict[str, torch.nn.Module]] = None,\n    If provided, loaded_modules will be used instead of loading from config/pretrained weights.\n    \"\"\"\n    if args is None or args.inference_mode:\n\n        kwargs['model_path'] = model_path\n        fastvideo_args = FastVideoArgs.from_kwargs(**kwargs)\n    else:\n        assert args is not None, \"args must be provided for training mode\"\n        fastvideo_args = TrainingArgs.from_cli_args(args)\n        # TODO(will): fix this so that its not so ugly\n        fastvideo_args.model_path = model_path\n        for key, value in kwargs.items():\n            setattr(fastvideo_args, key, value)\n\n        fastvideo_args.dit_cpu_offload = False\n        # we hijack the precision to be the master weight type so that the\n        # model is loaded with the correct precision. Subsequently we will\n        # use FSDP2's MixedPrecisionPolicy to set the precision for the\n        # fwd, bwd, and other operations' precision.\n        assert fastvideo_args.pipeline_config.dit_precision == 'fp32', 'only fp32 is supported for training'\n\n    logger.info(\"fastvideo_args in from_pretrained: %s\", fastvideo_args)\n\n    pipe = cls(model_path,\n               fastvideo_args,\n               required_config_modules=required_config_modules,\n               loaded_modules=loaded_modules)\n    pipe.post_init()\n    return pipe\n</code></pre> fastvideo.pipelines.composed_pipeline_base.ComposedPipelineBase.initialize_pipeline \u00b6 <pre><code>initialize_pipeline(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Initialize the pipeline.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def initialize_pipeline(self, fastvideo_args: FastVideoArgs):\n    \"\"\"\n    Initialize the pipeline.\n    \"\"\"\n    return\n</code></pre> fastvideo.pipelines.composed_pipeline_base.ComposedPipelineBase.load_modules \u00b6 <pre><code>load_modules(\n    fastvideo_args: FastVideoArgs,\n    loaded_modules: dict[str, Module] | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Load the modules from the config. loaded_modules: Optional[Dict[str, torch.nn.Module]] = None,  If provided, loaded_modules will be used instead of loading from config/pretrained weights.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def load_modules(\n    self,\n    fastvideo_args: FastVideoArgs,\n    loaded_modules: dict[str, torch.nn.Module] | None = None\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Load the modules from the config.\n    loaded_modules: Optional[Dict[str, torch.nn.Module]] = None, \n    If provided, loaded_modules will be used instead of loading from config/pretrained weights.\n    \"\"\"\n\n    model_index = self._load_config(self.model_path)\n    logger.info(\"Loading pipeline modules from config: %s\", model_index)\n\n    # remove keys that are not pipeline modules\n    model_index.pop(\"_class_name\")\n    model_index.pop(\"_diffusers_version\")\n    if \"boundary_ratio\" in model_index and model_index[\n            \"boundary_ratio\"] is not None:\n        logger.info(\n            \"MoE pipeline detected. Adding transformer_2 to self.required_config_modules...\"\n        )\n        self.required_config_modules.append(\"transformer_2\")\n        logger.info(\"MoE pipeline detected. Setting boundary ratio to %s\",\n                    model_index[\"boundary_ratio\"])\n        fastvideo_args.pipeline_config.dit_config.boundary_ratio = model_index[\n            \"boundary_ratio\"]\n\n    model_index.pop(\"boundary_ratio\", None)\n    # used by Wan2.2 ti2v\n    model_index.pop(\"expand_timesteps\", None)\n\n    # some sanity checks\n    assert len(\n        model_index\n    ) &gt; 1, \"model_index.json must contain at least one pipeline module\"\n\n    for module_name in self.required_config_modules:\n        if module_name not in model_index and module_name in self._extra_config_module_map:\n            extra_module_value = self._extra_config_module_map[module_name]\n            logger.warning(\n                \"model_index.json does not contain a %s module, but found {%s: %s} in _extra_config_module_map, adding to model_index.\",\n                module_name, module_name, extra_module_value)\n            if extra_module_value in model_index:\n                logger.info(\"Using module %s for %s\", extra_module_value,\n                            module_name)\n                model_index[module_name] = model_index[extra_module_value]\n                continue\n            else:\n                raise ValueError(\n                    f\"Required module key: {module_name} value: {model_index.get(module_name)} was not found in loaded modules {model_index.keys()}\"\n                )\n\n    # all the component models used by the pipeline\n    required_modules = self.required_config_modules\n    logger.info(\"Loading required modules: %s\", required_modules)\n\n    modules = {}\n    for module_name, (transformers_or_diffusers,\n                      architecture) in model_index.items():\n        if transformers_or_diffusers is None:\n            logger.warning(\n                \"Module %s in model_index.json has null value, removing from required_config_modules\",\n                module_name)\n            if module_name in self.required_config_modules:\n                self.required_config_modules.remove(module_name)\n            continue\n        if module_name not in required_modules:\n            logger.info(\"Skipping module %s\", module_name)\n            continue\n        if loaded_modules is not None and module_name in loaded_modules:\n            logger.info(\"Using module %s already provided\", module_name)\n            modules[module_name] = loaded_modules[module_name]\n            continue\n\n        # we load the module from the extra config module map if it exists\n        if module_name in self._extra_config_module_map:\n            load_module_name = self._extra_config_module_map[module_name]\n        else:\n            load_module_name = module_name\n\n        component_model_path = os.path.join(self.model_path,\n                                            load_module_name)\n        module = PipelineComponentLoader.load_module(\n            module_name=load_module_name,\n            component_model_path=component_model_path,\n            transformers_or_diffusers=transformers_or_diffusers,\n            fastvideo_args=fastvideo_args,\n        )\n        logger.info(\"Loaded module %s from %s\", module_name,\n                    component_model_path)\n\n        if module_name in modules:\n            logger.warning(\"Overwriting module %s\", module_name)\n        modules[module_name] = module\n\n    # Check if all required modules were loaded\n    for module_name in required_modules:\n        if module_name not in modules or modules[module_name] is None:\n            raise ValueError(\n                f\"Required module key: {module_name} value: {modules.get(module_name)} was not found in loaded modules {modules.keys()}\"\n            )\n\n    return modules\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.composed_pipeline_base-functions","title":"Functions","text":""},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.lora_pipeline","title":"fastvideo.pipelines.lora_pipeline","text":""},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.lora_pipeline-classes","title":"Classes","text":""},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.lora_pipeline.LoRAPipeline","title":"fastvideo.pipelines.lora_pipeline.LoRAPipeline","text":"<pre><code>LoRAPipeline(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ComposedPipelineBase</code></p> <p>Pipeline that supports injecting LoRA adapters into the diffusion transformer. TODO: support training.</p> Source code in <code>fastvideo/pipelines/lora_pipeline.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.device = get_local_torch_device()\n    self.exclude_lora_layers = self.modules[\n        \"transformer\"].config.arch_config.exclude_lora_layers\n    self.lora_target_modules = self.fastvideo_args.lora_target_modules\n    self.lora_path = self.fastvideo_args.lora_path\n    self.lora_nickname = self.fastvideo_args.lora_nickname\n    self.training_mode = self.fastvideo_args.training_mode\n    if self.training_mode and getattr(self.fastvideo_args, \"lora_training\",\n                                      False):\n        assert isinstance(self.fastvideo_args, TrainingArgs)\n        if self.fastvideo_args.lora_alpha is None:\n            self.fastvideo_args.lora_alpha = self.fastvideo_args.lora_rank\n        self.lora_rank = self.fastvideo_args.lora_rank  # type: ignore\n        self.lora_alpha = self.fastvideo_args.lora_alpha  # type: ignore\n        logger.info(\"Using LoRA training with rank %d and alpha %d\",\n                    self.lora_rank, self.lora_alpha)\n        if self.lora_target_modules is None:\n            self.lora_target_modules = [\n                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"to_q\", \"to_k\",\n                \"to_v\", \"to_out\", \"to_qkv\"\n            ]\n        self.convert_to_lora_layers()\n    # Inference\n    elif not self.training_mode and self.lora_path is not None:\n        self.convert_to_lora_layers()\n        self.set_lora_adapter(\n            self.lora_nickname,  # type: ignore\n            self.lora_path)  # type: ignore\n</code></pre> Functions\u00b6 fastvideo.pipelines.lora_pipeline.LoRAPipeline.convert_to_lora_layers \u00b6 <pre><code>convert_to_lora_layers() -&gt; None\n</code></pre> <p>Unified method to convert the transformer to a LoRA transformer.</p> Source code in <code>fastvideo/pipelines/lora_pipeline.py</code> <pre><code>def convert_to_lora_layers(self) -&gt; None:\n    \"\"\"\n    Unified method to convert the transformer to a LoRA transformer.\n    \"\"\"\n    if self.lora_initialized:\n        return\n    self.lora_initialized = True\n    converted_count = 0\n    for name, layer in self.modules[\"transformer\"].named_modules():\n        if not self.is_target_layer(name):\n            continue\n\n        excluded = False\n        for exclude_layer in self.exclude_lora_layers:\n            if exclude_layer in name:\n                excluded = True\n                break\n        if excluded:\n            continue\n\n        layer = get_lora_layer(layer,\n                               lora_rank=self.lora_rank,\n                               lora_alpha=self.lora_alpha,\n                               training_mode=self.training_mode)\n        if layer is not None:\n            self.lora_layers[name] = layer\n            replace_submodule(self.modules[\"transformer\"], name, layer)\n            converted_count += 1\n    logger.info(\"Converted %d layers to LoRA layers\", converted_count)\n\n    if \"fake_score_transformer\" in self.modules:\n        for name, layer in self.modules[\n                \"fake_score_transformer\"].named_modules():\n            if not self.is_target_layer(name):\n                continue\n            layer = get_lora_layer(layer,\n                                   lora_rank=self.lora_rank,\n                                   lora_alpha=self.lora_alpha,\n                                   training_mode=self.training_mode)\n            if layer is not None:\n                self.lora_layers_critic[name] = layer\n                replace_submodule(self.modules[\"fake_score_transformer\"],\n                                  name, layer)\n                converted_count += 1\n        logger.info(\n            \"Converted %d layers to LoRA layers in the critic model\",\n            converted_count)\n</code></pre> fastvideo.pipelines.lora_pipeline.LoRAPipeline.set_lora_adapter \u00b6 <pre><code>set_lora_adapter(\n    lora_nickname: str, lora_path: str | None = None\n)\n</code></pre> <p>Load a LoRA adapter into the pipeline and merge it into the transformer. Args:     lora_nickname: The \"nick name\" of the adapter when referenced in the pipeline.     lora_path: The path to the adapter, either a local path or a Hugging Face repo id.</p> Source code in <code>fastvideo/pipelines/lora_pipeline.py</code> <pre><code>def set_lora_adapter(self,\n                     lora_nickname: str,\n                     lora_path: str | None = None):  # type: ignore\n    \"\"\"\n    Load a LoRA adapter into the pipeline and merge it into the transformer.\n    Args:\n        lora_nickname: The \"nick name\" of the adapter when referenced in the pipeline.\n        lora_path: The path to the adapter, either a local path or a Hugging Face repo id.\n    \"\"\"\n\n    if lora_nickname not in self.lora_adapters and lora_path is None:\n        raise ValueError(\n            f\"Adapter {lora_nickname} not found in the pipeline. Please provide lora_path to load it.\"\n        )\n    if not self.lora_initialized:\n        self.convert_to_lora_layers()\n    adapter_updated = False\n    rank = dist.get_rank()\n    if lora_path is not None and lora_path != self.cur_adapter_path:\n        lora_local_path = maybe_download_lora(lora_path)\n        lora_state_dict = load_file(lora_local_path)\n\n        # Map the hf layer names to our custom layer names\n        param_names_mapping_fn = get_param_names_mapping(\n            self.modules[\"transformer\"].param_names_mapping)\n        lora_param_names_mapping_fn = get_param_names_mapping(\n            self.modules[\"transformer\"].lora_param_names_mapping)\n\n        to_merge_params: defaultdict[Hashable,\n                                     dict[Any, Any]] = defaultdict(dict)\n        for name, weight in lora_state_dict.items():\n            name = name.replace(\"diffusion_model.\", \"\")\n            name = name.replace(\".weight\", \"\")\n            name, _, _ = lora_param_names_mapping_fn(name)\n            target_name, merge_index, num_params_to_merge = param_names_mapping_fn(\n                name)\n            # for (in_dim, r) @ (r, out_dim), we only merge (r, out_dim * n) where n is the number of linear layers to fuse\n            # see param mapping in HunyuanVideoArchConfig\n            if merge_index is not None and \"lora_B\" in name:\n                to_merge_params[target_name][merge_index] = weight\n                if len(to_merge_params[target_name]) == num_params_to_merge:\n                    # cat at output dim according to the merge_index order\n                    sorted_tensors = [\n                        to_merge_params[target_name][i]\n                        for i in range(num_params_to_merge)\n                    ]\n                    weight = torch.cat(sorted_tensors, dim=1)\n                    del to_merge_params[target_name]\n                else:\n                    continue\n\n            if target_name in self.lora_adapters[lora_nickname]:\n                raise ValueError(\n                    f\"Target name {target_name} already exists in lora_adapters[{lora_nickname}]\"\n                )\n            self.lora_adapters[lora_nickname][target_name] = weight.to(\n                self.device)\n        adapter_updated = True\n        self.cur_adapter_path = lora_path\n        logger.info(\"Rank %d: loaded LoRA adapter %s\", rank, lora_path)\n\n    if not adapter_updated and self.cur_adapter_name == lora_nickname:\n        return\n    self.cur_adapter_name = lora_nickname\n\n    # Merge the new adapter\n    adapted_count = 0\n    for name, layer in self.lora_layers.items():\n        lora_A_name = name + \".lora_A\"\n        lora_B_name = name + \".lora_B\"\n        if lora_A_name in self.lora_adapters[lora_nickname]\\\n            and lora_B_name in self.lora_adapters[lora_nickname]:\n            layer.set_lora_weights(\n                self.lora_adapters[lora_nickname][lora_A_name],\n                self.lora_adapters[lora_nickname][lora_B_name],\n                training_mode=self.fastvideo_args.training_mode,\n                lora_path=lora_path)\n            adapted_count += 1\n        else:\n            if rank == 0:\n                logger.warning(\n                    \"LoRA adapter %s does not contain the weights for layer %s. LoRA will not be applied to it.\",\n                    lora_path, name)\n            layer.disable_lora = True\n    logger.info(\"Rank %d: LoRA adapter %s applied to %d layers\", rank,\n                lora_path, adapted_count)\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.lora_pipeline-functions","title":"Functions","text":""},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.pipeline_batch_info","title":"fastvideo.pipelines.pipeline_batch_info","text":"<p>Data structures for functional pipeline processing.</p> <p>This module defines the dataclasses used to pass state between pipeline components in a functional manner, reducing the need for explicit parameter passing.</p>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.pipeline_batch_info-classes","title":"Classes","text":""},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.pipeline_batch_info.ForwardBatch","title":"fastvideo.pipelines.pipeline_batch_info.ForwardBatch  <code>dataclass</code>","text":"<pre><code>ForwardBatch(\n    data_type: str,\n    generator: Generator | list[Generator] | None = None,\n    image_path: str | None = None,\n    image_embeds: list[Tensor] = list(),\n    pil_image: Tensor | Image | None = None,\n    preprocessed_image: Tensor | None = None,\n    prompt: str | list[str] | None = None,\n    negative_prompt: str | list[str] | None = None,\n    prompt_path: str | None = None,\n    output_path: str = \"outputs/\",\n    output_video_name: str | None = None,\n    video_path: str | None = None,\n    video_latent: Tensor | None = None,\n    prompt_embeds: list[Tensor] = list(),\n    negative_prompt_embeds: list[Tensor] | None = None,\n    prompt_attention_mask: list[Tensor] | None = None,\n    negative_attention_mask: list[Tensor] | None = None,\n    clip_embedding_pos: list[Tensor] | None = None,\n    clip_embedding_neg: list[Tensor] | None = None,\n    max_sequence_length: int | None = None,\n    prompt_template: dict[str, Any] | None = None,\n    do_classifier_free_guidance: bool = False,\n    batch_size: int | None = None,\n    num_videos_per_prompt: int = 1,\n    seed: int | None = None,\n    seeds: list[int] | None = None,\n    is_prompt_processed: bool = False,\n    latents: Tensor | None = None,\n    raw_latent_shape: Tensor | None = None,\n    noise_pred: Tensor | None = None,\n    image_latent: Tensor | None = None,\n    height_latents: list[int] | int | None = None,\n    width_latents: list[int] | int | None = None,\n    num_frames: list[int] | int = 1,\n    num_frames_round_down: bool = False,\n    height: list[int] | int | None = None,\n    width: list[int] | int | None = None,\n    fps: list[int] | int | None = None,\n    timesteps: Tensor | None = None,\n    timestep: Tensor | float | int | None = None,\n    step_index: int | None = None,\n    boundary_ratio: float | None = None,\n    num_inference_steps: int = 50,\n    guidance_scale: float = 1.0,\n    guidance_scale_2: float | None = None,\n    guidance_rescale: float = 0.0,\n    eta: float = 0.0,\n    sigmas: list[float] | None = None,\n    n_tokens: int | None = None,\n    extra_step_kwargs: dict[str, Any] = dict(),\n    modules: dict[str, Any] = dict(),\n    output: Tensor | None = None,\n    return_trajectory_latents: bool = False,\n    return_trajectory_decoded: bool = False,\n    trajectory_timesteps: list[Tensor] | None = None,\n    trajectory_latents: Tensor | None = None,\n    trajectory_decoded: list[Tensor] | None = None,\n    extra: dict[str, Any] = dict(),\n    save_video: bool = True,\n    return_frames: bool = False,\n    enable_teacache: bool = False,\n    teacache_params: TeaCacheParams\n    | WanTeaCacheParams\n    | None = None,\n    STA_param: list | None = None,\n    is_cfg_negative: bool = False,\n    mask_search_final_result_pos: list[list] | None = None,\n    mask_search_final_result_neg: list[list] | None = None,\n    VSA_sparsity: float = 0.0,\n    logging_info: PipelineLoggingInfo = PipelineLoggingInfo(),\n)\n</code></pre> <p>Complete state passed through the pipeline execution.</p> <p>This dataclass contains all information needed during the diffusion pipeline execution, allowing methods to update specific components without needing to manage numerous individual parameters.</p> Functions\u00b6 fastvideo.pipelines.pipeline_batch_info.ForwardBatch.__post_init__ \u00b6 <pre><code>__post_init__()\n</code></pre> <p>Initialize dependent fields after dataclass initialization.</p> Source code in <code>fastvideo/pipelines/pipeline_batch_info.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Initialize dependent fields after dataclass initialization.\"\"\"\n\n    # Set do_classifier_free_guidance based on guidance scale and negative prompt\n    if self.guidance_scale &gt; 1.0:\n        self.do_classifier_free_guidance = True\n    if self.negative_prompt_embeds is None:\n        self.negative_prompt_embeds = []\n    if self.guidance_scale_2 is None:\n        self.guidance_scale_2 = self.guidance_scale\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.pipeline_batch_info.PipelineLoggingInfo","title":"fastvideo.pipelines.pipeline_batch_info.PipelineLoggingInfo","text":"<pre><code>PipelineLoggingInfo()\n</code></pre> <p>Simple approach using OrderedDict to track stage metrics.</p> Source code in <code>fastvideo/pipelines/pipeline_batch_info.py</code> <pre><code>def __init__(self):\n    # OrderedDict preserves insertion order and allows easy access\n    self.stages: OrderedDict[str, dict[str, Any]] = OrderedDict()\n</code></pre> Functions\u00b6 fastvideo.pipelines.pipeline_batch_info.PipelineLoggingInfo.add_stage_execution_time \u00b6 <pre><code>add_stage_execution_time(\n    stage_name: str, execution_time: float\n)\n</code></pre> <p>Add execution time for a stage.</p> Source code in <code>fastvideo/pipelines/pipeline_batch_info.py</code> <pre><code>def add_stage_execution_time(self, stage_name: str, execution_time: float):\n    \"\"\"Add execution time for a stage.\"\"\"\n    if stage_name not in self.stages:\n        self.stages[stage_name] = {}\n    self.stages[stage_name]['execution_time'] = execution_time\n    self.stages[stage_name]['timestamp'] = time.time()\n</code></pre> fastvideo.pipelines.pipeline_batch_info.PipelineLoggingInfo.add_stage_metric \u00b6 <pre><code>add_stage_metric(\n    stage_name: str, metric_name: str, value: Any\n)\n</code></pre> <p>Add any metric for a stage.</p> Source code in <code>fastvideo/pipelines/pipeline_batch_info.py</code> <pre><code>def add_stage_metric(self, stage_name: str, metric_name: str, value: Any):\n    \"\"\"Add any metric for a stage.\"\"\"\n    if stage_name not in self.stages:\n        self.stages[stage_name] = {}\n    self.stages[stage_name][metric_name] = value\n</code></pre> fastvideo.pipelines.pipeline_batch_info.PipelineLoggingInfo.get_execution_order \u00b6 <pre><code>get_execution_order() -&gt; list[str]\n</code></pre> <p>Get stages in execution order.</p> Source code in <code>fastvideo/pipelines/pipeline_batch_info.py</code> <pre><code>def get_execution_order(self) -&gt; list[str]:\n    \"\"\"Get stages in execution order.\"\"\"\n    return list(self.stages.keys())\n</code></pre> fastvideo.pipelines.pipeline_batch_info.PipelineLoggingInfo.get_stage_info \u00b6 <pre><code>get_stage_info(stage_name: str) -&gt; dict[str, Any]\n</code></pre> <p>Get all info for a specific stage.</p> Source code in <code>fastvideo/pipelines/pipeline_batch_info.py</code> <pre><code>def get_stage_info(self, stage_name: str) -&gt; dict[str, Any]:\n    \"\"\"Get all info for a specific stage.\"\"\"\n    return self.stages.get(stage_name, {})\n</code></pre> fastvideo.pipelines.pipeline_batch_info.PipelineLoggingInfo.get_total_execution_time \u00b6 <pre><code>get_total_execution_time() -&gt; float\n</code></pre> <p>Get total pipeline execution time.</p> Source code in <code>fastvideo/pipelines/pipeline_batch_info.py</code> <pre><code>def get_total_execution_time(self) -&gt; float:\n    \"\"\"Get total pipeline execution time.\"\"\"\n    return sum(\n        stage.get('execution_time', 0) for stage in self.stages.values())\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.pipeline_registry","title":"fastvideo.pipelines.pipeline_registry","text":""},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.pipeline_registry-classes","title":"Classes","text":""},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.pipeline_registry.PipelineType","title":"fastvideo.pipelines.pipeline_registry.PipelineType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration for different pipeline types.</p> <p>Inherits from str to allow string comparison for backward compatibility.</p> Functions\u00b6 fastvideo.pipelines.pipeline_registry.PipelineType.choices <code>classmethod</code> \u00b6 <pre><code>choices() -&gt; list[str]\n</code></pre> <p>Get all available choices as strings.</p> Source code in <code>fastvideo/pipelines/pipeline_registry.py</code> <pre><code>@classmethod\ndef choices(cls) -&gt; list[str]:\n    \"\"\"Get all available choices as strings.\"\"\"\n    return [pipeline_type.value for pipeline_type in cls]\n</code></pre> fastvideo.pipelines.pipeline_registry.PipelineType.from_string <code>classmethod</code> \u00b6 <pre><code>from_string(value: str) -&gt; PipelineType\n</code></pre> <p>Convert string to PipelineType enum.</p> Source code in <code>fastvideo/pipelines/pipeline_registry.py</code> <pre><code>@classmethod\ndef from_string(cls, value: str) -&gt; \"PipelineType\":\n    \"\"\"Convert string to PipelineType enum.\"\"\"\n    try:\n        return cls(value.lower())\n    except ValueError:\n        raise ValueError(\n            f\"Invalid pipeline type: {value}. Must be one of: {', '.join([t.value for t in cls])}\"\n        ) from None\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.pipeline_registry-functions","title":"Functions","text":""},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.pipeline_registry.get_pipeline_registry","title":"fastvideo.pipelines.pipeline_registry.get_pipeline_registry","text":"<pre><code>get_pipeline_registry(\n    pipeline_type: PipelineType | str | None = None,\n) -&gt; _PipelineRegistry\n</code></pre> <p>Get a pipeline registry for the specified mode, pipeline type, and workload type.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_type</code> <code>PipelineType | str | None</code> <p>Pipeline type to load. If None and mode is provided, will be derived from mode.</p> <code>None</code> <p>Returns:</p> Type Description <code>_PipelineRegistry</code> <p>A pipeline registry instance.</p> Source code in <code>fastvideo/pipelines/pipeline_registry.py</code> <pre><code>def get_pipeline_registry(\n        pipeline_type: PipelineType | str | None = None) -&gt; _PipelineRegistry:\n    \"\"\"\n    Get a pipeline registry for the specified mode, pipeline type, and workload type.\n\n    Args:\n        pipeline_type: Pipeline type to load. If None and mode is provided, will be derived from mode.\n\n    Returns:\n        A pipeline registry instance.\n    \"\"\"\n    if isinstance(pipeline_type, str):\n        pipeline_type = PipelineType.from_string(pipeline_type)\n\n    pipeline_classes = import_pipeline_classes(pipeline_type)\n    return _PipelineRegistry(pipeline_classes)\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.pipeline_registry.import_pipeline_classes","title":"fastvideo.pipelines.pipeline_registry.import_pipeline_classes  <code>cached</code>","text":"<pre><code>import_pipeline_classes(\n    pipeline_types: list[PipelineType]\n    | PipelineType\n    | None = None,\n) -&gt; dict[\n    str,\n    dict[str, dict[str, type[ComposedPipelineBase] | None]],\n]\n</code></pre> <p>Import pipeline classes based on the pipeline type and workload type.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_types</code> <code>list[PipelineType] | PipelineType | None</code> <p>The pipeline types to load (basic, preprocess, training).            If None, loads all types.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, dict[str, dict[str, type[ComposedPipelineBase] | None]]]</code> <p>A three-level nested dictionary:</p> <code>dict[str, dict[str, dict[str, type[ComposedPipelineBase] | None]]]</code> <p>{pipeline_type: {architecture_name: {pipeline_name: pipeline_cls}}}</p> <code>dict[str, dict[str, dict[str, type[ComposedPipelineBase] | None]]]</code> <p>e.g., {\"basic\": {\"wan\": {\"WanPipeline\": WanPipeline}}}</p> Source code in <code>fastvideo/pipelines/pipeline_registry.py</code> <pre><code>@lru_cache\ndef import_pipeline_classes(\n    pipeline_types: list[PipelineType] | PipelineType | None = None\n) -&gt; dict[str, dict[str, dict[str, type[ComposedPipelineBase] | None]]]:\n    \"\"\"\n    Import pipeline classes based on the pipeline type and workload type.\n\n    Args:\n        pipeline_types: The pipeline types to load (basic, preprocess, training). \n                      If None, loads all types.\n\n    Returns:\n        A three-level nested dictionary:\n        {pipeline_type: {architecture_name: {pipeline_name: pipeline_cls}}}\n        e.g., {\"basic\": {\"wan\": {\"WanPipeline\": WanPipeline}}}\n    \"\"\"\n    type_to_arch_to_pipeline_dict: dict[str,\n                                        dict[str,\n                                             dict[str,\n                                                  type[ComposedPipelineBase]\n                                                  | None]]] = {}\n    package_name: str = \"fastvideo.pipelines\"\n\n    # Determine which pipeline types to scan\n    if isinstance(pipeline_types, list):\n        pipeline_types_to_scan = [\n            pipeline_type.value for pipeline_type in pipeline_types\n        ]\n    elif isinstance(pipeline_types, PipelineType):\n        pipeline_types_to_scan = [pipeline_types.value]\n    else:\n        pipeline_types_to_scan = [pt.value for pt in PipelineType]\n\n    logger.info(\"Loading pipelines for types: %s\", pipeline_types_to_scan)\n\n    for pipeline_type_str in pipeline_types_to_scan:\n        arch_to_pipeline_dict: dict[str, dict[str, type[ComposedPipelineBase]\n                                              | None]] = {}\n\n        # Try to load from pipeline-type-specific directory first\n        pipeline_type_package_name = f\"{package_name}.{pipeline_type_str}\"\n\n        try:\n            pipeline_type_package = importlib.import_module(\n                pipeline_type_package_name)\n            logger.debug(\"Successfully imported %s\", pipeline_type_package_name)\n\n            for _, arch, ispkg in pkgutil.iter_modules(\n                    pipeline_type_package.__path__):\n                pipeline_dict: dict[str, type[ComposedPipelineBase] | None] = {}\n\n                arch_package_name = f\"{pipeline_type_package_name}.{arch}\"\n                if ispkg:\n                    arch_package = importlib.import_module(arch_package_name)\n                    for _, module_name, ispkg in pkgutil.walk_packages(\n                            arch_package.__path__, arch_package_name + \".\"):\n                        if not ispkg:\n                            pipeline_module = importlib.import_module(\n                                module_name)\n                            if hasattr(pipeline_module, \"EntryClass\"):\n                                if isinstance(pipeline_module.EntryClass, list):\n                                    for pipeline in pipeline_module.EntryClass:\n                                        pipeline_name = pipeline.__name__\n                                        assert (\n                                            pipeline_name not in pipeline_dict\n                                        ), f\"Duplicated pipeline implementation for {pipeline_name} in {pipeline_type_str}.{arch_package_name}\"\n                                        pipeline_dict[pipeline_name] = pipeline\n                                else:\n                                    pipeline_name = pipeline_module.EntryClass.__name__\n                                    assert (\n                                        pipeline_name not in pipeline_dict\n                                    ), f\"Duplicated pipeline implementation for {pipeline_name} in {pipeline_type_str}.{arch_package_name}\"\n                                    pipeline_dict[\n                                        pipeline_name] = pipeline_module.EntryClass\n\n                arch_to_pipeline_dict[arch] = pipeline_dict\n\n        except ImportError as e:\n            raise ImportError(\n                f\"Could not import {pipeline_type_package_name} when importing pipeline classes: {e}\"\n            ) from None\n\n        type_to_arch_to_pipeline_dict[pipeline_type_str] = arch_to_pipeline_dict\n\n    # Log summary\n    total_pipelines = sum(\n        len(pipeline_dict)\n        for arch_to_pipeline_dict in type_to_arch_to_pipeline_dict.values()\n        for pipeline_dict in arch_to_pipeline_dict.values())\n    logger.info(\"Loaded %d pipeline classes across %d types\", total_pipelines,\n                len(pipeline_types_to_scan))\n\n    return type_to_arch_to_pipeline_dict\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.preprocess","title":"fastvideo.pipelines.preprocess","text":""},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.preprocess-modules","title":"Modules","text":""},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.preprocess.preprocess_pipeline_base","title":"fastvideo.pipelines.preprocess.preprocess_pipeline_base","text":"Classes\u00b6 fastvideo.pipelines.preprocess.preprocess_pipeline_base.BasePreprocessPipeline \u00b6 <pre><code>BasePreprocessPipeline(\n    model_path: str,\n    fastvideo_args: FastVideoArgs | TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>ComposedPipelineBase</code></p> <p>Base class for preprocessing pipelines that handles common functionality.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def __init__(self,\n             model_path: str,\n             fastvideo_args: FastVideoArgs | TrainingArgs,\n             required_config_modules: list[str] | None = None,\n             loaded_modules: dict[str, torch.nn.Module] | None = None):\n    \"\"\"\n    Initialize the pipeline. After __init__, the pipeline should be ready to\n    use. The pipeline should be stateless and not hold any batch state.\n    \"\"\"\n    self.fastvideo_args = fastvideo_args\n\n    self.model_path: str = model_path\n    self._stages: list[PipelineStage] = []\n    self._stage_name_mapping: dict[str, PipelineStage] = {}\n\n    if required_config_modules is not None:\n        self._required_config_modules = required_config_modules\n\n    if self._required_config_modules is None:\n        raise NotImplementedError(\n            \"Subclass must set _required_config_modules\")\n\n    maybe_init_distributed_environment_and_model_parallel(\n        fastvideo_args.tp_size, fastvideo_args.sp_size)\n\n    # Torch profiler. Enabled and configured through env vars:\n    # FASTVIDEO_TORCH_PROFILER_DIR=/path/to/save/trace\n    trace_dir = envs.FASTVIDEO_TORCH_PROFILER_DIR\n    self.profiler_controller = get_or_create_profiler(trace_dir)\n    self.profiler = self.profiler_controller.profiler\n\n    self.local_rank = get_world_group().local_rank\n\n    # Load modules directly in initialization\n    logger.info(\"Loading pipeline modules...\")\n    with self.profiler_controller.region(\"profiler_region_model_loading\"):\n        self.modules = self.load_modules(fastvideo_args, loaded_modules)\n</code></pre> Functions\u00b6 fastvideo.pipelines.preprocess.preprocess_pipeline_base.BasePreprocessPipeline.create_pipeline_stages \u00b6 <pre><code>create_pipeline_stages(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Set up pipeline stages with proper dependency injection.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_pipeline_base.py</code> <pre><code>def create_pipeline_stages(self, fastvideo_args: FastVideoArgs):\n    \"\"\"Set up pipeline stages with proper dependency injection.\"\"\"\n    self.add_stage(stage_name=\"prompt_encoding_stage\",\n                   stage=TextEncodingStage(\n                       text_encoders=[self.get_module(\"text_encoder\")],\n                       tokenizers=[self.get_module(\"tokenizer\")],\n                   ))\n</code></pre> fastvideo.pipelines.preprocess.preprocess_pipeline_base.BasePreprocessPipeline.create_record \u00b6 <pre><code>create_record(\n    video_name: str,\n    vae_latent: ndarray,\n    text_embedding: ndarray,\n    valid_data: dict[str, Any],\n    idx: int,\n    extra_features: dict[str, Any] | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Create a record for the Parquet dataset.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_pipeline_base.py</code> <pre><code>def create_record(\n        self,\n        video_name: str,\n        vae_latent: np.ndarray,\n        text_embedding: np.ndarray,\n        valid_data: dict[str, Any],\n        idx: int,\n        extra_features: dict[str, Any] | None = None) -&gt; dict[str, Any]:\n    \"\"\"Create a record for the Parquet dataset.\"\"\"\n    record = {\n        \"id\":\n        video_name,\n        \"vae_latent_bytes\":\n        vae_latent.tobytes(),\n        \"vae_latent_shape\":\n        list(vae_latent.shape),\n        \"vae_latent_dtype\":\n        str(vae_latent.dtype),\n        \"text_embedding_bytes\":\n        text_embedding.tobytes(),\n        \"text_embedding_shape\":\n        list(text_embedding.shape),\n        \"text_embedding_dtype\":\n        str(text_embedding.dtype),\n        \"file_name\":\n        video_name,\n        \"caption\":\n        valid_data[\"text\"][idx] if len(valid_data[\"text\"]) &gt; 0 else \"\",\n        \"media_type\":\n        \"video\",\n        \"width\":\n        valid_data[\"pixel_values\"][idx].shape[-2]\n        if len(valid_data[\"pixel_values\"]) &gt; 0 else 0,\n        \"height\":\n        valid_data[\"pixel_values\"][idx].shape[-1]\n        if len(valid_data[\"pixel_values\"]) &gt; 0 else 0,\n        \"num_frames\":\n        vae_latent.shape[1] if len(vae_latent.shape) &gt; 1 else 0,\n        \"duration_sec\":\n        float(valid_data[\"duration\"][idx])\n        if len(valid_data[\"duration\"]) &gt; 0 else 0.0,\n        \"fps\":\n        float(valid_data[\"fps\"][idx])\n        if len(valid_data[\"fps\"]) &gt; 0 else 0.0,\n    }\n    if extra_features:\n        record.update(extra_features)\n    return record\n</code></pre> fastvideo.pipelines.preprocess.preprocess_pipeline_base.BasePreprocessPipeline.create_record_for_schema \u00b6 <pre><code>create_record_for_schema(\n    preprocess_batch: PreprocessBatch,\n    schema: Schema,\n    strict: bool = False,\n) -&gt; dict[str, Any]\n</code></pre> <p>Create a record for the Parquet dataset using a generic schema-based approach.</p> <p>Parameters:</p> Name Type Description Default <code>preprocess_batch</code> <code>PreprocessBatch</code> <p>The batch containing the data to extract</p> required <code>schema</code> <code>Schema</code> <p>PyArrow schema defining the expected fields</p> required <code>strict</code> <code>bool</code> <p>If True, raises an exception when required fields are missing or unfilled</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary record matching the schema</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If strict=True and required fields are missing or unfilled</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_pipeline_base.py</code> <pre><code>def create_record_for_schema(self,\n                             preprocess_batch: PreprocessBatch,\n                             schema: pa.Schema,\n                             strict: bool = False) -&gt; dict[str, Any]:\n    \"\"\"Create a record for the Parquet dataset using a generic schema-based approach.\n\n    Args:\n        preprocess_batch: The batch containing the data to extract\n        schema: PyArrow schema defining the expected fields\n        strict: If True, raises an exception when required fields are missing or unfilled\n\n    Returns:\n        Dictionary record matching the schema\n\n    Raises:\n        ValueError: If strict=True and required fields are missing or unfilled\n    \"\"\"\n    record = {}\n    unfilled_fields = []\n\n    for field in schema.names:\n        field_filled = False\n\n        if field.endswith('_bytes'):\n            # Handle binary tensor data - convert numpy array or tensor to bytes\n            tensor_name = field.replace('_bytes', '')\n            tensor_data = getattr(preprocess_batch, tensor_name, None)\n            if tensor_data is not None:\n                try:\n                    if hasattr(tensor_data, 'numpy'):  # torch tensor\n                        record[field] = tensor_data.cpu().numpy().tobytes()\n                        field_filled = True\n                    elif hasattr(tensor_data, 'tobytes'):  # numpy array\n                        record[field] = tensor_data.tobytes()\n                        field_filled = True\n                    else:\n                        raise ValueError(\n                            f\"Unsupported tensor type for field {field}: {type(tensor_data)}\"\n                        )\n                except Exception as e:\n                    if strict:\n                        raise ValueError(\n                            f\"Failed to convert tensor {tensor_name} to bytes: {e}\"\n                        ) from e\n                    record[field] = b''  # Empty bytes for missing data\n            else:\n                record[field] = b''  # Empty bytes for missing data\n\n        elif field.endswith('_shape'):\n            # Handle tensor shape info\n            tensor_name = field.replace('_shape', '')\n            tensor_data = getattr(preprocess_batch, tensor_name, None)\n            if tensor_data is not None and hasattr(tensor_data, 'shape'):\n                record[field] = list(tensor_data.shape)\n                field_filled = True\n            else:\n                record[field] = []\n\n        elif field.endswith('_dtype'):\n            # Handle tensor dtype info\n            tensor_name = field.replace('_dtype', '')\n            tensor_data = getattr(preprocess_batch, tensor_name, None)\n            if tensor_data is not None and hasattr(tensor_data, 'dtype'):\n                record[field] = str(tensor_data.dtype)\n                field_filled = True\n            else:\n                record[field] = 'unknown'\n\n        elif field in ['width', 'height', 'num_frames']:\n            # Handle integer metadata fields\n            value = getattr(preprocess_batch, field, None)\n            if value is not None:\n                try:\n                    record[field] = int(value)\n                    field_filled = True\n                except (ValueError, TypeError) as e:\n                    if strict:\n                        raise ValueError(\n                            f\"Failed to convert field {field} to int: {e}\"\n                        ) from e\n                    record[field] = 0\n            else:\n                record[field] = 0\n\n        elif field in ['duration_sec', 'fps']:\n            # Handle float metadata fields\n            # Map schema field names to batch attribute names\n            attr_name = 'duration' if field == 'duration_sec' else field\n            value = getattr(preprocess_batch, attr_name, None)\n            if value is not None:\n                try:\n                    record[field] = float(value)\n                    field_filled = True\n                except (ValueError, TypeError) as e:\n                    if strict:\n                        raise ValueError(\n                            f\"Failed to convert field {field} to float: {e}\"\n                        ) from e\n                    record[field] = 0.0\n            else:\n                record[field] = 0.0\n\n        else:\n            # Handle string fields (id, file_name, caption, media_type, etc.)\n            # Map common schema field names to batch attribute names\n            attr_name = field\n            if field == 'caption':\n                attr_name = 'text'\n            elif field == 'file_name':\n                attr_name = 'path'\n            elif field == 'id':\n                # Generate ID from path if available\n                path_value = getattr(preprocess_batch, 'path', None)\n                if path_value:\n                    import os\n                    record[field] = os.path.basename(path_value).split(\n                        '.')[0]\n                    field_filled = True\n                else:\n                    record[field] = \"\"\n                continue\n            elif field == 'media_type':\n                # Determine media type from path\n                path_value = getattr(preprocess_batch, 'path', None)\n                if path_value:\n                    record[field] = 'video' if path_value.endswith(\n                        '.mp4') else 'image'\n                    field_filled = True\n                else:\n                    record[field] = \"\"\n                continue\n\n            value = getattr(preprocess_batch, attr_name, None)\n            if value is not None:\n                record[field] = str(value)\n                field_filled = True\n            else:\n                record[field] = \"\"\n\n        # Track unfilled fields\n        if not field_filled:\n            unfilled_fields.append(field)\n\n    # Handle strict mode\n    if strict and unfilled_fields:\n        raise ValueError(\n            f\"Required fields were not filled: {unfilled_fields}\")\n\n    # Log unfilled fields as warning if not in strict mode\n    if unfilled_fields:\n        logger.warning(\n            \"Some fields were not filled and got default values: %s\",\n            unfilled_fields)\n\n    return record\n</code></pre> fastvideo.pipelines.preprocess.preprocess_pipeline_base.BasePreprocessPipeline.get_extra_features \u00b6 <pre><code>get_extra_features(\n    valid_data: dict[str, Any],\n    fastvideo_args: FastVideoArgs,\n) -&gt; dict[str, Any]\n</code></pre> <p>Get additional features specific to the pipeline type. Override in subclasses.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_pipeline_base.py</code> <pre><code>def get_extra_features(self, valid_data: dict[str, Any],\n                       fastvideo_args: FastVideoArgs) -&gt; dict[str, Any]:\n    \"\"\"Get additional features specific to the pipeline type. Override in subclasses.\"\"\"\n    return {}\n</code></pre> fastvideo.pipelines.preprocess.preprocess_pipeline_base.BasePreprocessPipeline.get_pyarrow_schema \u00b6 <pre><code>get_pyarrow_schema() -&gt; pa.Schema\n</code></pre> <p>Return the PyArrow schema for this pipeline. Must be overridden.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_pipeline_base.py</code> <pre><code>def get_pyarrow_schema(self) -&gt; pa.Schema:\n    \"\"\"Return the PyArrow schema for this pipeline. Must be overridden.\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.pipelines.preprocess.preprocess_pipeline_base.BasePreprocessPipeline.get_schema_fields \u00b6 <pre><code>get_schema_fields() -&gt; list[str]\n</code></pre> <p>Get the schema fields for the pipeline type.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_pipeline_base.py</code> <pre><code>def get_schema_fields(self) -&gt; list[str]:\n    \"\"\"Get the schema fields for the pipeline type.\"\"\"\n    return [f.name for f in self.get_pyarrow_schema()]\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.preprocess.preprocess_pipeline_i2v","title":"fastvideo.pipelines.preprocess.preprocess_pipeline_i2v","text":"<p>I2V Data Preprocessing pipeline implementation.</p> <p>This module contains an implementation of the I2V Data Preprocessing pipeline using the modular pipeline architecture.</p> Classes\u00b6 fastvideo.pipelines.preprocess.preprocess_pipeline_i2v.PreprocessPipeline_I2V \u00b6 <pre><code>PreprocessPipeline_I2V(\n    model_path: str,\n    fastvideo_args: FastVideoArgs | TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>BasePreprocessPipeline</code></p> <p>I2V preprocessing pipeline implementation.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def __init__(self,\n             model_path: str,\n             fastvideo_args: FastVideoArgs | TrainingArgs,\n             required_config_modules: list[str] | None = None,\n             loaded_modules: dict[str, torch.nn.Module] | None = None):\n    \"\"\"\n    Initialize the pipeline. After __init__, the pipeline should be ready to\n    use. The pipeline should be stateless and not hold any batch state.\n    \"\"\"\n    self.fastvideo_args = fastvideo_args\n\n    self.model_path: str = model_path\n    self._stages: list[PipelineStage] = []\n    self._stage_name_mapping: dict[str, PipelineStage] = {}\n\n    if required_config_modules is not None:\n        self._required_config_modules = required_config_modules\n\n    if self._required_config_modules is None:\n        raise NotImplementedError(\n            \"Subclass must set _required_config_modules\")\n\n    maybe_init_distributed_environment_and_model_parallel(\n        fastvideo_args.tp_size, fastvideo_args.sp_size)\n\n    # Torch profiler. Enabled and configured through env vars:\n    # FASTVIDEO_TORCH_PROFILER_DIR=/path/to/save/trace\n    trace_dir = envs.FASTVIDEO_TORCH_PROFILER_DIR\n    self.profiler_controller = get_or_create_profiler(trace_dir)\n    self.profiler = self.profiler_controller.profiler\n\n    self.local_rank = get_world_group().local_rank\n\n    # Load modules directly in initialization\n    logger.info(\"Loading pipeline modules...\")\n    with self.profiler_controller.region(\"profiler_region_model_loading\"):\n        self.modules = self.load_modules(fastvideo_args, loaded_modules)\n</code></pre> Functions\u00b6 fastvideo.pipelines.preprocess.preprocess_pipeline_i2v.PreprocessPipeline_I2V.create_record \u00b6 <pre><code>create_record(\n    video_name: str,\n    vae_latent: ndarray,\n    text_embedding: ndarray,\n    valid_data: dict[str, Any],\n    idx: int,\n    extra_features: dict[str, Any] | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Create a record for the Parquet dataset with CLIP features.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_pipeline_i2v.py</code> <pre><code>def create_record(\n        self,\n        video_name: str,\n        vae_latent: np.ndarray,\n        text_embedding: np.ndarray,\n        valid_data: dict[str, Any],\n        idx: int,\n        extra_features: dict[str, Any] | None = None) -&gt; dict[str, Any]:\n    \"\"\"Create a record for the Parquet dataset with CLIP features.\"\"\"\n    record = super().create_record(video_name=video_name,\n                                   vae_latent=vae_latent,\n                                   text_embedding=text_embedding,\n                                   valid_data=valid_data,\n                                   idx=idx,\n                                   extra_features=extra_features)\n\n    if extra_features and \"clip_feature\" in extra_features:\n        clip_feature = extra_features[\"clip_feature\"]\n        record.update({\n            \"clip_feature_bytes\": clip_feature.tobytes(),\n            \"clip_feature_shape\": list(clip_feature.shape),\n            \"clip_feature_dtype\": str(clip_feature.dtype),\n        })\n    else:\n        record.update({\n            \"clip_feature_bytes\": b\"\",\n            \"clip_feature_shape\": [],\n            \"clip_feature_dtype\": \"\",\n        })\n\n    if extra_features and \"first_frame_latent\" in extra_features:\n        first_frame_latent = extra_features[\"first_frame_latent\"]\n        record.update({\n            \"first_frame_latent_bytes\":\n            first_frame_latent.tobytes(),\n            \"first_frame_latent_shape\":\n            list(first_frame_latent.shape),\n            \"first_frame_latent_dtype\":\n            str(first_frame_latent.dtype),\n        })\n    else:\n        record.update({\n            \"first_frame_latent_bytes\": b\"\",\n            \"first_frame_latent_shape\": [],\n            \"first_frame_latent_dtype\": \"\",\n        })\n\n    if extra_features and \"pil_image\" in extra_features:\n        pil_image = extra_features[\"pil_image\"]\n        record.update({\n            \"pil_image_bytes\": pil_image.tobytes(),\n            \"pil_image_shape\": list(pil_image.shape),\n            \"pil_image_dtype\": str(pil_image.dtype),\n        })\n    else:\n        record.update({\n            \"pil_image_bytes\": b\"\",\n            \"pil_image_shape\": [],\n            \"pil_image_dtype\": \"\",\n        })\n\n    return record\n</code></pre> fastvideo.pipelines.preprocess.preprocess_pipeline_i2v.PreprocessPipeline_I2V.get_pyarrow_schema \u00b6 <pre><code>get_pyarrow_schema()\n</code></pre> <p>Return the PyArrow schema for I2V pipeline.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_pipeline_i2v.py</code> <pre><code>def get_pyarrow_schema(self):\n    \"\"\"Return the PyArrow schema for I2V pipeline.\"\"\"\n    return pyarrow_schema_i2v\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.preprocess.preprocess_pipeline_ode_trajectory","title":"fastvideo.pipelines.preprocess.preprocess_pipeline_ode_trajectory","text":"<p>ODE Trajectory Data Preprocessing pipeline implementation.</p> <p>This module contains an implementation of the ODE Trajectory Data Preprocessing pipeline using the modular pipeline architecture.</p> <p>Sec 4.3 of CausVid paper: https://arxiv.org/pdf/2412.07772</p> Classes\u00b6 fastvideo.pipelines.preprocess.preprocess_pipeline_ode_trajectory.PreprocessPipeline_ODE_Trajectory \u00b6 <pre><code>PreprocessPipeline_ODE_Trajectory(\n    model_path: str,\n    fastvideo_args: FastVideoArgs | TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>BasePreprocessPipeline</code></p> <p>ODE Trajectory preprocessing pipeline implementation.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def __init__(self,\n             model_path: str,\n             fastvideo_args: FastVideoArgs | TrainingArgs,\n             required_config_modules: list[str] | None = None,\n             loaded_modules: dict[str, torch.nn.Module] | None = None):\n    \"\"\"\n    Initialize the pipeline. After __init__, the pipeline should be ready to\n    use. The pipeline should be stateless and not hold any batch state.\n    \"\"\"\n    self.fastvideo_args = fastvideo_args\n\n    self.model_path: str = model_path\n    self._stages: list[PipelineStage] = []\n    self._stage_name_mapping: dict[str, PipelineStage] = {}\n\n    if required_config_modules is not None:\n        self._required_config_modules = required_config_modules\n\n    if self._required_config_modules is None:\n        raise NotImplementedError(\n            \"Subclass must set _required_config_modules\")\n\n    maybe_init_distributed_environment_and_model_parallel(\n        fastvideo_args.tp_size, fastvideo_args.sp_size)\n\n    # Torch profiler. Enabled and configured through env vars:\n    # FASTVIDEO_TORCH_PROFILER_DIR=/path/to/save/trace\n    trace_dir = envs.FASTVIDEO_TORCH_PROFILER_DIR\n    self.profiler_controller = get_or_create_profiler(trace_dir)\n    self.profiler = self.profiler_controller.profiler\n\n    self.local_rank = get_world_group().local_rank\n\n    # Load modules directly in initialization\n    logger.info(\"Loading pipeline modules...\")\n    with self.profiler_controller.region(\"profiler_region_model_loading\"):\n        self.modules = self.load_modules(fastvideo_args, loaded_modules)\n</code></pre> Functions\u00b6 fastvideo.pipelines.preprocess.preprocess_pipeline_ode_trajectory.PreprocessPipeline_ODE_Trajectory.create_pipeline_stages \u00b6 <pre><code>create_pipeline_stages(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Set up pipeline stages with proper dependency injection.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_pipeline_ode_trajectory.py</code> <pre><code>def create_pipeline_stages(self, fastvideo_args: FastVideoArgs):\n    \"\"\"Set up pipeline stages with proper dependency injection.\"\"\"\n    assert fastvideo_args.pipeline_config.flow_shift == 5\n    self.modules[\"scheduler\"] = SelfForcingFlowMatchScheduler(\n        shift=fastvideo_args.pipeline_config.flow_shift,\n        sigma_min=0.0,\n        extra_one_step=True)\n    self.modules[\"scheduler\"].set_timesteps(num_inference_steps=48,\n                                            denoising_strength=1.0)\n\n    self.add_stage(stage_name=\"input_validation_stage\",\n                   stage=InputValidationStage())\n    self.add_stage(stage_name=\"prompt_encoding_stage\",\n                   stage=TextEncodingStage(\n                       text_encoders=[self.get_module(\"text_encoder\")],\n                       tokenizers=[self.get_module(\"tokenizer\")],\n                   ))\n    self.add_stage(stage_name=\"timestep_preparation_stage\",\n                   stage=TimestepPreparationStage(\n                       scheduler=self.get_module(\"scheduler\")))\n    self.add_stage(stage_name=\"latent_preparation_stage\",\n                   stage=LatentPreparationStage(\n                       scheduler=self.get_module(\"scheduler\"),\n                       transformer=self.get_module(\"transformer\", None)))\n    self.add_stage(stage_name=\"denoising_stage\",\n                   stage=DenoisingStage(\n                       transformer=self.get_module(\"transformer\"),\n                       scheduler=self.get_module(\"scheduler\"),\n                       pipeline=self,\n                   ))\n    self.add_stage(stage_name=\"decoding_stage\",\n                   stage=DecodingStage(vae=self.get_module(\"vae\")))\n</code></pre> fastvideo.pipelines.preprocess.preprocess_pipeline_ode_trajectory.PreprocessPipeline_ODE_Trajectory.get_pyarrow_schema \u00b6 <pre><code>get_pyarrow_schema() -&gt; pa.Schema\n</code></pre> <p>Return the PyArrow schema for ODE Trajectory pipeline.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_pipeline_ode_trajectory.py</code> <pre><code>def get_pyarrow_schema(self) -&gt; pa.Schema:\n    \"\"\"Return the PyArrow schema for ODE Trajectory pipeline.\"\"\"\n    return pyarrow_schema_ode_trajectory_text_only\n</code></pre> fastvideo.pipelines.preprocess.preprocess_pipeline_ode_trajectory.PreprocessPipeline_ODE_Trajectory.preprocess_text_and_trajectory \u00b6 <pre><code>preprocess_text_and_trajectory(\n    fastvideo_args: FastVideoArgs, args\n)\n</code></pre> <p>Preprocess text-only data and generate trajectory information.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_pipeline_ode_trajectory.py</code> <pre><code>def preprocess_text_and_trajectory(self, fastvideo_args: FastVideoArgs,\n                                   args):\n    \"\"\"Preprocess text-only data and generate trajectory information.\"\"\"\n\n    for batch_idx, data in enumerate(self.pbar):\n        if data is None:\n            continue\n\n        with torch.inference_mode():\n            # For text-only processing, we only need text data\n            # Filter out samples without text\n            valid_indices = []\n            for i, text in enumerate(data[\"text\"]):\n                if text and text.strip():  # Check if text is not empty\n                    valid_indices.append(i)\n            self.num_processed_samples += len(valid_indices)\n\n            if not valid_indices:\n                continue\n\n            # Create new batch with only valid samples (text-only)\n            valid_data = {\n                \"text\": [data[\"text\"][i] for i in valid_indices],\n                \"path\": [data[\"path\"][i] for i in valid_indices],\n            }\n\n            # Add fps and duration if available in data\n            if \"fps\" in data:\n                valid_data[\"fps\"] = [data[\"fps\"][i] for i in valid_indices]\n            if \"duration\" in data:\n                valid_data[\"duration\"] = [\n                    data[\"duration\"][i] for i in valid_indices\n                ]\n\n            batch_captions = valid_data[\"text\"]\n            # Encode text using the standalone TextEncodingStage API\n            prompt_embeds_list, prompt_masks_list = self.prompt_encoding_stage.encode_text(\n                batch_captions,\n                fastvideo_args,\n                encoder_index=[0],\n                return_attention_mask=True,\n            )\n            prompt_embeds = prompt_embeds_list[0]\n            prompt_attention_masks = prompt_masks_list[0]\n            assert prompt_embeds.shape[0] == prompt_attention_masks.shape[0]\n\n            sampling_params = SamplingParam.from_pretrained(args.model_path)\n\n            # encode negative prompt for trajectory collection\n            if sampling_params.guidance_scale &gt; 1 and sampling_params.negative_prompt is not None:\n                negative_prompt_embeds_list, negative_prompt_masks_list = self.prompt_encoding_stage.encode_text(\n                    sampling_params.negative_prompt,\n                    fastvideo_args,\n                    encoder_index=[0],\n                    return_attention_mask=True,\n                )\n                negative_prompt_embed = negative_prompt_embeds_list[0][0]\n                negative_prompt_attention_mask = negative_prompt_masks_list[\n                    0][0]\n            else:\n                negative_prompt_embed = None\n                negative_prompt_attention_mask = None\n\n            trajectory_latents = []\n            trajectory_timesteps = []\n            trajectory_decoded = []\n\n            for i, (prompt_embed, prompt_attention_mask) in enumerate(\n                    zip(prompt_embeds, prompt_attention_masks,\n                        strict=False)):\n                prompt_embed = prompt_embed.unsqueeze(0)\n                prompt_attention_mask = prompt_attention_mask.unsqueeze(0)\n\n                # Collect the trajectory data (text-to-video generation)\n                batch = ForwardBatch(**shallow_asdict(sampling_params), )\n                batch.prompt_embeds = [prompt_embed]\n                batch.prompt_attention_mask = [prompt_attention_mask]\n                batch.negative_prompt_embeds = [negative_prompt_embed]\n                batch.negative_attention_mask = [\n                    negative_prompt_attention_mask\n                ]\n                batch.num_inference_steps = 48\n                batch.return_trajectory_latents = True\n                # Enabling this will save the decoded trajectory videos.\n                # Used for debugging.\n                batch.return_trajectory_decoded = False\n                batch.height = args.max_height\n                batch.width = args.max_width\n                batch.fps = args.train_fps\n                batch.guidance_scale = 6.0\n                batch.do_classifier_free_guidance = True\n\n                result_batch = self.input_validation_stage(\n                    batch, fastvideo_args)\n                result_batch = self.timestep_preparation_stage(\n                    batch, fastvideo_args)\n                result_batch = self.latent_preparation_stage(\n                    result_batch, fastvideo_args)\n                result_batch = self.denoising_stage(result_batch,\n                                                    fastvideo_args)\n                result_batch = self.decoding_stage(result_batch,\n                                                   fastvideo_args)\n\n                trajectory_latents.append(\n                    result_batch.trajectory_latents.cpu())\n                trajectory_timesteps.append(\n                    result_batch.trajectory_timesteps.cpu())\n                trajectory_decoded.append(result_batch.trajectory_decoded)\n\n            # Prepare extra features for text-only processing\n            extra_features = {\n                \"trajectory_latents\": trajectory_latents,\n                \"trajectory_timesteps\": trajectory_timesteps\n            }\n\n            if batch.return_trajectory_decoded:\n                for i, decoded_frames in enumerate(trajectory_decoded):\n                    for j, decoded_frame in enumerate(decoded_frames):\n                        save_decoded_latents_as_video(\n                            decoded_frame,\n                            f\"decoded_videos/trajectory_decoded_{i}_{j}.mp4\",\n                            args.train_fps)\n\n            # Prepare batch data for Parquet dataset\n            batch_data: list[dict[str, Any]] = []\n\n            # Add progress bar for saving outputs\n            save_pbar = tqdm(enumerate(valid_data[\"path\"]),\n                             desc=\"Saving outputs\",\n                             unit=\"item\",\n                             leave=False)\n\n            for idx, video_path in save_pbar:\n                video_name = os.path.basename(video_path).split(\".\")[0]\n\n                # Convert tensors to numpy arrays\n                text_embedding = prompt_embeds[idx].cpu().numpy()\n\n                # Get extra features for this sample\n                sample_extra_features = {}\n                if extra_features:\n                    for key, value in extra_features.items():\n                        if isinstance(value, torch.Tensor):\n                            sample_extra_features[key] = value[idx].cpu(\n                            ).numpy()\n                        else:\n                            assert isinstance(value, list)\n                            if isinstance(value[idx], torch.Tensor):\n                                sample_extra_features[key] = value[idx].cpu(\n                                ).float().numpy()\n                            else:\n                                sample_extra_features[key] = value[idx]\n\n                # Create record for Parquet dataset (text-only ODE schema)\n                record: dict[str, Any] = ode_text_only_record_creator(\n                    video_name=video_name,\n                    text_embedding=text_embedding,\n                    caption=valid_data[\"text\"][idx],\n                    trajectory_latents=sample_extra_features[\n                        \"trajectory_latents\"],\n                    trajectory_timesteps=sample_extra_features[\n                        \"trajectory_timesteps\"],\n                )\n                batch_data.append(record)\n\n            if batch_data:\n                write_pbar = tqdm(total=1,\n                                  desc=\"Writing to Parquet dataset\",\n                                  unit=\"batch\")\n                table = records_to_table(batch_data,\n                                         self.get_pyarrow_schema())\n                write_pbar.update(1)\n                write_pbar.close()\n\n                if not hasattr(self, 'dataset_writer'):\n                    self.dataset_writer = ParquetDatasetWriter(\n                        out_dir=self.combined_parquet_dir,\n                        samples_per_file=args.samples_per_file,\n                    )\n                self.dataset_writer.append_table(table)\n\n                logger.info(\"Collected batch with %s samples\", len(table))\n\n            if self.num_processed_samples &gt;= args.flush_frequency:\n                written = self.dataset_writer.flush()\n                logger.info(\"Flushed %s samples to parquet\", written)\n                self.num_processed_samples = 0\n\n    # Final flush for any remaining samples\n    if hasattr(self, 'dataset_writer'):\n        written = self.dataset_writer.flush(write_remainder=True)\n        if written:\n            logger.info(\"Final flush wrote %s samples\", written)\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.preprocess.preprocess_pipeline_t2v","title":"fastvideo.pipelines.preprocess.preprocess_pipeline_t2v","text":"<p>T2V Data Preprocessing pipeline implementation.</p> <p>This module contains an implementation of the T2V Data Preprocessing pipeline using the modular pipeline architecture.</p> Classes\u00b6 fastvideo.pipelines.preprocess.preprocess_pipeline_t2v.PreprocessPipeline_T2V \u00b6 <pre><code>PreprocessPipeline_T2V(\n    model_path: str,\n    fastvideo_args: FastVideoArgs | TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>BasePreprocessPipeline</code></p> <p>T2V preprocessing pipeline implementation.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def __init__(self,\n             model_path: str,\n             fastvideo_args: FastVideoArgs | TrainingArgs,\n             required_config_modules: list[str] | None = None,\n             loaded_modules: dict[str, torch.nn.Module] | None = None):\n    \"\"\"\n    Initialize the pipeline. After __init__, the pipeline should be ready to\n    use. The pipeline should be stateless and not hold any batch state.\n    \"\"\"\n    self.fastvideo_args = fastvideo_args\n\n    self.model_path: str = model_path\n    self._stages: list[PipelineStage] = []\n    self._stage_name_mapping: dict[str, PipelineStage] = {}\n\n    if required_config_modules is not None:\n        self._required_config_modules = required_config_modules\n\n    if self._required_config_modules is None:\n        raise NotImplementedError(\n            \"Subclass must set _required_config_modules\")\n\n    maybe_init_distributed_environment_and_model_parallel(\n        fastvideo_args.tp_size, fastvideo_args.sp_size)\n\n    # Torch profiler. Enabled and configured through env vars:\n    # FASTVIDEO_TORCH_PROFILER_DIR=/path/to/save/trace\n    trace_dir = envs.FASTVIDEO_TORCH_PROFILER_DIR\n    self.profiler_controller = get_or_create_profiler(trace_dir)\n    self.profiler = self.profiler_controller.profiler\n\n    self.local_rank = get_world_group().local_rank\n\n    # Load modules directly in initialization\n    logger.info(\"Loading pipeline modules...\")\n    with self.profiler_controller.region(\"profiler_region_model_loading\"):\n        self.modules = self.load_modules(fastvideo_args, loaded_modules)\n</code></pre> Functions\u00b6 fastvideo.pipelines.preprocess.preprocess_pipeline_t2v.PreprocessPipeline_T2V.get_pyarrow_schema \u00b6 <pre><code>get_pyarrow_schema()\n</code></pre> <p>Return the PyArrow schema for T2V pipeline.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_pipeline_t2v.py</code> <pre><code>def get_pyarrow_schema(self):\n    \"\"\"Return the PyArrow schema for T2V pipeline.\"\"\"\n    return pyarrow_schema_t2v\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.preprocess.preprocess_pipeline_text","title":"fastvideo.pipelines.preprocess.preprocess_pipeline_text","text":"<p>Text-only Data Preprocessing pipeline implementation.</p> <p>This module contains an implementation of the Text-only Data Preprocessing pipeline using the modular pipeline architecture, based on the ODE Trajectory preprocessing.</p> Classes\u00b6 fastvideo.pipelines.preprocess.preprocess_pipeline_text.PreprocessPipeline_Text \u00b6 <pre><code>PreprocessPipeline_Text(\n    model_path: str,\n    fastvideo_args: FastVideoArgs | TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>BasePreprocessPipeline</code></p> <p>Text-only preprocessing pipeline implementation.</p> Source code in <code>fastvideo/pipelines/composed_pipeline_base.py</code> <pre><code>def __init__(self,\n             model_path: str,\n             fastvideo_args: FastVideoArgs | TrainingArgs,\n             required_config_modules: list[str] | None = None,\n             loaded_modules: dict[str, torch.nn.Module] | None = None):\n    \"\"\"\n    Initialize the pipeline. After __init__, the pipeline should be ready to\n    use. The pipeline should be stateless and not hold any batch state.\n    \"\"\"\n    self.fastvideo_args = fastvideo_args\n\n    self.model_path: str = model_path\n    self._stages: list[PipelineStage] = []\n    self._stage_name_mapping: dict[str, PipelineStage] = {}\n\n    if required_config_modules is not None:\n        self._required_config_modules = required_config_modules\n\n    if self._required_config_modules is None:\n        raise NotImplementedError(\n            \"Subclass must set _required_config_modules\")\n\n    maybe_init_distributed_environment_and_model_parallel(\n        fastvideo_args.tp_size, fastvideo_args.sp_size)\n\n    # Torch profiler. Enabled and configured through env vars:\n    # FASTVIDEO_TORCH_PROFILER_DIR=/path/to/save/trace\n    trace_dir = envs.FASTVIDEO_TORCH_PROFILER_DIR\n    self.profiler_controller = get_or_create_profiler(trace_dir)\n    self.profiler = self.profiler_controller.profiler\n\n    self.local_rank = get_world_group().local_rank\n\n    # Load modules directly in initialization\n    logger.info(\"Loading pipeline modules...\")\n    with self.profiler_controller.region(\"profiler_region_model_loading\"):\n        self.modules = self.load_modules(fastvideo_args, loaded_modules)\n</code></pre> Functions\u00b6 fastvideo.pipelines.preprocess.preprocess_pipeline_text.PreprocessPipeline_Text.create_pipeline_stages \u00b6 <pre><code>create_pipeline_stages(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Set up pipeline stages with proper dependency injection.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_pipeline_text.py</code> <pre><code>def create_pipeline_stages(self, fastvideo_args: FastVideoArgs):\n    \"\"\"Set up pipeline stages with proper dependency injection.\"\"\"\n    self.add_stage(stage_name=\"prompt_encoding_stage\",\n                   stage=TextEncodingStage(\n                       text_encoders=[self.get_module(\"text_encoder\")],\n                       tokenizers=[self.get_module(\"tokenizer\")],\n                   ))\n</code></pre> fastvideo.pipelines.preprocess.preprocess_pipeline_text.PreprocessPipeline_Text.get_pyarrow_schema \u00b6 <pre><code>get_pyarrow_schema()\n</code></pre> <p>Return the PyArrow schema for text-only pipeline.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_pipeline_text.py</code> <pre><code>def get_pyarrow_schema(self):\n    \"\"\"Return the PyArrow schema for text-only pipeline.\"\"\"\n    return pyarrow_schema_text_only\n</code></pre> fastvideo.pipelines.preprocess.preprocess_pipeline_text.PreprocessPipeline_Text.preprocess_text_only \u00b6 <pre><code>preprocess_text_only(fastvideo_args: FastVideoArgs, args)\n</code></pre> <p>Preprocess text-only data.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_pipeline_text.py</code> <pre><code>def preprocess_text_only(self, fastvideo_args: FastVideoArgs, args):\n    \"\"\"Preprocess text-only data.\"\"\"\n\n    for batch_idx, data in enumerate(self.pbar):\n        if data is None:\n            continue\n\n        with torch.inference_mode():\n            # For text-only processing, we only need text data\n            # Filter out samples without text\n            valid_indices = []\n            for i, text in enumerate(data[\"text\"]):\n                if text and text.strip():  # Check if text is not empty\n                    valid_indices.append(i)\n            self.num_processed_samples += len(valid_indices)\n\n            if not valid_indices:\n                continue\n\n            # Create new batch with only valid samples (text-only)\n            valid_data = {\n                \"text\": [data[\"text\"][i] for i in valid_indices],\n                \"path\": [data[\"path\"][i] for i in valid_indices],\n            }\n\n            batch_captions = valid_data[\"text\"]\n            # Encode text using the standalone TextEncodingStage API\n            prompt_embeds_list, prompt_masks_list = self.prompt_encoding_stage.encode_text(\n                batch_captions,\n                fastvideo_args,\n                encoder_index=[0],\n                return_attention_mask=True,\n            )\n            prompt_embeds = prompt_embeds_list[0]\n            prompt_attention_masks = prompt_masks_list[0]\n            assert prompt_embeds.shape[0] == prompt_attention_masks.shape[0]\n\n            logger.info(\"===== prompt_embeds: %s\", prompt_embeds.shape)\n            logger.info(\"===== prompt_attention_masks: %s\",\n                        prompt_attention_masks.shape)\n\n            # Prepare batch data for Parquet dataset\n            batch_data = []\n\n            # Add progress bar for saving outputs\n            save_pbar = tqdm(enumerate(valid_data[\"path\"]),\n                             desc=\"Saving outputs\",\n                             unit=\"item\",\n                             leave=False)\n\n            for idx, text_path in save_pbar:\n                text_name = os.path.basename(text_path).split(\".\")[0]\n\n                # Convert tensors to numpy arrays\n                text_embedding = prompt_embeds[idx].cpu().numpy()\n\n                # Create record for Parquet dataset (text-only schema)\n                record = text_only_record_creator(\n                    text_name=text_name,\n                    text_embedding=text_embedding,\n                    caption=valid_data[\"text\"][idx],\n                )\n                batch_data.append(record)\n\n            if batch_data:\n                write_pbar = tqdm(total=1,\n                                  desc=\"Writing to Parquet dataset\",\n                                  unit=\"batch\")\n                table = records_to_table(batch_data,\n                                         pyarrow_schema_text_only)\n                write_pbar.update(1)\n                write_pbar.close()\n\n                if not hasattr(self, 'dataset_writer'):\n                    self.dataset_writer = ParquetDatasetWriter(\n                        out_dir=self.combined_parquet_dir,\n                        samples_per_file=args.samples_per_file,\n                    )\n                self.dataset_writer.append_table(table)\n\n                logger.info(\"Collected batch with %s samples\", len(table))\n\n            if self.num_processed_samples &gt;= args.flush_frequency:\n                written = self.dataset_writer.flush()\n                logger.info(\"Flushed %s samples to parquet\", written)\n                self.num_processed_samples = 0\n\n    # Final flush for any remaining samples\n    if hasattr(self, 'dataset_writer'):\n        written = self.dataset_writer.flush(write_remainder=True)\n        if written:\n            logger.info(\"Final flush wrote %s samples\", written)\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.preprocess.preprocess_stages","title":"fastvideo.pipelines.preprocess.preprocess_stages","text":"Classes\u00b6 fastvideo.pipelines.preprocess.preprocess_stages.TextTransformStage \u00b6 <pre><code>TextTransformStage(\n    cfg_uncondition_drop_rate: float, seed: int\n)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Process text data according to the cfg rate.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_stages.py</code> <pre><code>def __init__(self, cfg_uncondition_drop_rate: float, seed: int) -&gt; None:\n    self.cfg_rate = cfg_uncondition_drop_rate\n    self.rng = random.Random(seed)\n</code></pre> fastvideo.pipelines.preprocess.preprocess_stages.VideoTransformStage \u00b6 <pre><code>VideoTransformStage(\n    train_fps: int,\n    num_frames: int,\n    max_height: int,\n    max_width: int,\n    do_temporal_sample: bool,\n)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Crop a video in temporal dimension.</p> Source code in <code>fastvideo/pipelines/preprocess/preprocess_stages.py</code> <pre><code>def __init__(self, train_fps: int, num_frames: int, max_height: int,\n             max_width: int, do_temporal_sample: bool) -&gt; None:\n    self.train_fps = train_fps\n    self.num_frames = num_frames\n    if do_temporal_sample:\n        self.temporal_sample_fn: Callable | None = TemporalRandomCrop(\n            num_frames)\n    else:\n        self.temporal_sample_fn = None\n\n    self.video_transform = transforms.Compose([\n        CenterCropResizeVideo((max_height, max_width)),\n    ])\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages","title":"fastvideo.pipelines.stages","text":"<p>Pipeline stages for diffusion models.</p> <p>This package contains the various stages that can be composed to create complete diffusion pipelines.</p>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages-classes","title":"Classes","text":""},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.CausalDMDDenosingStage","title":"fastvideo.pipelines.stages.CausalDMDDenosingStage","text":"<pre><code>CausalDMDDenosingStage(\n    transformer, scheduler, transformer_2=None\n)\n</code></pre> <p>               Bases: <code>DenoisingStage</code></p> <p>Denoising stage for causal diffusion.</p> Source code in <code>fastvideo/pipelines/stages/causal_denoising.py</code> <pre><code>def __init__(self, transformer, scheduler, transformer_2=None) -&gt; None:\n    super().__init__(transformer, scheduler, transformer_2)\n    # KV and cross-attention cache state (initialized on first forward)\n    self.transformer = transformer\n    self.transformer_2 = transformer_2\n    self.kv_cache1: list | None = None\n    self.crossattn_cache: list | None = None\n    # Model-dependent constants (aligned with causal_inference.py assumptions)\n    self.num_transformer_blocks = len(self.transformer.blocks)\n    self.num_frames_per_block = self.transformer.config.arch_config.num_frames_per_block\n    self.sliding_window_num_frames = self.transformer.config.arch_config.sliding_window_num_frames\n\n    try:\n        self.local_attn_size = getattr(self.transformer.model,\n                                       \"local_attn_size\",\n                                       -1)  # type: ignore\n    except Exception:\n        self.local_attn_size = -1\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.CausalDMDDenosingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify denoising stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/causal_denoising.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify denoising stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds, V.list_not_empty)\n    result.add_check(\"image_embeds\", batch.image_embeds, V.is_list)\n    result.add_check(\"image_latent\", batch.image_latent,\n                     V.none_or_tensor_with_dims(5))\n    result.add_check(\"num_inference_steps\", batch.num_inference_steps,\n                     V.positive_int)\n    result.add_check(\"guidance_scale\", batch.guidance_scale,\n                     V.positive_float)\n    result.add_check(\"eta\", batch.eta, V.non_negative_float)\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    result.add_check(\"do_classifier_free_guidance\",\n                     batch.do_classifier_free_guidance, V.bool_value)\n    result.add_check(\n        \"negative_prompt_embeds\", batch.negative_prompt_embeds, lambda x:\n        not batch.do_classifier_free_guidance or V.list_not_empty(x))\n    return result\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.ConditioningStage","title":"fastvideo.pipelines.stages.ConditioningStage","text":"<p>               Bases: <code>PipelineStage</code></p> <p>Stage for applying conditioning to the diffusion process.</p> <p>This stage handles the application of conditioning, such as classifier-free guidance, to the diffusion process.</p> Functions\u00b6 fastvideo.pipelines.stages.ConditioningStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Apply conditioning to the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with applied conditioning.</p> Source code in <code>fastvideo/pipelines/stages/conditioning.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Apply conditioning to the diffusion process.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with applied conditioning.\n    \"\"\"\n    # TODO!!\n    if not batch.do_classifier_free_guidance:\n        return batch\n    else:\n        return batch\n\n    logger.info(\"batch.negative_prompt_embeds: %s\",\n                batch.negative_prompt_embeds)\n    logger.info(\"do_classifier_free_guidance: %s\",\n                batch.do_classifier_free_guidance)\n    logger.info(\"cfg_scale: %s\", batch.guidance_scale)\n\n    # Ensure negative prompt embeddings are available\n    assert batch.negative_prompt_embeds is not None, (\n        \"Negative prompt embeddings are required for classifier-free guidance\"\n    )\n\n    # Concatenate primary embeddings and masks\n    batch.prompt_embeds = torch.cat(\n        [batch.negative_prompt_embeds, batch.prompt_embeds])\n    if batch.attention_mask is not None:\n        batch.attention_mask = torch.cat(\n            [batch.negative_attention_mask, batch.attention_mask])\n\n    # Concatenate secondary embeddings and masks if present\n    if batch.prompt_embeds_2 is not None:\n        batch.prompt_embeds_2 = torch.cat(\n            [batch.negative_prompt_embeds_2, batch.prompt_embeds_2])\n    if batch.attention_mask_2 is not None:\n        batch.attention_mask_2 = torch.cat(\n            [batch.negative_attention_mask_2, batch.attention_mask_2])\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.ConditioningStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify conditioning stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/conditioning.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify conditioning stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"do_classifier_free_guidance\",\n                     batch.do_classifier_free_guidance, V.bool_value)\n    result.add_check(\"guidance_scale\", batch.guidance_scale,\n                     V.positive_float)\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds, V.list_not_empty)\n    result.add_check(\n        \"negative_prompt_embeds\", batch.negative_prompt_embeds, lambda x:\n        not batch.do_classifier_free_guidance or V.list_not_empty(x))\n    return result\n</code></pre> fastvideo.pipelines.stages.ConditioningStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify conditioning stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/conditioning.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify conditioning stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds, V.list_not_empty)\n    return result\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.DecodingStage","title":"fastvideo.pipelines.stages.DecodingStage","text":"<pre><code>DecodingStage(vae, pipeline=None)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for decoding latent representations into pixel space.</p> <p>This stage handles the decoding of latent representations into the final output format (e.g., pixel values).</p> Source code in <code>fastvideo/pipelines/stages/decoding.py</code> <pre><code>def __init__(self, vae, pipeline=None) -&gt; None:\n    self.vae: ParallelTiledVAE = vae\n    self.pipeline = weakref.ref(pipeline) if pipeline else None\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.DecodingStage.decode \u00b6 <pre><code>decode(\n    latents: Tensor, fastvideo_args: FastVideoArgs\n) -&gt; torch.Tensor\n</code></pre> <p>Decode latent representations into pixel space using VAE.</p> <p>Parameters:</p> Name Type Description Default <code>latents</code> <code>Tensor</code> <p>Input latent tensor with shape (batch, channels, frames, height_latents, width_latents)</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>Configuration containing: - disable_autocast: Whether to disable automatic mixed precision (default: False) - pipeline_config.vae_precision: VAE computation precision (\"fp32\", \"fp16\", \"bf16\") - pipeline_config.vae_tiling: Whether to enable VAE tiling for memory efficiency</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Decoded video tensor with shape (batch, channels, frames, height, width), </p> <code>Tensor</code> <p>normalized to [0, 1] range and moved to CPU as float32</p> Source code in <code>fastvideo/pipelines/stages/decoding.py</code> <pre><code>@torch.no_grad()\ndef decode(self, latents: torch.Tensor,\n           fastvideo_args: FastVideoArgs) -&gt; torch.Tensor:\n    \"\"\"\n    Decode latent representations into pixel space using VAE.\n\n    Args:\n        latents: Input latent tensor with shape (batch, channels, frames, height_latents, width_latents)\n        fastvideo_args: Configuration containing:\n            - disable_autocast: Whether to disable automatic mixed precision (default: False)\n            - pipeline_config.vae_precision: VAE computation precision (\"fp32\", \"fp16\", \"bf16\")\n            - pipeline_config.vae_tiling: Whether to enable VAE tiling for memory efficiency\n\n    Returns:\n        Decoded video tensor with shape (batch, channels, frames, height, width), \n        normalized to [0, 1] range and moved to CPU as float32\n    \"\"\"\n    self.vae = self.vae.to(get_local_torch_device())\n    latents = latents.to(get_local_torch_device())\n\n    # Setup VAE precision\n    vae_dtype = PRECISION_TO_TYPE[\n        fastvideo_args.pipeline_config.vae_precision]\n    vae_autocast_enabled = (\n        vae_dtype != torch.float32) and not fastvideo_args.disable_autocast\n\n    if isinstance(self.vae.scaling_factor, torch.Tensor):\n        latents = latents / self.vae.scaling_factor.to(\n            latents.device, latents.dtype)\n    else:\n        latents = latents / self.vae.scaling_factor\n\n    # Apply shifting if needed\n    if (hasattr(self.vae, \"shift_factor\")\n            and self.vae.shift_factor is not None):\n        if isinstance(self.vae.shift_factor, torch.Tensor):\n            latents += self.vae.shift_factor.to(latents.device,\n                                                latents.dtype)\n        else:\n            latents += self.vae.shift_factor\n\n    # Decode latents\n    with torch.autocast(device_type=\"cuda\",\n                        dtype=vae_dtype,\n                        enabled=vae_autocast_enabled):\n        if fastvideo_args.pipeline_config.vae_tiling:\n            self.vae.enable_tiling()\n        # if fastvideo_args.vae_sp:\n        #     self.vae.enable_parallel()\n        if not vae_autocast_enabled:\n            latents = latents.to(vae_dtype)\n        image = self.vae.decode(latents)\n\n    # Normalize image to [0, 1] range\n    image = (image / 2 + 0.5).clamp(0, 1)\n    return image\n</code></pre> fastvideo.pipelines.stages.DecodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Decode latent representations into pixel space.</p> <p>This method processes the batch through the VAE decoder, converting latent representations to pixel-space video/images. It also optionally decodes trajectory latents for visualization purposes.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch containing: - latents: Tensor to decode (batch, channels, frames, height_latents, width_latents) - return_trajectory_decoded (optional): Flag to decode trajectory latents - trajectory_latents (optional): Latents at different timesteps - trajectory_timesteps (optional): Corresponding timesteps</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>Configuration containing: - output_type: \"latent\" to skip decoding, otherwise decode to pixels - vae_cpu_offload: Whether to offload VAE to CPU after decoding - model_loaded: Track VAE loading state - model_paths: Path to VAE model if loading needed</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>Modified batch with: - output: Decoded frames (batch, channels, frames, height, width) as CPU float32 - trajectory_decoded (if requested): List of decoded frames per timestep</p> Source code in <code>fastvideo/pipelines/stages/decoding.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Decode latent representations into pixel space.\n\n    This method processes the batch through the VAE decoder, converting latent\n    representations to pixel-space video/images. It also optionally decodes\n    trajectory latents for visualization purposes.\n\n    Args:\n        batch: The current batch containing:\n            - latents: Tensor to decode (batch, channels, frames, height_latents, width_latents)\n            - return_trajectory_decoded (optional): Flag to decode trajectory latents\n            - trajectory_latents (optional): Latents at different timesteps\n            - trajectory_timesteps (optional): Corresponding timesteps\n        fastvideo_args: Configuration containing:\n            - output_type: \"latent\" to skip decoding, otherwise decode to pixels\n            - vae_cpu_offload: Whether to offload VAE to CPU after decoding\n            - model_loaded: Track VAE loading state\n            - model_paths: Path to VAE model if loading needed\n\n    Returns:\n        Modified batch with:\n            - output: Decoded frames (batch, channels, frames, height, width) as CPU float32\n            - trajectory_decoded (if requested): List of decoded frames per timestep\n    \"\"\"\n    # load vae if not already loaded (used for memory constrained devices)\n    pipeline = self.pipeline() if self.pipeline else None\n    if not fastvideo_args.model_loaded[\"vae\"]:\n        loader = VAELoader()\n        self.vae = loader.load(fastvideo_args.model_paths[\"vae\"],\n                               fastvideo_args)\n        if pipeline:\n            pipeline.add_module(\"vae\", self.vae)\n        fastvideo_args.model_loaded[\"vae\"] = True\n\n    if fastvideo_args.output_type == \"latent\":\n        frames = batch.latents\n    else:\n        frames = self.decode(batch.latents, fastvideo_args)\n\n    # decode trajectory latents if needed\n    if batch.return_trajectory_decoded:\n        batch.trajectory_decoded = []\n        assert batch.trajectory_latents is not None, \"batch should have trajectory latents\"\n        for idx in range(batch.trajectory_latents.shape[1]):\n            # batch.trajectory_latents is [batch_size, timesteps, channels, frames, height, width]\n            cur_latent = batch.trajectory_latents[:, idx, :, :, :, :]\n            cur_timestep = batch.trajectory_timesteps[idx]\n            logger.info(\"decoding trajectory latent for timestep: %s\",\n                        cur_timestep)\n            decoded_frames = self.decode(cur_latent, fastvideo_args)\n            batch.trajectory_decoded.append(decoded_frames.cpu().float())\n\n    # Convert to CPU float32 for compatibility\n    frames = frames.cpu().float()\n\n    # Update batch with decoded image\n    batch.output = frames\n\n    # Offload models if needed\n    if hasattr(self, 'maybe_free_model_hooks'):\n        self.maybe_free_model_hooks()\n\n    if fastvideo_args.vae_cpu_offload:\n        self.vae.to(\"cpu\")\n\n    if torch.backends.mps.is_available():\n        del self.vae\n        if pipeline is not None and \"vae\" in pipeline.modules:\n            del pipeline.modules[\"vae\"]\n        fastvideo_args.model_loaded[\"vae\"] = False\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.DecodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify decoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/decoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify decoding stage inputs.\"\"\"\n    result = VerificationResult()\n    # Denoised latents for VAE decoding: [batch_size, channels, frames, height_latents, width_latents]\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre> fastvideo.pipelines.stages.DecodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify decoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/decoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify decoding stage outputs.\"\"\"\n    result = VerificationResult()\n    # Decoded video/images: [batch_size, channels, frames, height, width]\n    result.add_check(\"output\", batch.output, [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.DenoisingStage","title":"fastvideo.pipelines.stages.DenoisingStage","text":"<pre><code>DenoisingStage(\n    transformer,\n    scheduler,\n    pipeline=None,\n    transformer_2=None,\n    vae=None,\n)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for running the denoising loop in diffusion pipelines.</p> <p>This stage handles the iterative denoising process that transforms the initial noise into the final output.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def __init__(self,\n             transformer,\n             scheduler,\n             pipeline=None,\n             transformer_2=None,\n             vae=None) -&gt; None:\n    super().__init__()\n    self.transformer = transformer\n    self.transformer_2 = transformer_2\n    self.scheduler = scheduler\n    self.vae = vae\n    self.pipeline = weakref.ref(pipeline) if pipeline else None\n    attn_head_size = self.transformer.hidden_size // self.transformer.num_attention_heads\n    self.attn_backend = get_attn_backend(\n        head_size=attn_head_size,\n        dtype=torch.float16,  # TODO(will): hack\n        supported_attention_backends=(\n            AttentionBackendEnum.SLIDING_TILE_ATTN,\n            AttentionBackendEnum.VIDEO_SPARSE_ATTN,\n            AttentionBackendEnum.VMOBA_ATTN,\n            AttentionBackendEnum.FLASH_ATTN,\n            AttentionBackendEnum.TORCH_SDPA,\n            AttentionBackendEnum.SAGE_ATTN_THREE)  # hack\n    )\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.DenoisingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Run the denoising loop.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with denoised latents.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Run the denoising loop.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with denoised latents.\n    \"\"\"\n    pipeline = self.pipeline() if self.pipeline else None\n    if not fastvideo_args.model_loaded[\"transformer\"]:\n        loader = TransformerLoader()\n        self.transformer = loader.load(\n            fastvideo_args.model_paths[\"transformer\"], fastvideo_args)\n        if pipeline:\n            pipeline.add_module(\"transformer\", self.transformer)\n        fastvideo_args.model_loaded[\"transformer\"] = True\n\n    # Prepare extra step kwargs for scheduler\n    extra_step_kwargs = self.prepare_extra_func_kwargs(\n        self.scheduler.step,\n        {\n            \"generator\": batch.generator,\n            \"eta\": batch.eta\n        },\n    )\n\n    # Setup precision and autocast settings\n    # TODO(will): make the precision configurable for inference\n    # target_dtype = PRECISION_TO_TYPE[fastvideo_args.precision]\n    target_dtype = torch.bfloat16\n    autocast_enabled = (target_dtype != torch.float32\n                        ) and not fastvideo_args.disable_autocast\n\n    # Handle sequence parallelism if enabled\n    sp_world_size, rank_in_sp_group = get_sp_world_size(\n    ), get_sp_parallel_rank()\n    sp_group = sp_world_size &gt; 1\n    if sp_group:\n        latents = rearrange(batch.latents,\n                            \"b c (n t) h w -&gt; b c n t h w\",\n                            n=sp_world_size).contiguous()\n        latents = latents[:, :, rank_in_sp_group, :, :, :]\n        batch.latents = latents\n        if batch.image_latent is not None:\n            image_latent = rearrange(batch.image_latent,\n                                     \"b c (n t) h w -&gt; b c n t h w\",\n                                     n=sp_world_size).contiguous()\n            image_latent = image_latent[:, :, rank_in_sp_group, :, :, :]\n            batch.image_latent = image_latent\n    # Get timesteps and calculate warmup steps\n    timesteps = batch.timesteps\n    # TODO(will): remove this once we add input/output validation for stages\n    if timesteps is None:\n        raise ValueError(\"Timesteps must be provided\")\n    num_inference_steps = batch.num_inference_steps\n    num_warmup_steps = len(\n        timesteps) - num_inference_steps * self.scheduler.order\n\n    # Prepare image latents and embeddings for I2V generation\n    image_embeds = batch.image_embeds\n    if len(image_embeds) &gt; 0:\n        assert not torch.isnan(\n            image_embeds[0]).any(), \"image_embeds contains nan\"\n        image_embeds = [\n            image_embed.to(target_dtype) for image_embed in image_embeds\n        ]\n\n    image_kwargs = self.prepare_extra_func_kwargs(\n        self.transformer.forward,\n        {\n            \"encoder_hidden_states_image\": image_embeds,\n            \"mask_strategy\": dict_to_3d_list(\n                None, t_max=50, l_max=60, h_max=24)\n        },\n    )\n\n    pos_cond_kwargs = self.prepare_extra_func_kwargs(\n        self.transformer.forward,\n        {\n            \"encoder_hidden_states_2\": batch.clip_embedding_pos,\n            \"encoder_attention_mask\": batch.prompt_attention_mask,\n        },\n    )\n\n    neg_cond_kwargs = self.prepare_extra_func_kwargs(\n        self.transformer.forward,\n        {\n            \"encoder_hidden_states_2\": batch.clip_embedding_neg,\n            \"encoder_attention_mask\": batch.negative_attention_mask,\n        },\n    )\n\n    # Prepare STA parameters\n    if st_attn_available and self.attn_backend == SlidingTileAttentionBackend:\n        self.prepare_sta_param(batch, fastvideo_args)\n\n    # Get latents and embeddings\n    latents = batch.latents\n    prompt_embeds = batch.prompt_embeds\n    assert not torch.isnan(\n        prompt_embeds[0]).any(), \"prompt_embeds contains nan\"\n    if batch.do_classifier_free_guidance:\n        neg_prompt_embeds = batch.negative_prompt_embeds\n        assert neg_prompt_embeds is not None\n        assert not torch.isnan(\n            neg_prompt_embeds[0]).any(), \"neg_prompt_embeds contains nan\"\n\n    # (Wan2.2) Calculate timestep to switch from high noise expert to low noise expert\n    boundary_ratio = fastvideo_args.pipeline_config.dit_config.boundary_ratio\n    if batch.boundary_ratio is not None:\n        logger.info(\"Overriding boundary ratio from %s to %s\",\n                    boundary_ratio, batch.boundary_ratio)\n        boundary_ratio = batch.boundary_ratio\n\n    if boundary_ratio is not None:\n        boundary_timestep = boundary_ratio * self.scheduler.num_train_timesteps\n    else:\n        boundary_timestep = None\n    latent_model_input = latents.to(target_dtype)\n    assert latent_model_input.shape[0] == 1, \"only support batch size 1\"\n\n    if fastvideo_args.pipeline_config.ti2v_task and batch.pil_image is not None:\n        # TI2V directly replaces the first frame of the latent with\n        # the image latent instead of appending along the channel dim\n        assert batch.image_latent is None, \"TI2V task should not have image latents\"\n        assert self.vae is not None, \"VAE is not provided for TI2V task\"\n        z = self.vae.encode(batch.pil_image).mean.float()\n        if (hasattr(self.vae, \"shift_factor\")\n                and self.vae.shift_factor is not None):\n            if isinstance(self.vae.shift_factor, torch.Tensor):\n                z -= self.vae.shift_factor.to(z.device, z.dtype)\n            else:\n                z -= self.vae.shift_factor\n\n        if isinstance(self.vae.scaling_factor, torch.Tensor):\n            z = z * self.vae.scaling_factor.to(z.device, z.dtype)\n        else:\n            z = z * self.vae.scaling_factor\n\n        latent_model_input = latent_model_input.squeeze(0)\n        _, mask2 = masks_like([latent_model_input], zero=True)\n\n        latent_model_input = (1. -\n                              mask2[0]) * z + mask2[0] * latent_model_input\n        # latent_model_input = latent_model_input.unsqueeze(0)\n        latent_model_input = latent_model_input.to(get_local_torch_device())\n        latents = latent_model_input\n        F = batch.num_frames\n        temporal_scale = fastvideo_args.pipeline_config.vae_config.arch_config.scale_factor_temporal\n        spatial_scale = fastvideo_args.pipeline_config.vae_config.arch_config.scale_factor_spatial\n        patch_size = fastvideo_args.pipeline_config.dit_config.arch_config.patch_size\n        seq_len = ((F - 1) // temporal_scale +\n                   1) * (batch.height // spatial_scale) * (\n                       batch.width // spatial_scale) // (patch_size[1] *\n                                                         patch_size[2])\n        seq_len = int(math.ceil(seq_len / sp_world_size)) * sp_world_size\n\n    # Initialize lists for ODE trajectory\n    trajectory_timesteps: list[torch.Tensor] = []\n    trajectory_latents: list[torch.Tensor] = []\n\n    # Run denoising loop\n    with self.progress_bar(total=num_inference_steps) as progress_bar:\n        for i, t in enumerate(timesteps):\n            # Skip if interrupted\n            if hasattr(self, 'interrupt') and self.interrupt:\n                continue\n\n            if boundary_timestep is None or t &gt;= boundary_timestep:\n                if (fastvideo_args.dit_cpu_offload\n                        and self.transformer_2 is not None and next(\n                            self.transformer_2.parameters()).device.type\n                        == 'cuda'):\n                    self.transformer_2.to('cpu')\n                current_model = self.transformer\n                current_guidance_scale = batch.guidance_scale\n            else:\n                # low-noise stage in wan2.2\n                if fastvideo_args.dit_cpu_offload and next(\n                        self.transformer.parameters(\n                        )).device.type == 'cuda':\n                    self.transformer.to('cpu')\n                current_model = self.transformer_2\n                current_guidance_scale = batch.guidance_scale_2\n            assert current_model is not None, \"current_model is None\"\n\n            # Expand latents for V2V/I2V\n            latent_model_input = latents.to(target_dtype)\n            if batch.video_latent is not None:\n                latent_model_input = torch.cat([\n                    latent_model_input, batch.video_latent,\n                    torch.zeros_like(latents)\n                ],\n                                               dim=1).to(target_dtype)\n            elif batch.image_latent is not None:\n                assert not fastvideo_args.pipeline_config.ti2v_task, \"image latents should not be provided for TI2V task\"\n                latent_model_input = torch.cat(\n                    [latent_model_input, batch.image_latent],\n                    dim=1).to(target_dtype)\n\n            assert not torch.isnan(\n                latent_model_input).any(), \"latent_model_input contains nan\"\n            if fastvideo_args.pipeline_config.ti2v_task and batch.pil_image is not None:\n                timestep = torch.stack([t]).to(get_local_torch_device())\n                temp_ts = (mask2[0][0][:, ::2, ::2] * timestep).flatten()\n                temp_ts = torch.cat([\n                    temp_ts,\n                    temp_ts.new_ones(seq_len - temp_ts.size(0)) * timestep\n                ])\n                timestep = temp_ts.unsqueeze(0)\n                t_expand = timestep.repeat(latent_model_input.shape[0], 1)\n            else:\n                t_expand = t.repeat(latent_model_input.shape[0])\n\n            latent_model_input = self.scheduler.scale_model_input(\n                latent_model_input, t)\n\n            # Prepare inputs for transformer\n            guidance_expand = (\n                torch.tensor(\n                    [fastvideo_args.pipeline_config.embedded_cfg_scale] *\n                    latent_model_input.shape[0],\n                    dtype=torch.float32,\n                    device=get_local_torch_device(),\n                ).to(target_dtype) *\n                1000.0 if fastvideo_args.pipeline_config.embedded_cfg_scale\n                is not None else None)\n\n            # Predict noise residual\n            with torch.autocast(device_type=\"cuda\",\n                                dtype=target_dtype,\n                                enabled=autocast_enabled):\n                if (st_attn_available\n                        and self.attn_backend == SlidingTileAttentionBackend\n                    ) or (vsa_available and self.attn_backend\n                          == VideoSparseAttentionBackend):\n                    self.attn_metadata_builder_cls = self.attn_backend.get_builder_cls(\n                    )\n\n                    if self.attn_metadata_builder_cls is not None:\n                        self.attn_metadata_builder = self.attn_metadata_builder_cls(\n                        )\n                        # TODO(will): clean this up\n                        attn_metadata = self.attn_metadata_builder.build(  # type: ignore\n                            current_timestep=i,  # type: ignore\n                            raw_latent_shape=batch.\n                            raw_latent_shape[2:5],  # type: ignore\n                            patch_size=fastvideo_args.\n                            pipeline_config.  # type: ignore\n                            dit_config.patch_size,  # type: ignore\n                            STA_param=batch.STA_param,  # type: ignore\n                            VSA_sparsity=fastvideo_args.\n                            VSA_sparsity,  # type: ignore\n                            device=get_local_torch_device(),\n                        )\n                        assert attn_metadata is not None, \"attn_metadata cannot be None\"\n                    else:\n                        attn_metadata = None\n                elif (vmoba_attn_available\n                      and self.attn_backend == VMOBAAttentionBackend):\n                    self.attn_metadata_builder_cls = self.attn_backend.get_builder_cls(\n                    )\n                    if self.attn_metadata_builder_cls is not None:\n                        self.attn_metadata_builder = self.attn_metadata_builder_cls(\n                        )\n                        # Prepare V-MoBA parameters from config\n                        moba_params = fastvideo_args.moba_config.copy()\n                        moba_params.update({\n                            \"current_timestep\":\n                            i,\n                            \"raw_latent_shape\":\n                            batch.raw_latent_shape[2:5],\n                            \"patch_size\":\n                            fastvideo_args.pipeline_config.dit_config.\n                            patch_size,\n                            \"device\":\n                            get_local_torch_device(),\n                        })\n                        attn_metadata = self.attn_metadata_builder.build(\n                            **moba_params)\n                        assert attn_metadata is not None, \"attn_metadata cannot be None\"\n                    else:\n                        attn_metadata = None\n                else:\n                    attn_metadata = None\n                # TODO(will): finalize the interface. vLLM uses this to\n                # support torch dynamo compilation. They pass in\n                # attn_metadata, vllm_config, and num_tokens. We can pass in\n                # fastvideo_args or training_args, and attn_metadata.\n                batch.is_cfg_negative = False\n                with set_forward_context(\n                        current_timestep=i,\n                        attn_metadata=attn_metadata,\n                        forward_batch=batch,\n                        # fastvideo_args=fastvideo_args\n                ):\n                    # Run transformer\n                    noise_pred = current_model(\n                        latent_model_input,\n                        prompt_embeds,\n                        t_expand,\n                        guidance=guidance_expand,\n                        **image_kwargs,\n                        **pos_cond_kwargs,\n                    )\n\n                # Apply guidance\n                if batch.do_classifier_free_guidance:\n                    batch.is_cfg_negative = True\n                    with set_forward_context(\n                            current_timestep=i,\n                            attn_metadata=attn_metadata,\n                            forward_batch=batch,\n                            # fastvideo_args=fastvideo_args\n                    ):\n                        # Run transformer\n                        noise_pred_uncond = current_model(\n                            latent_model_input,\n                            neg_prompt_embeds,\n                            t_expand,\n                            guidance=guidance_expand,\n                            **image_kwargs,\n                            **neg_cond_kwargs,\n                        )\n                    noise_pred_text = noise_pred\n                    noise_pred = noise_pred_uncond + current_guidance_scale * (\n                        noise_pred_text - noise_pred_uncond)\n\n                    # Apply guidance rescale if needed\n                    if batch.guidance_rescale &gt; 0.0:\n                        # Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf\n                        noise_pred = self.rescale_noise_cfg(\n                            noise_pred,\n                            noise_pred_text,\n                            guidance_rescale=batch.guidance_rescale,\n                        )\n                # Compute the previous noisy sample\n                latents = self.scheduler.step(noise_pred,\n                                              t,\n                                              latents,\n                                              **extra_step_kwargs,\n                                              return_dict=False)[0]\n                if fastvideo_args.pipeline_config.ti2v_task and batch.pil_image is not None:\n                    latents = latents.squeeze(0)\n                    latents = (1. - mask2[0]) * z + mask2[0] * latents\n                    # latents = latents.unsqueeze(0)\n\n            # save trajectory latents if needed\n            if batch.return_trajectory_latents:\n                trajectory_timesteps.append(t)\n                trajectory_latents.append(latents)\n\n            # Update progress bar\n            if i == len(timesteps) - 1 or (\n                (i + 1) &gt; num_warmup_steps and\n                (i + 1) % self.scheduler.order == 0\n                    and progress_bar is not None):\n                progress_bar.update()\n\n    # Gather results if using sequence parallelism\n    trajectory_tensor: torch.Tensor | None = None\n    if trajectory_latents:\n        trajectory_tensor = torch.stack(trajectory_latents, dim=1)\n        trajectory_timesteps_tensor = torch.stack(trajectory_timesteps,\n                                                  dim=0)\n    else:\n        trajectory_tensor = None\n        trajectory_timesteps_tensor = None\n\n    # Gather results if using sequence parallelism\n    if sp_group:\n        latents = sequence_model_parallel_all_gather(latents, dim=2)\n        if batch.return_trajectory_latents:\n            trajectory_tensor = trajectory_tensor.to(\n                get_local_torch_device())\n            trajectory_tensor = sequence_model_parallel_all_gather(\n                trajectory_tensor, dim=3)\n\n    if trajectory_tensor is not None and trajectory_timesteps_tensor is not None:\n        batch.trajectory_timesteps = trajectory_timesteps_tensor.cpu()\n        batch.trajectory_latents = trajectory_tensor.cpu()\n\n    # Update batch with final latents\n    batch.latents = latents\n\n    # Save STA mask search results if needed\n    if st_attn_available and self.attn_backend == SlidingTileAttentionBackend and fastvideo_args.STA_mode == STA_Mode.STA_SEARCHING:\n        self.save_sta_search_results(batch)\n\n    # deallocate transformer if on mps\n    if torch.backends.mps.is_available():\n        logger.info(\"Memory before deallocating transformer: %s\",\n                    torch.mps.current_allocated_memory())\n        del self.transformer\n        if pipeline is not None and \"transformer\" in pipeline.modules:\n            del pipeline.modules[\"transformer\"]\n        fastvideo_args.model_loaded[\"transformer\"] = False\n        logger.info(\"Memory after deallocating transformer: %s\",\n                    torch.mps.current_allocated_memory())\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.DenoisingStage.prepare_extra_func_kwargs \u00b6 <pre><code>prepare_extra_func_kwargs(func, kwargs) -&gt; dict[str, Any]\n</code></pre> <p>Prepare extra kwargs for the scheduler step / denoise step.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <p>The function to prepare kwargs for.</p> required <code>kwargs</code> <p>The kwargs to prepare.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The prepared kwargs.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def prepare_extra_func_kwargs(self, func, kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Prepare extra kwargs for the scheduler step / denoise step.\n\n    Args:\n        func: The function to prepare kwargs for.\n        kwargs: The kwargs to prepare.\n\n    Returns:\n        The prepared kwargs.\n    \"\"\"\n    extra_step_kwargs = {}\n    for k, v in kwargs.items():\n        accepts = k in set(inspect.signature(func).parameters.keys())\n        if accepts:\n            extra_step_kwargs[k] = v\n    return extra_step_kwargs\n</code></pre> fastvideo.pipelines.stages.DenoisingStage.prepare_sta_param \u00b6 <pre><code>prepare_sta_param(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n)\n</code></pre> <p>Prepare Sliding Tile Attention (STA) parameters and settings.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def prepare_sta_param(self, batch: ForwardBatch,\n                      fastvideo_args: FastVideoArgs):\n    \"\"\"\n    Prepare Sliding Tile Attention (STA) parameters and settings.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n    \"\"\"\n    # TODO(kevin): STA mask search, currently only support Wan2.1 with 69x768x1280\n    from fastvideo.STA_configuration import configure_sta\n    STA_mode = fastvideo_args.STA_mode\n    skip_time_steps = fastvideo_args.skip_time_steps\n    if batch.timesteps is None:\n        raise ValueError(\"Timesteps must be provided\")\n    timesteps_num = batch.timesteps.shape[0]\n\n    logger.info(\"STA_mode: %s\", STA_mode)\n    if (batch.num_frames, batch.height,\n            batch.width) != (69, 768, 1280) and STA_mode != \"STA_inference\":\n        raise NotImplementedError(\n            \"STA mask search/tuning is not supported for this resolution\")\n\n    if STA_mode == STA_Mode.STA_SEARCHING or STA_mode == STA_Mode.STA_TUNING or STA_mode == STA_Mode.STA_TUNING_CFG:\n        size = (batch.width, batch.height)\n        if size == (1280, 768):\n            # TODO: make it configurable\n            sparse_mask_candidates_searching = [\n                \"3, 1, 10\", \"1, 5, 7\", \"3, 3, 3\", \"1, 6, 5\", \"1, 3, 10\",\n                \"3, 6, 1\"\n            ]\n            sparse_mask_candidates_tuning = [\n                \"3, 1, 10\", \"1, 5, 7\", \"3, 3, 3\", \"1, 6, 5\", \"1, 3, 10\",\n                \"3, 6, 1\"\n            ]\n            full_mask = [\"3,6,10\"]\n        else:\n            raise NotImplementedError(\n                \"STA mask search is not supported for this resolution\")\n    layer_num = self.transformer.config.num_layers\n    # specific for HunyuanVideo\n    if hasattr(self.transformer.config, \"num_single_layers\"):\n        layer_num += self.transformer.config.num_single_layers\n    head_num = self.transformer.config.num_attention_heads\n\n    if STA_mode == STA_Mode.STA_SEARCHING:\n        STA_param = configure_sta(\n            mode=STA_Mode.STA_SEARCHING,\n            layer_num=layer_num,\n            head_num=head_num,\n            time_step_num=timesteps_num,\n            mask_candidates=sparse_mask_candidates_searching +\n            full_mask,  # last is full mask; Can add more sparse masks while keep last one as full mask\n        )\n    elif STA_mode == STA_Mode.STA_TUNING:\n        STA_param = configure_sta(\n            mode=STA_Mode.STA_TUNING,\n            layer_num=layer_num,\n            head_num=head_num,\n            time_step_num=timesteps_num,\n            mask_search_files_path=\n            f'output/mask_search_result_pos_{size[0]}x{size[1]}/',\n            mask_candidates=sparse_mask_candidates_tuning,\n            full_attention_mask=[int(x) for x in full_mask[0].split(',')],\n            skip_time_steps=\n            skip_time_steps,  # Use full attention for first 12 steps\n            save_dir=\n            f'output/mask_search_strategy_{size[0]}x{size[1]}/',  # Custom save directory\n            timesteps=timesteps_num)\n    elif STA_mode == STA_Mode.STA_TUNING_CFG:\n        STA_param = configure_sta(\n            mode=STA_Mode.STA_TUNING_CFG,\n            layer_num=layer_num,\n            head_num=head_num,\n            time_step_num=timesteps_num,\n            mask_search_files_path_pos=\n            f'output/mask_search_result_pos_{size[0]}x{size[1]}/',\n            mask_search_files_path_neg=\n            f'output/mask_search_result_neg_{size[0]}x{size[1]}/',\n            mask_candidates=sparse_mask_candidates_tuning,\n            full_attention_mask=[int(x) for x in full_mask[0].split(',')],\n            skip_time_steps=skip_time_steps,\n            save_dir=f'output/mask_search_strategy_{size[0]}x{size[1]}/',\n            timesteps=timesteps_num)\n    elif STA_mode == STA_Mode.STA_INFERENCE:\n        import fastvideo.envs as envs\n        config_file = envs.FASTVIDEO_ATTENTION_CONFIG\n        if config_file is None:\n            raise ValueError(\"FASTVIDEO_ATTENTION_CONFIG is not set\")\n        STA_param = configure_sta(mode=STA_Mode.STA_INFERENCE,\n                                  layer_num=layer_num,\n                                  head_num=head_num,\n                                  time_step_num=timesteps_num,\n                                  load_path=config_file)\n\n    batch.STA_param = STA_param\n    batch.mask_search_final_result_pos = [[] for _ in range(timesteps_num)]\n    batch.mask_search_final_result_neg = [[] for _ in range(timesteps_num)]\n</code></pre> fastvideo.pipelines.stages.DenoisingStage.progress_bar \u00b6 <pre><code>progress_bar(\n    iterable: Iterable | None = None,\n    total: int | None = None,\n) -&gt; tqdm\n</code></pre> <p>Create a progress bar for the denoising process.</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>Iterable | None</code> <p>The iterable to iterate over.</p> <code>None</code> <code>total</code> <code>int | None</code> <p>The total number of items.</p> <code>None</code> <p>Returns:</p> Type Description <code>tqdm</code> <p>A tqdm progress bar.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def progress_bar(self,\n                 iterable: Iterable | None = None,\n                 total: int | None = None) -&gt; tqdm:\n    \"\"\"\n    Create a progress bar for the denoising process.\n\n    Args:\n        iterable: The iterable to iterate over.\n        total: The total number of items.\n\n    Returns:\n        A tqdm progress bar.\n    \"\"\"\n    local_rank = get_world_group().local_rank\n    if local_rank == 0:\n        return tqdm(iterable=iterable, total=total)\n    else:\n        return tqdm(iterable=iterable, total=total, disable=True)\n</code></pre> fastvideo.pipelines.stages.DenoisingStage.rescale_noise_cfg \u00b6 <pre><code>rescale_noise_cfg(\n    noise_cfg, noise_pred_text, guidance_rescale=0.0\n) -&gt; torch.Tensor\n</code></pre> <p>Rescale noise prediction according to guidance_rescale.</p> <p>Based on findings of \"Common Diffusion Noise Schedules and Sample Steps are Flawed\" (https://arxiv.org/pdf/2305.08891.pdf), Section 3.4.</p> <p>Parameters:</p> Name Type Description Default <code>noise_cfg</code> <p>The noise prediction with guidance.</p> required <code>noise_pred_text</code> <p>The text-conditioned noise prediction.</p> required <code>guidance_rescale</code> <p>The guidance rescale factor.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The rescaled noise prediction.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def rescale_noise_cfg(self,\n                      noise_cfg,\n                      noise_pred_text,\n                      guidance_rescale=0.0) -&gt; torch.Tensor:\n    \"\"\"\n    Rescale noise prediction according to guidance_rescale.\n\n    Based on findings of \"Common Diffusion Noise Schedules and Sample Steps are Flawed\"\n    (https://arxiv.org/pdf/2305.08891.pdf), Section 3.4.\n\n    Args:\n        noise_cfg: The noise prediction with guidance.\n        noise_pred_text: The text-conditioned noise prediction.\n        guidance_rescale: The guidance rescale factor.\n\n    Returns:\n        The rescaled noise prediction.\n    \"\"\"\n    std_text = noise_pred_text.std(dim=list(range(1, noise_pred_text.ndim)),\n                                   keepdim=True)\n    std_cfg = noise_cfg.std(dim=list(range(1, noise_cfg.ndim)),\n                            keepdim=True)\n    # Rescale the results from guidance (fixes overexposure)\n    noise_pred_rescaled = noise_cfg * (std_text / std_cfg)\n    # Mix with the original results from guidance by factor guidance_rescale\n    noise_cfg = (guidance_rescale * noise_pred_rescaled +\n                 (1 - guidance_rescale) * noise_cfg)\n    return noise_cfg\n</code></pre> fastvideo.pipelines.stages.DenoisingStage.save_sta_search_results \u00b6 <pre><code>save_sta_search_results(batch: ForwardBatch)\n</code></pre> <p>Save the STA mask search results.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def save_sta_search_results(self, batch: ForwardBatch):\n    \"\"\"\n    Save the STA mask search results.\n\n    Args:\n        batch: The current batch information.\n    \"\"\"\n    size = (batch.width, batch.height)\n    if size == (1280, 768):\n        # TODO: make it configurable\n        sparse_mask_candidates_searching = [\n            \"3, 1, 10\", \"1, 5, 7\", \"3, 3, 3\", \"1, 6, 5\", \"1, 3, 10\",\n            \"3, 6, 1\"\n        ]\n    else:\n        raise NotImplementedError(\n            \"STA mask search is not supported for this resolution\")\n\n    from fastvideo.STA_configuration import save_mask_search_results\n    if batch.mask_search_final_result_pos is not None and batch.prompt is not None:\n        save_mask_search_results(\n            [\n                dict(layer_data)\n                for layer_data in batch.mask_search_final_result_pos\n            ],\n            prompt=str(batch.prompt),\n            mask_strategies=sparse_mask_candidates_searching,\n            output_dir=f'output/mask_search_result_pos_{size[0]}x{size[1]}/'\n        )\n    if batch.mask_search_final_result_neg is not None and batch.prompt is not None:\n        save_mask_search_results(\n            [\n                dict(layer_data)\n                for layer_data in batch.mask_search_final_result_neg\n            ],\n            prompt=str(batch.prompt),\n            mask_strategies=sparse_mask_candidates_searching,\n            output_dir=f'output/mask_search_result_neg_{size[0]}x{size[1]}/'\n        )\n</code></pre> fastvideo.pipelines.stages.DenoisingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify denoising stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify denoising stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"timesteps\", batch.timesteps,\n                     [V.is_tensor, V.min_dims(1)])\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds, V.list_not_empty)\n    result.add_check(\"image_embeds\", batch.image_embeds, V.is_list)\n    result.add_check(\"image_latent\", batch.image_latent,\n                     V.none_or_tensor_with_dims(5))\n    result.add_check(\"num_inference_steps\", batch.num_inference_steps,\n                     V.positive_int)\n    result.add_check(\"guidance_scale\", batch.guidance_scale,\n                     V.positive_float)\n    result.add_check(\"eta\", batch.eta, V.non_negative_float)\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    result.add_check(\"do_classifier_free_guidance\",\n                     batch.do_classifier_free_guidance, V.bool_value)\n    result.add_check(\n        \"negative_prompt_embeds\", batch.negative_prompt_embeds, lambda x:\n        not batch.do_classifier_free_guidance or V.list_not_empty(x))\n    return result\n</code></pre> fastvideo.pipelines.stages.DenoisingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify denoising stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify denoising stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.DmdDenoisingStage","title":"fastvideo.pipelines.stages.DmdDenoisingStage","text":"<pre><code>DmdDenoisingStage(transformer, scheduler)\n</code></pre> <p>               Bases: <code>DenoisingStage</code></p> <p>Denoising stage for DMD.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def __init__(self, transformer, scheduler) -&gt; None:\n    super().__init__(transformer, scheduler)\n    self.scheduler = FlowMatchEulerDiscreteScheduler(shift=8.0)\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.DmdDenoisingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Run the denoising loop.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with denoised latents.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Run the denoising loop.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with denoised latents.\n    \"\"\"\n    # Setup precision and autocast settings\n    # TODO(will): make the precision configurable for inference\n    # target_dtype = PRECISION_TO_TYPE[fastvideo_args.precision]\n    target_dtype = torch.bfloat16\n    autocast_enabled = (target_dtype != torch.float32\n                        ) and not fastvideo_args.disable_autocast\n\n    # Get timesteps and calculate warmup steps\n    timesteps = batch.timesteps\n\n    # TODO(will): remove this once we add input/output validation for stages\n    if timesteps is None:\n        raise ValueError(\"Timesteps must be provided\")\n    num_inference_steps = batch.num_inference_steps\n    num_warmup_steps = len(\n        timesteps) - num_inference_steps * self.scheduler.order\n\n    # Prepare image latents and embeddings for I2V generation\n    image_embeds = batch.image_embeds\n    if len(image_embeds) &gt; 0:\n        assert torch.isnan(image_embeds[0]).sum() == 0\n        image_embeds = [\n            image_embed.to(target_dtype) for image_embed in image_embeds\n        ]\n\n    image_kwargs = self.prepare_extra_func_kwargs(\n        self.transformer.forward,\n        {\n            \"encoder_hidden_states_image\": image_embeds,\n            \"mask_strategy\": dict_to_3d_list(\n                None, t_max=50, l_max=60, h_max=24)\n        },\n    )\n\n    pos_cond_kwargs = self.prepare_extra_func_kwargs(\n        self.transformer.forward,\n        {\n            \"encoder_hidden_states_2\": batch.clip_embedding_pos,\n            \"encoder_attention_mask\": batch.prompt_attention_mask,\n        },\n    )\n\n    # Prepare STA parameters\n    if st_attn_available and self.attn_backend == SlidingTileAttentionBackend:\n        self.prepare_sta_param(batch, fastvideo_args)\n\n    # Get latents and embeddings\n    assert batch.latents is not None, \"latents must be provided\"\n    latents = batch.latents\n    latents = latents.permute(0, 2, 1, 3, 4)\n\n    video_raw_latent_shape = latents.shape\n    prompt_embeds = batch.prompt_embeds\n    assert not torch.isnan(\n        prompt_embeds[0]).any(), \"prompt_embeds contains nan\"\n    timesteps = torch.tensor(\n        fastvideo_args.pipeline_config.dmd_denoising_steps,\n        dtype=torch.long,\n        device=get_local_torch_device())\n\n    # Handle sequence parallelism if enabled\n    sp_world_size, rank_in_sp_group = get_sp_world_size(\n    ), get_sp_parallel_rank()\n    sp_group = sp_world_size &gt; 1\n    if sp_group:\n        latents = rearrange(latents,\n                            \"b (n t) c h w -&gt; b n t c h w\",\n                            n=sp_world_size).contiguous()\n        latents = latents[:, rank_in_sp_group, :, :, :, :]\n        if batch.image_latent is not None:\n            image_latent = rearrange(batch.image_latent,\n                                     \"b c (n t) h w -&gt; b c n t h w\",\n                                     n=sp_world_size).contiguous()\n\n            image_latent = image_latent[:, :, rank_in_sp_group, :, :, :]\n            batch.image_latent = image_latent\n\n    # Run denoising loop\n    with self.progress_bar(total=len(timesteps)) as progress_bar:\n        for i, t in enumerate(timesteps):\n            # Skip if interrupted\n            if hasattr(self, 'interrupt') and self.interrupt:\n                continue\n            # Expand latents for I2V\n            noise_latents = latents.clone()\n            latent_model_input = latents.to(target_dtype)\n\n            if batch.image_latent is not None:\n                latent_model_input = torch.cat([\n                    latent_model_input,\n                    batch.image_latent.permute(0, 2, 1, 3, 4)\n                ],\n                                               dim=2).to(target_dtype)\n            assert not torch.isnan(\n                latent_model_input).any(), \"latent_model_input contains nan\"\n\n            # Prepare inputs for transformer\n            t_expand = t.repeat(latent_model_input.shape[0])\n            guidance_expand = (\n                torch.tensor(\n                    [fastvideo_args.pipeline_config.embedded_cfg_scale] *\n                    latent_model_input.shape[0],\n                    dtype=torch.float32,\n                    device=get_local_torch_device(),\n                ).to(target_dtype) *\n                1000.0 if fastvideo_args.pipeline_config.embedded_cfg_scale\n                is not None else None)\n\n            # Predict noise residual\n            with torch.autocast(device_type=\"cuda\",\n                                dtype=target_dtype,\n                                enabled=autocast_enabled):\n                if (vsa_available and self.attn_backend\n                        == VideoSparseAttentionBackend):\n                    self.attn_metadata_builder_cls = self.attn_backend.get_builder_cls(\n                    )\n\n                    if self.attn_metadata_builder_cls is not None:\n                        self.attn_metadata_builder = self.attn_metadata_builder_cls(\n                        )\n                        # TODO(will): clean this up\n                        attn_metadata = self.attn_metadata_builder.build(  # type: ignore\n                            current_timestep=i,  # type: ignore\n                            raw_latent_shape=batch.\n                            raw_latent_shape[2:5],  # type: ignore\n                            patch_size=fastvideo_args.\n                            pipeline_config.  # type: ignore\n                            dit_config.patch_size,  # type: ignore\n                            STA_param=batch.STA_param,  # type: ignore\n                            VSA_sparsity=fastvideo_args.\n                            VSA_sparsity,  # type: ignore\n                            device=get_local_torch_device(),  # type: ignore\n                        )  # type: ignore\n                        assert attn_metadata is not None, \"attn_metadata cannot be None\"\n                    else:\n                        attn_metadata = None\n                else:\n                    attn_metadata = None\n\n                batch.is_cfg_negative = False\n                with set_forward_context(\n                        current_timestep=i,\n                        attn_metadata=attn_metadata,\n                        forward_batch=batch,\n                        # fastvideo_args=fastvideo_args\n                ):\n                    # Run transformer\n                    pred_noise = self.transformer(\n                        latent_model_input.permute(0, 2, 1, 3, 4),\n                        prompt_embeds,\n                        t_expand,\n                        guidance=guidance_expand,\n                        **image_kwargs,\n                        **pos_cond_kwargs,\n                    ).permute(0, 2, 1, 3, 4)\n\n                pred_video = pred_noise_to_pred_video(\n                    pred_noise=pred_noise.flatten(0, 1),\n                    noise_input_latent=noise_latents.flatten(0, 1),\n                    timestep=t_expand,\n                    scheduler=self.scheduler).unflatten(\n                        0, pred_noise.shape[:2])\n\n                if i &lt; len(timesteps) - 1:\n                    next_timestep = timesteps[i + 1] * torch.ones(\n                        [1], dtype=torch.long, device=pred_video.device)\n                    noise = torch.randn(video_raw_latent_shape,\n                                        dtype=pred_video.dtype,\n                                        generator=batch.generator[0]).to(\n                                            self.device)\n                    if sp_group:\n                        noise = rearrange(noise,\n                                          \"b (n t) c h w -&gt; b n t c h w\",\n                                          n=sp_world_size).contiguous()\n                        noise = noise[:, rank_in_sp_group, :, :, :, :]\n                    latents = self.scheduler.add_noise(\n                        pred_video.flatten(0, 1), noise.flatten(0, 1),\n                        next_timestep).unflatten(0, pred_video.shape[:2])\n                else:\n                    latents = pred_video\n\n                # Update progress bar\n                if i == len(timesteps) - 1 or (\n                    (i + 1) &gt; num_warmup_steps and\n                    (i + 1) % self.scheduler.order == 0\n                        and progress_bar is not None):\n                    progress_bar.update()\n\n    # Gather results if using sequence parallelism\n    if sp_group:\n        latents = sequence_model_parallel_all_gather(latents, dim=1)\n    latents = latents.permute(0, 2, 1, 3, 4)\n    # Update batch with final latents\n    batch.latents = latents\n\n    return batch\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.EncodingStage","title":"fastvideo.pipelines.stages.EncodingStage","text":"<pre><code>EncodingStage(vae: ParallelTiledVAE)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for encoding pixel space representations into latent space.</p> <p>This stage handles the encoding of pixel-space video/images into latent representations for further processing in the diffusion pipeline.</p> Source code in <code>fastvideo/pipelines/stages/encoding.py</code> <pre><code>def __init__(self, vae: ParallelTiledVAE) -&gt; None:\n    self.vae: ParallelTiledVAE = vae\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.EncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode pixel space representations into latent space.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded latents.</p> Source code in <code>fastvideo/pipelines/stages/encoding.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode pixel space representations into latent space.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded latents.\n    \"\"\"\n    assert batch.latents is not None and isinstance(batch.latents,\n                                                    torch.Tensor)\n\n    self.vae = self.vae.to(get_local_torch_device())\n\n    # Setup VAE precision\n    vae_dtype = PRECISION_TO_TYPE[\n        fastvideo_args.pipeline_config.vae_precision]\n    vae_autocast_enabled = (\n        vae_dtype != torch.float32) and not fastvideo_args.disable_autocast\n\n    # Normalize input to [-1, 1] range (reverse of decoding normalization)\n    latents = (batch.latents * 2.0 - 1.0).clamp(-1, 1)\n\n    # Move to appropriate device and dtype\n    latents = latents.to(get_local_torch_device())\n\n    # Encode image to latents\n    with torch.autocast(device_type=\"cuda\",\n                        dtype=vae_dtype,\n                        enabled=vae_autocast_enabled):\n        if fastvideo_args.pipeline_config.vae_tiling:\n            self.vae.enable_tiling()\n        # if fastvideo_args.vae_sp:\n        #     self.vae.enable_parallel()\n        if not vae_autocast_enabled:\n            latents = latents.to(vae_dtype)\n        latents = self.vae.encode(latents).mean\n\n    # Update batch with encoded latents\n    batch.latents = latents\n\n    # Offload models if needed\n    if hasattr(self, 'maybe_free_model_hooks'):\n        self.maybe_free_model_hooks()\n\n    if fastvideo_args.vae_cpu_offload:\n        self.vae.to(\"cpu\")\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.EncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/encoding.py</code> <pre><code>@torch.no_grad()\ndef verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    # Input video/images for VAE encoding: [batch_size, channels, frames, height, width]\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre> fastvideo.pipelines.stages.EncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    # Encoded latents: [batch_size, channels, frames, height_latents, width_latents]\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.ImageEncodingStage","title":"fastvideo.pipelines.stages.ImageEncodingStage","text":"<pre><code>ImageEncodingStage(image_encoder, image_processor)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for encoding image prompts into embeddings for diffusion models.</p> <p>This stage handles the encoding of image prompts into the embedding space expected by the diffusion model.</p> <p>Initialize the prompt encoding stage.</p> <p>Parameters:</p> Name Type Description Default <code>enable_logging</code> <p>Whether to enable logging for this stage.</p> required <code>is_secondary</code> <p>Whether this is a secondary image encoder.</p> required Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def __init__(self, image_encoder, image_processor) -&gt; None:\n    \"\"\"\n    Initialize the prompt encoding stage.\n\n    Args:\n        enable_logging: Whether to enable logging for this stage.\n        is_secondary: Whether this is a secondary image encoder.\n    \"\"\"\n    super().__init__()\n    self.image_processor = image_processor\n    self.image_encoder = image_encoder\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.ImageEncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode the prompt into image encoder hidden states.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded prompt embeddings.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode the prompt into image encoder hidden states.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded prompt embeddings.\n    \"\"\"\n    self.image_encoder = self.image_encoder.to(get_local_torch_device())\n\n    image = batch.pil_image\n\n    image_inputs = self.image_processor(\n        images=image, return_tensors=\"pt\").to(get_local_torch_device())\n    with set_forward_context(current_timestep=0, attn_metadata=None):\n        outputs = self.image_encoder(**image_inputs)\n        image_embeds = outputs.last_hidden_state\n\n    batch.image_embeds.append(image_embeds)\n\n    if fastvideo_args.image_encoder_cpu_offload:\n        self.image_encoder.to('cpu')\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.ImageEncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify image encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify image encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"pil_image\", batch.pil_image, V.not_none)\n    result.add_check(\"image_embeds\", batch.image_embeds, V.is_list)\n    return result\n</code></pre> fastvideo.pipelines.stages.ImageEncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify image encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify image encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"image_embeds\", batch.image_embeds,\n                     V.list_of_tensors_dims(3))\n    return result\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.ImageVAEEncodingStage","title":"fastvideo.pipelines.stages.ImageVAEEncodingStage","text":"<pre><code>ImageVAEEncodingStage(vae: ParallelTiledVAE)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for encoding image pixel representations into latent space.</p> <p>This stage handles the encoding of image pixel representations into the final input format (e.g., latents) for image-to-video generation.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def __init__(self, vae: ParallelTiledVAE) -&gt; None:\n    self.vae: ParallelTiledVAE = vae\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.ImageVAEEncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode pixel representations into latent space.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded outputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode pixel representations into latent space.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded outputs.\n    \"\"\"\n    assert batch.pil_image is not None\n    if fastvideo_args.mode == ExecutionMode.INFERENCE:\n        assert batch.pil_image is not None and isinstance(\n            batch.pil_image, PIL.Image.Image)\n        assert batch.height is not None and isinstance(batch.height, int)\n        assert batch.width is not None and isinstance(batch.width, int)\n        assert batch.num_frames is not None and isinstance(\n            batch.num_frames, int)\n        height = batch.height\n        width = batch.width\n        num_frames = batch.num_frames\n    elif fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        assert batch.pil_image is not None and isinstance(\n            batch.pil_image, torch.Tensor)\n        assert batch.height is not None and isinstance(batch.height, list)\n        assert batch.width is not None and isinstance(batch.width, list)\n        assert batch.num_frames is not None and isinstance(\n            batch.num_frames, list)\n        num_frames = batch.num_frames[0]\n        height = batch.height[0]\n        width = batch.width[0]\n\n    self.vae = self.vae.to(get_local_torch_device())\n\n    # Process single image for I2V\n    latent_height = height // self.vae.spatial_compression_ratio\n    latent_width = width // self.vae.spatial_compression_ratio\n    image = batch.pil_image\n    image = self.preprocess(\n        image,\n        vae_scale_factor=self.vae.spatial_compression_ratio,\n        height=height,\n        width=width).to(get_local_torch_device(), dtype=torch.float32)\n\n    # (B, C, H, W) -&gt; (B, C, 1, H, W)\n    image = image.unsqueeze(2)\n\n    video_condition = torch.cat([\n        image,\n        image.new_zeros(image.shape[0], image.shape[1], num_frames - 1,\n                        image.shape[3], image.shape[4])\n    ],\n                                dim=2)\n    video_condition = video_condition.to(device=get_local_torch_device(),\n                                         dtype=torch.float32)\n\n    # Setup VAE precision\n    vae_dtype = PRECISION_TO_TYPE[\n        fastvideo_args.pipeline_config.vae_precision]\n    vae_autocast_enabled = (\n        vae_dtype != torch.float32) and not fastvideo_args.disable_autocast\n\n    # Encode Image\n    with torch.autocast(device_type=\"cuda\",\n                        dtype=vae_dtype,\n                        enabled=vae_autocast_enabled):\n        if fastvideo_args.pipeline_config.vae_tiling:\n            self.vae.enable_tiling()\n        # if fastvideo_args.vae_sp:\n        #     self.vae.enable_parallel()\n        if not vae_autocast_enabled:\n            video_condition = video_condition.to(vae_dtype)\n        encoder_output = self.vae.encode(video_condition)\n\n    if fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        latent_condition = encoder_output.mean\n    else:\n        generator = batch.generator\n        if generator is None:\n            raise ValueError(\"Generator must be provided\")\n        latent_condition = self.retrieve_latents(encoder_output, generator)\n\n    # Apply shifting if needed\n    if (hasattr(self.vae, \"shift_factor\")\n            and self.vae.shift_factor is not None):\n        if isinstance(self.vae.shift_factor, torch.Tensor):\n            latent_condition -= self.vae.shift_factor.to(\n                latent_condition.device, latent_condition.dtype)\n        else:\n            latent_condition -= self.vae.shift_factor\n\n    if isinstance(self.vae.scaling_factor, torch.Tensor):\n        latent_condition = latent_condition * self.vae.scaling_factor.to(\n            latent_condition.device, latent_condition.dtype)\n    else:\n        latent_condition = latent_condition * self.vae.scaling_factor\n\n    if fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        batch.image_latent = latent_condition\n    else:\n        mask_lat_size = torch.ones(1, 1, num_frames, latent_height,\n                                   latent_width)\n        mask_lat_size[:, :, list(range(1, num_frames))] = 0\n        first_frame_mask = mask_lat_size[:, :, 0:1]\n        first_frame_mask = torch.repeat_interleave(\n            first_frame_mask,\n            dim=2,\n            repeats=self.vae.temporal_compression_ratio)\n        mask_lat_size = torch.concat(\n            [first_frame_mask, mask_lat_size[:, :, 1:, :]], dim=2)\n        mask_lat_size = mask_lat_size.view(\n            1, -1, self.vae.temporal_compression_ratio, latent_height,\n            latent_width)\n        mask_lat_size = mask_lat_size.transpose(1, 2)\n        mask_lat_size = mask_lat_size.to(latent_condition.device)\n\n        batch.image_latent = torch.concat([mask_lat_size, latent_condition],\n                                          dim=1)\n\n    # Offload models if needed\n    if hasattr(self, 'maybe_free_model_hooks'):\n        self.maybe_free_model_hooks()\n\n    self.vae.to(\"cpu\")\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.ImageVAEEncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    if fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        result.add_check(\"height\", batch.height, V.list_not_empty)\n        result.add_check(\"width\", batch.width, V.list_not_empty)\n        result.add_check(\"num_frames\", batch.num_frames, V.list_not_empty)\n    else:\n        result.add_check(\"height\", batch.height, V.positive_int)\n        result.add_check(\"width\", batch.width, V.positive_int)\n        result.add_check(\"num_frames\", batch.num_frames, V.positive_int)\n    return result\n</code></pre> fastvideo.pipelines.stages.ImageVAEEncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"image_latent\", batch.image_latent,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.InputValidationStage","title":"fastvideo.pipelines.stages.InputValidationStage","text":"<p>               Bases: <code>PipelineStage</code></p> <p>Stage for validating and preparing inputs for diffusion pipelines.</p> <p>This stage validates that all required inputs are present and properly formatted before proceeding with the diffusion process.</p> Functions\u00b6 fastvideo.pipelines.stages.InputValidationStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Validate and prepare inputs.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The validated batch information.</p> Source code in <code>fastvideo/pipelines/stages/input_validation.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Validate and prepare inputs.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The validated batch information.\n    \"\"\"\n\n    self._generate_seeds(batch, fastvideo_args)\n\n    # Ensure prompt is properly formatted\n    if batch.prompt is None and batch.prompt_embeds is None:\n        raise ValueError(\n            \"Either `prompt` or `prompt_embeds` must be provided\")\n\n    # Ensure negative prompt is properly formatted if using classifier-free guidance\n    if (batch.do_classifier_free_guidance and batch.negative_prompt is None\n            and batch.negative_prompt_embeds is None):\n        raise ValueError(\n            \"For classifier-free guidance, either `negative_prompt` or \"\n            \"`negative_prompt_embeds` must be provided\")\n\n    # Validate height and width\n    if batch.height is None or batch.width is None:\n        raise ValueError(\n            \"Height and width must be provided. Please set `height` and `width`.\"\n        )\n    if batch.height % 8 != 0 or batch.width % 8 != 0:\n        raise ValueError(\n            f\"Height and width must be divisible by 8 but are {batch.height} and {batch.width}.\"\n        )\n\n    # Validate number of inference steps\n    if batch.num_inference_steps &lt;= 0:\n        raise ValueError(\n            f\"Number of inference steps must be positive, but got {batch.num_inference_steps}\"\n        )\n\n    # Validate guidance scale if using classifier-free guidance\n    if batch.do_classifier_free_guidance and batch.guidance_scale &lt;= 0:\n        raise ValueError(\n            f\"Guidance scale must be positive, but got {batch.guidance_scale}\"\n        )\n\n    # for i2v, get image from image_path\n    # @TODO(Wei) hard-coded for wan2.2 5b ti2v for now. Should put this in image_encoding stage\n    if batch.image_path is not None:\n        if batch.image_path.endswith(\".mp4\"):\n            image = load_video(batch.image_path)[0]\n        else:\n            image = load_image(batch.image_path)\n        batch.pil_image = image\n\n    # further processing for ti2v task\n    if fastvideo_args.pipeline_config.ti2v_task and batch.pil_image is not None:\n        img = batch.pil_image\n        ih, iw = img.height, img.width\n        patch_size = fastvideo_args.pipeline_config.dit_config.arch_config.patch_size\n        vae_stride = fastvideo_args.pipeline_config.vae_config.arch_config.scale_factor_spatial\n        dh, dw = patch_size[1] * vae_stride, patch_size[2] * vae_stride\n        max_area = 704 * 1280\n        ow, oh = best_output_size(iw, ih, dw, dh, max_area)\n\n        scale = max(ow / iw, oh / ih)\n        img = img.resize((round(iw * scale), round(ih * scale)),\n                         Image.LANCZOS)\n        logger.info(\"resized img height: %s, img width: %s\", img.height,\n                    img.width)\n\n        # center-crop\n        x1 = (img.width - ow) // 2\n        y1 = (img.height - oh) // 2\n        img = img.crop((x1, y1, x1 + ow, y1 + oh))\n        assert img.width == ow and img.height == oh\n\n        # to tensor\n        img = TF.to_tensor(img).sub_(0.5).div_(0.5).to(\n            self.device).unsqueeze(1)\n        img = img.unsqueeze(0)\n        batch.height = oh\n        batch.width = ow\n        batch.pil_image = img\n\n    # for v2v, get control video from video path\n    if batch.video_path is not None:\n        pil_images, original_fps = load_video(batch.video_path,\n                                              return_fps=True)\n        logger.info(\"Loaded video with %s frames, original FPS: %s\",\n                    len(pil_images), original_fps)\n\n        # Get target parameters from batch\n        target_fps = batch.fps\n        target_num_frames = batch.num_frames\n        target_height = batch.height\n        target_width = batch.width\n\n        if target_fps is not None and original_fps is not None:\n            frame_skip = max(1, int(original_fps // target_fps))\n            if frame_skip &gt; 1:\n                pil_images = pil_images[::frame_skip]\n                effective_fps = original_fps / frame_skip\n                logger.info(\n                    \"Resampled video from %.1f fps to %.1f fps (skip=%s)\",\n                    original_fps, effective_fps, frame_skip)\n\n        # Limit to target number of frames\n        if target_num_frames is not None and len(\n                pil_images) &gt; target_num_frames:\n            pil_images = pil_images[:target_num_frames]\n            logger.info(\"Limited video to %s frames (from %s total)\",\n                        target_num_frames, len(pil_images))\n\n        # Resize each PIL image to target dimensions\n        resized_images = []\n        for pil_img in pil_images:\n            resized_img = resize(pil_img,\n                                 target_height,\n                                 target_width,\n                                 resize_mode=\"default\",\n                                 resample=\"lanczos\")\n            resized_images.append(resized_img)\n\n        # Convert PIL images to numpy array\n        video_numpy = pil_to_numpy(resized_images)\n        video_numpy = normalize(video_numpy)\n        video_tensor = numpy_to_pt(video_numpy)\n\n        # Rearrange to [C, T, H, W] and add batch dimension -&gt; [B, C, T, H, W]\n        input_video = video_tensor.permute(1, 0, 2, 3).unsqueeze(0)\n\n        batch.video_latent = input_video\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.InputValidationStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify input validation stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/input_validation.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify input validation stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"seed\", batch.seed, [V.not_none, V.positive_int])\n    result.add_check(\"num_videos_per_prompt\", batch.num_videos_per_prompt,\n                     V.positive_int)\n    result.add_check(\n        \"prompt_or_embeds\", None, lambda _: V.string_or_list_strings(\n            batch.prompt) or V.list_not_empty(batch.prompt_embeds))\n    result.add_check(\"height\", batch.height, V.positive_int)\n    result.add_check(\"width\", batch.width, V.positive_int)\n    result.add_check(\"num_inference_steps\", batch.num_inference_steps,\n                     V.positive_int)\n    result.add_check(\n        \"guidance_scale\", batch.guidance_scale, lambda x: not batch.\n        do_classifier_free_guidance or V.positive_float(x))\n    return result\n</code></pre> fastvideo.pipelines.stages.InputValidationStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify input validation stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/input_validation.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify input validation stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"seeds\", batch.seeds, V.list_not_empty)\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    return result\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.LatentPreparationStage","title":"fastvideo.pipelines.stages.LatentPreparationStage","text":"<pre><code>LatentPreparationStage(scheduler, transformer)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for preparing initial latent variables for the diffusion process.</p> <p>This stage handles the preparation of the initial latent variables that will be denoised during the diffusion process.</p> Source code in <code>fastvideo/pipelines/stages/latent_preparation.py</code> <pre><code>def __init__(self, scheduler, transformer) -&gt; None:\n    super().__init__()\n    self.scheduler = scheduler\n    self.transformer = transformer\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.LatentPreparationStage.adjust_video_length \u00b6 <pre><code>adjust_video_length(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; int\n</code></pre> <p>Adjust video length based on VAE version.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The batch with adjusted video length.</p> Source code in <code>fastvideo/pipelines/stages/latent_preparation.py</code> <pre><code>def adjust_video_length(self, batch: ForwardBatch,\n                        fastvideo_args: FastVideoArgs) -&gt; int:\n    \"\"\"\n    Adjust video length based on VAE version.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with adjusted video length.\n    \"\"\"\n\n    video_length = batch.num_frames\n    use_temporal_scaling_frames = fastvideo_args.pipeline_config.vae_config.use_temporal_scaling_frames\n    if use_temporal_scaling_frames:\n        temporal_scale_factor = fastvideo_args.pipeline_config.vae_config.arch_config.temporal_compression_ratio\n        latent_num_frames = (video_length - 1) // temporal_scale_factor + 1\n    else:  # stepvideo only\n        latent_num_frames = video_length // 17 * 3\n    return int(latent_num_frames)\n</code></pre> fastvideo.pipelines.stages.LatentPreparationStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Prepare initial latent variables for the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with prepared latent variables.</p> Source code in <code>fastvideo/pipelines/stages/latent_preparation.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Prepare initial latent variables for the diffusion process.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with prepared latent variables.\n    \"\"\"\n\n    latent_num_frames = None\n    # Adjust video length based on VAE version if needed\n    if hasattr(self, 'adjust_video_length'):\n        latent_num_frames = self.adjust_video_length(batch, fastvideo_args)\n    # Determine batch size\n    if isinstance(batch.prompt, list):\n        batch_size = len(batch.prompt)\n    elif batch.prompt is not None:\n        batch_size = 1\n    else:\n        batch_size = batch.prompt_embeds[0].shape[0]\n\n    # Adjust batch size for number of videos per prompt\n    batch_size *= batch.num_videos_per_prompt\n\n    # Get required parameters\n    dtype = batch.prompt_embeds[0].dtype\n    device = get_local_torch_device()\n    generator = batch.generator\n    latents = batch.latents\n    num_frames = latent_num_frames if latent_num_frames is not None else batch.num_frames\n    height = batch.height\n    width = batch.width\n\n    # TODO(will): remove this once we add input/output validation for stages\n    if height is None or width is None:\n        raise ValueError(\"Height and width must be provided\")\n\n    # Calculate latent shape\n    shape = (\n        batch_size,\n        self.transformer.num_channels_latents,\n        num_frames,\n        height // fastvideo_args.pipeline_config.vae_config.arch_config.\n        spatial_compression_ratio,\n        width // fastvideo_args.pipeline_config.vae_config.arch_config.\n        spatial_compression_ratio,\n    )\n\n    # Validate generator if it's a list\n    if isinstance(generator, list) and len(generator) != batch_size:\n        raise ValueError(\n            f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n            f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n        )\n    # Generate or use provided latents\n    if latents is None:\n        latents = randn_tensor(shape,\n                               generator=generator,\n                               device=device,\n                               dtype=dtype)\n    else:\n        latents = latents.to(device)\n\n    # Scale the initial noise if needed\n    if hasattr(self.scheduler, \"init_noise_sigma\"):\n        latents = latents * self.scheduler.init_noise_sigma\n    # Update batch with prepared latents\n    batch.latents = latents\n    batch.raw_latent_shape = latents.shape\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.LatentPreparationStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify latent preparation stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/latent_preparation.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify latent preparation stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\n        \"prompt_or_embeds\", None, lambda _: V.string_or_list_strings(\n            batch.prompt) or V.list_not_empty(batch.prompt_embeds))\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds,\n                     V.list_of_tensors)\n    result.add_check(\"num_videos_per_prompt\", batch.num_videos_per_prompt,\n                     V.positive_int)\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    result.add_check(\"num_frames\", batch.num_frames, V.positive_int)\n    result.add_check(\"height\", batch.height, V.positive_int)\n    result.add_check(\"width\", batch.width, V.positive_int)\n    result.add_check(\"latents\", batch.latents, V.none_or_tensor)\n    return result\n</code></pre> fastvideo.pipelines.stages.LatentPreparationStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify latent preparation stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/latent_preparation.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify latent preparation stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    result.add_check(\"raw_latent_shape\", batch.raw_latent_shape, V.is_tuple)\n    return result\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.PipelineStage","title":"fastvideo.pipelines.stages.PipelineStage","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all pipeline stages.</p> <p>A pipeline stage represents a discrete step in the diffusion process that can be composed with other stages to create a complete pipeline. Each stage is responsible for a specific part of the process, such as prompt encoding, latent preparation, etc.</p> Attributes\u00b6 fastvideo.pipelines.stages.PipelineStage.device <code>property</code> \u00b6 <pre><code>device: device\n</code></pre> <p>Get the device for this stage.</p> Functions\u00b6 fastvideo.pipelines.stages.PipelineStage.__call__ \u00b6 <pre><code>__call__(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Execute the stage's processing on the batch with optional verification and logging. Should not be overridden by subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The updated batch information after this stage's processing.</p> Source code in <code>fastvideo/pipelines/stages/base.py</code> <pre><code>def __call__(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Execute the stage's processing on the batch with optional verification and logging.\n    Should not be overridden by subclasses.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The updated batch information after this stage's processing.\n    \"\"\"\n    stage_name = self.__class__.__name__\n\n    # Check if verification is enabled (simple approach for prototype)\n    enable_verification = getattr(fastvideo_args,\n                                  'enable_stage_verification', False)\n\n    if enable_verification:\n        # Pre-execution input verification\n        try:\n            input_result = self.verify_input(batch, fastvideo_args)\n            self._run_verification(input_result, stage_name, \"input\")\n        except Exception as e:\n            logger.error(\"Input verification failed for %s: %s\", stage_name,\n                         str(e))\n            raise\n\n    # Execute the actual stage logic\n    if envs.FASTVIDEO_STAGE_LOGGING:\n        logger.info(\"[%s] Starting execution\", stage_name)\n        start_time = time.perf_counter()\n\n        try:\n            result = self.forward(batch, fastvideo_args)\n            execution_time = time.perf_counter() - start_time\n            logger.info(\"[%s] Execution completed in %s ms\", stage_name,\n                        execution_time * 1000)\n            batch.logging_info.add_stage_execution_time(\n                stage_name, execution_time)\n        except Exception as e:\n            execution_time = time.perf_counter() - start_time\n            logger.error(\"[%s] Error during execution after %s ms: %s\",\n                         stage_name, execution_time * 1000, e)\n            logger.error(\"[%s] Traceback: %s\", stage_name,\n                         traceback.format_exc())\n            raise\n    else:\n        # Direct execution (current behavior)\n        result = self.forward(batch, fastvideo_args)\n\n    if enable_verification:\n        # Post-execution output verification\n        try:\n            output_result = self.verify_output(result, fastvideo_args)\n            self._run_verification(output_result, stage_name, \"output\")\n        except Exception as e:\n            logger.error(\"Output verification failed for %s: %s\",\n                         stage_name, str(e))\n            raise\n\n    return result\n</code></pre> fastvideo.pipelines.stages.PipelineStage.forward <code>abstractmethod</code> \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Forward pass of the stage's processing.</p> <p>This method should be implemented by subclasses to provide the forward processing logic for the stage.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The updated batch information after this stage's processing.</p> Source code in <code>fastvideo/pipelines/stages/base.py</code> <pre><code>@abstractmethod\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Forward pass of the stage's processing.\n\n    This method should be implemented by subclasses to provide the forward\n    processing logic for the stage.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The updated batch information after this stage's processing.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.pipelines.stages.PipelineStage.set_logging \u00b6 <pre><code>set_logging(enable: bool)\n</code></pre> <p>Enable or disable logging for this stage.</p> <p>Parameters:</p> Name Type Description Default <code>enable</code> <code>bool</code> <p>Whether to enable logging.</p> required Source code in <code>fastvideo/pipelines/stages/base.py</code> <pre><code>def set_logging(self, enable: bool):\n    \"\"\"\n    Enable or disable logging for this stage.\n\n    Args:\n        enable: Whether to enable logging.\n    \"\"\"\n    self._enable_logging = enable\n</code></pre> fastvideo.pipelines.stages.PipelineStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify the input for the stage.</p> Example <p>from fastvideo.pipelines.stages.validators import V, VerificationResult</p> <p>def verify_input(self, batch, fastvideo_args):     result = VerificationResult()     result.add_check(\"height\", batch.height, V.positive_int_divisible(8))     result.add_check(\"width\", batch.width, V.positive_int_divisible(8))     result.add_check(\"image_latent\", batch.image_latent, V.is_tensor)     return result</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>VerificationResult</code> <p>A VerificationResult containing the verification status.</p> Source code in <code>fastvideo/pipelines/stages/base.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"\n    Verify the input for the stage.\n\n    Example:\n        from fastvideo.pipelines.stages.validators import V, VerificationResult\n\n        def verify_input(self, batch, fastvideo_args):\n            result = VerificationResult()\n            result.add_check(\"height\", batch.height, V.positive_int_divisible(8))\n            result.add_check(\"width\", batch.width, V.positive_int_divisible(8))\n            result.add_check(\"image_latent\", batch.image_latent, V.is_tensor)\n            return result\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        A VerificationResult containing the verification status.\n\n    \"\"\"\n    # Default implementation - no verification\n    return VerificationResult()\n</code></pre> fastvideo.pipelines.stages.PipelineStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify the output for the stage.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>VerificationResult</code> <p>A VerificationResult containing the verification status.</p> Source code in <code>fastvideo/pipelines/stages/base.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"\n    Verify the output for the stage.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        A VerificationResult containing the verification status.\n    \"\"\"\n    # Default implementation - no verification\n    return VerificationResult()\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.RefImageEncodingStage","title":"fastvideo.pipelines.stages.RefImageEncodingStage","text":"<pre><code>RefImageEncodingStage(image_encoder, image_processor)\n</code></pre> <p>               Bases: <code>ImageEncodingStage</code></p> <p>Stage for encoding reference image prompts into embeddings for Wan2.1 Control models.</p> <p>This stage extends ImageEncodingStage with specialized preprocessing for reference images.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def __init__(self, image_encoder, image_processor) -&gt; None:\n    \"\"\"\n    Initialize the prompt encoding stage.\n\n    Args:\n        enable_logging: Whether to enable logging for this stage.\n        is_secondary: Whether this is a secondary image encoder.\n    \"\"\"\n    super().__init__()\n    self.image_processor = image_processor\n    self.image_encoder = image_encoder\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.RefImageEncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode the prompt into image encoder hidden states.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded prompt embeddings.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode the prompt into image encoder hidden states.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded prompt embeddings.\n    \"\"\"\n    self.image_encoder = self.image_encoder.to(get_local_torch_device())\n\n    image = batch.pil_image\n    if image is None:\n        image = create_default_image()\n    # Preprocess reference image for CLIP encoder\n    image_tensor = preprocess_reference_image_for_clip(\n        image, get_local_torch_device())\n\n    image_inputs = self.image_processor(images=image_tensor,\n                                        return_tensors=\"pt\").to(\n                                            get_local_torch_device())\n    with set_forward_context(current_timestep=0, attn_metadata=None):\n        outputs = self.image_encoder(**image_inputs)\n        image_embeds = outputs.last_hidden_state\n    batch.image_embeds.append(image_embeds)\n\n    if batch.pil_image is None:\n        batch.image_embeds = [\n            torch.zeros_like(x) for x in batch.image_embeds\n        ]\n\n    return batch\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.StepvideoPromptEncodingStage","title":"fastvideo.pipelines.stages.StepvideoPromptEncodingStage","text":"<pre><code>StepvideoPromptEncodingStage(stepllm, clip)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for encoding prompts using the remote caption API.</p> <p>This stage applies the magic string transformations and calls the remote caption service asynchronously to get:   - primary prompt embeddings,   - an attention mask,   - and a clip embedding.</p> Source code in <code>fastvideo/pipelines/stages/stepvideo_encoding.py</code> <pre><code>def __init__(self, stepllm, clip) -&gt; None:\n    super().__init__()\n    # self.caption_client = caption_client  # This should have a call_caption(prompts: List[str]) method.\n    self.stepllm = stepllm\n    self.clip = clip\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.StepvideoPromptEncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify stepvideo encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/stepvideo_encoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify stepvideo encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"prompt\", batch.prompt, V.string_not_empty)\n    return result\n</code></pre> fastvideo.pipelines.stages.StepvideoPromptEncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify stepvideo encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/stepvideo_encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify stepvideo encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds,\n                     [V.is_tensor, V.with_dims(3)])\n    result.add_check(\"negative_prompt_embeds\", batch.negative_prompt_embeds,\n                     [V.is_tensor, V.with_dims(3)])\n    result.add_check(\"prompt_attention_mask\", batch.prompt_attention_mask,\n                     [V.is_tensor, V.with_dims(2)])\n    result.add_check(\"negative_attention_mask\",\n                     batch.negative_attention_mask,\n                     [V.is_tensor, V.with_dims(2)])\n    result.add_check(\"clip_embedding_pos\", batch.clip_embedding_pos,\n                     [V.is_tensor, V.with_dims(2)])\n    result.add_check(\"clip_embedding_neg\", batch.clip_embedding_neg,\n                     [V.is_tensor, V.with_dims(2)])\n    return result\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.TextEncodingStage","title":"fastvideo.pipelines.stages.TextEncodingStage","text":"<pre><code>TextEncodingStage(text_encoders, tokenizers)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for encoding text prompts into embeddings for diffusion models.</p> <p>This stage handles the encoding of text prompts into the embedding space expected by the diffusion model.</p> <p>Initialize the prompt encoding stage.</p> <p>Parameters:</p> Name Type Description Default <code>enable_logging</code> <p>Whether to enable logging for this stage.</p> required <code>is_secondary</code> <p>Whether this is a secondary text encoder.</p> required Source code in <code>fastvideo/pipelines/stages/text_encoding.py</code> <pre><code>def __init__(self, text_encoders, tokenizers) -&gt; None:\n    \"\"\"\n    Initialize the prompt encoding stage.\n\n    Args:\n        enable_logging: Whether to enable logging for this stage.\n        is_secondary: Whether this is a secondary text encoder.\n    \"\"\"\n    super().__init__()\n    self.tokenizers = tokenizers\n    self.text_encoders = text_encoders\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.TextEncodingStage.encode_text \u00b6 <pre><code>encode_text(\n    text: str | list[str],\n    fastvideo_args: FastVideoArgs,\n    encoder_index: int | list[int] | None = None,\n    return_attention_mask: bool = False,\n    return_type: str = \"list\",\n    device: device | str | None = None,\n    dtype: dtype | None = None,\n    max_length: int | None = None,\n    truncation: bool | None = None,\n    padding: bool | str | None = None,\n)\n</code></pre> <p>Encode plain text using selected text encoder(s) and return embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str | list[str]</code> <p>A single string or a list of strings to encode.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments providing pipeline config, including tokenizer and encoder settings, preprocess and postprocess functions.</p> required <code>encoder_index</code> <code>int | list[int] | None</code> <p>Encoder selector by index. Accepts an int or list of ints.</p> <code>None</code> <code>return_attention_mask</code> <code>bool</code> <p>If True, also return attention masks for each selected encoder.</p> <code>False</code> <code>return_type</code> <code>str</code> <p>\"list\" (default) returns a list aligned with selection; \"dict\" returns a dict keyed by encoder index as a string; \"stack\" stacks along a new first dimension (requires matching shapes).</p> <code>'list'</code> <code>device</code> <code>device | str | None</code> <p>Optional device override for inputs; defaults to local torch device.</p> <code>None</code> <code>dtype</code> <code>dtype | None</code> <p>Optional dtype to cast returned embeddings to.</p> <code>None</code> <code>max_length</code> <code>int | None</code> <p>Optional per-call tokenizer override.</p> <code>None</code> <code>truncation</code> <code>bool | None</code> <p>Optional per-call tokenizer override.</p> <code>None</code> <code>padding</code> <code>bool | str | None</code> <p>Optional per-call tokenizer override.</p> <code>None</code> <p>Returns:</p> Type Description <p>Depending on return_type and return_attention_mask:</p> <ul> <li>list: List[Tensor] or (List[Tensor], List[Tensor])</li> </ul> <ul> <li>dict: Dict[str, Tensor] or (Dict[str, Tensor], Dict[str, Tensor])</li> </ul> <ul> <li>stack: Tensor of shape [num_encoders, ...] or a tuple with stacked attention masks</li> </ul> Source code in <code>fastvideo/pipelines/stages/text_encoding.py</code> <pre><code>@torch.no_grad()\ndef encode_text(\n    self,\n    text: str | list[str],\n    fastvideo_args: FastVideoArgs,\n    encoder_index: int | list[int] | None = None,\n    return_attention_mask: bool = False,\n    return_type: str = \"list\",  # one of: \"list\", \"dict\", \"stack\"\n    device: torch.device | str | None = None,\n    dtype: torch.dtype | None = None,\n    max_length: int | None = None,\n    truncation: bool | None = None,\n    padding: bool | str | None = None,\n):\n    \"\"\"\n    Encode plain text using selected text encoder(s) and return embeddings.\n\n    Args:\n        text: A single string or a list of strings to encode.\n        fastvideo_args: The inference arguments providing pipeline config,\n            including tokenizer and encoder settings, preprocess and postprocess\n            functions.\n        encoder_index: Encoder selector by index. Accepts an int or list of ints.\n        return_attention_mask: If True, also return attention masks for each\n            selected encoder.\n        return_type: \"list\" (default) returns a list aligned with selection;\n            \"dict\" returns a dict keyed by encoder index as a string; \"stack\" stacks along a\n            new first dimension (requires matching shapes).\n        device: Optional device override for inputs; defaults to local torch device.\n        dtype: Optional dtype to cast returned embeddings to.\n        max_length: Optional per-call tokenizer override.\n        truncation: Optional per-call tokenizer override.\n        padding: Optional per-call tokenizer override.\n\n    Returns:\n        Depending on return_type and return_attention_mask:\n        - list: List[Tensor] or (List[Tensor], List[Tensor])\n        - dict: Dict[str, Tensor] or (Dict[str, Tensor], Dict[str, Tensor])\n        - stack: Tensor of shape [num_encoders, ...] or a tuple with stacked\n          attention masks\n    \"\"\"\n\n    assert len(self.tokenizers) == len(self.text_encoders)\n    assert len(self.text_encoders) == len(\n        fastvideo_args.pipeline_config.text_encoder_configs)\n\n    # Resolve selection into indices\n    encoder_cfgs = fastvideo_args.pipeline_config.text_encoder_configs\n    if encoder_index is None:\n        indices: list[int] = [0]\n    elif isinstance(encoder_index, int):\n        indices = [encoder_index]\n    else:\n        indices = list(encoder_index)\n    # validate range\n    num_encoders = len(self.text_encoders)\n    for idx in indices:\n        if idx &lt; 0 or idx &gt;= num_encoders:\n            raise IndexError(\n                f\"encoder index {idx} out of range [0, {num_encoders-1}]\")\n\n    # Validate indices are within range\n    num_encoders = len(self.text_encoders)\n\n    # Normalize input to list[str]\n    assert isinstance(text, str | list)\n    if isinstance(text, str):\n        texts: list[str] = [text]\n    else:\n        texts = text\n\n    embeds_list: list[torch.Tensor] = []\n    attn_masks_list: list[torch.Tensor] = []\n\n    preprocess_funcs = fastvideo_args.pipeline_config.preprocess_text_funcs\n    postprocess_funcs = fastvideo_args.pipeline_config.postprocess_text_funcs\n    encoder_cfgs = fastvideo_args.pipeline_config.text_encoder_configs\n\n    if return_type not in (\"list\", \"dict\", \"stack\"):\n        raise ValueError(\n            f\"Invalid return_type '{return_type}'. Expected one of: 'list', 'dict', 'stack'\"\n        )\n\n    target_device = device if device is not None else get_local_torch_device(\n    )\n\n    for i in indices:\n        tokenizer = self.tokenizers[i]\n        text_encoder = self.text_encoders[i]\n        encoder_config = encoder_cfgs[i]\n        preprocess_func = preprocess_funcs[i]\n        postprocess_func = postprocess_funcs[i]\n\n        processed_texts: list[str] = []\n        for prompt_str in texts:\n            processed_texts.append(preprocess_func(prompt_str))\n\n        tok_kwargs = dict(encoder_config.tokenizer_kwargs)\n        if max_length is not None:\n            tok_kwargs[\"max_length\"] = max_length\n        if truncation is not None:\n            tok_kwargs[\"truncation\"] = truncation\n        if padding is not None:\n            tok_kwargs[\"padding\"] = padding\n\n        text_inputs = tokenizer(processed_texts,\n                                **tok_kwargs).to(target_device)\n\n        input_ids = text_inputs[\"input_ids\"]\n        attention_mask = text_inputs[\"attention_mask\"]\n\n        with set_forward_context(current_timestep=0, attn_metadata=None):\n            outputs = text_encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                output_hidden_states=True,\n            )\n\n        prompt_embeds = postprocess_func(outputs)\n        if dtype is not None:\n            prompt_embeds = prompt_embeds.to(dtype=dtype)\n        embeds_list.append(prompt_embeds)\n        if return_attention_mask:\n            attn_masks_list.append(attention_mask)\n\n    # Shape results according to return_type\n    if return_type == \"list\":\n        if return_attention_mask:\n            return embeds_list, attn_masks_list\n        return embeds_list\n\n    if return_type == \"dict\":\n        key_strs = [str(i) for i in indices]\n        embeds_dict = {\n            k: v\n            for k, v in zip(key_strs, embeds_list, strict=False)\n        }\n        if return_attention_mask:\n            attn_dict = {\n                k: v\n                for k, v in zip(key_strs, attn_masks_list, strict=False)\n            }\n            return embeds_dict, attn_dict\n        return embeds_dict\n\n    # return_type == \"stack\"\n    # Validate shapes are compatible\n    base_shape = list(embeds_list[0].shape)\n    for t in embeds_list[1:]:\n        if list(t.shape) != base_shape:\n            raise ValueError(\n                f\"Cannot stack embeddings with differing shapes: {[list(t.shape) for t in embeds_list]}\"\n            )\n    stacked_embeds = torch.stack(embeds_list, dim=0)\n    if return_attention_mask:\n        base_mask_shape = list(attn_masks_list[0].shape)\n        for m in attn_masks_list[1:]:\n            if list(m.shape) != base_mask_shape:\n                raise ValueError(\n                    f\"Cannot stack attention masks with differing shapes: {[list(m.shape) for m in attn_masks_list]}\"\n                )\n        stacked_masks = torch.stack(attn_masks_list, dim=0)\n        return stacked_embeds, stacked_masks\n    return stacked_embeds\n</code></pre> fastvideo.pipelines.stages.TextEncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode the prompt into text encoder hidden states.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded prompt embeddings.</p> Source code in <code>fastvideo/pipelines/stages/text_encoding.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode the prompt into text encoder hidden states.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded prompt embeddings.\n    \"\"\"\n    assert len(self.tokenizers) == len(self.text_encoders)\n    assert len(self.text_encoders) == len(\n        fastvideo_args.pipeline_config.text_encoder_configs)\n\n    # Encode positive prompt with all available encoders\n    assert batch.prompt is not None\n    prompt_text: str | list[str] = batch.prompt\n    all_indices: list[int] = list(range(len(self.text_encoders)))\n    prompt_embeds_list, prompt_masks_list = self.encode_text(\n        prompt_text,\n        fastvideo_args,\n        encoder_index=all_indices,\n        return_attention_mask=True,\n    )\n    for pe in prompt_embeds_list:\n        batch.prompt_embeds.append(pe)\n    if batch.prompt_attention_mask is not None:\n        for am in prompt_masks_list:\n            batch.prompt_attention_mask.append(am)\n\n    # Encode negative prompt if CFG is enabled\n    if batch.do_classifier_free_guidance:\n        assert isinstance(batch.negative_prompt, str)\n        neg_embeds_list, neg_masks_list = self.encode_text(\n            batch.negative_prompt,\n            fastvideo_args,\n            encoder_index=all_indices,\n            return_attention_mask=True,\n        )\n        assert batch.negative_prompt_embeds is not None\n        for ne in neg_embeds_list:\n            batch.negative_prompt_embeds.append(ne)\n        if batch.negative_attention_mask is not None:\n            for nm in neg_masks_list:\n                batch.negative_attention_mask.append(nm)\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.TextEncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify text encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/text_encoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify text encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"prompt\", batch.prompt, V.string_or_list_strings)\n    result.add_check(\n        \"negative_prompt\", batch.negative_prompt, lambda x: not batch.\n        do_classifier_free_guidance or V.string_not_empty(x))\n    result.add_check(\"do_classifier_free_guidance\",\n                     batch.do_classifier_free_guidance, V.bool_value)\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds, V.is_list)\n    result.add_check(\"negative_prompt_embeds\", batch.negative_prompt_embeds,\n                     V.none_or_list)\n    return result\n</code></pre> fastvideo.pipelines.stages.TextEncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify text encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/text_encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify text encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds,\n                     V.list_of_tensors_min_dims(2))\n    result.add_check(\n        \"negative_prompt_embeds\", batch.negative_prompt_embeds,\n        lambda x: not batch.do_classifier_free_guidance or V.\n        list_of_tensors_with_min_dims(x, 2))\n    return result\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.TimestepPreparationStage","title":"fastvideo.pipelines.stages.TimestepPreparationStage","text":"<pre><code>TimestepPreparationStage(scheduler)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for preparing timesteps for the diffusion process.</p> <p>This stage handles the preparation of the timestep sequence that will be used during the diffusion process.</p> Source code in <code>fastvideo/pipelines/stages/timestep_preparation.py</code> <pre><code>def __init__(self, scheduler) -&gt; None:\n    self.scheduler = scheduler\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.TimestepPreparationStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Prepare timesteps for the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with prepared timesteps.</p> Source code in <code>fastvideo/pipelines/stages/timestep_preparation.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Prepare timesteps for the diffusion process.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with prepared timesteps.\n    \"\"\"\n    scheduler = self.scheduler\n    device = get_local_torch_device()\n    num_inference_steps = batch.num_inference_steps\n    timesteps = batch.timesteps\n    sigmas = batch.sigmas\n    n_tokens = batch.n_tokens\n\n    # Prepare extra kwargs for set_timesteps\n    extra_set_timesteps_kwargs = {}\n    if n_tokens is not None and \"n_tokens\" in inspect.signature(\n            scheduler.set_timesteps).parameters:\n        extra_set_timesteps_kwargs[\"n_tokens\"] = n_tokens\n\n    # Handle custom timesteps or sigmas\n    if timesteps is not None and sigmas is not None:\n        raise ValueError(\n            \"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\"\n        )\n\n    if timesteps is not None:\n        accepts_timesteps = \"timesteps\" in inspect.signature(\n            scheduler.set_timesteps).parameters\n        if not accepts_timesteps:\n            raise ValueError(\n                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n            )\n        scheduler.set_timesteps(timesteps=timesteps,\n                                device=device,\n                                **extra_set_timesteps_kwargs)\n        timesteps = scheduler.timesteps\n    elif sigmas is not None:\n        accept_sigmas = \"sigmas\" in inspect.signature(\n            scheduler.set_timesteps).parameters\n        if not accept_sigmas:\n            raise ValueError(\n                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n                f\" sigmas schedules. Please check whether you are using the correct scheduler.\"\n            )\n        scheduler.set_timesteps(sigmas=sigmas,\n                                device=device,\n                                **extra_set_timesteps_kwargs)\n        timesteps = scheduler.timesteps\n    else:\n        scheduler.set_timesteps(num_inference_steps,\n                                device=device,\n                                **extra_set_timesteps_kwargs)\n        timesteps = scheduler.timesteps\n\n    # Update batch with prepared timesteps\n    batch.timesteps = timesteps\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.TimestepPreparationStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify timestep preparation stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/timestep_preparation.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify timestep preparation stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"num_inference_steps\", batch.num_inference_steps,\n                     V.positive_int)\n    result.add_check(\"timesteps\", batch.timesteps, V.none_or_tensor)\n    result.add_check(\"sigmas\", batch.sigmas, V.none_or_list)\n    result.add_check(\"n_tokens\", batch.n_tokens, V.none_or_positive_int)\n    return result\n</code></pre> fastvideo.pipelines.stages.TimestepPreparationStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify timestep preparation stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/timestep_preparation.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify timestep preparation stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"timesteps\", batch.timesteps,\n                     [V.is_tensor, V.with_dims(1)])\n    return result\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.VideoVAEEncodingStage","title":"fastvideo.pipelines.stages.VideoVAEEncodingStage","text":"<pre><code>VideoVAEEncodingStage(vae: ParallelTiledVAE)\n</code></pre> <p>               Bases: <code>ImageVAEEncodingStage</code></p> <p>Stage for encoding video pixel representations into latent space.</p> <p>This stage handles the encoding of video pixel representations for video-to-video generation and control. Inherits from ImageVAEEncodingStage to reuse common functionality.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def __init__(self, vae: ParallelTiledVAE) -&gt; None:\n    self.vae: ParallelTiledVAE = vae\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.VideoVAEEncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode video pixel representations into latent space.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded outputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode video pixel representations into latent space.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded outputs.\n    \"\"\"\n    assert batch.video_latent is not None, \"Video latent input is required for VideoVAEEncodingStage\"\n\n    if fastvideo_args.mode == ExecutionMode.INFERENCE:\n        assert batch.height is not None and isinstance(batch.height, int)\n        assert batch.width is not None and isinstance(batch.width, int)\n        assert batch.num_frames is not None and isinstance(\n            batch.num_frames, int)\n        height = batch.height\n        width = batch.width\n        num_frames = batch.num_frames\n    elif fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        assert batch.height is not None and isinstance(batch.height, list)\n        assert batch.width is not None and isinstance(batch.width, list)\n        assert batch.num_frames is not None and isinstance(\n            batch.num_frames, list)\n        num_frames = batch.num_frames[0]\n        height = batch.height[0]\n        width = batch.width[0]\n\n    self.vae = self.vae.to(get_local_torch_device())\n\n    # Prepare video tensor from control video\n    video_condition = self._prepare_control_video_tensor(\n        batch.video_latent, num_frames, height,\n        width).to(get_local_torch_device(), dtype=torch.float32)\n\n    # Setup VAE precision\n    vae_dtype = PRECISION_TO_TYPE[\n        fastvideo_args.pipeline_config.vae_precision]\n    vae_autocast_enabled = (\n        vae_dtype != torch.float32) and not fastvideo_args.disable_autocast\n\n    # Encode control video\n    with torch.autocast(device_type=\"cuda\",\n                        dtype=vae_dtype,\n                        enabled=vae_autocast_enabled):\n        if fastvideo_args.pipeline_config.vae_tiling:\n            self.vae.enable_tiling()\n        if not vae_autocast_enabled:\n            video_condition = video_condition.to(vae_dtype)\n        encoder_output = self.vae.encode(video_condition)\n\n    generator = batch.generator\n    if generator is None:\n        raise ValueError(\"Generator must be provided\")\n    latent_condition = self.retrieve_latents(encoder_output, generator)\n\n    if (hasattr(self.vae, \"shift_factor\")\n            and self.vae.shift_factor is not None):\n        if isinstance(self.vae.shift_factor, torch.Tensor):\n            latent_condition -= self.vae.shift_factor.to(\n                latent_condition.device, latent_condition.dtype)\n        else:\n            latent_condition -= self.vae.shift_factor\n\n    if isinstance(self.vae.scaling_factor, torch.Tensor):\n        latent_condition = latent_condition * self.vae.scaling_factor.to(\n            latent_condition.device, latent_condition.dtype)\n    else:\n        latent_condition = latent_condition * self.vae.scaling_factor\n\n    batch.video_latent = latent_condition\n\n    # Offload models if needed\n    if hasattr(self, 'maybe_free_model_hooks'):\n        self.maybe_free_model_hooks()\n\n    self.vae.to(\"cpu\")\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.VideoVAEEncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify video encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify video encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"video_latent\", batch.video_latent, V.not_none)\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    if fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        result.add_check(\"height\", batch.height, V.list_not_empty)\n        result.add_check(\"width\", batch.width, V.list_not_empty)\n        result.add_check(\"num_frames\", batch.num_frames, V.list_not_empty)\n    else:\n        result.add_check(\"height\", batch.height, V.positive_int)\n        result.add_check(\"width\", batch.width, V.positive_int)\n        result.add_check(\"num_frames\", batch.num_frames, V.positive_int)\n    return result\n</code></pre> fastvideo.pipelines.stages.VideoVAEEncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify video encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify video encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"video_latent\", batch.video_latent,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages-modules","title":"Modules","text":""},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.base","title":"fastvideo.pipelines.stages.base","text":"<p>Base classes for pipeline stages.</p> <p>This module defines the abstract base classes for pipeline stages that can be composed to create complete diffusion pipelines.</p> Classes\u00b6 fastvideo.pipelines.stages.base.PipelineStage \u00b6 <p>               Bases: <code>ABC</code></p> <p>Abstract base class for all pipeline stages.</p> <p>A pipeline stage represents a discrete step in the diffusion process that can be composed with other stages to create a complete pipeline. Each stage is responsible for a specific part of the process, such as prompt encoding, latent preparation, etc.</p> Attributes\u00b6 fastvideo.pipelines.stages.base.PipelineStage.device <code>property</code> \u00b6 <pre><code>device: device\n</code></pre> <p>Get the device for this stage.</p> Functions\u00b6 fastvideo.pipelines.stages.base.PipelineStage.__call__ \u00b6 <pre><code>__call__(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Execute the stage's processing on the batch with optional verification and logging. Should not be overridden by subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The updated batch information after this stage's processing.</p> Source code in <code>fastvideo/pipelines/stages/base.py</code> <pre><code>def __call__(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Execute the stage's processing on the batch with optional verification and logging.\n    Should not be overridden by subclasses.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The updated batch information after this stage's processing.\n    \"\"\"\n    stage_name = self.__class__.__name__\n\n    # Check if verification is enabled (simple approach for prototype)\n    enable_verification = getattr(fastvideo_args,\n                                  'enable_stage_verification', False)\n\n    if enable_verification:\n        # Pre-execution input verification\n        try:\n            input_result = self.verify_input(batch, fastvideo_args)\n            self._run_verification(input_result, stage_name, \"input\")\n        except Exception as e:\n            logger.error(\"Input verification failed for %s: %s\", stage_name,\n                         str(e))\n            raise\n\n    # Execute the actual stage logic\n    if envs.FASTVIDEO_STAGE_LOGGING:\n        logger.info(\"[%s] Starting execution\", stage_name)\n        start_time = time.perf_counter()\n\n        try:\n            result = self.forward(batch, fastvideo_args)\n            execution_time = time.perf_counter() - start_time\n            logger.info(\"[%s] Execution completed in %s ms\", stage_name,\n                        execution_time * 1000)\n            batch.logging_info.add_stage_execution_time(\n                stage_name, execution_time)\n        except Exception as e:\n            execution_time = time.perf_counter() - start_time\n            logger.error(\"[%s] Error during execution after %s ms: %s\",\n                         stage_name, execution_time * 1000, e)\n            logger.error(\"[%s] Traceback: %s\", stage_name,\n                         traceback.format_exc())\n            raise\n    else:\n        # Direct execution (current behavior)\n        result = self.forward(batch, fastvideo_args)\n\n    if enable_verification:\n        # Post-execution output verification\n        try:\n            output_result = self.verify_output(result, fastvideo_args)\n            self._run_verification(output_result, stage_name, \"output\")\n        except Exception as e:\n            logger.error(\"Output verification failed for %s: %s\",\n                         stage_name, str(e))\n            raise\n\n    return result\n</code></pre> fastvideo.pipelines.stages.base.PipelineStage.forward <code>abstractmethod</code> \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Forward pass of the stage's processing.</p> <p>This method should be implemented by subclasses to provide the forward processing logic for the stage.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The updated batch information after this stage's processing.</p> Source code in <code>fastvideo/pipelines/stages/base.py</code> <pre><code>@abstractmethod\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Forward pass of the stage's processing.\n\n    This method should be implemented by subclasses to provide the forward\n    processing logic for the stage.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The updated batch information after this stage's processing.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.pipelines.stages.base.PipelineStage.set_logging \u00b6 <pre><code>set_logging(enable: bool)\n</code></pre> <p>Enable or disable logging for this stage.</p> <p>Parameters:</p> Name Type Description Default <code>enable</code> <code>bool</code> <p>Whether to enable logging.</p> required Source code in <code>fastvideo/pipelines/stages/base.py</code> <pre><code>def set_logging(self, enable: bool):\n    \"\"\"\n    Enable or disable logging for this stage.\n\n    Args:\n        enable: Whether to enable logging.\n    \"\"\"\n    self._enable_logging = enable\n</code></pre> fastvideo.pipelines.stages.base.PipelineStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify the input for the stage.</p> Example <p>from fastvideo.pipelines.stages.validators import V, VerificationResult</p> <p>def verify_input(self, batch, fastvideo_args):     result = VerificationResult()     result.add_check(\"height\", batch.height, V.positive_int_divisible(8))     result.add_check(\"width\", batch.width, V.positive_int_divisible(8))     result.add_check(\"image_latent\", batch.image_latent, V.is_tensor)     return result</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>VerificationResult</code> <p>A VerificationResult containing the verification status.</p> Source code in <code>fastvideo/pipelines/stages/base.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"\n    Verify the input for the stage.\n\n    Example:\n        from fastvideo.pipelines.stages.validators import V, VerificationResult\n\n        def verify_input(self, batch, fastvideo_args):\n            result = VerificationResult()\n            result.add_check(\"height\", batch.height, V.positive_int_divisible(8))\n            result.add_check(\"width\", batch.width, V.positive_int_divisible(8))\n            result.add_check(\"image_latent\", batch.image_latent, V.is_tensor)\n            return result\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        A VerificationResult containing the verification status.\n\n    \"\"\"\n    # Default implementation - no verification\n    return VerificationResult()\n</code></pre> fastvideo.pipelines.stages.base.PipelineStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify the output for the stage.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>VerificationResult</code> <p>A VerificationResult containing the verification status.</p> Source code in <code>fastvideo/pipelines/stages/base.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"\n    Verify the output for the stage.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        A VerificationResult containing the verification status.\n    \"\"\"\n    # Default implementation - no verification\n    return VerificationResult()\n</code></pre> fastvideo.pipelines.stages.base.StageVerificationError \u00b6 <p>               Bases: <code>Exception</code></p> <p>Exception raised when stage verification fails.</p> Functions\u00b6"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.causal_denoising","title":"fastvideo.pipelines.stages.causal_denoising","text":"Classes\u00b6 fastvideo.pipelines.stages.causal_denoising.CausalDMDDenosingStage \u00b6 <pre><code>CausalDMDDenosingStage(\n    transformer, scheduler, transformer_2=None\n)\n</code></pre> <p>               Bases: <code>DenoisingStage</code></p> <p>Denoising stage for causal diffusion.</p> Source code in <code>fastvideo/pipelines/stages/causal_denoising.py</code> <pre><code>def __init__(self, transformer, scheduler, transformer_2=None) -&gt; None:\n    super().__init__(transformer, scheduler, transformer_2)\n    # KV and cross-attention cache state (initialized on first forward)\n    self.transformer = transformer\n    self.transformer_2 = transformer_2\n    self.kv_cache1: list | None = None\n    self.crossattn_cache: list | None = None\n    # Model-dependent constants (aligned with causal_inference.py assumptions)\n    self.num_transformer_blocks = len(self.transformer.blocks)\n    self.num_frames_per_block = self.transformer.config.arch_config.num_frames_per_block\n    self.sliding_window_num_frames = self.transformer.config.arch_config.sliding_window_num_frames\n\n    try:\n        self.local_attn_size = getattr(self.transformer.model,\n                                       \"local_attn_size\",\n                                       -1)  # type: ignore\n    except Exception:\n        self.local_attn_size = -1\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.causal_denoising.CausalDMDDenosingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify denoising stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/causal_denoising.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify denoising stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds, V.list_not_empty)\n    result.add_check(\"image_embeds\", batch.image_embeds, V.is_list)\n    result.add_check(\"image_latent\", batch.image_latent,\n                     V.none_or_tensor_with_dims(5))\n    result.add_check(\"num_inference_steps\", batch.num_inference_steps,\n                     V.positive_int)\n    result.add_check(\"guidance_scale\", batch.guidance_scale,\n                     V.positive_float)\n    result.add_check(\"eta\", batch.eta, V.non_negative_float)\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    result.add_check(\"do_classifier_free_guidance\",\n                     batch.do_classifier_free_guidance, V.bool_value)\n    result.add_check(\n        \"negative_prompt_embeds\", batch.negative_prompt_embeds, lambda x:\n        not batch.do_classifier_free_guidance or V.list_not_empty(x))\n    return result\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.conditioning","title":"fastvideo.pipelines.stages.conditioning","text":"<p>Conditioning stage for diffusion pipelines.</p> Classes\u00b6 fastvideo.pipelines.stages.conditioning.ConditioningStage \u00b6 <p>               Bases: <code>PipelineStage</code></p> <p>Stage for applying conditioning to the diffusion process.</p> <p>This stage handles the application of conditioning, such as classifier-free guidance, to the diffusion process.</p> Functions\u00b6 fastvideo.pipelines.stages.conditioning.ConditioningStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Apply conditioning to the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with applied conditioning.</p> Source code in <code>fastvideo/pipelines/stages/conditioning.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Apply conditioning to the diffusion process.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with applied conditioning.\n    \"\"\"\n    # TODO!!\n    if not batch.do_classifier_free_guidance:\n        return batch\n    else:\n        return batch\n\n    logger.info(\"batch.negative_prompt_embeds: %s\",\n                batch.negative_prompt_embeds)\n    logger.info(\"do_classifier_free_guidance: %s\",\n                batch.do_classifier_free_guidance)\n    logger.info(\"cfg_scale: %s\", batch.guidance_scale)\n\n    # Ensure negative prompt embeddings are available\n    assert batch.negative_prompt_embeds is not None, (\n        \"Negative prompt embeddings are required for classifier-free guidance\"\n    )\n\n    # Concatenate primary embeddings and masks\n    batch.prompt_embeds = torch.cat(\n        [batch.negative_prompt_embeds, batch.prompt_embeds])\n    if batch.attention_mask is not None:\n        batch.attention_mask = torch.cat(\n            [batch.negative_attention_mask, batch.attention_mask])\n\n    # Concatenate secondary embeddings and masks if present\n    if batch.prompt_embeds_2 is not None:\n        batch.prompt_embeds_2 = torch.cat(\n            [batch.negative_prompt_embeds_2, batch.prompt_embeds_2])\n    if batch.attention_mask_2 is not None:\n        batch.attention_mask_2 = torch.cat(\n            [batch.negative_attention_mask_2, batch.attention_mask_2])\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.conditioning.ConditioningStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify conditioning stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/conditioning.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify conditioning stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"do_classifier_free_guidance\",\n                     batch.do_classifier_free_guidance, V.bool_value)\n    result.add_check(\"guidance_scale\", batch.guidance_scale,\n                     V.positive_float)\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds, V.list_not_empty)\n    result.add_check(\n        \"negative_prompt_embeds\", batch.negative_prompt_embeds, lambda x:\n        not batch.do_classifier_free_guidance or V.list_not_empty(x))\n    return result\n</code></pre> fastvideo.pipelines.stages.conditioning.ConditioningStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify conditioning stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/conditioning.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify conditioning stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds, V.list_not_empty)\n    return result\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.decoding","title":"fastvideo.pipelines.stages.decoding","text":"<p>Decoding stage for diffusion pipelines.</p> Classes\u00b6 fastvideo.pipelines.stages.decoding.DecodingStage \u00b6 <pre><code>DecodingStage(vae, pipeline=None)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for decoding latent representations into pixel space.</p> <p>This stage handles the decoding of latent representations into the final output format (e.g., pixel values).</p> Source code in <code>fastvideo/pipelines/stages/decoding.py</code> <pre><code>def __init__(self, vae, pipeline=None) -&gt; None:\n    self.vae: ParallelTiledVAE = vae\n    self.pipeline = weakref.ref(pipeline) if pipeline else None\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.decoding.DecodingStage.decode \u00b6 <pre><code>decode(\n    latents: Tensor, fastvideo_args: FastVideoArgs\n) -&gt; torch.Tensor\n</code></pre> <p>Decode latent representations into pixel space using VAE.</p> <p>Parameters:</p> Name Type Description Default <code>latents</code> <code>Tensor</code> <p>Input latent tensor with shape (batch, channels, frames, height_latents, width_latents)</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>Configuration containing: - disable_autocast: Whether to disable automatic mixed precision (default: False) - pipeline_config.vae_precision: VAE computation precision (\"fp32\", \"fp16\", \"bf16\") - pipeline_config.vae_tiling: Whether to enable VAE tiling for memory efficiency</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Decoded video tensor with shape (batch, channels, frames, height, width), </p> <code>Tensor</code> <p>normalized to [0, 1] range and moved to CPU as float32</p> Source code in <code>fastvideo/pipelines/stages/decoding.py</code> <pre><code>@torch.no_grad()\ndef decode(self, latents: torch.Tensor,\n           fastvideo_args: FastVideoArgs) -&gt; torch.Tensor:\n    \"\"\"\n    Decode latent representations into pixel space using VAE.\n\n    Args:\n        latents: Input latent tensor with shape (batch, channels, frames, height_latents, width_latents)\n        fastvideo_args: Configuration containing:\n            - disable_autocast: Whether to disable automatic mixed precision (default: False)\n            - pipeline_config.vae_precision: VAE computation precision (\"fp32\", \"fp16\", \"bf16\")\n            - pipeline_config.vae_tiling: Whether to enable VAE tiling for memory efficiency\n\n    Returns:\n        Decoded video tensor with shape (batch, channels, frames, height, width), \n        normalized to [0, 1] range and moved to CPU as float32\n    \"\"\"\n    self.vae = self.vae.to(get_local_torch_device())\n    latents = latents.to(get_local_torch_device())\n\n    # Setup VAE precision\n    vae_dtype = PRECISION_TO_TYPE[\n        fastvideo_args.pipeline_config.vae_precision]\n    vae_autocast_enabled = (\n        vae_dtype != torch.float32) and not fastvideo_args.disable_autocast\n\n    if isinstance(self.vae.scaling_factor, torch.Tensor):\n        latents = latents / self.vae.scaling_factor.to(\n            latents.device, latents.dtype)\n    else:\n        latents = latents / self.vae.scaling_factor\n\n    # Apply shifting if needed\n    if (hasattr(self.vae, \"shift_factor\")\n            and self.vae.shift_factor is not None):\n        if isinstance(self.vae.shift_factor, torch.Tensor):\n            latents += self.vae.shift_factor.to(latents.device,\n                                                latents.dtype)\n        else:\n            latents += self.vae.shift_factor\n\n    # Decode latents\n    with torch.autocast(device_type=\"cuda\",\n                        dtype=vae_dtype,\n                        enabled=vae_autocast_enabled):\n        if fastvideo_args.pipeline_config.vae_tiling:\n            self.vae.enable_tiling()\n        # if fastvideo_args.vae_sp:\n        #     self.vae.enable_parallel()\n        if not vae_autocast_enabled:\n            latents = latents.to(vae_dtype)\n        image = self.vae.decode(latents)\n\n    # Normalize image to [0, 1] range\n    image = (image / 2 + 0.5).clamp(0, 1)\n    return image\n</code></pre> fastvideo.pipelines.stages.decoding.DecodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Decode latent representations into pixel space.</p> <p>This method processes the batch through the VAE decoder, converting latent representations to pixel-space video/images. It also optionally decodes trajectory latents for visualization purposes.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch containing: - latents: Tensor to decode (batch, channels, frames, height_latents, width_latents) - return_trajectory_decoded (optional): Flag to decode trajectory latents - trajectory_latents (optional): Latents at different timesteps - trajectory_timesteps (optional): Corresponding timesteps</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>Configuration containing: - output_type: \"latent\" to skip decoding, otherwise decode to pixels - vae_cpu_offload: Whether to offload VAE to CPU after decoding - model_loaded: Track VAE loading state - model_paths: Path to VAE model if loading needed</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>Modified batch with: - output: Decoded frames (batch, channels, frames, height, width) as CPU float32 - trajectory_decoded (if requested): List of decoded frames per timestep</p> Source code in <code>fastvideo/pipelines/stages/decoding.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Decode latent representations into pixel space.\n\n    This method processes the batch through the VAE decoder, converting latent\n    representations to pixel-space video/images. It also optionally decodes\n    trajectory latents for visualization purposes.\n\n    Args:\n        batch: The current batch containing:\n            - latents: Tensor to decode (batch, channels, frames, height_latents, width_latents)\n            - return_trajectory_decoded (optional): Flag to decode trajectory latents\n            - trajectory_latents (optional): Latents at different timesteps\n            - trajectory_timesteps (optional): Corresponding timesteps\n        fastvideo_args: Configuration containing:\n            - output_type: \"latent\" to skip decoding, otherwise decode to pixels\n            - vae_cpu_offload: Whether to offload VAE to CPU after decoding\n            - model_loaded: Track VAE loading state\n            - model_paths: Path to VAE model if loading needed\n\n    Returns:\n        Modified batch with:\n            - output: Decoded frames (batch, channels, frames, height, width) as CPU float32\n            - trajectory_decoded (if requested): List of decoded frames per timestep\n    \"\"\"\n    # load vae if not already loaded (used for memory constrained devices)\n    pipeline = self.pipeline() if self.pipeline else None\n    if not fastvideo_args.model_loaded[\"vae\"]:\n        loader = VAELoader()\n        self.vae = loader.load(fastvideo_args.model_paths[\"vae\"],\n                               fastvideo_args)\n        if pipeline:\n            pipeline.add_module(\"vae\", self.vae)\n        fastvideo_args.model_loaded[\"vae\"] = True\n\n    if fastvideo_args.output_type == \"latent\":\n        frames = batch.latents\n    else:\n        frames = self.decode(batch.latents, fastvideo_args)\n\n    # decode trajectory latents if needed\n    if batch.return_trajectory_decoded:\n        batch.trajectory_decoded = []\n        assert batch.trajectory_latents is not None, \"batch should have trajectory latents\"\n        for idx in range(batch.trajectory_latents.shape[1]):\n            # batch.trajectory_latents is [batch_size, timesteps, channels, frames, height, width]\n            cur_latent = batch.trajectory_latents[:, idx, :, :, :, :]\n            cur_timestep = batch.trajectory_timesteps[idx]\n            logger.info(\"decoding trajectory latent for timestep: %s\",\n                        cur_timestep)\n            decoded_frames = self.decode(cur_latent, fastvideo_args)\n            batch.trajectory_decoded.append(decoded_frames.cpu().float())\n\n    # Convert to CPU float32 for compatibility\n    frames = frames.cpu().float()\n\n    # Update batch with decoded image\n    batch.output = frames\n\n    # Offload models if needed\n    if hasattr(self, 'maybe_free_model_hooks'):\n        self.maybe_free_model_hooks()\n\n    if fastvideo_args.vae_cpu_offload:\n        self.vae.to(\"cpu\")\n\n    if torch.backends.mps.is_available():\n        del self.vae\n        if pipeline is not None and \"vae\" in pipeline.modules:\n            del pipeline.modules[\"vae\"]\n        fastvideo_args.model_loaded[\"vae\"] = False\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.decoding.DecodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify decoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/decoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify decoding stage inputs.\"\"\"\n    result = VerificationResult()\n    # Denoised latents for VAE decoding: [batch_size, channels, frames, height_latents, width_latents]\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre> fastvideo.pipelines.stages.decoding.DecodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify decoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/decoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify decoding stage outputs.\"\"\"\n    result = VerificationResult()\n    # Decoded video/images: [batch_size, channels, frames, height, width]\n    result.add_check(\"output\", batch.output, [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.denoising","title":"fastvideo.pipelines.stages.denoising","text":"<p>Denoising stage for diffusion pipelines.</p> Classes\u00b6 fastvideo.pipelines.stages.denoising.DenoisingStage \u00b6 <pre><code>DenoisingStage(\n    transformer,\n    scheduler,\n    pipeline=None,\n    transformer_2=None,\n    vae=None,\n)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for running the denoising loop in diffusion pipelines.</p> <p>This stage handles the iterative denoising process that transforms the initial noise into the final output.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def __init__(self,\n             transformer,\n             scheduler,\n             pipeline=None,\n             transformer_2=None,\n             vae=None) -&gt; None:\n    super().__init__()\n    self.transformer = transformer\n    self.transformer_2 = transformer_2\n    self.scheduler = scheduler\n    self.vae = vae\n    self.pipeline = weakref.ref(pipeline) if pipeline else None\n    attn_head_size = self.transformer.hidden_size // self.transformer.num_attention_heads\n    self.attn_backend = get_attn_backend(\n        head_size=attn_head_size,\n        dtype=torch.float16,  # TODO(will): hack\n        supported_attention_backends=(\n            AttentionBackendEnum.SLIDING_TILE_ATTN,\n            AttentionBackendEnum.VIDEO_SPARSE_ATTN,\n            AttentionBackendEnum.VMOBA_ATTN,\n            AttentionBackendEnum.FLASH_ATTN,\n            AttentionBackendEnum.TORCH_SDPA,\n            AttentionBackendEnum.SAGE_ATTN_THREE)  # hack\n    )\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.denoising.DenoisingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Run the denoising loop.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with denoised latents.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Run the denoising loop.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with denoised latents.\n    \"\"\"\n    pipeline = self.pipeline() if self.pipeline else None\n    if not fastvideo_args.model_loaded[\"transformer\"]:\n        loader = TransformerLoader()\n        self.transformer = loader.load(\n            fastvideo_args.model_paths[\"transformer\"], fastvideo_args)\n        if pipeline:\n            pipeline.add_module(\"transformer\", self.transformer)\n        fastvideo_args.model_loaded[\"transformer\"] = True\n\n    # Prepare extra step kwargs for scheduler\n    extra_step_kwargs = self.prepare_extra_func_kwargs(\n        self.scheduler.step,\n        {\n            \"generator\": batch.generator,\n            \"eta\": batch.eta\n        },\n    )\n\n    # Setup precision and autocast settings\n    # TODO(will): make the precision configurable for inference\n    # target_dtype = PRECISION_TO_TYPE[fastvideo_args.precision]\n    target_dtype = torch.bfloat16\n    autocast_enabled = (target_dtype != torch.float32\n                        ) and not fastvideo_args.disable_autocast\n\n    # Handle sequence parallelism if enabled\n    sp_world_size, rank_in_sp_group = get_sp_world_size(\n    ), get_sp_parallel_rank()\n    sp_group = sp_world_size &gt; 1\n    if sp_group:\n        latents = rearrange(batch.latents,\n                            \"b c (n t) h w -&gt; b c n t h w\",\n                            n=sp_world_size).contiguous()\n        latents = latents[:, :, rank_in_sp_group, :, :, :]\n        batch.latents = latents\n        if batch.image_latent is not None:\n            image_latent = rearrange(batch.image_latent,\n                                     \"b c (n t) h w -&gt; b c n t h w\",\n                                     n=sp_world_size).contiguous()\n            image_latent = image_latent[:, :, rank_in_sp_group, :, :, :]\n            batch.image_latent = image_latent\n    # Get timesteps and calculate warmup steps\n    timesteps = batch.timesteps\n    # TODO(will): remove this once we add input/output validation for stages\n    if timesteps is None:\n        raise ValueError(\"Timesteps must be provided\")\n    num_inference_steps = batch.num_inference_steps\n    num_warmup_steps = len(\n        timesteps) - num_inference_steps * self.scheduler.order\n\n    # Prepare image latents and embeddings for I2V generation\n    image_embeds = batch.image_embeds\n    if len(image_embeds) &gt; 0:\n        assert not torch.isnan(\n            image_embeds[0]).any(), \"image_embeds contains nan\"\n        image_embeds = [\n            image_embed.to(target_dtype) for image_embed in image_embeds\n        ]\n\n    image_kwargs = self.prepare_extra_func_kwargs(\n        self.transformer.forward,\n        {\n            \"encoder_hidden_states_image\": image_embeds,\n            \"mask_strategy\": dict_to_3d_list(\n                None, t_max=50, l_max=60, h_max=24)\n        },\n    )\n\n    pos_cond_kwargs = self.prepare_extra_func_kwargs(\n        self.transformer.forward,\n        {\n            \"encoder_hidden_states_2\": batch.clip_embedding_pos,\n            \"encoder_attention_mask\": batch.prompt_attention_mask,\n        },\n    )\n\n    neg_cond_kwargs = self.prepare_extra_func_kwargs(\n        self.transformer.forward,\n        {\n            \"encoder_hidden_states_2\": batch.clip_embedding_neg,\n            \"encoder_attention_mask\": batch.negative_attention_mask,\n        },\n    )\n\n    # Prepare STA parameters\n    if st_attn_available and self.attn_backend == SlidingTileAttentionBackend:\n        self.prepare_sta_param(batch, fastvideo_args)\n\n    # Get latents and embeddings\n    latents = batch.latents\n    prompt_embeds = batch.prompt_embeds\n    assert not torch.isnan(\n        prompt_embeds[0]).any(), \"prompt_embeds contains nan\"\n    if batch.do_classifier_free_guidance:\n        neg_prompt_embeds = batch.negative_prompt_embeds\n        assert neg_prompt_embeds is not None\n        assert not torch.isnan(\n            neg_prompt_embeds[0]).any(), \"neg_prompt_embeds contains nan\"\n\n    # (Wan2.2) Calculate timestep to switch from high noise expert to low noise expert\n    boundary_ratio = fastvideo_args.pipeline_config.dit_config.boundary_ratio\n    if batch.boundary_ratio is not None:\n        logger.info(\"Overriding boundary ratio from %s to %s\",\n                    boundary_ratio, batch.boundary_ratio)\n        boundary_ratio = batch.boundary_ratio\n\n    if boundary_ratio is not None:\n        boundary_timestep = boundary_ratio * self.scheduler.num_train_timesteps\n    else:\n        boundary_timestep = None\n    latent_model_input = latents.to(target_dtype)\n    assert latent_model_input.shape[0] == 1, \"only support batch size 1\"\n\n    if fastvideo_args.pipeline_config.ti2v_task and batch.pil_image is not None:\n        # TI2V directly replaces the first frame of the latent with\n        # the image latent instead of appending along the channel dim\n        assert batch.image_latent is None, \"TI2V task should not have image latents\"\n        assert self.vae is not None, \"VAE is not provided for TI2V task\"\n        z = self.vae.encode(batch.pil_image).mean.float()\n        if (hasattr(self.vae, \"shift_factor\")\n                and self.vae.shift_factor is not None):\n            if isinstance(self.vae.shift_factor, torch.Tensor):\n                z -= self.vae.shift_factor.to(z.device, z.dtype)\n            else:\n                z -= self.vae.shift_factor\n\n        if isinstance(self.vae.scaling_factor, torch.Tensor):\n            z = z * self.vae.scaling_factor.to(z.device, z.dtype)\n        else:\n            z = z * self.vae.scaling_factor\n\n        latent_model_input = latent_model_input.squeeze(0)\n        _, mask2 = masks_like([latent_model_input], zero=True)\n\n        latent_model_input = (1. -\n                              mask2[0]) * z + mask2[0] * latent_model_input\n        # latent_model_input = latent_model_input.unsqueeze(0)\n        latent_model_input = latent_model_input.to(get_local_torch_device())\n        latents = latent_model_input\n        F = batch.num_frames\n        temporal_scale = fastvideo_args.pipeline_config.vae_config.arch_config.scale_factor_temporal\n        spatial_scale = fastvideo_args.pipeline_config.vae_config.arch_config.scale_factor_spatial\n        patch_size = fastvideo_args.pipeline_config.dit_config.arch_config.patch_size\n        seq_len = ((F - 1) // temporal_scale +\n                   1) * (batch.height // spatial_scale) * (\n                       batch.width // spatial_scale) // (patch_size[1] *\n                                                         patch_size[2])\n        seq_len = int(math.ceil(seq_len / sp_world_size)) * sp_world_size\n\n    # Initialize lists for ODE trajectory\n    trajectory_timesteps: list[torch.Tensor] = []\n    trajectory_latents: list[torch.Tensor] = []\n\n    # Run denoising loop\n    with self.progress_bar(total=num_inference_steps) as progress_bar:\n        for i, t in enumerate(timesteps):\n            # Skip if interrupted\n            if hasattr(self, 'interrupt') and self.interrupt:\n                continue\n\n            if boundary_timestep is None or t &gt;= boundary_timestep:\n                if (fastvideo_args.dit_cpu_offload\n                        and self.transformer_2 is not None and next(\n                            self.transformer_2.parameters()).device.type\n                        == 'cuda'):\n                    self.transformer_2.to('cpu')\n                current_model = self.transformer\n                current_guidance_scale = batch.guidance_scale\n            else:\n                # low-noise stage in wan2.2\n                if fastvideo_args.dit_cpu_offload and next(\n                        self.transformer.parameters(\n                        )).device.type == 'cuda':\n                    self.transformer.to('cpu')\n                current_model = self.transformer_2\n                current_guidance_scale = batch.guidance_scale_2\n            assert current_model is not None, \"current_model is None\"\n\n            # Expand latents for V2V/I2V\n            latent_model_input = latents.to(target_dtype)\n            if batch.video_latent is not None:\n                latent_model_input = torch.cat([\n                    latent_model_input, batch.video_latent,\n                    torch.zeros_like(latents)\n                ],\n                                               dim=1).to(target_dtype)\n            elif batch.image_latent is not None:\n                assert not fastvideo_args.pipeline_config.ti2v_task, \"image latents should not be provided for TI2V task\"\n                latent_model_input = torch.cat(\n                    [latent_model_input, batch.image_latent],\n                    dim=1).to(target_dtype)\n\n            assert not torch.isnan(\n                latent_model_input).any(), \"latent_model_input contains nan\"\n            if fastvideo_args.pipeline_config.ti2v_task and batch.pil_image is not None:\n                timestep = torch.stack([t]).to(get_local_torch_device())\n                temp_ts = (mask2[0][0][:, ::2, ::2] * timestep).flatten()\n                temp_ts = torch.cat([\n                    temp_ts,\n                    temp_ts.new_ones(seq_len - temp_ts.size(0)) * timestep\n                ])\n                timestep = temp_ts.unsqueeze(0)\n                t_expand = timestep.repeat(latent_model_input.shape[0], 1)\n            else:\n                t_expand = t.repeat(latent_model_input.shape[0])\n\n            latent_model_input = self.scheduler.scale_model_input(\n                latent_model_input, t)\n\n            # Prepare inputs for transformer\n            guidance_expand = (\n                torch.tensor(\n                    [fastvideo_args.pipeline_config.embedded_cfg_scale] *\n                    latent_model_input.shape[0],\n                    dtype=torch.float32,\n                    device=get_local_torch_device(),\n                ).to(target_dtype) *\n                1000.0 if fastvideo_args.pipeline_config.embedded_cfg_scale\n                is not None else None)\n\n            # Predict noise residual\n            with torch.autocast(device_type=\"cuda\",\n                                dtype=target_dtype,\n                                enabled=autocast_enabled):\n                if (st_attn_available\n                        and self.attn_backend == SlidingTileAttentionBackend\n                    ) or (vsa_available and self.attn_backend\n                          == VideoSparseAttentionBackend):\n                    self.attn_metadata_builder_cls = self.attn_backend.get_builder_cls(\n                    )\n\n                    if self.attn_metadata_builder_cls is not None:\n                        self.attn_metadata_builder = self.attn_metadata_builder_cls(\n                        )\n                        # TODO(will): clean this up\n                        attn_metadata = self.attn_metadata_builder.build(  # type: ignore\n                            current_timestep=i,  # type: ignore\n                            raw_latent_shape=batch.\n                            raw_latent_shape[2:5],  # type: ignore\n                            patch_size=fastvideo_args.\n                            pipeline_config.  # type: ignore\n                            dit_config.patch_size,  # type: ignore\n                            STA_param=batch.STA_param,  # type: ignore\n                            VSA_sparsity=fastvideo_args.\n                            VSA_sparsity,  # type: ignore\n                            device=get_local_torch_device(),\n                        )\n                        assert attn_metadata is not None, \"attn_metadata cannot be None\"\n                    else:\n                        attn_metadata = None\n                elif (vmoba_attn_available\n                      and self.attn_backend == VMOBAAttentionBackend):\n                    self.attn_metadata_builder_cls = self.attn_backend.get_builder_cls(\n                    )\n                    if self.attn_metadata_builder_cls is not None:\n                        self.attn_metadata_builder = self.attn_metadata_builder_cls(\n                        )\n                        # Prepare V-MoBA parameters from config\n                        moba_params = fastvideo_args.moba_config.copy()\n                        moba_params.update({\n                            \"current_timestep\":\n                            i,\n                            \"raw_latent_shape\":\n                            batch.raw_latent_shape[2:5],\n                            \"patch_size\":\n                            fastvideo_args.pipeline_config.dit_config.\n                            patch_size,\n                            \"device\":\n                            get_local_torch_device(),\n                        })\n                        attn_metadata = self.attn_metadata_builder.build(\n                            **moba_params)\n                        assert attn_metadata is not None, \"attn_metadata cannot be None\"\n                    else:\n                        attn_metadata = None\n                else:\n                    attn_metadata = None\n                # TODO(will): finalize the interface. vLLM uses this to\n                # support torch dynamo compilation. They pass in\n                # attn_metadata, vllm_config, and num_tokens. We can pass in\n                # fastvideo_args or training_args, and attn_metadata.\n                batch.is_cfg_negative = False\n                with set_forward_context(\n                        current_timestep=i,\n                        attn_metadata=attn_metadata,\n                        forward_batch=batch,\n                        # fastvideo_args=fastvideo_args\n                ):\n                    # Run transformer\n                    noise_pred = current_model(\n                        latent_model_input,\n                        prompt_embeds,\n                        t_expand,\n                        guidance=guidance_expand,\n                        **image_kwargs,\n                        **pos_cond_kwargs,\n                    )\n\n                # Apply guidance\n                if batch.do_classifier_free_guidance:\n                    batch.is_cfg_negative = True\n                    with set_forward_context(\n                            current_timestep=i,\n                            attn_metadata=attn_metadata,\n                            forward_batch=batch,\n                            # fastvideo_args=fastvideo_args\n                    ):\n                        # Run transformer\n                        noise_pred_uncond = current_model(\n                            latent_model_input,\n                            neg_prompt_embeds,\n                            t_expand,\n                            guidance=guidance_expand,\n                            **image_kwargs,\n                            **neg_cond_kwargs,\n                        )\n                    noise_pred_text = noise_pred\n                    noise_pred = noise_pred_uncond + current_guidance_scale * (\n                        noise_pred_text - noise_pred_uncond)\n\n                    # Apply guidance rescale if needed\n                    if batch.guidance_rescale &gt; 0.0:\n                        # Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf\n                        noise_pred = self.rescale_noise_cfg(\n                            noise_pred,\n                            noise_pred_text,\n                            guidance_rescale=batch.guidance_rescale,\n                        )\n                # Compute the previous noisy sample\n                latents = self.scheduler.step(noise_pred,\n                                              t,\n                                              latents,\n                                              **extra_step_kwargs,\n                                              return_dict=False)[0]\n                if fastvideo_args.pipeline_config.ti2v_task and batch.pil_image is not None:\n                    latents = latents.squeeze(0)\n                    latents = (1. - mask2[0]) * z + mask2[0] * latents\n                    # latents = latents.unsqueeze(0)\n\n            # save trajectory latents if needed\n            if batch.return_trajectory_latents:\n                trajectory_timesteps.append(t)\n                trajectory_latents.append(latents)\n\n            # Update progress bar\n            if i == len(timesteps) - 1 or (\n                (i + 1) &gt; num_warmup_steps and\n                (i + 1) % self.scheduler.order == 0\n                    and progress_bar is not None):\n                progress_bar.update()\n\n    # Gather results if using sequence parallelism\n    trajectory_tensor: torch.Tensor | None = None\n    if trajectory_latents:\n        trajectory_tensor = torch.stack(trajectory_latents, dim=1)\n        trajectory_timesteps_tensor = torch.stack(trajectory_timesteps,\n                                                  dim=0)\n    else:\n        trajectory_tensor = None\n        trajectory_timesteps_tensor = None\n\n    # Gather results if using sequence parallelism\n    if sp_group:\n        latents = sequence_model_parallel_all_gather(latents, dim=2)\n        if batch.return_trajectory_latents:\n            trajectory_tensor = trajectory_tensor.to(\n                get_local_torch_device())\n            trajectory_tensor = sequence_model_parallel_all_gather(\n                trajectory_tensor, dim=3)\n\n    if trajectory_tensor is not None and trajectory_timesteps_tensor is not None:\n        batch.trajectory_timesteps = trajectory_timesteps_tensor.cpu()\n        batch.trajectory_latents = trajectory_tensor.cpu()\n\n    # Update batch with final latents\n    batch.latents = latents\n\n    # Save STA mask search results if needed\n    if st_attn_available and self.attn_backend == SlidingTileAttentionBackend and fastvideo_args.STA_mode == STA_Mode.STA_SEARCHING:\n        self.save_sta_search_results(batch)\n\n    # deallocate transformer if on mps\n    if torch.backends.mps.is_available():\n        logger.info(\"Memory before deallocating transformer: %s\",\n                    torch.mps.current_allocated_memory())\n        del self.transformer\n        if pipeline is not None and \"transformer\" in pipeline.modules:\n            del pipeline.modules[\"transformer\"]\n        fastvideo_args.model_loaded[\"transformer\"] = False\n        logger.info(\"Memory after deallocating transformer: %s\",\n                    torch.mps.current_allocated_memory())\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.denoising.DenoisingStage.prepare_extra_func_kwargs \u00b6 <pre><code>prepare_extra_func_kwargs(func, kwargs) -&gt; dict[str, Any]\n</code></pre> <p>Prepare extra kwargs for the scheduler step / denoise step.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <p>The function to prepare kwargs for.</p> required <code>kwargs</code> <p>The kwargs to prepare.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The prepared kwargs.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def prepare_extra_func_kwargs(self, func, kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Prepare extra kwargs for the scheduler step / denoise step.\n\n    Args:\n        func: The function to prepare kwargs for.\n        kwargs: The kwargs to prepare.\n\n    Returns:\n        The prepared kwargs.\n    \"\"\"\n    extra_step_kwargs = {}\n    for k, v in kwargs.items():\n        accepts = k in set(inspect.signature(func).parameters.keys())\n        if accepts:\n            extra_step_kwargs[k] = v\n    return extra_step_kwargs\n</code></pre> fastvideo.pipelines.stages.denoising.DenoisingStage.prepare_sta_param \u00b6 <pre><code>prepare_sta_param(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n)\n</code></pre> <p>Prepare Sliding Tile Attention (STA) parameters and settings.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def prepare_sta_param(self, batch: ForwardBatch,\n                      fastvideo_args: FastVideoArgs):\n    \"\"\"\n    Prepare Sliding Tile Attention (STA) parameters and settings.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n    \"\"\"\n    # TODO(kevin): STA mask search, currently only support Wan2.1 with 69x768x1280\n    from fastvideo.STA_configuration import configure_sta\n    STA_mode = fastvideo_args.STA_mode\n    skip_time_steps = fastvideo_args.skip_time_steps\n    if batch.timesteps is None:\n        raise ValueError(\"Timesteps must be provided\")\n    timesteps_num = batch.timesteps.shape[0]\n\n    logger.info(\"STA_mode: %s\", STA_mode)\n    if (batch.num_frames, batch.height,\n            batch.width) != (69, 768, 1280) and STA_mode != \"STA_inference\":\n        raise NotImplementedError(\n            \"STA mask search/tuning is not supported for this resolution\")\n\n    if STA_mode == STA_Mode.STA_SEARCHING or STA_mode == STA_Mode.STA_TUNING or STA_mode == STA_Mode.STA_TUNING_CFG:\n        size = (batch.width, batch.height)\n        if size == (1280, 768):\n            # TODO: make it configurable\n            sparse_mask_candidates_searching = [\n                \"3, 1, 10\", \"1, 5, 7\", \"3, 3, 3\", \"1, 6, 5\", \"1, 3, 10\",\n                \"3, 6, 1\"\n            ]\n            sparse_mask_candidates_tuning = [\n                \"3, 1, 10\", \"1, 5, 7\", \"3, 3, 3\", \"1, 6, 5\", \"1, 3, 10\",\n                \"3, 6, 1\"\n            ]\n            full_mask = [\"3,6,10\"]\n        else:\n            raise NotImplementedError(\n                \"STA mask search is not supported for this resolution\")\n    layer_num = self.transformer.config.num_layers\n    # specific for HunyuanVideo\n    if hasattr(self.transformer.config, \"num_single_layers\"):\n        layer_num += self.transformer.config.num_single_layers\n    head_num = self.transformer.config.num_attention_heads\n\n    if STA_mode == STA_Mode.STA_SEARCHING:\n        STA_param = configure_sta(\n            mode=STA_Mode.STA_SEARCHING,\n            layer_num=layer_num,\n            head_num=head_num,\n            time_step_num=timesteps_num,\n            mask_candidates=sparse_mask_candidates_searching +\n            full_mask,  # last is full mask; Can add more sparse masks while keep last one as full mask\n        )\n    elif STA_mode == STA_Mode.STA_TUNING:\n        STA_param = configure_sta(\n            mode=STA_Mode.STA_TUNING,\n            layer_num=layer_num,\n            head_num=head_num,\n            time_step_num=timesteps_num,\n            mask_search_files_path=\n            f'output/mask_search_result_pos_{size[0]}x{size[1]}/',\n            mask_candidates=sparse_mask_candidates_tuning,\n            full_attention_mask=[int(x) for x in full_mask[0].split(',')],\n            skip_time_steps=\n            skip_time_steps,  # Use full attention for first 12 steps\n            save_dir=\n            f'output/mask_search_strategy_{size[0]}x{size[1]}/',  # Custom save directory\n            timesteps=timesteps_num)\n    elif STA_mode == STA_Mode.STA_TUNING_CFG:\n        STA_param = configure_sta(\n            mode=STA_Mode.STA_TUNING_CFG,\n            layer_num=layer_num,\n            head_num=head_num,\n            time_step_num=timesteps_num,\n            mask_search_files_path_pos=\n            f'output/mask_search_result_pos_{size[0]}x{size[1]}/',\n            mask_search_files_path_neg=\n            f'output/mask_search_result_neg_{size[0]}x{size[1]}/',\n            mask_candidates=sparse_mask_candidates_tuning,\n            full_attention_mask=[int(x) for x in full_mask[0].split(',')],\n            skip_time_steps=skip_time_steps,\n            save_dir=f'output/mask_search_strategy_{size[0]}x{size[1]}/',\n            timesteps=timesteps_num)\n    elif STA_mode == STA_Mode.STA_INFERENCE:\n        import fastvideo.envs as envs\n        config_file = envs.FASTVIDEO_ATTENTION_CONFIG\n        if config_file is None:\n            raise ValueError(\"FASTVIDEO_ATTENTION_CONFIG is not set\")\n        STA_param = configure_sta(mode=STA_Mode.STA_INFERENCE,\n                                  layer_num=layer_num,\n                                  head_num=head_num,\n                                  time_step_num=timesteps_num,\n                                  load_path=config_file)\n\n    batch.STA_param = STA_param\n    batch.mask_search_final_result_pos = [[] for _ in range(timesteps_num)]\n    batch.mask_search_final_result_neg = [[] for _ in range(timesteps_num)]\n</code></pre> fastvideo.pipelines.stages.denoising.DenoisingStage.progress_bar \u00b6 <pre><code>progress_bar(\n    iterable: Iterable | None = None,\n    total: int | None = None,\n) -&gt; tqdm\n</code></pre> <p>Create a progress bar for the denoising process.</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>Iterable | None</code> <p>The iterable to iterate over.</p> <code>None</code> <code>total</code> <code>int | None</code> <p>The total number of items.</p> <code>None</code> <p>Returns:</p> Type Description <code>tqdm</code> <p>A tqdm progress bar.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def progress_bar(self,\n                 iterable: Iterable | None = None,\n                 total: int | None = None) -&gt; tqdm:\n    \"\"\"\n    Create a progress bar for the denoising process.\n\n    Args:\n        iterable: The iterable to iterate over.\n        total: The total number of items.\n\n    Returns:\n        A tqdm progress bar.\n    \"\"\"\n    local_rank = get_world_group().local_rank\n    if local_rank == 0:\n        return tqdm(iterable=iterable, total=total)\n    else:\n        return tqdm(iterable=iterable, total=total, disable=True)\n</code></pre> fastvideo.pipelines.stages.denoising.DenoisingStage.rescale_noise_cfg \u00b6 <pre><code>rescale_noise_cfg(\n    noise_cfg, noise_pred_text, guidance_rescale=0.0\n) -&gt; torch.Tensor\n</code></pre> <p>Rescale noise prediction according to guidance_rescale.</p> <p>Based on findings of \"Common Diffusion Noise Schedules and Sample Steps are Flawed\" (https://arxiv.org/pdf/2305.08891.pdf), Section 3.4.</p> <p>Parameters:</p> Name Type Description Default <code>noise_cfg</code> <p>The noise prediction with guidance.</p> required <code>noise_pred_text</code> <p>The text-conditioned noise prediction.</p> required <code>guidance_rescale</code> <p>The guidance rescale factor.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The rescaled noise prediction.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def rescale_noise_cfg(self,\n                      noise_cfg,\n                      noise_pred_text,\n                      guidance_rescale=0.0) -&gt; torch.Tensor:\n    \"\"\"\n    Rescale noise prediction according to guidance_rescale.\n\n    Based on findings of \"Common Diffusion Noise Schedules and Sample Steps are Flawed\"\n    (https://arxiv.org/pdf/2305.08891.pdf), Section 3.4.\n\n    Args:\n        noise_cfg: The noise prediction with guidance.\n        noise_pred_text: The text-conditioned noise prediction.\n        guidance_rescale: The guidance rescale factor.\n\n    Returns:\n        The rescaled noise prediction.\n    \"\"\"\n    std_text = noise_pred_text.std(dim=list(range(1, noise_pred_text.ndim)),\n                                   keepdim=True)\n    std_cfg = noise_cfg.std(dim=list(range(1, noise_cfg.ndim)),\n                            keepdim=True)\n    # Rescale the results from guidance (fixes overexposure)\n    noise_pred_rescaled = noise_cfg * (std_text / std_cfg)\n    # Mix with the original results from guidance by factor guidance_rescale\n    noise_cfg = (guidance_rescale * noise_pred_rescaled +\n                 (1 - guidance_rescale) * noise_cfg)\n    return noise_cfg\n</code></pre> fastvideo.pipelines.stages.denoising.DenoisingStage.save_sta_search_results \u00b6 <pre><code>save_sta_search_results(batch: ForwardBatch)\n</code></pre> <p>Save the STA mask search results.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def save_sta_search_results(self, batch: ForwardBatch):\n    \"\"\"\n    Save the STA mask search results.\n\n    Args:\n        batch: The current batch information.\n    \"\"\"\n    size = (batch.width, batch.height)\n    if size == (1280, 768):\n        # TODO: make it configurable\n        sparse_mask_candidates_searching = [\n            \"3, 1, 10\", \"1, 5, 7\", \"3, 3, 3\", \"1, 6, 5\", \"1, 3, 10\",\n            \"3, 6, 1\"\n        ]\n    else:\n        raise NotImplementedError(\n            \"STA mask search is not supported for this resolution\")\n\n    from fastvideo.STA_configuration import save_mask_search_results\n    if batch.mask_search_final_result_pos is not None and batch.prompt is not None:\n        save_mask_search_results(\n            [\n                dict(layer_data)\n                for layer_data in batch.mask_search_final_result_pos\n            ],\n            prompt=str(batch.prompt),\n            mask_strategies=sparse_mask_candidates_searching,\n            output_dir=f'output/mask_search_result_pos_{size[0]}x{size[1]}/'\n        )\n    if batch.mask_search_final_result_neg is not None and batch.prompt is not None:\n        save_mask_search_results(\n            [\n                dict(layer_data)\n                for layer_data in batch.mask_search_final_result_neg\n            ],\n            prompt=str(batch.prompt),\n            mask_strategies=sparse_mask_candidates_searching,\n            output_dir=f'output/mask_search_result_neg_{size[0]}x{size[1]}/'\n        )\n</code></pre> fastvideo.pipelines.stages.denoising.DenoisingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify denoising stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify denoising stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"timesteps\", batch.timesteps,\n                     [V.is_tensor, V.min_dims(1)])\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds, V.list_not_empty)\n    result.add_check(\"image_embeds\", batch.image_embeds, V.is_list)\n    result.add_check(\"image_latent\", batch.image_latent,\n                     V.none_or_tensor_with_dims(5))\n    result.add_check(\"num_inference_steps\", batch.num_inference_steps,\n                     V.positive_int)\n    result.add_check(\"guidance_scale\", batch.guidance_scale,\n                     V.positive_float)\n    result.add_check(\"eta\", batch.eta, V.non_negative_float)\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    result.add_check(\"do_classifier_free_guidance\",\n                     batch.do_classifier_free_guidance, V.bool_value)\n    result.add_check(\n        \"negative_prompt_embeds\", batch.negative_prompt_embeds, lambda x:\n        not batch.do_classifier_free_guidance or V.list_not_empty(x))\n    return result\n</code></pre> fastvideo.pipelines.stages.denoising.DenoisingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify denoising stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify denoising stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre> fastvideo.pipelines.stages.denoising.DmdDenoisingStage \u00b6 <pre><code>DmdDenoisingStage(transformer, scheduler)\n</code></pre> <p>               Bases: <code>DenoisingStage</code></p> <p>Denoising stage for DMD.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def __init__(self, transformer, scheduler) -&gt; None:\n    super().__init__(transformer, scheduler)\n    self.scheduler = FlowMatchEulerDiscreteScheduler(shift=8.0)\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.denoising.DmdDenoisingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Run the denoising loop.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with denoised latents.</p> Source code in <code>fastvideo/pipelines/stages/denoising.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Run the denoising loop.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with denoised latents.\n    \"\"\"\n    # Setup precision and autocast settings\n    # TODO(will): make the precision configurable for inference\n    # target_dtype = PRECISION_TO_TYPE[fastvideo_args.precision]\n    target_dtype = torch.bfloat16\n    autocast_enabled = (target_dtype != torch.float32\n                        ) and not fastvideo_args.disable_autocast\n\n    # Get timesteps and calculate warmup steps\n    timesteps = batch.timesteps\n\n    # TODO(will): remove this once we add input/output validation for stages\n    if timesteps is None:\n        raise ValueError(\"Timesteps must be provided\")\n    num_inference_steps = batch.num_inference_steps\n    num_warmup_steps = len(\n        timesteps) - num_inference_steps * self.scheduler.order\n\n    # Prepare image latents and embeddings for I2V generation\n    image_embeds = batch.image_embeds\n    if len(image_embeds) &gt; 0:\n        assert torch.isnan(image_embeds[0]).sum() == 0\n        image_embeds = [\n            image_embed.to(target_dtype) for image_embed in image_embeds\n        ]\n\n    image_kwargs = self.prepare_extra_func_kwargs(\n        self.transformer.forward,\n        {\n            \"encoder_hidden_states_image\": image_embeds,\n            \"mask_strategy\": dict_to_3d_list(\n                None, t_max=50, l_max=60, h_max=24)\n        },\n    )\n\n    pos_cond_kwargs = self.prepare_extra_func_kwargs(\n        self.transformer.forward,\n        {\n            \"encoder_hidden_states_2\": batch.clip_embedding_pos,\n            \"encoder_attention_mask\": batch.prompt_attention_mask,\n        },\n    )\n\n    # Prepare STA parameters\n    if st_attn_available and self.attn_backend == SlidingTileAttentionBackend:\n        self.prepare_sta_param(batch, fastvideo_args)\n\n    # Get latents and embeddings\n    assert batch.latents is not None, \"latents must be provided\"\n    latents = batch.latents\n    latents = latents.permute(0, 2, 1, 3, 4)\n\n    video_raw_latent_shape = latents.shape\n    prompt_embeds = batch.prompt_embeds\n    assert not torch.isnan(\n        prompt_embeds[0]).any(), \"prompt_embeds contains nan\"\n    timesteps = torch.tensor(\n        fastvideo_args.pipeline_config.dmd_denoising_steps,\n        dtype=torch.long,\n        device=get_local_torch_device())\n\n    # Handle sequence parallelism if enabled\n    sp_world_size, rank_in_sp_group = get_sp_world_size(\n    ), get_sp_parallel_rank()\n    sp_group = sp_world_size &gt; 1\n    if sp_group:\n        latents = rearrange(latents,\n                            \"b (n t) c h w -&gt; b n t c h w\",\n                            n=sp_world_size).contiguous()\n        latents = latents[:, rank_in_sp_group, :, :, :, :]\n        if batch.image_latent is not None:\n            image_latent = rearrange(batch.image_latent,\n                                     \"b c (n t) h w -&gt; b c n t h w\",\n                                     n=sp_world_size).contiguous()\n\n            image_latent = image_latent[:, :, rank_in_sp_group, :, :, :]\n            batch.image_latent = image_latent\n\n    # Run denoising loop\n    with self.progress_bar(total=len(timesteps)) as progress_bar:\n        for i, t in enumerate(timesteps):\n            # Skip if interrupted\n            if hasattr(self, 'interrupt') and self.interrupt:\n                continue\n            # Expand latents for I2V\n            noise_latents = latents.clone()\n            latent_model_input = latents.to(target_dtype)\n\n            if batch.image_latent is not None:\n                latent_model_input = torch.cat([\n                    latent_model_input,\n                    batch.image_latent.permute(0, 2, 1, 3, 4)\n                ],\n                                               dim=2).to(target_dtype)\n            assert not torch.isnan(\n                latent_model_input).any(), \"latent_model_input contains nan\"\n\n            # Prepare inputs for transformer\n            t_expand = t.repeat(latent_model_input.shape[0])\n            guidance_expand = (\n                torch.tensor(\n                    [fastvideo_args.pipeline_config.embedded_cfg_scale] *\n                    latent_model_input.shape[0],\n                    dtype=torch.float32,\n                    device=get_local_torch_device(),\n                ).to(target_dtype) *\n                1000.0 if fastvideo_args.pipeline_config.embedded_cfg_scale\n                is not None else None)\n\n            # Predict noise residual\n            with torch.autocast(device_type=\"cuda\",\n                                dtype=target_dtype,\n                                enabled=autocast_enabled):\n                if (vsa_available and self.attn_backend\n                        == VideoSparseAttentionBackend):\n                    self.attn_metadata_builder_cls = self.attn_backend.get_builder_cls(\n                    )\n\n                    if self.attn_metadata_builder_cls is not None:\n                        self.attn_metadata_builder = self.attn_metadata_builder_cls(\n                        )\n                        # TODO(will): clean this up\n                        attn_metadata = self.attn_metadata_builder.build(  # type: ignore\n                            current_timestep=i,  # type: ignore\n                            raw_latent_shape=batch.\n                            raw_latent_shape[2:5],  # type: ignore\n                            patch_size=fastvideo_args.\n                            pipeline_config.  # type: ignore\n                            dit_config.patch_size,  # type: ignore\n                            STA_param=batch.STA_param,  # type: ignore\n                            VSA_sparsity=fastvideo_args.\n                            VSA_sparsity,  # type: ignore\n                            device=get_local_torch_device(),  # type: ignore\n                        )  # type: ignore\n                        assert attn_metadata is not None, \"attn_metadata cannot be None\"\n                    else:\n                        attn_metadata = None\n                else:\n                    attn_metadata = None\n\n                batch.is_cfg_negative = False\n                with set_forward_context(\n                        current_timestep=i,\n                        attn_metadata=attn_metadata,\n                        forward_batch=batch,\n                        # fastvideo_args=fastvideo_args\n                ):\n                    # Run transformer\n                    pred_noise = self.transformer(\n                        latent_model_input.permute(0, 2, 1, 3, 4),\n                        prompt_embeds,\n                        t_expand,\n                        guidance=guidance_expand,\n                        **image_kwargs,\n                        **pos_cond_kwargs,\n                    ).permute(0, 2, 1, 3, 4)\n\n                pred_video = pred_noise_to_pred_video(\n                    pred_noise=pred_noise.flatten(0, 1),\n                    noise_input_latent=noise_latents.flatten(0, 1),\n                    timestep=t_expand,\n                    scheduler=self.scheduler).unflatten(\n                        0, pred_noise.shape[:2])\n\n                if i &lt; len(timesteps) - 1:\n                    next_timestep = timesteps[i + 1] * torch.ones(\n                        [1], dtype=torch.long, device=pred_video.device)\n                    noise = torch.randn(video_raw_latent_shape,\n                                        dtype=pred_video.dtype,\n                                        generator=batch.generator[0]).to(\n                                            self.device)\n                    if sp_group:\n                        noise = rearrange(noise,\n                                          \"b (n t) c h w -&gt; b n t c h w\",\n                                          n=sp_world_size).contiguous()\n                        noise = noise[:, rank_in_sp_group, :, :, :, :]\n                    latents = self.scheduler.add_noise(\n                        pred_video.flatten(0, 1), noise.flatten(0, 1),\n                        next_timestep).unflatten(0, pred_video.shape[:2])\n                else:\n                    latents = pred_video\n\n                # Update progress bar\n                if i == len(timesteps) - 1 or (\n                    (i + 1) &gt; num_warmup_steps and\n                    (i + 1) % self.scheduler.order == 0\n                        and progress_bar is not None):\n                    progress_bar.update()\n\n    # Gather results if using sequence parallelism\n    if sp_group:\n        latents = sequence_model_parallel_all_gather(latents, dim=1)\n    latents = latents.permute(0, 2, 1, 3, 4)\n    # Update batch with final latents\n    batch.latents = latents\n\n    return batch\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.encoding","title":"fastvideo.pipelines.stages.encoding","text":"<p>Encoding stage for diffusion pipelines.</p> Classes\u00b6 fastvideo.pipelines.stages.encoding.EncodingStage \u00b6 <pre><code>EncodingStage(vae: ParallelTiledVAE)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for encoding pixel space representations into latent space.</p> <p>This stage handles the encoding of pixel-space video/images into latent representations for further processing in the diffusion pipeline.</p> Source code in <code>fastvideo/pipelines/stages/encoding.py</code> <pre><code>def __init__(self, vae: ParallelTiledVAE) -&gt; None:\n    self.vae: ParallelTiledVAE = vae\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.encoding.EncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode pixel space representations into latent space.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded latents.</p> Source code in <code>fastvideo/pipelines/stages/encoding.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode pixel space representations into latent space.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded latents.\n    \"\"\"\n    assert batch.latents is not None and isinstance(batch.latents,\n                                                    torch.Tensor)\n\n    self.vae = self.vae.to(get_local_torch_device())\n\n    # Setup VAE precision\n    vae_dtype = PRECISION_TO_TYPE[\n        fastvideo_args.pipeline_config.vae_precision]\n    vae_autocast_enabled = (\n        vae_dtype != torch.float32) and not fastvideo_args.disable_autocast\n\n    # Normalize input to [-1, 1] range (reverse of decoding normalization)\n    latents = (batch.latents * 2.0 - 1.0).clamp(-1, 1)\n\n    # Move to appropriate device and dtype\n    latents = latents.to(get_local_torch_device())\n\n    # Encode image to latents\n    with torch.autocast(device_type=\"cuda\",\n                        dtype=vae_dtype,\n                        enabled=vae_autocast_enabled):\n        if fastvideo_args.pipeline_config.vae_tiling:\n            self.vae.enable_tiling()\n        # if fastvideo_args.vae_sp:\n        #     self.vae.enable_parallel()\n        if not vae_autocast_enabled:\n            latents = latents.to(vae_dtype)\n        latents = self.vae.encode(latents).mean\n\n    # Update batch with encoded latents\n    batch.latents = latents\n\n    # Offload models if needed\n    if hasattr(self, 'maybe_free_model_hooks'):\n        self.maybe_free_model_hooks()\n\n    if fastvideo_args.vae_cpu_offload:\n        self.vae.to(\"cpu\")\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.encoding.EncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/encoding.py</code> <pre><code>@torch.no_grad()\ndef verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    # Input video/images for VAE encoding: [batch_size, channels, frames, height, width]\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre> fastvideo.pipelines.stages.encoding.EncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    # Encoded latents: [batch_size, channels, frames, height_latents, width_latents]\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.image_encoding","title":"fastvideo.pipelines.stages.image_encoding","text":"<p>Image and video encoding stages for diffusion pipelines.</p> <p>This module contains implementations of encoding stages for diffusion pipelines: - ImageEncodingStage: Encodes images using image encoders (e.g., CLIP) - RefImageEncodingStage: Encodes reference image for Wan2.1 control pipeline - ImageVAEEncodingStage: Encodes images to latent space using VAE for I2V generation - VideoVAEEncodingStage: Encodes videos to latent space using VAE for V2V and control tasks</p> Classes\u00b6 fastvideo.pipelines.stages.image_encoding.ImageEncodingStage \u00b6 <pre><code>ImageEncodingStage(image_encoder, image_processor)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for encoding image prompts into embeddings for diffusion models.</p> <p>This stage handles the encoding of image prompts into the embedding space expected by the diffusion model.</p> <p>Initialize the prompt encoding stage.</p> <p>Parameters:</p> Name Type Description Default <code>enable_logging</code> <p>Whether to enable logging for this stage.</p> required <code>is_secondary</code> <p>Whether this is a secondary image encoder.</p> required Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def __init__(self, image_encoder, image_processor) -&gt; None:\n    \"\"\"\n    Initialize the prompt encoding stage.\n\n    Args:\n        enable_logging: Whether to enable logging for this stage.\n        is_secondary: Whether this is a secondary image encoder.\n    \"\"\"\n    super().__init__()\n    self.image_processor = image_processor\n    self.image_encoder = image_encoder\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.image_encoding.ImageEncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode the prompt into image encoder hidden states.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded prompt embeddings.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode the prompt into image encoder hidden states.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded prompt embeddings.\n    \"\"\"\n    self.image_encoder = self.image_encoder.to(get_local_torch_device())\n\n    image = batch.pil_image\n\n    image_inputs = self.image_processor(\n        images=image, return_tensors=\"pt\").to(get_local_torch_device())\n    with set_forward_context(current_timestep=0, attn_metadata=None):\n        outputs = self.image_encoder(**image_inputs)\n        image_embeds = outputs.last_hidden_state\n\n    batch.image_embeds.append(image_embeds)\n\n    if fastvideo_args.image_encoder_cpu_offload:\n        self.image_encoder.to('cpu')\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.image_encoding.ImageEncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify image encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify image encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"pil_image\", batch.pil_image, V.not_none)\n    result.add_check(\"image_embeds\", batch.image_embeds, V.is_list)\n    return result\n</code></pre> fastvideo.pipelines.stages.image_encoding.ImageEncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify image encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify image encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"image_embeds\", batch.image_embeds,\n                     V.list_of_tensors_dims(3))\n    return result\n</code></pre> fastvideo.pipelines.stages.image_encoding.ImageVAEEncodingStage \u00b6 <pre><code>ImageVAEEncodingStage(vae: ParallelTiledVAE)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for encoding image pixel representations into latent space.</p> <p>This stage handles the encoding of image pixel representations into the final input format (e.g., latents) for image-to-video generation.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def __init__(self, vae: ParallelTiledVAE) -&gt; None:\n    self.vae: ParallelTiledVAE = vae\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.image_encoding.ImageVAEEncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode pixel representations into latent space.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded outputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode pixel representations into latent space.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded outputs.\n    \"\"\"\n    assert batch.pil_image is not None\n    if fastvideo_args.mode == ExecutionMode.INFERENCE:\n        assert batch.pil_image is not None and isinstance(\n            batch.pil_image, PIL.Image.Image)\n        assert batch.height is not None and isinstance(batch.height, int)\n        assert batch.width is not None and isinstance(batch.width, int)\n        assert batch.num_frames is not None and isinstance(\n            batch.num_frames, int)\n        height = batch.height\n        width = batch.width\n        num_frames = batch.num_frames\n    elif fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        assert batch.pil_image is not None and isinstance(\n            batch.pil_image, torch.Tensor)\n        assert batch.height is not None and isinstance(batch.height, list)\n        assert batch.width is not None and isinstance(batch.width, list)\n        assert batch.num_frames is not None and isinstance(\n            batch.num_frames, list)\n        num_frames = batch.num_frames[0]\n        height = batch.height[0]\n        width = batch.width[0]\n\n    self.vae = self.vae.to(get_local_torch_device())\n\n    # Process single image for I2V\n    latent_height = height // self.vae.spatial_compression_ratio\n    latent_width = width // self.vae.spatial_compression_ratio\n    image = batch.pil_image\n    image = self.preprocess(\n        image,\n        vae_scale_factor=self.vae.spatial_compression_ratio,\n        height=height,\n        width=width).to(get_local_torch_device(), dtype=torch.float32)\n\n    # (B, C, H, W) -&gt; (B, C, 1, H, W)\n    image = image.unsqueeze(2)\n\n    video_condition = torch.cat([\n        image,\n        image.new_zeros(image.shape[0], image.shape[1], num_frames - 1,\n                        image.shape[3], image.shape[4])\n    ],\n                                dim=2)\n    video_condition = video_condition.to(device=get_local_torch_device(),\n                                         dtype=torch.float32)\n\n    # Setup VAE precision\n    vae_dtype = PRECISION_TO_TYPE[\n        fastvideo_args.pipeline_config.vae_precision]\n    vae_autocast_enabled = (\n        vae_dtype != torch.float32) and not fastvideo_args.disable_autocast\n\n    # Encode Image\n    with torch.autocast(device_type=\"cuda\",\n                        dtype=vae_dtype,\n                        enabled=vae_autocast_enabled):\n        if fastvideo_args.pipeline_config.vae_tiling:\n            self.vae.enable_tiling()\n        # if fastvideo_args.vae_sp:\n        #     self.vae.enable_parallel()\n        if not vae_autocast_enabled:\n            video_condition = video_condition.to(vae_dtype)\n        encoder_output = self.vae.encode(video_condition)\n\n    if fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        latent_condition = encoder_output.mean\n    else:\n        generator = batch.generator\n        if generator is None:\n            raise ValueError(\"Generator must be provided\")\n        latent_condition = self.retrieve_latents(encoder_output, generator)\n\n    # Apply shifting if needed\n    if (hasattr(self.vae, \"shift_factor\")\n            and self.vae.shift_factor is not None):\n        if isinstance(self.vae.shift_factor, torch.Tensor):\n            latent_condition -= self.vae.shift_factor.to(\n                latent_condition.device, latent_condition.dtype)\n        else:\n            latent_condition -= self.vae.shift_factor\n\n    if isinstance(self.vae.scaling_factor, torch.Tensor):\n        latent_condition = latent_condition * self.vae.scaling_factor.to(\n            latent_condition.device, latent_condition.dtype)\n    else:\n        latent_condition = latent_condition * self.vae.scaling_factor\n\n    if fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        batch.image_latent = latent_condition\n    else:\n        mask_lat_size = torch.ones(1, 1, num_frames, latent_height,\n                                   latent_width)\n        mask_lat_size[:, :, list(range(1, num_frames))] = 0\n        first_frame_mask = mask_lat_size[:, :, 0:1]\n        first_frame_mask = torch.repeat_interleave(\n            first_frame_mask,\n            dim=2,\n            repeats=self.vae.temporal_compression_ratio)\n        mask_lat_size = torch.concat(\n            [first_frame_mask, mask_lat_size[:, :, 1:, :]], dim=2)\n        mask_lat_size = mask_lat_size.view(\n            1, -1, self.vae.temporal_compression_ratio, latent_height,\n            latent_width)\n        mask_lat_size = mask_lat_size.transpose(1, 2)\n        mask_lat_size = mask_lat_size.to(latent_condition.device)\n\n        batch.image_latent = torch.concat([mask_lat_size, latent_condition],\n                                          dim=1)\n\n    # Offload models if needed\n    if hasattr(self, 'maybe_free_model_hooks'):\n        self.maybe_free_model_hooks()\n\n    self.vae.to(\"cpu\")\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.image_encoding.ImageVAEEncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    if fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        result.add_check(\"height\", batch.height, V.list_not_empty)\n        result.add_check(\"width\", batch.width, V.list_not_empty)\n        result.add_check(\"num_frames\", batch.num_frames, V.list_not_empty)\n    else:\n        result.add_check(\"height\", batch.height, V.positive_int)\n        result.add_check(\"width\", batch.width, V.positive_int)\n        result.add_check(\"num_frames\", batch.num_frames, V.positive_int)\n    return result\n</code></pre> fastvideo.pipelines.stages.image_encoding.ImageVAEEncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"image_latent\", batch.image_latent,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre> fastvideo.pipelines.stages.image_encoding.RefImageEncodingStage \u00b6 <pre><code>RefImageEncodingStage(image_encoder, image_processor)\n</code></pre> <p>               Bases: <code>ImageEncodingStage</code></p> <p>Stage for encoding reference image prompts into embeddings for Wan2.1 Control models.</p> <p>This stage extends ImageEncodingStage with specialized preprocessing for reference images.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def __init__(self, image_encoder, image_processor) -&gt; None:\n    \"\"\"\n    Initialize the prompt encoding stage.\n\n    Args:\n        enable_logging: Whether to enable logging for this stage.\n        is_secondary: Whether this is a secondary image encoder.\n    \"\"\"\n    super().__init__()\n    self.image_processor = image_processor\n    self.image_encoder = image_encoder\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.image_encoding.RefImageEncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode the prompt into image encoder hidden states.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded prompt embeddings.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode the prompt into image encoder hidden states.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded prompt embeddings.\n    \"\"\"\n    self.image_encoder = self.image_encoder.to(get_local_torch_device())\n\n    image = batch.pil_image\n    if image is None:\n        image = create_default_image()\n    # Preprocess reference image for CLIP encoder\n    image_tensor = preprocess_reference_image_for_clip(\n        image, get_local_torch_device())\n\n    image_inputs = self.image_processor(images=image_tensor,\n                                        return_tensors=\"pt\").to(\n                                            get_local_torch_device())\n    with set_forward_context(current_timestep=0, attn_metadata=None):\n        outputs = self.image_encoder(**image_inputs)\n        image_embeds = outputs.last_hidden_state\n    batch.image_embeds.append(image_embeds)\n\n    if batch.pil_image is None:\n        batch.image_embeds = [\n            torch.zeros_like(x) for x in batch.image_embeds\n        ]\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.image_encoding.VideoVAEEncodingStage \u00b6 <pre><code>VideoVAEEncodingStage(vae: ParallelTiledVAE)\n</code></pre> <p>               Bases: <code>ImageVAEEncodingStage</code></p> <p>Stage for encoding video pixel representations into latent space.</p> <p>This stage handles the encoding of video pixel representations for video-to-video generation and control. Inherits from ImageVAEEncodingStage to reuse common functionality.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def __init__(self, vae: ParallelTiledVAE) -&gt; None:\n    self.vae: ParallelTiledVAE = vae\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.image_encoding.VideoVAEEncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode video pixel representations into latent space.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded outputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode video pixel representations into latent space.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded outputs.\n    \"\"\"\n    assert batch.video_latent is not None, \"Video latent input is required for VideoVAEEncodingStage\"\n\n    if fastvideo_args.mode == ExecutionMode.INFERENCE:\n        assert batch.height is not None and isinstance(batch.height, int)\n        assert batch.width is not None and isinstance(batch.width, int)\n        assert batch.num_frames is not None and isinstance(\n            batch.num_frames, int)\n        height = batch.height\n        width = batch.width\n        num_frames = batch.num_frames\n    elif fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        assert batch.height is not None and isinstance(batch.height, list)\n        assert batch.width is not None and isinstance(batch.width, list)\n        assert batch.num_frames is not None and isinstance(\n            batch.num_frames, list)\n        num_frames = batch.num_frames[0]\n        height = batch.height[0]\n        width = batch.width[0]\n\n    self.vae = self.vae.to(get_local_torch_device())\n\n    # Prepare video tensor from control video\n    video_condition = self._prepare_control_video_tensor(\n        batch.video_latent, num_frames, height,\n        width).to(get_local_torch_device(), dtype=torch.float32)\n\n    # Setup VAE precision\n    vae_dtype = PRECISION_TO_TYPE[\n        fastvideo_args.pipeline_config.vae_precision]\n    vae_autocast_enabled = (\n        vae_dtype != torch.float32) and not fastvideo_args.disable_autocast\n\n    # Encode control video\n    with torch.autocast(device_type=\"cuda\",\n                        dtype=vae_dtype,\n                        enabled=vae_autocast_enabled):\n        if fastvideo_args.pipeline_config.vae_tiling:\n            self.vae.enable_tiling()\n        if not vae_autocast_enabled:\n            video_condition = video_condition.to(vae_dtype)\n        encoder_output = self.vae.encode(video_condition)\n\n    generator = batch.generator\n    if generator is None:\n        raise ValueError(\"Generator must be provided\")\n    latent_condition = self.retrieve_latents(encoder_output, generator)\n\n    if (hasattr(self.vae, \"shift_factor\")\n            and self.vae.shift_factor is not None):\n        if isinstance(self.vae.shift_factor, torch.Tensor):\n            latent_condition -= self.vae.shift_factor.to(\n                latent_condition.device, latent_condition.dtype)\n        else:\n            latent_condition -= self.vae.shift_factor\n\n    if isinstance(self.vae.scaling_factor, torch.Tensor):\n        latent_condition = latent_condition * self.vae.scaling_factor.to(\n            latent_condition.device, latent_condition.dtype)\n    else:\n        latent_condition = latent_condition * self.vae.scaling_factor\n\n    batch.video_latent = latent_condition\n\n    # Offload models if needed\n    if hasattr(self, 'maybe_free_model_hooks'):\n        self.maybe_free_model_hooks()\n\n    self.vae.to(\"cpu\")\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.image_encoding.VideoVAEEncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify video encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify video encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"video_latent\", batch.video_latent, V.not_none)\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    if fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        result.add_check(\"height\", batch.height, V.list_not_empty)\n        result.add_check(\"width\", batch.width, V.list_not_empty)\n        result.add_check(\"num_frames\", batch.num_frames, V.list_not_empty)\n    else:\n        result.add_check(\"height\", batch.height, V.positive_int)\n        result.add_check(\"width\", batch.width, V.positive_int)\n        result.add_check(\"num_frames\", batch.num_frames, V.positive_int)\n    return result\n</code></pre> fastvideo.pipelines.stages.image_encoding.VideoVAEEncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify video encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/image_encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify video encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"video_latent\", batch.video_latent,\n                     [V.is_tensor, V.with_dims(5)])\n    return result\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.input_validation","title":"fastvideo.pipelines.stages.input_validation","text":"<p>Input validation stage for diffusion pipelines.</p> Classes\u00b6 fastvideo.pipelines.stages.input_validation.InputValidationStage \u00b6 <p>               Bases: <code>PipelineStage</code></p> <p>Stage for validating and preparing inputs for diffusion pipelines.</p> <p>This stage validates that all required inputs are present and properly formatted before proceeding with the diffusion process.</p> Functions\u00b6 fastvideo.pipelines.stages.input_validation.InputValidationStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Validate and prepare inputs.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The validated batch information.</p> Source code in <code>fastvideo/pipelines/stages/input_validation.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Validate and prepare inputs.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The validated batch information.\n    \"\"\"\n\n    self._generate_seeds(batch, fastvideo_args)\n\n    # Ensure prompt is properly formatted\n    if batch.prompt is None and batch.prompt_embeds is None:\n        raise ValueError(\n            \"Either `prompt` or `prompt_embeds` must be provided\")\n\n    # Ensure negative prompt is properly formatted if using classifier-free guidance\n    if (batch.do_classifier_free_guidance and batch.negative_prompt is None\n            and batch.negative_prompt_embeds is None):\n        raise ValueError(\n            \"For classifier-free guidance, either `negative_prompt` or \"\n            \"`negative_prompt_embeds` must be provided\")\n\n    # Validate height and width\n    if batch.height is None or batch.width is None:\n        raise ValueError(\n            \"Height and width must be provided. Please set `height` and `width`.\"\n        )\n    if batch.height % 8 != 0 or batch.width % 8 != 0:\n        raise ValueError(\n            f\"Height and width must be divisible by 8 but are {batch.height} and {batch.width}.\"\n        )\n\n    # Validate number of inference steps\n    if batch.num_inference_steps &lt;= 0:\n        raise ValueError(\n            f\"Number of inference steps must be positive, but got {batch.num_inference_steps}\"\n        )\n\n    # Validate guidance scale if using classifier-free guidance\n    if batch.do_classifier_free_guidance and batch.guidance_scale &lt;= 0:\n        raise ValueError(\n            f\"Guidance scale must be positive, but got {batch.guidance_scale}\"\n        )\n\n    # for i2v, get image from image_path\n    # @TODO(Wei) hard-coded for wan2.2 5b ti2v for now. Should put this in image_encoding stage\n    if batch.image_path is not None:\n        if batch.image_path.endswith(\".mp4\"):\n            image = load_video(batch.image_path)[0]\n        else:\n            image = load_image(batch.image_path)\n        batch.pil_image = image\n\n    # further processing for ti2v task\n    if fastvideo_args.pipeline_config.ti2v_task and batch.pil_image is not None:\n        img = batch.pil_image\n        ih, iw = img.height, img.width\n        patch_size = fastvideo_args.pipeline_config.dit_config.arch_config.patch_size\n        vae_stride = fastvideo_args.pipeline_config.vae_config.arch_config.scale_factor_spatial\n        dh, dw = patch_size[1] * vae_stride, patch_size[2] * vae_stride\n        max_area = 704 * 1280\n        ow, oh = best_output_size(iw, ih, dw, dh, max_area)\n\n        scale = max(ow / iw, oh / ih)\n        img = img.resize((round(iw * scale), round(ih * scale)),\n                         Image.LANCZOS)\n        logger.info(\"resized img height: %s, img width: %s\", img.height,\n                    img.width)\n\n        # center-crop\n        x1 = (img.width - ow) // 2\n        y1 = (img.height - oh) // 2\n        img = img.crop((x1, y1, x1 + ow, y1 + oh))\n        assert img.width == ow and img.height == oh\n\n        # to tensor\n        img = TF.to_tensor(img).sub_(0.5).div_(0.5).to(\n            self.device).unsqueeze(1)\n        img = img.unsqueeze(0)\n        batch.height = oh\n        batch.width = ow\n        batch.pil_image = img\n\n    # for v2v, get control video from video path\n    if batch.video_path is not None:\n        pil_images, original_fps = load_video(batch.video_path,\n                                              return_fps=True)\n        logger.info(\"Loaded video with %s frames, original FPS: %s\",\n                    len(pil_images), original_fps)\n\n        # Get target parameters from batch\n        target_fps = batch.fps\n        target_num_frames = batch.num_frames\n        target_height = batch.height\n        target_width = batch.width\n\n        if target_fps is not None and original_fps is not None:\n            frame_skip = max(1, int(original_fps // target_fps))\n            if frame_skip &gt; 1:\n                pil_images = pil_images[::frame_skip]\n                effective_fps = original_fps / frame_skip\n                logger.info(\n                    \"Resampled video from %.1f fps to %.1f fps (skip=%s)\",\n                    original_fps, effective_fps, frame_skip)\n\n        # Limit to target number of frames\n        if target_num_frames is not None and len(\n                pil_images) &gt; target_num_frames:\n            pil_images = pil_images[:target_num_frames]\n            logger.info(\"Limited video to %s frames (from %s total)\",\n                        target_num_frames, len(pil_images))\n\n        # Resize each PIL image to target dimensions\n        resized_images = []\n        for pil_img in pil_images:\n            resized_img = resize(pil_img,\n                                 target_height,\n                                 target_width,\n                                 resize_mode=\"default\",\n                                 resample=\"lanczos\")\n            resized_images.append(resized_img)\n\n        # Convert PIL images to numpy array\n        video_numpy = pil_to_numpy(resized_images)\n        video_numpy = normalize(video_numpy)\n        video_tensor = numpy_to_pt(video_numpy)\n\n        # Rearrange to [C, T, H, W] and add batch dimension -&gt; [B, C, T, H, W]\n        input_video = video_tensor.permute(1, 0, 2, 3).unsqueeze(0)\n\n        batch.video_latent = input_video\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.input_validation.InputValidationStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify input validation stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/input_validation.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify input validation stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"seed\", batch.seed, [V.not_none, V.positive_int])\n    result.add_check(\"num_videos_per_prompt\", batch.num_videos_per_prompt,\n                     V.positive_int)\n    result.add_check(\n        \"prompt_or_embeds\", None, lambda _: V.string_or_list_strings(\n            batch.prompt) or V.list_not_empty(batch.prompt_embeds))\n    result.add_check(\"height\", batch.height, V.positive_int)\n    result.add_check(\"width\", batch.width, V.positive_int)\n    result.add_check(\"num_inference_steps\", batch.num_inference_steps,\n                     V.positive_int)\n    result.add_check(\n        \"guidance_scale\", batch.guidance_scale, lambda x: not batch.\n        do_classifier_free_guidance or V.positive_float(x))\n    return result\n</code></pre> fastvideo.pipelines.stages.input_validation.InputValidationStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify input validation stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/input_validation.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify input validation stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"seeds\", batch.seeds, V.list_not_empty)\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    return result\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.latent_preparation","title":"fastvideo.pipelines.stages.latent_preparation","text":"<p>Latent preparation stage for diffusion pipelines.</p> Classes\u00b6 fastvideo.pipelines.stages.latent_preparation.LatentPreparationStage \u00b6 <pre><code>LatentPreparationStage(scheduler, transformer)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for preparing initial latent variables for the diffusion process.</p> <p>This stage handles the preparation of the initial latent variables that will be denoised during the diffusion process.</p> Source code in <code>fastvideo/pipelines/stages/latent_preparation.py</code> <pre><code>def __init__(self, scheduler, transformer) -&gt; None:\n    super().__init__()\n    self.scheduler = scheduler\n    self.transformer = transformer\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.latent_preparation.LatentPreparationStage.adjust_video_length \u00b6 <pre><code>adjust_video_length(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; int\n</code></pre> <p>Adjust video length based on VAE version.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The batch with adjusted video length.</p> Source code in <code>fastvideo/pipelines/stages/latent_preparation.py</code> <pre><code>def adjust_video_length(self, batch: ForwardBatch,\n                        fastvideo_args: FastVideoArgs) -&gt; int:\n    \"\"\"\n    Adjust video length based on VAE version.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with adjusted video length.\n    \"\"\"\n\n    video_length = batch.num_frames\n    use_temporal_scaling_frames = fastvideo_args.pipeline_config.vae_config.use_temporal_scaling_frames\n    if use_temporal_scaling_frames:\n        temporal_scale_factor = fastvideo_args.pipeline_config.vae_config.arch_config.temporal_compression_ratio\n        latent_num_frames = (video_length - 1) // temporal_scale_factor + 1\n    else:  # stepvideo only\n        latent_num_frames = video_length // 17 * 3\n    return int(latent_num_frames)\n</code></pre> fastvideo.pipelines.stages.latent_preparation.LatentPreparationStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Prepare initial latent variables for the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with prepared latent variables.</p> Source code in <code>fastvideo/pipelines/stages/latent_preparation.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Prepare initial latent variables for the diffusion process.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with prepared latent variables.\n    \"\"\"\n\n    latent_num_frames = None\n    # Adjust video length based on VAE version if needed\n    if hasattr(self, 'adjust_video_length'):\n        latent_num_frames = self.adjust_video_length(batch, fastvideo_args)\n    # Determine batch size\n    if isinstance(batch.prompt, list):\n        batch_size = len(batch.prompt)\n    elif batch.prompt is not None:\n        batch_size = 1\n    else:\n        batch_size = batch.prompt_embeds[0].shape[0]\n\n    # Adjust batch size for number of videos per prompt\n    batch_size *= batch.num_videos_per_prompt\n\n    # Get required parameters\n    dtype = batch.prompt_embeds[0].dtype\n    device = get_local_torch_device()\n    generator = batch.generator\n    latents = batch.latents\n    num_frames = latent_num_frames if latent_num_frames is not None else batch.num_frames\n    height = batch.height\n    width = batch.width\n\n    # TODO(will): remove this once we add input/output validation for stages\n    if height is None or width is None:\n        raise ValueError(\"Height and width must be provided\")\n\n    # Calculate latent shape\n    shape = (\n        batch_size,\n        self.transformer.num_channels_latents,\n        num_frames,\n        height // fastvideo_args.pipeline_config.vae_config.arch_config.\n        spatial_compression_ratio,\n        width // fastvideo_args.pipeline_config.vae_config.arch_config.\n        spatial_compression_ratio,\n    )\n\n    # Validate generator if it's a list\n    if isinstance(generator, list) and len(generator) != batch_size:\n        raise ValueError(\n            f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n            f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n        )\n    # Generate or use provided latents\n    if latents is None:\n        latents = randn_tensor(shape,\n                               generator=generator,\n                               device=device,\n                               dtype=dtype)\n    else:\n        latents = latents.to(device)\n\n    # Scale the initial noise if needed\n    if hasattr(self.scheduler, \"init_noise_sigma\"):\n        latents = latents * self.scheduler.init_noise_sigma\n    # Update batch with prepared latents\n    batch.latents = latents\n    batch.raw_latent_shape = latents.shape\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.latent_preparation.LatentPreparationStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify latent preparation stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/latent_preparation.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify latent preparation stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\n        \"prompt_or_embeds\", None, lambda _: V.string_or_list_strings(\n            batch.prompt) or V.list_not_empty(batch.prompt_embeds))\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds,\n                     V.list_of_tensors)\n    result.add_check(\"num_videos_per_prompt\", batch.num_videos_per_prompt,\n                     V.positive_int)\n    result.add_check(\"generator\", batch.generator,\n                     V.generator_or_list_generators)\n    result.add_check(\"num_frames\", batch.num_frames, V.positive_int)\n    result.add_check(\"height\", batch.height, V.positive_int)\n    result.add_check(\"width\", batch.width, V.positive_int)\n    result.add_check(\"latents\", batch.latents, V.none_or_tensor)\n    return result\n</code></pre> fastvideo.pipelines.stages.latent_preparation.LatentPreparationStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify latent preparation stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/latent_preparation.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify latent preparation stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"latents\", batch.latents,\n                     [V.is_tensor, V.with_dims(5)])\n    result.add_check(\"raw_latent_shape\", batch.raw_latent_shape, V.is_tuple)\n    return result\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.stepvideo_encoding","title":"fastvideo.pipelines.stages.stepvideo_encoding","text":"Classes\u00b6 fastvideo.pipelines.stages.stepvideo_encoding.StepvideoPromptEncodingStage \u00b6 <pre><code>StepvideoPromptEncodingStage(stepllm, clip)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for encoding prompts using the remote caption API.</p> <p>This stage applies the magic string transformations and calls the remote caption service asynchronously to get:   - primary prompt embeddings,   - an attention mask,   - and a clip embedding.</p> Source code in <code>fastvideo/pipelines/stages/stepvideo_encoding.py</code> <pre><code>def __init__(self, stepllm, clip) -&gt; None:\n    super().__init__()\n    # self.caption_client = caption_client  # This should have a call_caption(prompts: List[str]) method.\n    self.stepllm = stepllm\n    self.clip = clip\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.stepvideo_encoding.StepvideoPromptEncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify stepvideo encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/stepvideo_encoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify stepvideo encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"prompt\", batch.prompt, V.string_not_empty)\n    return result\n</code></pre> fastvideo.pipelines.stages.stepvideo_encoding.StepvideoPromptEncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify stepvideo encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/stepvideo_encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify stepvideo encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds,\n                     [V.is_tensor, V.with_dims(3)])\n    result.add_check(\"negative_prompt_embeds\", batch.negative_prompt_embeds,\n                     [V.is_tensor, V.with_dims(3)])\n    result.add_check(\"prompt_attention_mask\", batch.prompt_attention_mask,\n                     [V.is_tensor, V.with_dims(2)])\n    result.add_check(\"negative_attention_mask\",\n                     batch.negative_attention_mask,\n                     [V.is_tensor, V.with_dims(2)])\n    result.add_check(\"clip_embedding_pos\", batch.clip_embedding_pos,\n                     [V.is_tensor, V.with_dims(2)])\n    result.add_check(\"clip_embedding_neg\", batch.clip_embedding_neg,\n                     [V.is_tensor, V.with_dims(2)])\n    return result\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.text_encoding","title":"fastvideo.pipelines.stages.text_encoding","text":"<p>Prompt encoding stages for diffusion pipelines.</p> <p>This module contains implementations of prompt encoding stages for diffusion pipelines.</p> Classes\u00b6 fastvideo.pipelines.stages.text_encoding.TextEncodingStage \u00b6 <pre><code>TextEncodingStage(text_encoders, tokenizers)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for encoding text prompts into embeddings for diffusion models.</p> <p>This stage handles the encoding of text prompts into the embedding space expected by the diffusion model.</p> <p>Initialize the prompt encoding stage.</p> <p>Parameters:</p> Name Type Description Default <code>enable_logging</code> <p>Whether to enable logging for this stage.</p> required <code>is_secondary</code> <p>Whether this is a secondary text encoder.</p> required Source code in <code>fastvideo/pipelines/stages/text_encoding.py</code> <pre><code>def __init__(self, text_encoders, tokenizers) -&gt; None:\n    \"\"\"\n    Initialize the prompt encoding stage.\n\n    Args:\n        enable_logging: Whether to enable logging for this stage.\n        is_secondary: Whether this is a secondary text encoder.\n    \"\"\"\n    super().__init__()\n    self.tokenizers = tokenizers\n    self.text_encoders = text_encoders\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.text_encoding.TextEncodingStage.encode_text \u00b6 <pre><code>encode_text(\n    text: str | list[str],\n    fastvideo_args: FastVideoArgs,\n    encoder_index: int | list[int] | None = None,\n    return_attention_mask: bool = False,\n    return_type: str = \"list\",\n    device: device | str | None = None,\n    dtype: dtype | None = None,\n    max_length: int | None = None,\n    truncation: bool | None = None,\n    padding: bool | str | None = None,\n)\n</code></pre> <p>Encode plain text using selected text encoder(s) and return embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str | list[str]</code> <p>A single string or a list of strings to encode.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments providing pipeline config, including tokenizer and encoder settings, preprocess and postprocess functions.</p> required <code>encoder_index</code> <code>int | list[int] | None</code> <p>Encoder selector by index. Accepts an int or list of ints.</p> <code>None</code> <code>return_attention_mask</code> <code>bool</code> <p>If True, also return attention masks for each selected encoder.</p> <code>False</code> <code>return_type</code> <code>str</code> <p>\"list\" (default) returns a list aligned with selection; \"dict\" returns a dict keyed by encoder index as a string; \"stack\" stacks along a new first dimension (requires matching shapes).</p> <code>'list'</code> <code>device</code> <code>device | str | None</code> <p>Optional device override for inputs; defaults to local torch device.</p> <code>None</code> <code>dtype</code> <code>dtype | None</code> <p>Optional dtype to cast returned embeddings to.</p> <code>None</code> <code>max_length</code> <code>int | None</code> <p>Optional per-call tokenizer override.</p> <code>None</code> <code>truncation</code> <code>bool | None</code> <p>Optional per-call tokenizer override.</p> <code>None</code> <code>padding</code> <code>bool | str | None</code> <p>Optional per-call tokenizer override.</p> <code>None</code> <p>Returns:</p> Type Description <p>Depending on return_type and return_attention_mask:</p> <ul> <li>list: List[Tensor] or (List[Tensor], List[Tensor])</li> </ul> <ul> <li>dict: Dict[str, Tensor] or (Dict[str, Tensor], Dict[str, Tensor])</li> </ul> <ul> <li>stack: Tensor of shape [num_encoders, ...] or a tuple with stacked attention masks</li> </ul> Source code in <code>fastvideo/pipelines/stages/text_encoding.py</code> <pre><code>@torch.no_grad()\ndef encode_text(\n    self,\n    text: str | list[str],\n    fastvideo_args: FastVideoArgs,\n    encoder_index: int | list[int] | None = None,\n    return_attention_mask: bool = False,\n    return_type: str = \"list\",  # one of: \"list\", \"dict\", \"stack\"\n    device: torch.device | str | None = None,\n    dtype: torch.dtype | None = None,\n    max_length: int | None = None,\n    truncation: bool | None = None,\n    padding: bool | str | None = None,\n):\n    \"\"\"\n    Encode plain text using selected text encoder(s) and return embeddings.\n\n    Args:\n        text: A single string or a list of strings to encode.\n        fastvideo_args: The inference arguments providing pipeline config,\n            including tokenizer and encoder settings, preprocess and postprocess\n            functions.\n        encoder_index: Encoder selector by index. Accepts an int or list of ints.\n        return_attention_mask: If True, also return attention masks for each\n            selected encoder.\n        return_type: \"list\" (default) returns a list aligned with selection;\n            \"dict\" returns a dict keyed by encoder index as a string; \"stack\" stacks along a\n            new first dimension (requires matching shapes).\n        device: Optional device override for inputs; defaults to local torch device.\n        dtype: Optional dtype to cast returned embeddings to.\n        max_length: Optional per-call tokenizer override.\n        truncation: Optional per-call tokenizer override.\n        padding: Optional per-call tokenizer override.\n\n    Returns:\n        Depending on return_type and return_attention_mask:\n        - list: List[Tensor] or (List[Tensor], List[Tensor])\n        - dict: Dict[str, Tensor] or (Dict[str, Tensor], Dict[str, Tensor])\n        - stack: Tensor of shape [num_encoders, ...] or a tuple with stacked\n          attention masks\n    \"\"\"\n\n    assert len(self.tokenizers) == len(self.text_encoders)\n    assert len(self.text_encoders) == len(\n        fastvideo_args.pipeline_config.text_encoder_configs)\n\n    # Resolve selection into indices\n    encoder_cfgs = fastvideo_args.pipeline_config.text_encoder_configs\n    if encoder_index is None:\n        indices: list[int] = [0]\n    elif isinstance(encoder_index, int):\n        indices = [encoder_index]\n    else:\n        indices = list(encoder_index)\n    # validate range\n    num_encoders = len(self.text_encoders)\n    for idx in indices:\n        if idx &lt; 0 or idx &gt;= num_encoders:\n            raise IndexError(\n                f\"encoder index {idx} out of range [0, {num_encoders-1}]\")\n\n    # Validate indices are within range\n    num_encoders = len(self.text_encoders)\n\n    # Normalize input to list[str]\n    assert isinstance(text, str | list)\n    if isinstance(text, str):\n        texts: list[str] = [text]\n    else:\n        texts = text\n\n    embeds_list: list[torch.Tensor] = []\n    attn_masks_list: list[torch.Tensor] = []\n\n    preprocess_funcs = fastvideo_args.pipeline_config.preprocess_text_funcs\n    postprocess_funcs = fastvideo_args.pipeline_config.postprocess_text_funcs\n    encoder_cfgs = fastvideo_args.pipeline_config.text_encoder_configs\n\n    if return_type not in (\"list\", \"dict\", \"stack\"):\n        raise ValueError(\n            f\"Invalid return_type '{return_type}'. Expected one of: 'list', 'dict', 'stack'\"\n        )\n\n    target_device = device if device is not None else get_local_torch_device(\n    )\n\n    for i in indices:\n        tokenizer = self.tokenizers[i]\n        text_encoder = self.text_encoders[i]\n        encoder_config = encoder_cfgs[i]\n        preprocess_func = preprocess_funcs[i]\n        postprocess_func = postprocess_funcs[i]\n\n        processed_texts: list[str] = []\n        for prompt_str in texts:\n            processed_texts.append(preprocess_func(prompt_str))\n\n        tok_kwargs = dict(encoder_config.tokenizer_kwargs)\n        if max_length is not None:\n            tok_kwargs[\"max_length\"] = max_length\n        if truncation is not None:\n            tok_kwargs[\"truncation\"] = truncation\n        if padding is not None:\n            tok_kwargs[\"padding\"] = padding\n\n        text_inputs = tokenizer(processed_texts,\n                                **tok_kwargs).to(target_device)\n\n        input_ids = text_inputs[\"input_ids\"]\n        attention_mask = text_inputs[\"attention_mask\"]\n\n        with set_forward_context(current_timestep=0, attn_metadata=None):\n            outputs = text_encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                output_hidden_states=True,\n            )\n\n        prompt_embeds = postprocess_func(outputs)\n        if dtype is not None:\n            prompt_embeds = prompt_embeds.to(dtype=dtype)\n        embeds_list.append(prompt_embeds)\n        if return_attention_mask:\n            attn_masks_list.append(attention_mask)\n\n    # Shape results according to return_type\n    if return_type == \"list\":\n        if return_attention_mask:\n            return embeds_list, attn_masks_list\n        return embeds_list\n\n    if return_type == \"dict\":\n        key_strs = [str(i) for i in indices]\n        embeds_dict = {\n            k: v\n            for k, v in zip(key_strs, embeds_list, strict=False)\n        }\n        if return_attention_mask:\n            attn_dict = {\n                k: v\n                for k, v in zip(key_strs, attn_masks_list, strict=False)\n            }\n            return embeds_dict, attn_dict\n        return embeds_dict\n\n    # return_type == \"stack\"\n    # Validate shapes are compatible\n    base_shape = list(embeds_list[0].shape)\n    for t in embeds_list[1:]:\n        if list(t.shape) != base_shape:\n            raise ValueError(\n                f\"Cannot stack embeddings with differing shapes: {[list(t.shape) for t in embeds_list]}\"\n            )\n    stacked_embeds = torch.stack(embeds_list, dim=0)\n    if return_attention_mask:\n        base_mask_shape = list(attn_masks_list[0].shape)\n        for m in attn_masks_list[1:]:\n            if list(m.shape) != base_mask_shape:\n                raise ValueError(\n                    f\"Cannot stack attention masks with differing shapes: {[list(m.shape) for m in attn_masks_list]}\"\n                )\n        stacked_masks = torch.stack(attn_masks_list, dim=0)\n        return stacked_embeds, stacked_masks\n    return stacked_embeds\n</code></pre> fastvideo.pipelines.stages.text_encoding.TextEncodingStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Encode the prompt into text encoder hidden states.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with encoded prompt embeddings.</p> Source code in <code>fastvideo/pipelines/stages/text_encoding.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Encode the prompt into text encoder hidden states.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with encoded prompt embeddings.\n    \"\"\"\n    assert len(self.tokenizers) == len(self.text_encoders)\n    assert len(self.text_encoders) == len(\n        fastvideo_args.pipeline_config.text_encoder_configs)\n\n    # Encode positive prompt with all available encoders\n    assert batch.prompt is not None\n    prompt_text: str | list[str] = batch.prompt\n    all_indices: list[int] = list(range(len(self.text_encoders)))\n    prompt_embeds_list, prompt_masks_list = self.encode_text(\n        prompt_text,\n        fastvideo_args,\n        encoder_index=all_indices,\n        return_attention_mask=True,\n    )\n    for pe in prompt_embeds_list:\n        batch.prompt_embeds.append(pe)\n    if batch.prompt_attention_mask is not None:\n        for am in prompt_masks_list:\n            batch.prompt_attention_mask.append(am)\n\n    # Encode negative prompt if CFG is enabled\n    if batch.do_classifier_free_guidance:\n        assert isinstance(batch.negative_prompt, str)\n        neg_embeds_list, neg_masks_list = self.encode_text(\n            batch.negative_prompt,\n            fastvideo_args,\n            encoder_index=all_indices,\n            return_attention_mask=True,\n        )\n        assert batch.negative_prompt_embeds is not None\n        for ne in neg_embeds_list:\n            batch.negative_prompt_embeds.append(ne)\n        if batch.negative_attention_mask is not None:\n            for nm in neg_masks_list:\n                batch.negative_attention_mask.append(nm)\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.text_encoding.TextEncodingStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify text encoding stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/text_encoding.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify text encoding stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"prompt\", batch.prompt, V.string_or_list_strings)\n    result.add_check(\n        \"negative_prompt\", batch.negative_prompt, lambda x: not batch.\n        do_classifier_free_guidance or V.string_not_empty(x))\n    result.add_check(\"do_classifier_free_guidance\",\n                     batch.do_classifier_free_guidance, V.bool_value)\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds, V.is_list)\n    result.add_check(\"negative_prompt_embeds\", batch.negative_prompt_embeds,\n                     V.none_or_list)\n    return result\n</code></pre> fastvideo.pipelines.stages.text_encoding.TextEncodingStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify text encoding stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/text_encoding.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify text encoding stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"prompt_embeds\", batch.prompt_embeds,\n                     V.list_of_tensors_min_dims(2))\n    result.add_check(\n        \"negative_prompt_embeds\", batch.negative_prompt_embeds,\n        lambda x: not batch.do_classifier_free_guidance or V.\n        list_of_tensors_with_min_dims(x, 2))\n    return result\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.timestep_preparation","title":"fastvideo.pipelines.stages.timestep_preparation","text":"<p>Timestep preparation stages for diffusion pipelines.</p> <p>This module contains implementations of timestep preparation stages for diffusion pipelines.</p> Classes\u00b6 fastvideo.pipelines.stages.timestep_preparation.TimestepPreparationStage \u00b6 <pre><code>TimestepPreparationStage(scheduler)\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Stage for preparing timesteps for the diffusion process.</p> <p>This stage handles the preparation of the timestep sequence that will be used during the diffusion process.</p> Source code in <code>fastvideo/pipelines/stages/timestep_preparation.py</code> <pre><code>def __init__(self, scheduler) -&gt; None:\n    self.scheduler = scheduler\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.timestep_preparation.TimestepPreparationStage.forward \u00b6 <pre><code>forward(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; ForwardBatch\n</code></pre> <p>Prepare timesteps for the diffusion process.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ForwardBatch</code> <p>The current batch information.</p> required <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>The inference arguments.</p> required <p>Returns:</p> Type Description <code>ForwardBatch</code> <p>The batch with prepared timesteps.</p> Source code in <code>fastvideo/pipelines/stages/timestep_preparation.py</code> <pre><code>def forward(\n    self,\n    batch: ForwardBatch,\n    fastvideo_args: FastVideoArgs,\n) -&gt; ForwardBatch:\n    \"\"\"\n    Prepare timesteps for the diffusion process.\n\n    Args:\n        batch: The current batch information.\n        fastvideo_args: The inference arguments.\n\n    Returns:\n        The batch with prepared timesteps.\n    \"\"\"\n    scheduler = self.scheduler\n    device = get_local_torch_device()\n    num_inference_steps = batch.num_inference_steps\n    timesteps = batch.timesteps\n    sigmas = batch.sigmas\n    n_tokens = batch.n_tokens\n\n    # Prepare extra kwargs for set_timesteps\n    extra_set_timesteps_kwargs = {}\n    if n_tokens is not None and \"n_tokens\" in inspect.signature(\n            scheduler.set_timesteps).parameters:\n        extra_set_timesteps_kwargs[\"n_tokens\"] = n_tokens\n\n    # Handle custom timesteps or sigmas\n    if timesteps is not None and sigmas is not None:\n        raise ValueError(\n            \"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\"\n        )\n\n    if timesteps is not None:\n        accepts_timesteps = \"timesteps\" in inspect.signature(\n            scheduler.set_timesteps).parameters\n        if not accepts_timesteps:\n            raise ValueError(\n                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n            )\n        scheduler.set_timesteps(timesteps=timesteps,\n                                device=device,\n                                **extra_set_timesteps_kwargs)\n        timesteps = scheduler.timesteps\n    elif sigmas is not None:\n        accept_sigmas = \"sigmas\" in inspect.signature(\n            scheduler.set_timesteps).parameters\n        if not accept_sigmas:\n            raise ValueError(\n                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n                f\" sigmas schedules. Please check whether you are using the correct scheduler.\"\n            )\n        scheduler.set_timesteps(sigmas=sigmas,\n                                device=device,\n                                **extra_set_timesteps_kwargs)\n        timesteps = scheduler.timesteps\n    else:\n        scheduler.set_timesteps(num_inference_steps,\n                                device=device,\n                                **extra_set_timesteps_kwargs)\n        timesteps = scheduler.timesteps\n\n    # Update batch with prepared timesteps\n    batch.timesteps = timesteps\n\n    return batch\n</code></pre> fastvideo.pipelines.stages.timestep_preparation.TimestepPreparationStage.verify_input \u00b6 <pre><code>verify_input(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify timestep preparation stage inputs.</p> Source code in <code>fastvideo/pipelines/stages/timestep_preparation.py</code> <pre><code>def verify_input(self, batch: ForwardBatch,\n                 fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify timestep preparation stage inputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"num_inference_steps\", batch.num_inference_steps,\n                     V.positive_int)\n    result.add_check(\"timesteps\", batch.timesteps, V.none_or_tensor)\n    result.add_check(\"sigmas\", batch.sigmas, V.none_or_list)\n    result.add_check(\"n_tokens\", batch.n_tokens, V.none_or_positive_int)\n    return result\n</code></pre> fastvideo.pipelines.stages.timestep_preparation.TimestepPreparationStage.verify_output \u00b6 <pre><code>verify_output(\n    batch: ForwardBatch, fastvideo_args: FastVideoArgs\n) -&gt; VerificationResult\n</code></pre> <p>Verify timestep preparation stage outputs.</p> Source code in <code>fastvideo/pipelines/stages/timestep_preparation.py</code> <pre><code>def verify_output(self, batch: ForwardBatch,\n                  fastvideo_args: FastVideoArgs) -&gt; VerificationResult:\n    \"\"\"Verify timestep preparation stage outputs.\"\"\"\n    result = VerificationResult()\n    result.add_check(\"timesteps\", batch.timesteps,\n                     [V.is_tensor, V.with_dims(1)])\n    return result\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.validators","title":"fastvideo.pipelines.stages.validators","text":"<p>Common validators for pipeline stage verification.</p> <p>This module provides reusable validation functions that can be used across all pipeline stages for input/output verification.</p> Classes\u00b6 fastvideo.pipelines.stages.validators.StageValidators \u00b6 <p>Common validators for pipeline stages.</p> Functions\u00b6 fastvideo.pipelines.stages.validators.StageValidators.bool_value <code>staticmethod</code> \u00b6 <pre><code>bool_value(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a boolean.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef bool_value(value: Any) -&gt; bool:\n    \"\"\"Check if value is a boolean.\"\"\"\n    return isinstance(value, bool)\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.divisible <code>staticmethod</code> \u00b6 <pre><code>divisible(divisor: int) -&gt; Callable[[Any], bool]\n</code></pre> <p>Return a validator that checks if value is divisible by divisor.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef divisible(divisor: int) -&gt; Callable[[Any], bool]:\n    \"\"\"Return a validator that checks if value is divisible by divisor.\"\"\"\n\n    def validator(value: Any) -&gt; bool:\n        return StageValidators.divisible_by(value, divisor)\n\n    return validator\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.divisible_by <code>staticmethod</code> \u00b6 <pre><code>divisible_by(value: Any, divisor: int) -&gt; bool\n</code></pre> <p>Check if value is divisible by divisor.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef divisible_by(value: Any, divisor: int) -&gt; bool:\n    \"\"\"Check if value is divisible by divisor.\"\"\"\n    return value is not None and isinstance(value,\n                                            int) and value % divisor == 0\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.generator_or_list_generators <code>staticmethod</code> \u00b6 <pre><code>generator_or_list_generators(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a Generator or list of Generators.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef generator_or_list_generators(value: Any) -&gt; bool:\n    \"\"\"Check if value is a Generator or list of Generators.\"\"\"\n    if isinstance(value, torch.Generator):\n        return True\n    if isinstance(value, list):\n        return all(isinstance(item, torch.Generator) for item in value)\n    return False\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.is_list <code>staticmethod</code> \u00b6 <pre><code>is_list(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a list (can be empty).</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef is_list(value: Any) -&gt; bool:\n    \"\"\"Check if value is a list (can be empty).\"\"\"\n    return isinstance(value, list)\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.is_tensor <code>staticmethod</code> \u00b6 <pre><code>is_tensor(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a torch tensor and doesn't contain NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef is_tensor(value: Any) -&gt; bool:\n    \"\"\"Check if value is a torch tensor and doesn't contain NaN values.\"\"\"\n    if not isinstance(value, torch.Tensor):\n        return False\n    return not torch.isnan(value).any().item()\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.is_tuple <code>staticmethod</code> \u00b6 <pre><code>is_tuple(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a tuple.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef is_tuple(value: Any) -&gt; bool:\n    \"\"\"Check if value is a tuple.\"\"\"\n    return isinstance(value, tuple)\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.list_length <code>staticmethod</code> \u00b6 <pre><code>list_length(value: Any, length: int) -&gt; bool\n</code></pre> <p>Check if list has specific length.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef list_length(value: Any, length: int) -&gt; bool:\n    \"\"\"Check if list has specific length.\"\"\"\n    return isinstance(value, list) and len(value) == length\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.list_min_length <code>staticmethod</code> \u00b6 <pre><code>list_min_length(value: Any, min_length: int) -&gt; bool\n</code></pre> <p>Check if list has at least min_length items.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef list_min_length(value: Any, min_length: int) -&gt; bool:\n    \"\"\"Check if list has at least min_length items.\"\"\"\n    return isinstance(value, list) and len(value) &gt;= min_length\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.list_not_empty <code>staticmethod</code> \u00b6 <pre><code>list_not_empty(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a non-empty list.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef list_not_empty(value: Any) -&gt; bool:\n    \"\"\"Check if value is a non-empty list.\"\"\"\n    return isinstance(value, list) and len(value) &gt; 0\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.list_of_tensors <code>staticmethod</code> \u00b6 <pre><code>list_of_tensors(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a non-empty list where all items are tensors without NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef list_of_tensors(value: Any) -&gt; bool:\n    \"\"\"Check if value is a non-empty list where all items are tensors without NaN values.\"\"\"\n    if not isinstance(value, list) or len(value) == 0:\n        return False\n    for item in value:\n        if not isinstance(item, torch.Tensor):\n            return False\n        if torch.isnan(item).any().item():\n            return False\n    return True\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.list_of_tensors_dims <code>staticmethod</code> \u00b6 <pre><code>list_of_tensors_dims(dims: int) -&gt; Callable[[Any], bool]\n</code></pre> <p>Return a validator that checks if value is a list of tensors with specific dimensions and no NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef list_of_tensors_dims(dims: int) -&gt; Callable[[Any], bool]:\n    \"\"\"Return a validator that checks if value is a list of tensors with specific dimensions and no NaN values.\"\"\"\n\n    def validator(value: Any) -&gt; bool:\n        return StageValidators.list_of_tensors_with_dims(value, dims)\n\n    return validator\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.list_of_tensors_min_dims <code>staticmethod</code> \u00b6 <pre><code>list_of_tensors_min_dims(\n    min_dims: int,\n) -&gt; Callable[[Any], bool]\n</code></pre> <p>Return a validator that checks if value is a list of tensors with at least min_dims dimensions and no NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef list_of_tensors_min_dims(min_dims: int) -&gt; Callable[[Any], bool]:\n    \"\"\"Return a validator that checks if value is a list of tensors with at least min_dims dimensions and no NaN values.\"\"\"\n\n    def validator(value: Any) -&gt; bool:\n        return StageValidators.list_of_tensors_with_min_dims(\n            value, min_dims)\n\n    return validator\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.list_of_tensors_with_dims <code>staticmethod</code> \u00b6 <pre><code>list_of_tensors_with_dims(value: Any, dims: int) -&gt; bool\n</code></pre> <p>Check if value is a non-empty list where all items are tensors with specific dimensions and no NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef list_of_tensors_with_dims(value: Any, dims: int) -&gt; bool:\n    \"\"\"Check if value is a non-empty list where all items are tensors with specific dimensions and no NaN values.\"\"\"\n    if not isinstance(value, list) or len(value) == 0:\n        return False\n    for item in value:\n        if not isinstance(item, torch.Tensor):\n            return False\n        if item.dim() != dims:\n            return False\n        if torch.isnan(item).any().item():\n            return False\n    return True\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.list_of_tensors_with_min_dims <code>staticmethod</code> \u00b6 <pre><code>list_of_tensors_with_min_dims(\n    value: Any, min_dims: int\n) -&gt; bool\n</code></pre> <p>Check if value is a non-empty list where all items are tensors with at least min_dims dimensions and no NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef list_of_tensors_with_min_dims(value: Any, min_dims: int) -&gt; bool:\n    \"\"\"Check if value is a non-empty list where all items are tensors with at least min_dims dimensions and no NaN values.\"\"\"\n    if not isinstance(value, list) or len(value) == 0:\n        return False\n    for item in value:\n        if not isinstance(item, torch.Tensor):\n            return False\n        if item.dim() &lt; min_dims:\n            return False\n        if torch.isnan(item).any().item():\n            return False\n    return True\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.min_dims <code>staticmethod</code> \u00b6 <pre><code>min_dims(min_dims: int) -&gt; Callable[[Any], bool]\n</code></pre> <p>Return a validator that checks if tensor has at least min_dims dimensions and no NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef min_dims(min_dims: int) -&gt; Callable[[Any], bool]:\n    \"\"\"Return a validator that checks if tensor has at least min_dims dimensions and no NaN values.\"\"\"\n\n    def validator(value: Any) -&gt; bool:\n        return StageValidators.tensor_min_dims(value, min_dims)\n\n    return validator\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.non_negative_float <code>staticmethod</code> \u00b6 <pre><code>non_negative_float(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a non-negative float.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef non_negative_float(value: Any) -&gt; bool:\n    \"\"\"Check if value is a non-negative float.\"\"\"\n    return isinstance(value, int | float) and value &gt;= 0\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.none_or_list <code>staticmethod</code> \u00b6 <pre><code>none_or_list(value: Any) -&gt; bool\n</code></pre> <p>Check if value is None or a list.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef none_or_list(value: Any) -&gt; bool:\n    \"\"\"Check if value is None or a list.\"\"\"\n    return value is None or isinstance(value, list)\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.none_or_positive_int <code>staticmethod</code> \u00b6 <pre><code>none_or_positive_int(value: Any) -&gt; bool\n</code></pre> <p>Check if value is None or a positive integer.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef none_or_positive_int(value: Any) -&gt; bool:\n    \"\"\"Check if value is None or a positive integer.\"\"\"\n    return value is None or (isinstance(value, int) and value &gt; 0)\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.none_or_tensor <code>staticmethod</code> \u00b6 <pre><code>none_or_tensor(value: Any) -&gt; bool\n</code></pre> <p>Check if value is None or a tensor without NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef none_or_tensor(value: Any) -&gt; bool:\n    \"\"\"Check if value is None or a tensor without NaN values.\"\"\"\n    if value is None:\n        return True\n    if not isinstance(value, torch.Tensor):\n        return False\n    return not torch.isnan(value).any().item()\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.none_or_tensor_with_dims <code>staticmethod</code> \u00b6 <pre><code>none_or_tensor_with_dims(\n    dims: int,\n) -&gt; Callable[[Any], bool]\n</code></pre> <p>Return a validator that checks if value is None or a tensor with specific dimensions and no NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef none_or_tensor_with_dims(dims: int) -&gt; Callable[[Any], bool]:\n    \"\"\"Return a validator that checks if value is None or a tensor with specific dimensions and no NaN values.\"\"\"\n\n    def validator(value: Any) -&gt; bool:\n        if value is None:\n            return True\n        if not isinstance(value, torch.Tensor):\n            return False\n        if value.dim() != dims:\n            return False\n        return not torch.isnan(value).any().item()\n\n    return validator\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.not_none <code>staticmethod</code> \u00b6 <pre><code>not_none(value: Any) -&gt; bool\n</code></pre> <p>Check if value is not None.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef not_none(value: Any) -&gt; bool:\n    \"\"\"Check if value is not None.\"\"\"\n    return value is not None\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.positive_float <code>staticmethod</code> \u00b6 <pre><code>positive_float(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a positive float.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef positive_float(value: Any) -&gt; bool:\n    \"\"\"Check if value is a positive float.\"\"\"\n    return isinstance(value, int | float) and value &gt; 0\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.positive_int <code>staticmethod</code> \u00b6 <pre><code>positive_int(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a positive integer.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef positive_int(value: Any) -&gt; bool:\n    \"\"\"Check if value is a positive integer.\"\"\"\n    return isinstance(value, int) and value &gt; 0\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.positive_int_divisible <code>staticmethod</code> \u00b6 <pre><code>positive_int_divisible(\n    divisor: int,\n) -&gt; Callable[[Any], bool]\n</code></pre> <p>Return a validator that checks if value is a positive integer divisible by divisor.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef positive_int_divisible(divisor: int) -&gt; Callable[[Any], bool]:\n    \"\"\"Return a validator that checks if value is a positive integer divisible by divisor.\"\"\"\n\n    def validator(value: Any) -&gt; bool:\n        return (isinstance(value, int) and value &gt; 0\n                and StageValidators.divisible_by(value, divisor))\n\n    return validator\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.string_not_empty <code>staticmethod</code> \u00b6 <pre><code>string_not_empty(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a non-empty string.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef string_not_empty(value: Any) -&gt; bool:\n    \"\"\"Check if value is a non-empty string.\"\"\"\n    return isinstance(value, str) and len(value.strip()) &gt; 0\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.string_or_list_strings <code>staticmethod</code> \u00b6 <pre><code>string_or_list_strings(value: Any) -&gt; bool\n</code></pre> <p>Check if value is a string or list of strings.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef string_or_list_strings(value: Any) -&gt; bool:\n    \"\"\"Check if value is a string or list of strings.\"\"\"\n    if isinstance(value, str):\n        return True\n    if isinstance(value, list):\n        return all(isinstance(item, str) for item in value)\n    return False\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.tensor_min_dims <code>staticmethod</code> \u00b6 <pre><code>tensor_min_dims(value: Any, min_dims: int) -&gt; bool\n</code></pre> <p>Check if value is a tensor with at least min_dims dimensions and no NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef tensor_min_dims(value: Any, min_dims: int) -&gt; bool:\n    \"\"\"Check if value is a tensor with at least min_dims dimensions and no NaN values.\"\"\"\n    if not isinstance(value, torch.Tensor):\n        return False\n    if value.dim() &lt; min_dims:\n        return False\n    return not torch.isnan(value).any().item()\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.tensor_shape_matches <code>staticmethod</code> \u00b6 <pre><code>tensor_shape_matches(\n    value: Any, expected_shape: tuple\n) -&gt; bool\n</code></pre> <p>Check if tensor shape matches expected shape (None for any size) and no NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef tensor_shape_matches(value: Any, expected_shape: tuple) -&gt; bool:\n    \"\"\"Check if tensor shape matches expected shape (None for any size) and no NaN values.\"\"\"\n    if not isinstance(value, torch.Tensor):\n        return False\n    if len(value.shape) != len(expected_shape):\n        return False\n    for actual, expected in zip(value.shape, expected_shape, strict=True):\n        if expected is not None and actual != expected:\n            return False\n    return not torch.isnan(value).any().item()\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.tensor_with_dims <code>staticmethod</code> \u00b6 <pre><code>tensor_with_dims(value: Any, dims: int) -&gt; bool\n</code></pre> <p>Check if value is a tensor with specific dimensions and no NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef tensor_with_dims(value: Any, dims: int) -&gt; bool:\n    \"\"\"Check if value is a tensor with specific dimensions and no NaN values.\"\"\"\n    if not isinstance(value, torch.Tensor):\n        return False\n    if value.dim() != dims:\n        return False\n    return not torch.isnan(value).any().item()\n</code></pre> fastvideo.pipelines.stages.validators.StageValidators.with_dims <code>staticmethod</code> \u00b6 <pre><code>with_dims(dims: int) -&gt; Callable[[Any], bool]\n</code></pre> <p>Return a validator that checks if tensor has specific dimensions and no NaN values.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>@staticmethod\ndef with_dims(dims: int) -&gt; Callable[[Any], bool]:\n    \"\"\"Return a validator that checks if tensor has specific dimensions and no NaN values.\"\"\"\n\n    def validator(value: Any) -&gt; bool:\n        return StageValidators.tensor_with_dims(value, dims)\n\n    return validator\n</code></pre> fastvideo.pipelines.stages.validators.ValidationFailure \u00b6 <pre><code>ValidationFailure(\n    validator_name: str,\n    actual_value: Any,\n    expected: str | None = None,\n    error_msg: str | None = None,\n)\n</code></pre> <p>Details about a specific validation failure.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>def __init__(self,\n             validator_name: str,\n             actual_value: Any,\n             expected: str | None = None,\n             error_msg: str | None = None):\n    self.validator_name = validator_name\n    self.actual_value = actual_value\n    self.expected = expected\n    self.error_msg = error_msg\n</code></pre> fastvideo.pipelines.stages.validators.VerificationResult \u00b6 <pre><code>VerificationResult()\n</code></pre> <p>Wrapper class for stage verification results.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>def __init__(self) -&gt; None:\n    self._checks: dict[str, bool] = {}\n    self._failures: dict[str, list[ValidationFailure]] = {}\n</code></pre> Functions\u00b6 fastvideo.pipelines.stages.validators.VerificationResult.add_check \u00b6 <pre><code>add_check(\n    field_name: str,\n    value: Any,\n    validators: Callable[[Any], bool]\n    | list[Callable[[Any], bool]],\n) -&gt; VerificationResult\n</code></pre> <p>Add a validation check for a field.</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the field being checked</p> required <code>value</code> <code>Any</code> <p>The actual value to validate</p> required <code>validators</code> <code>Callable[[Any], bool] | list[Callable[[Any], bool]]</code> <p>Single validation function or list of validation functions.        Each function will be called with the value as its first argument.</p> required <p>Returns:</p> Type Description <code>VerificationResult</code> <p>Self for method chaining</p> <p>Examples:</p> fastvideo.pipelines.stages.validators.VerificationResult.get_detailed_failures \u00b6 <pre><code>get_detailed_failures() -&gt; dict[\n    str, list[ValidationFailure]\n]\n</code></pre> <p>Get detailed failure information for each failed field.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>def get_detailed_failures(self) -&gt; dict[str, list[ValidationFailure]]:\n    \"\"\"Get detailed failure information for each failed field.\"\"\"\n    return self._failures.copy()\n</code></pre> fastvideo.pipelines.stages.validators.VerificationResult.get_failed_fields \u00b6 <pre><code>get_failed_fields() -&gt; list[str]\n</code></pre> <p>Get list of fields that failed validation.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>def get_failed_fields(self) -&gt; list[str]:\n    \"\"\"Get list of fields that failed validation.\"\"\"\n    return [field for field, passed in self._checks.items() if not passed]\n</code></pre> fastvideo.pipelines.stages.validators.VerificationResult.get_failure_summary \u00b6 <pre><code>get_failure_summary() -&gt; str\n</code></pre> <p>Get a comprehensive summary of all validation failures.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>def get_failure_summary(self) -&gt; str:\n    \"\"\"Get a comprehensive summary of all validation failures.\"\"\"\n    if self.is_valid():\n        return \"All validations passed\"\n\n    summary_parts = []\n    for field_name, failures in self._failures.items():\n        field_summary = f\"\\n  Field '{field_name}':\"\n        for i, failure in enumerate(failures, 1):\n            field_summary += f\"\\n    {i}. {failure}\"\n        summary_parts.append(field_summary)\n\n    return \"Validation failures:\" + \"\".join(summary_parts)\n</code></pre> fastvideo.pipelines.stages.validators.VerificationResult.is_valid \u00b6 <pre><code>is_valid() -&gt; bool\n</code></pre> <p>Check if all validations passed.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>def is_valid(self) -&gt; bool:\n    \"\"\"Check if all validations passed.\"\"\"\n    return all(self._checks.values())\n</code></pre> fastvideo.pipelines.stages.validators.VerificationResult.to_dict \u00b6 <pre><code>to_dict() -&gt; dict\n</code></pre> <p>Convert to dictionary for backward compatibility.</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Convert to dictionary for backward compatibility.\"\"\"\n    return self._checks.copy()\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.validators.VerificationResult.add_check--single-validator","title":"Single validator","text":"<p>result.add_check(\"tensor\", my_tensor, V.is_tensor)</p>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.validators.VerificationResult.add_check--multiple-validators-all-must-pass","title":"Multiple validators (all must pass)","text":"<p>result.add_check(\"latents\", batch.latents, [V.is_tensor, V.with_dims(5)])</p>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.stages.validators.VerificationResult.add_check--using-partial-functions-for-parameters","title":"Using partial functions for parameters","text":"<p>result.add_check(\"height\", batch.height, [V.not_none, V.divisible(8)])</p> Source code in <code>fastvideo/pipelines/stages/validators.py</code> <pre><code>def add_check(\n    self, field_name: str, value: Any,\n    validators: Callable[[Any], bool] | list[Callable[[Any], bool]]\n) -&gt; 'VerificationResult':\n    \"\"\"\n    Add a validation check for a field.\n\n    Args:\n        field_name: Name of the field being checked\n        value: The actual value to validate\n        validators: Single validation function or list of validation functions.\n                   Each function will be called with the value as its first argument.\n\n    Returns:\n        Self for method chaining\n\n    Examples:\n        # Single validator\n        result.add_check(\"tensor\", my_tensor, V.is_tensor)\n\n        # Multiple validators (all must pass)\n        result.add_check(\"latents\", batch.latents, [V.is_tensor, V.with_dims(5)])\n\n        # Using partial functions for parameters\n        result.add_check(\"height\", batch.height, [V.not_none, V.divisible(8)])\n    \"\"\"\n    if not isinstance(validators, list):\n        validators = [validators]\n\n    failures = []\n    all_passed = True\n\n    # Apply all validators and collect detailed failure info\n    for validator in validators:\n        try:\n            passed = validator(value)\n            if not passed:\n                all_passed = False\n                failure = self._create_validation_failure(validator, value)\n                failures.append(failure)\n        except Exception as e:\n            # If any validator raises an exception, consider the check failed\n            all_passed = False\n            validator_name = getattr(validator, '__name__', str(validator))\n            failure = ValidationFailure(\n                validator_name=validator_name,\n                actual_value=value,\n                error_msg=f\"Exception during validation: {str(e)}\")\n            failures.append(failure)\n\n    self._checks[field_name] = all_passed\n    if not all_passed:\n        self._failures[field_name] = failures\n\n    return self\n</code></pre>"},{"location":"api/fastvideo/pipelines/#fastvideo.pipelines.training","title":"fastvideo.pipelines.training","text":"<p>Training pipelines for fastvideo.v1.</p> <p>This package contains pipelines for training diffusion models.</p>"},{"location":"api/fastvideo/platforms/","title":"platforms","text":""},{"location":"api/fastvideo/platforms/#fastvideo.platforms","title":"platforms","text":""},{"location":"api/fastvideo/platforms/#fastvideo.platforms-classes","title":"Classes","text":""},{"location":"api/fastvideo/platforms/#fastvideo.platforms.Platform","title":"fastvideo.platforms.Platform","text":""},{"location":"api/fastvideo/platforms/#fastvideo.platforms.Platform-functions","title":"Functions","text":""},{"location":"api/fastvideo/platforms/#fastvideo.platforms.Platform.get_attn_backend_cls","title":"fastvideo.platforms.Platform.get_attn_backend_cls  <code>classmethod</code>","text":"<pre><code>get_attn_backend_cls(\n    selected_backend: AttentionBackendEnum | None,\n    head_size: int,\n    dtype: dtype,\n) -&gt; str\n</code></pre> <p>Get the attention backend class of a device.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_attn_backend_cls(cls, selected_backend: AttentionBackendEnum | None,\n                         head_size: int, dtype: torch.dtype) -&gt; str:\n    \"\"\"Get the attention backend class of a device.\"\"\"\n    return \"\"\n</code></pre>"},{"location":"api/fastvideo/platforms/#fastvideo.platforms.Platform.get_cpu_architecture","title":"fastvideo.platforms.Platform.get_cpu_architecture  <code>classmethod</code>","text":"<pre><code>get_cpu_architecture() -&gt; CpuArchEnum\n</code></pre> <p>Get the CPU architecture of the current platform.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_cpu_architecture(cls) -&gt; CpuArchEnum:\n    \"\"\"Get the CPU architecture of the current platform.\"\"\"\n    return CpuArchEnum.UNSPECIFIED\n</code></pre>"},{"location":"api/fastvideo/platforms/#fastvideo.platforms.Platform.get_current_memory_usage","title":"fastvideo.platforms.Platform.get_current_memory_usage  <code>classmethod</code>","text":"<pre><code>get_current_memory_usage(\n    device: Device | None = None,\n) -&gt; float\n</code></pre> <p>Return the memory usage in bytes.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_current_memory_usage(cls,\n                             device: torch.types.Device | None = None\n                             ) -&gt; float:\n    \"\"\"\n    Return the memory usage in bytes.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fastvideo/platforms/#fastvideo.platforms.Platform.get_device_capability","title":"fastvideo.platforms.Platform.get_device_capability  <code>classmethod</code>","text":"<pre><code>get_device_capability(\n    device_id: int = 0,\n) -&gt; DeviceCapability | None\n</code></pre> <p>Stateless version of :func:<code>torch.cuda.get_device_capability</code>.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_device_capability(\n    cls,\n    device_id: int = 0,\n) -&gt; DeviceCapability | None:\n    \"\"\"Stateless version of :func:`torch.cuda.get_device_capability`.\"\"\"\n    return None\n</code></pre>"},{"location":"api/fastvideo/platforms/#fastvideo.platforms.Platform.get_device_communicator_cls","title":"fastvideo.platforms.Platform.get_device_communicator_cls  <code>classmethod</code>","text":"<pre><code>get_device_communicator_cls() -&gt; str\n</code></pre> <p>Get device specific communicator class for distributed communication.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_device_communicator_cls(cls) -&gt; str:\n    \"\"\"\n    Get device specific communicator class for distributed communication.\n    \"\"\"\n    return \"fastvideo.distributed.device_communicators.base_device_communicator.DeviceCommunicatorBase\"  # noqa\n</code></pre>"},{"location":"api/fastvideo/platforms/#fastvideo.platforms.Platform.get_device_name","title":"fastvideo.platforms.Platform.get_device_name  <code>classmethod</code>","text":"<pre><code>get_device_name(device_id: int = 0) -&gt; str\n</code></pre> <p>Get the name of a device.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_device_name(cls, device_id: int = 0) -&gt; str:\n    \"\"\"Get the name of a device.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fastvideo/platforms/#fastvideo.platforms.Platform.get_device_total_memory","title":"fastvideo.platforms.Platform.get_device_total_memory  <code>classmethod</code>","text":"<pre><code>get_device_total_memory(device_id: int = 0) -&gt; int\n</code></pre> <p>Get the total memory of a device in bytes.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_device_total_memory(cls, device_id: int = 0) -&gt; int:\n    \"\"\"Get the total memory of a device in bytes.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fastvideo/platforms/#fastvideo.platforms.Platform.get_device_uuid","title":"fastvideo.platforms.Platform.get_device_uuid  <code>classmethod</code>","text":"<pre><code>get_device_uuid(device_id: int = 0) -&gt; str\n</code></pre> <p>Get the uuid of a device, e.g. the PCI bus ID.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_device_uuid(cls, device_id: int = 0) -&gt; str:\n    \"\"\"Get the uuid of a device, e.g. the PCI bus ID.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fastvideo/platforms/#fastvideo.platforms.Platform.get_torch_device","title":"fastvideo.platforms.Platform.get_torch_device  <code>classmethod</code>","text":"<pre><code>get_torch_device()\n</code></pre> <p>Check if the current platform supports torch device.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_torch_device(cls):\n    \"\"\"\n    Check if the current platform supports torch device.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fastvideo/platforms/#fastvideo.platforms.Platform.has_device_capability","title":"fastvideo.platforms.Platform.has_device_capability  <code>classmethod</code>","text":"<pre><code>has_device_capability(\n    capability: tuple[int, int] | int, device_id: int = 0\n) -&gt; bool\n</code></pre> <p>Test whether this platform is compatible with a device capability.</p> <p>The <code>capability</code> argument can either be:</p> <ul> <li>A tuple <code>(major, minor)</code>.</li> <li>An integer <code>&lt;major&gt;&lt;minor&gt;</code>. (See :meth:<code>DeviceCapability.to_int</code>)</li> </ul> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef has_device_capability(\n    cls,\n    capability: tuple[int, int] | int,\n    device_id: int = 0,\n) -&gt; bool:\n    \"\"\"\n    Test whether this platform is compatible with a device capability.\n\n    The ``capability`` argument can either be:\n\n    - A tuple ``(major, minor)``.\n    - An integer ``&lt;major&gt;&lt;minor&gt;``. (See :meth:`DeviceCapability.to_int`)\n    \"\"\"\n    current_capability = cls.get_device_capability(device_id=device_id)\n    if current_capability is None:\n        return False\n\n    if isinstance(capability, tuple):\n        return current_capability &gt;= capability\n\n    return current_capability.to_int() &gt;= capability\n</code></pre>"},{"location":"api/fastvideo/platforms/#fastvideo.platforms.Platform.inference_mode","title":"fastvideo.platforms.Platform.inference_mode  <code>classmethod</code>","text":"<pre><code>inference_mode()\n</code></pre> <p>A device-specific wrapper of <code>torch.inference_mode</code>.</p> <p>This wrapper is recommended because some hardware backends such as TPU do not support <code>torch.inference_mode</code>. In such a case, they will fall back to <code>torch.no_grad</code> by overriding this method.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef inference_mode(cls):\n    \"\"\"A device-specific wrapper of `torch.inference_mode`.\n\n    This wrapper is recommended because some hardware backends such as TPU\n    do not support `torch.inference_mode`. In such a case, they will fall\n    back to `torch.no_grad` by overriding this method.\n    \"\"\"\n    return torch.inference_mode(mode=True)\n</code></pre>"},{"location":"api/fastvideo/platforms/#fastvideo.platforms.Platform.is_async_output_supported","title":"fastvideo.platforms.Platform.is_async_output_supported  <code>classmethod</code>","text":"<pre><code>is_async_output_supported(\n    enforce_eager: bool | None,\n) -&gt; bool\n</code></pre> <p>Check if the current platform supports async output.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef is_async_output_supported(cls, enforce_eager: bool | None) -&gt; bool:\n    \"\"\"\n    Check if the current platform supports async output.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fastvideo/platforms/#fastvideo.platforms.Platform.is_cuda_alike","title":"fastvideo.platforms.Platform.is_cuda_alike","text":"<pre><code>is_cuda_alike() -&gt; bool\n</code></pre> <p>Stateless version of :func:<code>torch.cuda.is_available</code>.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>def is_cuda_alike(self) -&gt; bool:\n    \"\"\"Stateless version of :func:`torch.cuda.is_available`.\"\"\"\n    return self._enum in (PlatformEnum.CUDA, PlatformEnum.ROCM)\n</code></pre>"},{"location":"api/fastvideo/platforms/#fastvideo.platforms.Platform.seed_everything","title":"fastvideo.platforms.Platform.seed_everything  <code>classmethod</code>","text":"<pre><code>seed_everything(seed: int | None = None) -&gt; None\n</code></pre> <p>Set the seed of each random module. <code>torch.manual_seed</code> will set seed on all devices.</p> <p>Loosely based on: https://github.com/Lightning-AI/pytorch-lightning/blob/2.4.0/src/lightning/fabric/utilities/seed.py#L20</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef seed_everything(cls, seed: int | None = None) -&gt; None:\n    \"\"\"\n    Set the seed of each random module.\n    `torch.manual_seed` will set seed on all devices.\n\n    Loosely based on: https://github.com/Lightning-AI/pytorch-lightning/blob/2.4.0/src/lightning/fabric/utilities/seed.py#L20\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n</code></pre>"},{"location":"api/fastvideo/platforms/#fastvideo.platforms.Platform.verify_model_arch","title":"fastvideo.platforms.Platform.verify_model_arch  <code>classmethod</code>","text":"<pre><code>verify_model_arch(model_arch: str) -&gt; None\n</code></pre> <p>Verify whether the current platform supports the specified model architecture.</p> <ul> <li>This will raise an Error or Warning based on the model support on the current platform.</li> <li>By default all models are considered supported.</li> </ul> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef verify_model_arch(cls, model_arch: str) -&gt; None:\n    \"\"\"\n    Verify whether the current platform supports the specified model\n    architecture.\n\n    - This will raise an Error or Warning based on the model support on\n    the current platform.\n    - By default all models are considered supported.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fastvideo/platforms/#fastvideo.platforms.Platform.verify_quantization","title":"fastvideo.platforms.Platform.verify_quantization  <code>classmethod</code>","text":"<pre><code>verify_quantization(quant: str) -&gt; None\n</code></pre> <p>Verify whether the quantization is supported by the current platform.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef verify_quantization(cls, quant: str) -&gt; None:\n    \"\"\"\n    Verify whether the quantization is supported by the current platform.\n    \"\"\"\n    if cls.supported_quantization and \\\n        quant not in cls.supported_quantization:\n        raise ValueError(\n            f\"{quant} quantization is currently not supported in \"\n            f\"{cls.device_name}.\")\n</code></pre>"},{"location":"api/fastvideo/platforms/#fastvideo.platforms-functions","title":"Functions","text":""},{"location":"api/fastvideo/platforms/#fastvideo.platforms.cpu_platform_plugin","title":"fastvideo.platforms.cpu_platform_plugin","text":"<pre><code>cpu_platform_plugin() -&gt; str | None\n</code></pre> <p>Detect if CPU platform should be used.</p> Source code in <code>fastvideo/platforms/__init__.py</code> <pre><code>def cpu_platform_plugin() -&gt; str | None:\n    \"\"\"Detect if CPU platform should be used.\"\"\"\n    # CPU is always available as a fallback\n    return \"fastvideo.platforms.cpu.CpuPlatform\"\n</code></pre>"},{"location":"api/fastvideo/platforms/#fastvideo.platforms.mps_platform_plugin","title":"fastvideo.platforms.mps_platform_plugin","text":"<pre><code>mps_platform_plugin() -&gt; str | None\n</code></pre> <p>Detect if MPS (Metal Performance Shaders) is available on macOS.</p> Source code in <code>fastvideo/platforms/__init__.py</code> <pre><code>def mps_platform_plugin() -&gt; str | None:\n    \"\"\"Detect if MPS (Metal Performance Shaders) is available on macOS.\"\"\"\n    is_mps = False\n\n    try:\n        import torch\n        if torch.backends.mps.is_available():\n            is_mps = True\n            logger.info(\"MPS (Metal Performance Shaders) is available\")\n    except Exception as e:\n        logger.info(\"MPS detection failed: %s\", e)\n\n    return \"fastvideo.platforms.mps.MpsPlatform\" if is_mps else None\n</code></pre>"},{"location":"api/fastvideo/platforms/#fastvideo.platforms-modules","title":"Modules","text":""},{"location":"api/fastvideo/platforms/#fastvideo.platforms.cpu","title":"fastvideo.platforms.cpu","text":""},{"location":"api/fastvideo/platforms/#fastvideo.platforms.cpu-classes","title":"Classes","text":""},{"location":"api/fastvideo/platforms/#fastvideo.platforms.cpu.CpuPlatform","title":"fastvideo.platforms.cpu.CpuPlatform","text":"<p>               Bases: <code>Platform</code></p> Functions\u00b6 fastvideo.platforms.cpu.CpuPlatform.get_cpu_architecture <code>classmethod</code> \u00b6 <pre><code>get_cpu_architecture() -&gt; CpuArchEnum\n</code></pre> <p>Get the CPU architecture.</p> Source code in <code>fastvideo/platforms/cpu.py</code> <pre><code>@classmethod\ndef get_cpu_architecture(cls) -&gt; CpuArchEnum:\n    \"\"\"Get the CPU architecture.\"\"\"\n    machine = platform.machine().lower()\n    if machine in (\"x86_64\", \"amd64\", \"i386\", \"i686\"):\n        return CpuArchEnum.X86\n    elif machine in (\"arm64\", \"aarch64\"):\n        return CpuArchEnum.ARM\n    else:\n        return CpuArchEnum.UNSPECIFIED\n</code></pre>"},{"location":"api/fastvideo/platforms/#fastvideo.platforms.cuda","title":"fastvideo.platforms.cuda","text":"<p>Code inside this file can safely assume cuda platform, e.g. importing pynvml. However, it should not initialize cuda context.</p>"},{"location":"api/fastvideo/platforms/#fastvideo.platforms.cuda-classes","title":"Classes","text":""},{"location":"api/fastvideo/platforms/#fastvideo.platforms.cuda.CudaPlatformBase","title":"fastvideo.platforms.cuda.CudaPlatformBase","text":"<p>               Bases: <code>Platform</code></p> Functions\u00b6 fastvideo.platforms.cuda.CudaPlatformBase.get_torch_device <code>classmethod</code> \u00b6 <pre><code>get_torch_device()\n</code></pre> <p>Return torch.cuda</p> Source code in <code>fastvideo/platforms/cuda.py</code> <pre><code>@classmethod\ndef get_torch_device(cls):\n    \"\"\"\n    Return torch.cuda\n    \"\"\"\n    return torch.cuda\n</code></pre>"},{"location":"api/fastvideo/platforms/#fastvideo.platforms.cuda.NvmlCudaPlatform","title":"fastvideo.platforms.cuda.NvmlCudaPlatform","text":"<p>               Bases: <code>CudaPlatformBase</code></p> Functions\u00b6 fastvideo.platforms.cuda.NvmlCudaPlatform.is_full_nvlink <code>classmethod</code> \u00b6 <pre><code>is_full_nvlink(physical_device_ids: list[int]) -&gt; bool\n</code></pre> <p>query if the set of gpus are fully connected by nvlink (1 hop)</p> Source code in <code>fastvideo/platforms/cuda.py</code> <pre><code>@classmethod\n@with_nvml_context\ndef is_full_nvlink(cls, physical_device_ids: list[int]) -&gt; bool:\n    \"\"\"\n    query if the set of gpus are fully connected by nvlink (1 hop)\n    \"\"\"\n    handles = [\n        pynvml.nvmlDeviceGetHandleByIndex(i) for i in physical_device_ids\n    ]\n    for i, handle in enumerate(handles):\n        for j, peer_handle in enumerate(handles):\n            if i &lt; j:\n                try:\n                    p2p_status = pynvml.nvmlDeviceGetP2PStatus(\n                        handle,\n                        peer_handle,\n                        pynvml.NVML_P2P_CAPS_INDEX_NVLINK,\n                    )\n                    if p2p_status != pynvml.NVML_P2P_STATUS_OK:\n                        return False\n                except pynvml.NVMLError:\n                    logger.exception(\n                        \"NVLink detection failed. This is normal if\"\n                        \" your machine has no NVLink equipped.\")\n                    return False\n    return True\n</code></pre>"},{"location":"api/fastvideo/platforms/#fastvideo.platforms.cuda-functions","title":"Functions","text":""},{"location":"api/fastvideo/platforms/#fastvideo.platforms.interface","title":"fastvideo.platforms.interface","text":""},{"location":"api/fastvideo/platforms/#fastvideo.platforms.interface-classes","title":"Classes","text":""},{"location":"api/fastvideo/platforms/#fastvideo.platforms.interface.DeviceCapability","title":"fastvideo.platforms.interface.DeviceCapability","text":"<p>               Bases: <code>NamedTuple</code></p> Functions\u00b6 fastvideo.platforms.interface.DeviceCapability.to_int \u00b6 <pre><code>to_int() -&gt; int\n</code></pre> <p>Express device capability as an integer <code>&lt;major&gt;&lt;minor&gt;</code>.</p> <p>It is assumed that the minor version is always a single digit.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>def to_int(self) -&gt; int:\n    \"\"\"\n    Express device capability as an integer ``&lt;major&gt;&lt;minor&gt;``.\n\n    It is assumed that the minor version is always a single digit.\n    \"\"\"\n    assert 0 &lt;= self.minor &lt; 10\n    return self.major * 10 + self.minor\n</code></pre>"},{"location":"api/fastvideo/platforms/#fastvideo.platforms.interface.Platform","title":"fastvideo.platforms.interface.Platform","text":"Functions\u00b6 fastvideo.platforms.interface.Platform.get_attn_backend_cls <code>classmethod</code> \u00b6 <pre><code>get_attn_backend_cls(\n    selected_backend: AttentionBackendEnum | None,\n    head_size: int,\n    dtype: dtype,\n) -&gt; str\n</code></pre> <p>Get the attention backend class of a device.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_attn_backend_cls(cls, selected_backend: AttentionBackendEnum | None,\n                         head_size: int, dtype: torch.dtype) -&gt; str:\n    \"\"\"Get the attention backend class of a device.\"\"\"\n    return \"\"\n</code></pre> fastvideo.platforms.interface.Platform.get_cpu_architecture <code>classmethod</code> \u00b6 <pre><code>get_cpu_architecture() -&gt; CpuArchEnum\n</code></pre> <p>Get the CPU architecture of the current platform.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_cpu_architecture(cls) -&gt; CpuArchEnum:\n    \"\"\"Get the CPU architecture of the current platform.\"\"\"\n    return CpuArchEnum.UNSPECIFIED\n</code></pre> fastvideo.platforms.interface.Platform.get_current_memory_usage <code>classmethod</code> \u00b6 <pre><code>get_current_memory_usage(\n    device: Device | None = None,\n) -&gt; float\n</code></pre> <p>Return the memory usage in bytes.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_current_memory_usage(cls,\n                             device: torch.types.Device | None = None\n                             ) -&gt; float:\n    \"\"\"\n    Return the memory usage in bytes.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.platforms.interface.Platform.get_device_capability <code>classmethod</code> \u00b6 <pre><code>get_device_capability(\n    device_id: int = 0,\n) -&gt; DeviceCapability | None\n</code></pre> <p>Stateless version of :func:<code>torch.cuda.get_device_capability</code>.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_device_capability(\n    cls,\n    device_id: int = 0,\n) -&gt; DeviceCapability | None:\n    \"\"\"Stateless version of :func:`torch.cuda.get_device_capability`.\"\"\"\n    return None\n</code></pre> fastvideo.platforms.interface.Platform.get_device_communicator_cls <code>classmethod</code> \u00b6 <pre><code>get_device_communicator_cls() -&gt; str\n</code></pre> <p>Get device specific communicator class for distributed communication.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_device_communicator_cls(cls) -&gt; str:\n    \"\"\"\n    Get device specific communicator class for distributed communication.\n    \"\"\"\n    return \"fastvideo.distributed.device_communicators.base_device_communicator.DeviceCommunicatorBase\"  # noqa\n</code></pre> fastvideo.platforms.interface.Platform.get_device_name <code>classmethod</code> \u00b6 <pre><code>get_device_name(device_id: int = 0) -&gt; str\n</code></pre> <p>Get the name of a device.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_device_name(cls, device_id: int = 0) -&gt; str:\n    \"\"\"Get the name of a device.\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.platforms.interface.Platform.get_device_total_memory <code>classmethod</code> \u00b6 <pre><code>get_device_total_memory(device_id: int = 0) -&gt; int\n</code></pre> <p>Get the total memory of a device in bytes.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_device_total_memory(cls, device_id: int = 0) -&gt; int:\n    \"\"\"Get the total memory of a device in bytes.\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.platforms.interface.Platform.get_device_uuid <code>classmethod</code> \u00b6 <pre><code>get_device_uuid(device_id: int = 0) -&gt; str\n</code></pre> <p>Get the uuid of a device, e.g. the PCI bus ID.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_device_uuid(cls, device_id: int = 0) -&gt; str:\n    \"\"\"Get the uuid of a device, e.g. the PCI bus ID.\"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.platforms.interface.Platform.get_torch_device <code>classmethod</code> \u00b6 <pre><code>get_torch_device()\n</code></pre> <p>Check if the current platform supports torch device.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef get_torch_device(cls):\n    \"\"\"\n    Check if the current platform supports torch device.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.platforms.interface.Platform.has_device_capability <code>classmethod</code> \u00b6 <pre><code>has_device_capability(\n    capability: tuple[int, int] | int, device_id: int = 0\n) -&gt; bool\n</code></pre> <p>Test whether this platform is compatible with a device capability.</p> <p>The <code>capability</code> argument can either be:</p> <ul> <li>A tuple <code>(major, minor)</code>.</li> <li>An integer <code>&lt;major&gt;&lt;minor&gt;</code>. (See :meth:<code>DeviceCapability.to_int</code>)</li> </ul> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef has_device_capability(\n    cls,\n    capability: tuple[int, int] | int,\n    device_id: int = 0,\n) -&gt; bool:\n    \"\"\"\n    Test whether this platform is compatible with a device capability.\n\n    The ``capability`` argument can either be:\n\n    - A tuple ``(major, minor)``.\n    - An integer ``&lt;major&gt;&lt;minor&gt;``. (See :meth:`DeviceCapability.to_int`)\n    \"\"\"\n    current_capability = cls.get_device_capability(device_id=device_id)\n    if current_capability is None:\n        return False\n\n    if isinstance(capability, tuple):\n        return current_capability &gt;= capability\n\n    return current_capability.to_int() &gt;= capability\n</code></pre> fastvideo.platforms.interface.Platform.inference_mode <code>classmethod</code> \u00b6 <pre><code>inference_mode()\n</code></pre> <p>A device-specific wrapper of <code>torch.inference_mode</code>.</p> <p>This wrapper is recommended because some hardware backends such as TPU do not support <code>torch.inference_mode</code>. In such a case, they will fall back to <code>torch.no_grad</code> by overriding this method.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef inference_mode(cls):\n    \"\"\"A device-specific wrapper of `torch.inference_mode`.\n\n    This wrapper is recommended because some hardware backends such as TPU\n    do not support `torch.inference_mode`. In such a case, they will fall\n    back to `torch.no_grad` by overriding this method.\n    \"\"\"\n    return torch.inference_mode(mode=True)\n</code></pre> fastvideo.platforms.interface.Platform.is_async_output_supported <code>classmethod</code> \u00b6 <pre><code>is_async_output_supported(\n    enforce_eager: bool | None,\n) -&gt; bool\n</code></pre> <p>Check if the current platform supports async output.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef is_async_output_supported(cls, enforce_eager: bool | None) -&gt; bool:\n    \"\"\"\n    Check if the current platform supports async output.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.platforms.interface.Platform.is_cuda_alike \u00b6 <pre><code>is_cuda_alike() -&gt; bool\n</code></pre> <p>Stateless version of :func:<code>torch.cuda.is_available</code>.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>def is_cuda_alike(self) -&gt; bool:\n    \"\"\"Stateless version of :func:`torch.cuda.is_available`.\"\"\"\n    return self._enum in (PlatformEnum.CUDA, PlatformEnum.ROCM)\n</code></pre> fastvideo.platforms.interface.Platform.seed_everything <code>classmethod</code> \u00b6 <pre><code>seed_everything(seed: int | None = None) -&gt; None\n</code></pre> <p>Set the seed of each random module. <code>torch.manual_seed</code> will set seed on all devices.</p> <p>Loosely based on: https://github.com/Lightning-AI/pytorch-lightning/blob/2.4.0/src/lightning/fabric/utilities/seed.py#L20</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef seed_everything(cls, seed: int | None = None) -&gt; None:\n    \"\"\"\n    Set the seed of each random module.\n    `torch.manual_seed` will set seed on all devices.\n\n    Loosely based on: https://github.com/Lightning-AI/pytorch-lightning/blob/2.4.0/src/lightning/fabric/utilities/seed.py#L20\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n</code></pre> fastvideo.platforms.interface.Platform.verify_model_arch <code>classmethod</code> \u00b6 <pre><code>verify_model_arch(model_arch: str) -&gt; None\n</code></pre> <p>Verify whether the current platform supports the specified model architecture.</p> <ul> <li>This will raise an Error or Warning based on the model support on the current platform.</li> <li>By default all models are considered supported.</li> </ul> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef verify_model_arch(cls, model_arch: str) -&gt; None:\n    \"\"\"\n    Verify whether the current platform supports the specified model\n    architecture.\n\n    - This will raise an Error or Warning based on the model support on\n    the current platform.\n    - By default all models are considered supported.\n    \"\"\"\n    pass\n</code></pre> fastvideo.platforms.interface.Platform.verify_quantization <code>classmethod</code> \u00b6 <pre><code>verify_quantization(quant: str) -&gt; None\n</code></pre> <p>Verify whether the quantization is supported by the current platform.</p> Source code in <code>fastvideo/platforms/interface.py</code> <pre><code>@classmethod\ndef verify_quantization(cls, quant: str) -&gt; None:\n    \"\"\"\n    Verify whether the quantization is supported by the current platform.\n    \"\"\"\n    if cls.supported_quantization and \\\n        quant not in cls.supported_quantization:\n        raise ValueError(\n            f\"{quant} quantization is currently not supported in \"\n            f\"{cls.device_name}.\")\n</code></pre>"},{"location":"api/fastvideo/platforms/#fastvideo.platforms.interface-functions","title":"Functions","text":""},{"location":"api/fastvideo/platforms/#fastvideo.platforms.mps","title":"fastvideo.platforms.mps","text":""},{"location":"api/fastvideo/platforms/#fastvideo.platforms.mps-classes","title":"Classes","text":""},{"location":"api/fastvideo/platforms/#fastvideo.platforms.mps.MpsPlatform","title":"fastvideo.platforms.mps.MpsPlatform","text":"<p>               Bases: <code>Platform</code></p> Functions\u00b6 fastvideo.platforms.mps.MpsPlatform.seed_everything <code>classmethod</code> \u00b6 <pre><code>seed_everything(seed: int | None = None) -&gt; None\n</code></pre> <p>Set the seed for MPS device.</p> Source code in <code>fastvideo/platforms/mps.py</code> <pre><code>@classmethod\ndef seed_everything(cls, seed: int | None = None) -&gt; None:\n    \"\"\"Set the seed for MPS device.\"\"\"\n    if seed is not None:\n        import random\n\n        import numpy as np\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n</code></pre>"},{"location":"api/fastvideo/platforms/#fastvideo.platforms.mps-functions","title":"Functions","text":""},{"location":"api/fastvideo/platforms/#fastvideo.platforms.npu","title":"fastvideo.platforms.npu","text":""},{"location":"api/fastvideo/platforms/#fastvideo.platforms.npu-classes","title":"Classes","text":""},{"location":"api/fastvideo/platforms/#fastvideo.platforms.npu.NPUPlatform","title":"fastvideo.platforms.npu.NPUPlatform","text":"<p>               Bases: <code>Platform</code></p> Functions\u00b6 fastvideo.platforms.npu.NPUPlatform.get_torch_device <code>classmethod</code> \u00b6 <pre><code>get_torch_device()\n</code></pre> <p>Return torch.npu</p> Source code in <code>fastvideo/platforms/npu.py</code> <pre><code>@classmethod\ndef get_torch_device(cls):\n    \"\"\"\n    Return torch.npu\n    \"\"\"\n    return torch.npu\n</code></pre>"},{"location":"api/fastvideo/platforms/#fastvideo.platforms.npu-functions","title":"Functions","text":""},{"location":"api/fastvideo/platforms/#fastvideo.platforms.rocm","title":"fastvideo.platforms.rocm","text":"<p>This file is a platform abstraction for ROCm GPUs, adjusted to match the structure and interface of <code>cuda.py</code>.</p>"},{"location":"api/fastvideo/platforms/#fastvideo.platforms.rocm-classes","title":"Classes","text":""},{"location":"api/fastvideo/platforms/#fastvideo.platforms.rocm-functions","title":"Functions","text":""},{"location":"api/fastvideo/profiler/","title":"profiler","text":""},{"location":"api/fastvideo/profiler/#fastvideo.profiler","title":"profiler","text":"<p>Utilities for managing the PyTorch profiler within FastVideo.</p> <p>The profiler is shared across the process; this module adds a light-weight controller that gates collection based on named regions. Regions may be enabled through dedicated environment variables (e.g. <code>FASTVIDEO_TORCH_PROFILE_MODEL_LOADING=1</code>) or via the consolidated <code>FASTVIDEO_TORCH_PROFILE_REGIONS</code> comma-separated list (e.g. <code>FASTVIDEO_TORCH_PROFILE_REGIONS=model_loading,training_dit</code>).</p> <p>Typical usage from client code::</p> <pre><code>controller = TorchProfilerController(profiler, activities)\nwith controller.region(\"training_dit\"):\n    run_training_step()\n</code></pre> <p>To introduce a new region, register it via :func:<code>register_profiler_region</code> and wrap the corresponding code in :meth:<code>TorchProfilerController.region</code>.</p>"},{"location":"api/fastvideo/profiler/#fastvideo.profiler-classes","title":"Classes","text":""},{"location":"api/fastvideo/profiler/#fastvideo.profiler.ProfilerRegion","title":"fastvideo.profiler.ProfilerRegion  <code>dataclass</code>","text":"<pre><code>ProfilerRegion(\n    name: str,\n    description: str,\n    default_enabled: bool = False,\n)\n</code></pre> <p>Metadata describing a profiler region.</p>"},{"location":"api/fastvideo/profiler/#fastvideo.profiler.TorchProfilerConfig","title":"fastvideo.profiler.TorchProfilerConfig  <code>dataclass</code>","text":"<pre><code>TorchProfilerConfig(regions: dict[str, bool])\n</code></pre> <p>Configuration for torch profiler region control.</p> <p>Use :meth:<code>from_env</code> to construct an instance with defaults inherited from registered regions and optional overrides from the <code>FASTVIDEO_TORCH_PROFILE_REGIONS</code> environment variable. The resulting <code>regions</code> map is consumed by :class:<code>TorchProfilerController</code> to decide when collection should be enabled.</p>"},{"location":"api/fastvideo/profiler/#fastvideo.profiler.TorchProfilerConfig-functions","title":"Functions","text":""},{"location":"api/fastvideo/profiler/#fastvideo.profiler.TorchProfilerConfig.from_env","title":"fastvideo.profiler.TorchProfilerConfig.from_env  <code>classmethod</code>","text":"<pre><code>from_env() -&gt; TorchProfilerConfig\n</code></pre> <p>Build a configuration from process environment variables.</p> Source code in <code>fastvideo/profiler.py</code> <pre><code>@classmethod\ndef from_env(cls) -&gt; TorchProfilerConfig:\n    \"\"\"Build a configuration from process environment variables.\"\"\"\n\n    requested_regions = {\n        token.strip()\n        for token in (getattr(envs, \"FASTVIDEO_TORCH_PROFILE_REGIONS\", \"\")\n                      or \"\").split(\",\") if token.strip()\n    }\n\n    if not requested_regions:\n        available = \", \".join(region.name\n                              for region in list_profiler_regions())\n        raise ValueError(\n            \"FASTVIDEO_TORCH_PROFILE_REGIONS must list at least one region; \"\n            f\"available regions: {available}\")\n\n    regions: dict[str, bool] = {}\n    available_regions = list_profiler_regions()\n    available_names = \", \".join(region.name for region in available_regions)\n\n    for token in requested_regions:\n        resolved = resolve_profiler_region(token)\n        if resolved is None:\n            logger.warning(\n                \"Unknown profiler region '%s'; available regions: %s\",\n                token, available_names)\n            continue\n        regions[resolved.name] = True\n\n    if not regions:\n        raise ValueError(\n            \"FASTVIDEO_TORCH_PROFILE_REGIONS did not match any known regions; \"\n            f\"requested={sorted(requested_regions)}, available={available_names}\"\n        )\n\n    return cls(regions=regions)\n</code></pre>"},{"location":"api/fastvideo/profiler/#fastvideo.profiler.TorchProfilerController","title":"fastvideo.profiler.TorchProfilerController","text":"<pre><code>TorchProfilerController(\n    profiler: Any,\n    activities: Iterable[ProfilerActivity],\n    config: TorchProfilerConfig | None = None,\n    disabled: bool = False,\n)\n</code></pre> <p>Helper that toggles torch profiler collection for named regions.</p>"},{"location":"api/fastvideo/profiler/#fastvideo.profiler.TorchProfilerController--parameters","title":"Parameters","text":"<p>profiler:     The shared :class:<code>torch.profiler.profile</code> instance, or <code>None</code> if     profiling is disabled. activities:     Iterable of :class:<code>torch.profiler.ProfilerActivity</code> recorded by the     profiler. config:     Optional :class:<code>TorchProfilerConfig</code>. If omitted, :meth:<code>from_env</code>     constructs one during initialization.</p>"},{"location":"api/fastvideo/profiler/#fastvideo.profiler.TorchProfilerController--examples","title":"Examples","text":"<p>Enabling an existing region from the command line::</p> <pre><code>FASTVIDEO_TORCH_PROFILE_REGIONS=model_loading,training_dit         python fastvideo/training/wan_training_pipeline.py ...\n</code></pre> <p>Wrapping a code block in a custom region::</p> <pre><code>controller = TorchProfilerController(profiler, activities)\nwith controller.region(\"training_validation\"):\n    run_validation_epoch()\n</code></pre> Adding a new region requires three steps <ol> <li>Define an env var in <code>envs.py</code>.</li> <li>Add a default entry to <code>register_profiler_region</code> in this module.</li> <li>Wrap the target code in :meth:<code>region</code> using the new name.</li> </ol> Source code in <code>fastvideo/profiler.py</code> <pre><code>def __init__(\n    self,\n    profiler: Any,\n    activities: Iterable[torch.profiler.ProfilerActivity],\n    config: TorchProfilerConfig | None = None,\n    disabled: bool = False,\n) -&gt; None:\n    activities_tuple = tuple(activities)\n    existing = get_global_controller()\n    if existing is not None and not disabled:\n        raise RuntimeError(\n            \"TorchProfilerController already initialized globally. Use get_global_controller().\"\n        )\n    if disabled:\n        self._profiler = None\n        return\n\n    self._profiler = profiler\n    self._activities = activities_tuple\n    self._config = config or TorchProfilerConfig.from_env()\n    self._collection_enabled = False\n    self._active_region_depth = 0\n    logger.info(\n        \"PROFILER: TorchProfilerController initialized with config: %s\",\n        self._config)\n    set_global_profiler(self._profiler)\n    set_global_controller(self)\n</code></pre>"},{"location":"api/fastvideo/profiler/#fastvideo.profiler.TorchProfilerController-attributes","title":"Attributes","text":""},{"location":"api/fastvideo/profiler/#fastvideo.profiler.TorchProfilerController.has_profiler","title":"fastvideo.profiler.TorchProfilerController.has_profiler  <code>property</code>","text":"<pre><code>has_profiler: bool\n</code></pre> <p>Return <code>True</code> when a profiler instance is available.</p>"},{"location":"api/fastvideo/profiler/#fastvideo.profiler.TorchProfilerController.is_enabled","title":"fastvideo.profiler.TorchProfilerController.is_enabled  <code>property</code>","text":"<pre><code>is_enabled: bool\n</code></pre> <p>Return <code>True</code> when the underlying profiler is collecting.</p>"},{"location":"api/fastvideo/profiler/#fastvideo.profiler.TorchProfilerController-functions","title":"Functions","text":""},{"location":"api/fastvideo/profiler/#fastvideo.profiler.TorchProfilerController.is_region_enabled","title":"fastvideo.profiler.TorchProfilerController.is_region_enabled","text":"<pre><code>is_region_enabled(region: str) -&gt; bool\n</code></pre> <p>Return <code>True</code> if <code>region</code> should be collected.</p> Source code in <code>fastvideo/profiler.py</code> <pre><code>def is_region_enabled(self, region: str) -&gt; bool:\n    \"\"\"Return ``True`` if ``region`` should be collected.\"\"\"\n\n    if self._profiler is None:\n        return False\n    return self._config.regions.get(region, False)\n</code></pre>"},{"location":"api/fastvideo/profiler/#fastvideo.profiler.TorchProfilerController.region","title":"fastvideo.profiler.TorchProfilerController.region","text":"<pre><code>region(region: str)\n</code></pre> <p>Context manager that enables profiling for <code>region</code> if configured.</p> Source code in <code>fastvideo/profiler.py</code> <pre><code>@contextlib.contextmanager\ndef region(self, region: str):\n    \"\"\"Context manager that enables profiling for ``region`` if configured.\"\"\"\n\n    if self._profiler is None:\n        yield\n        return\n\n    if not self.is_region_enabled(region):\n        yield\n        return\n\n    with torch.profiler.record_function(f\"fastvideo.region::{region}\"):\n        self._active_region_depth += 1\n        if self._active_region_depth == 1:\n            logger.info(\n                \"PROFILER: Setting collection to True (depth=%s) for region %s\",\n                self._active_region_depth, region)\n            self._set_collection(True)\n        try:\n            yield\n        finally:\n            self._active_region_depth -= 1\n            logger.info(\"PROFILER: Decreasing active region depth to %s\",\n                        self._active_region_depth)\n            if self._active_region_depth == 0:\n                logger.info(\n                    \"PROFILER: Setting collection to False upon exiting region %s\",\n                    region)\n                self._set_collection(False)\n</code></pre>"},{"location":"api/fastvideo/profiler/#fastvideo.profiler.TorchProfilerController.start","title":"fastvideo.profiler.TorchProfilerController.start","text":"<pre><code>start() -&gt; None\n</code></pre> <p>Start the profiler and pause collection until a region is entered.</p> Source code in <code>fastvideo/profiler.py</code> <pre><code>def start(self) -&gt; None:\n    \"\"\"Start the profiler and pause collection until a region is entered.\"\"\"\n\n    logger.info(\"PROFILER: Starting profiler...\")\n    if self._profiler is None:\n        return\n    self._profiler.start()\n    logger.info(\"PROFILER: Profiler started\")\n    # Profiler starts with collection disabled by default.\n    logger.info(\"PROFILER: Setting collection to False\")\n    self._set_collection(False)\n    logger.info(\"PROFILER: Profiler started with collection disabled\")\n</code></pre>"},{"location":"api/fastvideo/profiler/#fastvideo.profiler.TorchProfilerController.stop","title":"fastvideo.profiler.TorchProfilerController.stop","text":"<pre><code>stop() -&gt; None\n</code></pre> <p>Stop the profiler after disabling collection and clearing state.</p> Source code in <code>fastvideo/profiler.py</code> <pre><code>def stop(self) -&gt; None:\n    \"\"\"Stop the profiler after disabling collection and clearing state.\"\"\"\n\n    if self._profiler is None:\n        return\n\n    logger.info(\"PROFILER: Stopping profiler...\")\n    self._profiler.stop()\n    logger.info(\"PROFILER: Profiler stopped\")\n    self._active_region_depth = 0\n    set_global_profiler(None)\n    set_global_controller(None)\n</code></pre>"},{"location":"api/fastvideo/profiler/#fastvideo.profiler-functions","title":"Functions","text":""},{"location":"api/fastvideo/profiler/#fastvideo.profiler.get_global_profiler","title":"fastvideo.profiler.get_global_profiler","text":"<pre><code>get_global_profiler() -&gt; torch.profiler.profile | None\n</code></pre> <p>Return the global profiler instance if one was created.</p> Source code in <code>fastvideo/profiler.py</code> <pre><code>def get_global_profiler() -&gt; torch.profiler.profile | None:\n    \"\"\"Return the global profiler instance if one was created.\"\"\"\n\n    return _GLOBAL_PROFILER\n</code></pre>"},{"location":"api/fastvideo/profiler/#fastvideo.profiler.get_or_create_profiler","title":"fastvideo.profiler.get_or_create_profiler","text":"<pre><code>get_or_create_profiler(\n    trace_dir: str | None,\n) -&gt; TorchProfilerController\n</code></pre> <p>Create or reuse the process-wide torch profiler controller.</p> Source code in <code>fastvideo/profiler.py</code> <pre><code>def get_or_create_profiler(trace_dir: str | None) -&gt; TorchProfilerController:\n    \"\"\"Create or reuse the process-wide torch profiler controller.\"\"\"\n\n    existing = get_global_controller()\n    if existing is not None:\n        if trace_dir:\n            logger.info(\"Reusing existing global torch profiler controller\")\n        return existing\n\n    if not trace_dir:\n        logger.info(\"Torch profiler disabled; returning no-op controller\")\n        return TorchProfilerController(None, _DEFAULT_ACTIVITIES, disabled=True)\n\n    logger.info(\"Profiling enabled. Traces will be saved to: %s\", trace_dir)\n    logger.info(\n        \"Profiler config: record_shapes=%s, profile_memory=%s, with_stack=%s, with_flops=%s\",\n        envs.FASTVIDEO_TORCH_PROFILER_RECORD_SHAPES,\n        envs.FASTVIDEO_TORCH_PROFILER_WITH_PROFILE_MEMORY,\n        envs.FASTVIDEO_TORCH_PROFILER_WITH_STACK,\n        envs.FASTVIDEO_TORCH_PROFILER_WITH_FLOPS,\n    )\n    logger.info(\"FASTVIDEO_TORCH_PROFILE_REGIONS=%s\",\n                envs.FASTVIDEO_TORCH_PROFILE_REGIONS)\n\n    profiler = torch.profiler.profile(\n        activities=_DEFAULT_ACTIVITIES,\n        record_shapes=envs.FASTVIDEO_TORCH_PROFILER_RECORD_SHAPES,\n        profile_memory=envs.FASTVIDEO_TORCH_PROFILER_WITH_PROFILE_MEMORY,\n        with_stack=envs.FASTVIDEO_TORCH_PROFILER_WITH_STACK,\n        with_flops=envs.FASTVIDEO_TORCH_PROFILER_WITH_FLOPS,\n        on_trace_ready=torch.profiler.tensorboard_trace_handler(trace_dir,\n                                                                use_gzip=True),\n    )\n    controller = TorchProfilerController(profiler, _DEFAULT_ACTIVITIES)\n    controller.start()\n    logger.info(\"Torch profiler started\")\n    return controller\n</code></pre>"},{"location":"api/fastvideo/profiler/#fastvideo.profiler.list_profiler_regions","title":"fastvideo.profiler.list_profiler_regions","text":"<pre><code>list_profiler_regions() -&gt; list[ProfilerRegion]\n</code></pre> <p>Return all registered profiler regions sorted by canonical name.</p> Source code in <code>fastvideo/profiler.py</code> <pre><code>def list_profiler_regions() -&gt; list[ProfilerRegion]:\n    \"\"\"Return all registered profiler regions sorted by canonical name.\"\"\"\n\n    return [_REGISTERED_REGIONS[name] for name in sorted(_REGISTERED_REGIONS)]\n</code></pre>"},{"location":"api/fastvideo/profiler/#fastvideo.profiler.profile_region","title":"fastvideo.profiler.profile_region","text":"<pre><code>profile_region(\n    region: str,\n) -&gt; Callable[[Callable[..., Any]], Callable[..., Any]]\n</code></pre> <p>Wrap a bound method so it runs inside a profiler region if available.</p> Source code in <code>fastvideo/profiler.py</code> <pre><code>def profile_region(\n        region: str) -&gt; Callable[[Callable[..., Any]], Callable[..., Any]]:\n    \"\"\"Wrap a bound method so it runs inside a profiler region if available.\"\"\"\n\n    def decorator(fn: Callable[..., Any]) -&gt; Callable[..., Any]:\n\n        @functools.wraps(fn)\n        def wrapped(self, *args, **kwargs):\n            controller = getattr(self, \"profiler_controller\", None)\n            if controller is None or not controller.has_profiler:\n                return fn(self, *args, **kwargs)\n            with controller.region(region):\n                return fn(self, *args, **kwargs)\n\n        return wrapped\n\n    return decorator\n</code></pre>"},{"location":"api/fastvideo/profiler/#fastvideo.profiler.register_profiler_region","title":"fastvideo.profiler.register_profiler_region","text":"<pre><code>register_profiler_region(\n    name: str,\n    description: str,\n    *,\n    default_enabled: bool = False\n) -&gt; None\n</code></pre> <p>Register a profiler region so configuration can validate inputs.</p> Source code in <code>fastvideo/profiler.py</code> <pre><code>def register_profiler_region(\n    name: str,\n    description: str,\n    *,\n    default_enabled: bool = False,\n) -&gt; None:\n    \"\"\"Register a profiler region so configuration can validate inputs.\"\"\"\n\n    canonical = _normalize_token(name)\n    if canonical in _REGISTERED_REGIONS:\n        raise ValueError(f\"Profiler region {name!r} is already registered\")\n\n    region = ProfilerRegion(\n        name=canonical,\n        description=description,\n        default_enabled=bool(default_enabled),\n    )\n    _REGISTERED_REGIONS[canonical] = region\n</code></pre>"},{"location":"api/fastvideo/profiler/#fastvideo.profiler.resolve_profiler_region","title":"fastvideo.profiler.resolve_profiler_region","text":"<pre><code>resolve_profiler_region(name: str) -&gt; ProfilerRegion | None\n</code></pre> <p>Return the registered region matching <code>name</code> or <code>None</code> if absent.</p> Source code in <code>fastvideo/profiler.py</code> <pre><code>def resolve_profiler_region(name: str) -&gt; ProfilerRegion | None:\n    \"\"\"Return the registered region matching ``name`` or ``None`` if absent.\"\"\"\n\n    canonical = _normalize_token(name)\n    return _REGISTERED_REGIONS.get(canonical)\n</code></pre>"},{"location":"api/fastvideo/tests/","title":"tests","text":""},{"location":"api/fastvideo/tests/#fastvideo.tests","title":"tests","text":""},{"location":"api/fastvideo/tests/#fastvideo.tests-modules","title":"Modules","text":""},{"location":"api/fastvideo/tests/#fastvideo.tests.conftest","title":"fastvideo.tests.conftest","text":""},{"location":"api/fastvideo/tests/#fastvideo.tests.conftest-functions","title":"Functions","text":""},{"location":"api/fastvideo/tests/#fastvideo.tests.conftest.distributed_setup","title":"fastvideo.tests.conftest.distributed_setup","text":"<pre><code>distributed_setup()\n</code></pre> <p>Fixture to set up and tear down the distributed environment for tests.</p> <p>This ensures proper cleanup even if tests fail.</p> Source code in <code>fastvideo/tests/conftest.py</code> <pre><code>@pytest.fixture(scope=\"function\")\ndef distributed_setup():\n    \"\"\"\n    Fixture to set up and tear down the distributed environment for tests.\n\n    This ensures proper cleanup even if tests fail.\n    \"\"\"\n    torch.manual_seed(42)\n    np.random.seed(42)\n    maybe_init_distributed_environment_and_model_parallel(1, 1)\n    yield\n\n    cleanup_dist_env_and_memory()\n</code></pre>"},{"location":"api/fastvideo/tests/#fastvideo.tests.utils","title":"fastvideo.tests.utils","text":""},{"location":"api/fastvideo/tests/#fastvideo.tests.utils-functions","title":"Functions","text":""},{"location":"api/fastvideo/tests/#fastvideo.tests.utils.compare_folders","title":"fastvideo.tests.utils.compare_folders","text":"<pre><code>compare_folders(\n    reference_folder, generated_folder, use_ms_ssim=True\n)\n</code></pre> <pre><code>Compare videos with the same filename between reference_folder and generated_folder\n\nExample usage:\n    results = compare_folders(reference_folder, generated_folder,\n                          args.use_ms_ssim)\n    for video_name, ssim_value in results.items():\n        if ssim_value is not None:\n            print(\n                f\"{video_name}: {ssim_value[0]:.4f}, Min SSIM: {ssim_value[1]:.4f}, Max SSIM: {ssim_value[2]:.4f}\"\n            )\n        else:\n            print(f\"{video_name}: Error during comparison\")\n\n    valid_ssims = [v for v in results.values() if v is not None]\n    if valid_ssims:\n        avg_ssim = np.mean([v[0] for v in valid_ssims])\n        print(f\"\n</code></pre> <p>Average SSIM across all videos: {avg_ssim:.4f}\")         else:             print(\" No valid SSIM values to average\")</p> Source code in <code>fastvideo/tests/utils.py</code> <pre><code>def compare_folders(reference_folder, generated_folder, use_ms_ssim=True):\n    \"\"\"\n    Compare videos with the same filename between reference_folder and generated_folder\n\n    Example usage:\n        results = compare_folders(reference_folder, generated_folder,\n                              args.use_ms_ssim)\n        for video_name, ssim_value in results.items():\n            if ssim_value is not None:\n                print(\n                    f\"{video_name}: {ssim_value[0]:.4f}, Min SSIM: {ssim_value[1]:.4f}, Max SSIM: {ssim_value[2]:.4f}\"\n                )\n            else:\n                print(f\"{video_name}: Error during comparison\")\n\n        valid_ssims = [v for v in results.values() if v is not None]\n        if valid_ssims:\n            avg_ssim = np.mean([v[0] for v in valid_ssims])\n            print(f\"\\nAverage SSIM across all videos: {avg_ssim:.4f}\")\n        else:\n            print(\"\\nNo valid SSIM values to average\")\n    \"\"\"\n\n    reference_videos = [\n        f for f in os.listdir(reference_folder) if f.endswith('.mp4')\n    ]\n\n    results = {}\n\n    for video_name in reference_videos:\n        ref_path = os.path.join(reference_folder, video_name)\n        gen_path = os.path.join(generated_folder, video_name)\n\n        if os.path.exists(gen_path):\n            print(f\"\\nComparing {video_name}...\")\n            try:\n                ssim_value = compute_video_ssim_torchvision(\n                    ref_path, gen_path, use_ms_ssim)\n                results[video_name] = ssim_value\n            except Exception as e:\n                print(f\"Error comparing {video_name}: {e}\")\n                results[video_name] = None\n        else:\n            print(\n                f\"\\nSkipping {video_name} - no matching file in generated folder\"\n            )\n\n    return results\n</code></pre>"},{"location":"api/fastvideo/tests/#fastvideo.tests.utils.compute_video_ssim_torchvision","title":"fastvideo.tests.utils.compute_video_ssim_torchvision","text":"<pre><code>compute_video_ssim_torchvision(\n    video1_path, video2_path, use_ms_ssim=True\n)\n</code></pre> <p>Compute SSIM between two videos.</p> <p>Parameters:</p> Name Type Description Default <code>video1_path</code> <p>Path to the first video.</p> required <code>video2_path</code> <p>Path to the second video.</p> required <code>use_ms_ssim</code> <p>Whether to use Multi-Scale Structural Similarity(MS-SSIM) instead of SSIM.</p> <code>True</code> Source code in <code>fastvideo/tests/utils.py</code> <pre><code>def compute_video_ssim_torchvision(video1_path, video2_path, use_ms_ssim=True):\n    \"\"\"\n    Compute SSIM between two videos.\n\n    Args:\n        video1_path: Path to the first video.\n        video2_path: Path to the second video.\n        use_ms_ssim: Whether to use Multi-Scale Structural Similarity(MS-SSIM) instead of SSIM.\n    \"\"\"\n    print(f\"Computing SSIM between {video1_path} and {video2_path}...\")\n    if not os.path.exists(video1_path):\n        raise FileNotFoundError(f\"Video1 not found: {video1_path}\")\n    if not os.path.exists(video2_path):\n        raise FileNotFoundError(f\"Video2 not found: {video2_path}\")\n\n    frames1, _, _ = read_video(video1_path,\n                               pts_unit='sec',\n                               output_format=\"TCHW\")\n    frames2, _, _ = read_video(video2_path,\n                               pts_unit='sec',\n                               output_format=\"TCHW\")\n\n    # Ensure same number of frames\n    min_frames = min(frames1.shape[0], frames2.shape[0])\n    frames1 = frames1[:min_frames]\n    frames2 = frames2[:min_frames]\n\n    frames1 = frames1.float() / 255.0\n    frames2 = frames2.float() / 255.0\n\n    if torch.cuda.is_available():\n        frames1 = frames1.cuda()\n        frames2 = frames2.cuda()\n\n    ssim_values = []\n\n    # Process each frame individually\n    for i in range(min_frames):\n        img1 = frames1[i:i + 1]\n        img2 = frames2[i:i + 1]\n\n        with torch.no_grad():\n            if use_ms_ssim:\n                value = ms_ssim(img1, img2, data_range=1.0)\n            else:\n                value = ssim(img1, img2, data_range=1.0)\n\n            ssim_values.append(value.item())\n\n    if ssim_values:\n        mean_ssim = np.mean(ssim_values)\n        min_ssim = np.min(ssim_values)\n        max_ssim = np.max(ssim_values)\n        min_frame_idx = np.argmin(ssim_values)\n        max_frame_idx = np.argmax(ssim_values)\n\n        print(f\"Mean SSIM: {mean_ssim:.4f}\")\n        print(f\"Min SSIM: {min_ssim:.4f} (at frame {min_frame_idx})\")\n        print(f\"Max SSIM: {max_ssim:.4f} (at frame {max_frame_idx})\")\n\n        return mean_ssim, min_ssim, max_ssim\n    else:\n        print('No SSIM values calculated')\n        return 0, 0, 0\n</code></pre>"},{"location":"api/fastvideo/tests/#fastvideo.tests.utils.write_ssim_results","title":"fastvideo.tests.utils.write_ssim_results","text":"<pre><code>write_ssim_results(\n    output_dir,\n    ssim_values,\n    reference_path,\n    generated_path,\n    num_inference_steps,\n    prompt,\n)\n</code></pre> <p>Write SSIM results to a JSON file in the same directory as the generated videos.</p> Source code in <code>fastvideo/tests/utils.py</code> <pre><code>def write_ssim_results(output_dir, ssim_values, reference_path, generated_path,\n                       num_inference_steps, prompt):\n    \"\"\"\n    Write SSIM results to a JSON file in the same directory as the generated videos.\n    \"\"\"\n    try:\n        logger.info(\n            f\"Attempting to write SSIM results to directory: {output_dir}\")\n\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir, exist_ok=True)\n\n        mean_ssim, min_ssim, max_ssim = ssim_values\n\n        result = {\n            \"mean_ssim\": mean_ssim,\n            \"min_ssim\": min_ssim,\n            \"max_ssim\": max_ssim,\n            \"reference_video\": reference_path,\n            \"generated_video\": generated_path,\n            \"parameters\": {\n                \"num_inference_steps\": num_inference_steps,\n                \"prompt\": prompt\n            }\n        }\n\n        test_name = f\"steps{num_inference_steps}_{prompt[:100]}\"\n        result_file = os.path.join(output_dir, f\"{test_name}_ssim.json\")\n        logger.info(f\"Writing JSON results to: {result_file}\")\n        with open(result_file, 'w') as f:\n            json.dump(result, f, indent=2)\n\n        logger.info(f\"SSIM results written to {result_file}\")\n        return True\n    except Exception as e:\n        logger.error(f\"ERROR writing SSIM results: {str(e)}\")\n        return False\n</code></pre>"},{"location":"api/fastvideo/third_party/","title":"third_party","text":""},{"location":"api/fastvideo/third_party/#fastvideo.third_party","title":"third_party","text":""},{"location":"api/fastvideo/third_party/#fastvideo.third_party-modules","title":"Modules","text":""},{"location":"api/fastvideo/third_party/#fastvideo.third_party.pynvml","title":"fastvideo.third_party.pynvml","text":""},{"location":"api/fastvideo/third_party/#fastvideo.third_party.pynvml-attributes","title":"Attributes","text":""},{"location":"api/fastvideo/third_party/#fastvideo.third_party.pynvml.NVML_VALUE_NOT_AVAILABLE_uint","title":"fastvideo.third_party.pynvml.NVML_VALUE_NOT_AVAILABLE_uint  <code>module-attribute</code>","text":"<pre><code>NVML_VALUE_NOT_AVAILABLE_uint = c_uint(-1)\n</code></pre> <p>Field Identifiers.</p> <p>All Identifiers pertain to a device. Each ID is only used once and is guaranteed never to change.</p>"},{"location":"api/fastvideo/third_party/#fastvideo.third_party.pynvml-classes","title":"Classes","text":""},{"location":"api/fastvideo/third_party/#fastvideo.third_party.pynvml.NVMLError","title":"fastvideo.third_party.pynvml.NVMLError","text":"<p>               Bases: <code>Exception</code></p> Functions\u00b6 fastvideo.third_party.pynvml.NVMLError.__new__ \u00b6 <pre><code>__new__(typ, value)\n</code></pre> <p>Maps value to a proper subclass of NVMLError. See _extractNVMLErrorsAsClasses function for more details</p> Source code in <code>fastvideo/third_party/pynvml.py</code> <pre><code>def __new__(typ, value):\n    '''\n    Maps value to a proper subclass of NVMLError.\n    See _extractNVMLErrorsAsClasses function for more details\n    '''\n    if typ == NVMLError:\n        typ = NVMLError._valClassMapping.get(value, typ)\n    obj = Exception.__new__(typ)\n    obj.value = value\n    return obj\n</code></pre>"},{"location":"api/fastvideo/third_party/#fastvideo.third_party.pynvml-functions","title":"Functions","text":""},{"location":"api/fastvideo/third_party/#fastvideo.third_party.pynvml.convertStrBytes","title":"fastvideo.third_party.pynvml.convertStrBytes","text":"<pre><code>convertStrBytes(func)\n</code></pre> <p>In python 3, strings are unicode instead of bytes, and need to be converted for ctypes Args from caller: (1, 'string', &lt;main.c_nvmlDevice_t at 0xFFFFFFFF&gt;) Args passed to function: (1, b'string', &lt;main.c_nvmlDevice_t at 0xFFFFFFFF)&gt;</p> <p>Returned from function: b'returned string' Returned to caller: 'returned string'</p> Source code in <code>fastvideo/third_party/pynvml.py</code> <pre><code>def convertStrBytes(func):\n    '''\n    In python 3, strings are unicode instead of bytes, and need to be converted for ctypes\n    Args from caller: (1, 'string', &lt;__main__.c_nvmlDevice_t at 0xFFFFFFFF&gt;)\n    Args passed to function: (1, b'string', &lt;__main__.c_nvmlDevice_t at 0xFFFFFFFF)&gt;\n    ----\n    Returned from function: b'returned string'\n    Returned to caller: 'returned string'\n    '''\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        # encoding a str returns bytes in python 2 and 3\n        args = [arg.encode() if isinstance(arg, str) else arg for arg in args]\n        res = func(*args, **kwargs)\n        # In python 2, str and bytes are the same\n        # In python 3, str is unicode and should be decoded.\n        # Ctypes handles most conversions, this only effects c_char and char arrays.\n        if isinstance(res, bytes):\n            if isinstance(res, str):\n                return res\n            return res.decode()\n        return res\n\n    if sys.version_info &gt;= (3,):\n        return wrapper\n    return func\n</code></pre>"},{"location":"api/fastvideo/training/","title":"training","text":""},{"location":"api/fastvideo/training/#fastvideo.training","title":"training","text":""},{"location":"api/fastvideo/training/#fastvideo.training-classes","title":"Classes","text":""},{"location":"api/fastvideo/training/#fastvideo.training.DistillationPipeline","title":"fastvideo.training.DistillationPipeline","text":"<pre><code>DistillationPipeline(\n    model_path: str,\n    fastvideo_args: TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>TrainingPipeline</code></p> <p>A distillation pipeline for training a 3 step model. Inherits from TrainingPipeline to reuse training infrastructure.</p> Source code in <code>fastvideo/training/training_pipeline.py</code> <pre><code>def __init__(\n        self,\n        model_path: str,\n        fastvideo_args: TrainingArgs,\n        required_config_modules: list[str] | None = None,\n        loaded_modules: dict[str, torch.nn.Module] | None = None) -&gt; None:\n    fastvideo_args.inference_mode = False\n    self.lora_training = fastvideo_args.lora_training\n    if self.lora_training and fastvideo_args.lora_rank is None:\n        raise ValueError(\"lora rank must be set when using lora training\")\n\n    set_random_seed(fastvideo_args.seed)  # for lora param init\n    super().__init__(model_path, fastvideo_args, required_config_modules,\n                     loaded_modules)  # type: ignore\n    self.tracker = DummyTracker()\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.DistillationPipeline-functions","title":"Functions","text":""},{"location":"api/fastvideo/training/#fastvideo.training.DistillationPipeline.apply_ema_to_model","title":"fastvideo.training.DistillationPipeline.apply_ema_to_model","text":"<pre><code>apply_ema_to_model(model)\n</code></pre> <p>Apply EMA weights to the model for validation or inference.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def apply_ema_to_model(self, model):\n    \"\"\"Apply EMA weights to the model for validation or inference.\"\"\"\n    if model is self.transformer and self.generator_ema is not None:\n        with self.generator_ema.apply_to_model(model):\n            return model\n    elif model is self.transformer_2 and self.generator_ema_2 is not None:\n        with self.generator_ema_2.apply_to_model(model):\n            return model\n    return model\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.DistillationPipeline.get_ema_2_model_copy","title":"fastvideo.training.DistillationPipeline.get_ema_2_model_copy","text":"<pre><code>get_ema_2_model_copy() -&gt; torch.nn.Module | None\n</code></pre> <p>Get a copy of the transformer_2 model with EMA weights applied.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def get_ema_2_model_copy(self) -&gt; torch.nn.Module | None:\n    \"\"\"Get a copy of the transformer_2 model with EMA weights applied.\"\"\"\n    if self.generator_ema_2 is not None and self.transformer_2 is not None:\n        ema_2_model = copy.deepcopy(self.transformer_2)\n        self.generator_ema_2.copy_to_unwrapped(ema_2_model)\n        return ema_2_model\n    return None\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.DistillationPipeline.get_ema_model_copy","title":"fastvideo.training.DistillationPipeline.get_ema_model_copy","text":"<pre><code>get_ema_model_copy() -&gt; torch.nn.Module | None\n</code></pre> <p>Get a copy of the model with EMA weights applied.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def get_ema_model_copy(self) -&gt; torch.nn.Module | None:\n    \"\"\"Get a copy of the model with EMA weights applied.\"\"\"\n    if self.generator_ema is not None:\n        ema_model = copy.deepcopy(self.transformer)\n        self.generator_ema.copy_to_unwrapped(ema_model)\n        return ema_model\n    return None\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.DistillationPipeline.get_ema_stats","title":"fastvideo.training.DistillationPipeline.get_ema_stats","text":"<pre><code>get_ema_stats() -&gt; dict[str, Any]\n</code></pre> <p>Get EMA statistics for monitoring.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def get_ema_stats(self) -&gt; dict[str, Any]:\n    \"\"\"Get EMA statistics for monitoring.\"\"\"\n    ema_enabled = self.generator_ema is not None\n    ema_2_enabled = self.generator_ema_2 is not None\n\n    if not ema_enabled and not ema_2_enabled:\n        return {\n            \"ema_enabled\": False,\n            \"ema_2_enabled\": False,\n            \"ema_decay\": None,\n            \"ema_start_step\": self.training_args.ema_start_step,\n            \"ema_ready\": False,\n            \"ema_2_ready\": False,\n            \"ema_step\": self.current_trainstep,\n        }\n\n    return {\n        \"ema_enabled\": ema_enabled,\n        \"ema_2_enabled\": ema_2_enabled,\n        \"ema_decay\": self.training_args.ema_decay,\n        \"ema_start_step\": self.training_args.ema_start_step,\n        \"ema_ready\": self.is_ema_ready() if ema_enabled else False,\n        \"ema_2_ready\": self.is_ema_ready() if ema_2_enabled else False,\n        \"ema_step\": self.current_trainstep,\n    }\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.DistillationPipeline.initialize_training_pipeline","title":"fastvideo.training.DistillationPipeline.initialize_training_pipeline","text":"<pre><code>initialize_training_pipeline(training_args: TrainingArgs)\n</code></pre> <p>Initialize the distillation training pipeline with multiple models.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def initialize_training_pipeline(self, training_args: TrainingArgs):\n    \"\"\"Initialize the distillation training pipeline with multiple models.\"\"\"\n    logger.info(\"Initializing distillation pipeline...\")\n\n    super().initialize_training_pipeline(training_args)\n\n    self.noise_scheduler = self.get_module(\"scheduler\")\n    self.vae = self.get_module(\"vae\")\n    self.vae.requires_grad_(False)\n\n    self.timestep_shift = self.training_args.pipeline_config.flow_shift\n    self.noise_scheduler = FlowMatchEulerDiscreteScheduler(\n        shift=self.timestep_shift)\n\n    if self.training_args.boundary_ratio is not None:\n        self.boundary_timestep = self.training_args.boundary_ratio * self.noise_scheduler.num_train_timesteps\n    else:\n        self.boundary_timestep = None\n\n    if training_args.real_score_model_path:\n        logger.info(\"Loading real score transformer from: %s\",\n                    training_args.real_score_model_path)\n        training_args.override_transformer_cls_name = \"WanTransformer3DModel\"\n        self.real_score_transformer = self.load_module_from_path(\n            training_args.real_score_model_path, \"transformer\",\n            training_args)\n        try:\n            self.real_score_transformer_2 = self.load_module_from_path(\n                training_args.real_score_model_path, \"transformer_2\",\n                training_args)\n            logger.info(\"Loaded real score transformer_2 for MoE support\")\n        except Exception:\n            logger.info(\n                \"real score transformer_2 not found, using single transformer\"\n            )\n            self.real_score_transformer_2 = None\n    else:\n        self.real_score_transformer = self.get_module(\n            \"real_score_transformer\")\n        self.real_score_transformer_2 = self.get_module(\n            \"real_score_transformer_2\")\n\n    if training_args.fake_score_model_path:\n        logger.info(\"Loading fake score transformer from: %s\",\n                    training_args.fake_score_model_path)\n        training_args.override_transformer_cls_name = \"WanTransformer3DModel\"\n        self.fake_score_transformer = self.load_module_from_path(\n            training_args.fake_score_model_path, \"transformer\",\n            training_args)\n        try:\n            self.fake_score_transformer_2 = self.load_module_from_path(\n                training_args.fake_score_model_path, \"transformer_2\",\n                training_args)\n            logger.info(\"Loaded fake score transformer_2 for MoE support\")\n        except Exception:\n            logger.info(\n                \"fake score transformer_2 not found, using single transformer\"\n            )\n            self.fake_score_transformer_2 = None\n    else:\n        self.fake_score_transformer = self.get_module(\n            \"fake_score_transformer\")\n        self.fake_score_transformer_2 = self.get_module(\n            \"fake_score_transformer_2\")\n\n    self.real_score_transformer.requires_grad_(False)\n    self.real_score_transformer.eval()\n    if self.real_score_transformer_2 is not None:\n        self.real_score_transformer_2.requires_grad_(False)\n        self.real_score_transformer_2.eval()\n\n    # Set training modes for fake score transformers (trainable)\n    self.fake_score_transformer.requires_grad_(True)\n    self.fake_score_transformer.train()\n    if self.fake_score_transformer_2 is not None:\n        self.fake_score_transformer_2.requires_grad_(True)\n        self.fake_score_transformer_2.train()\n\n    if training_args.enable_gradient_checkpointing_type is not None:\n        self.fake_score_transformer = apply_activation_checkpointing(\n            self.fake_score_transformer,\n            checkpointing_type=training_args.\n            enable_gradient_checkpointing_type)\n        if self.fake_score_transformer_2 is not None:\n            self.fake_score_transformer_2 = apply_activation_checkpointing(\n                self.fake_score_transformer_2,\n                checkpointing_type=training_args.\n                enable_gradient_checkpointing_type)\n\n        self.real_score_transformer = apply_activation_checkpointing(\n            self.real_score_transformer,\n            checkpointing_type=training_args.\n            enable_gradient_checkpointing_type)\n        if self.real_score_transformer_2 is not None:\n            self.real_score_transformer_2 = apply_activation_checkpointing(\n                self.real_score_transformer_2,\n                checkpointing_type=training_args.\n                enable_gradient_checkpointing_type)\n\n    # Initialize optimizers\n    fake_score_params = list(\n        filter(lambda p: p.requires_grad,\n               self.fake_score_transformer.parameters()))\n\n    # Use separate learning rate for fake_score_transformer if specified\n    fake_score_lr = training_args.fake_score_learning_rate\n    if fake_score_lr == 0.0:\n        fake_score_lr = training_args.learning_rate\n\n    betas_str = training_args.fake_score_betas\n    betas = tuple(float(x.strip()) for x in betas_str.split(\",\"))\n\n    self.fake_score_optimizer = torch.optim.AdamW(\n        fake_score_params,\n        lr=fake_score_lr,\n        betas=betas,\n        weight_decay=training_args.weight_decay,\n        eps=1e-8,\n    )\n\n    self.fake_score_lr_scheduler = get_scheduler(\n        training_args.fake_score_lr_scheduler,\n        optimizer=self.fake_score_optimizer,\n        num_warmup_steps=training_args.lr_warmup_steps,\n        num_training_steps=training_args.max_train_steps,\n        num_cycles=training_args.lr_num_cycles,\n        power=training_args.lr_power,\n        min_lr_ratio=training_args.min_lr_ratio,\n        last_epoch=self.init_steps - 1,\n    )\n\n    if self.fake_score_transformer_2 is not None:\n        fake_score_params_2 = list(\n            filter(lambda p: p.requires_grad,\n                   self.fake_score_transformer_2.parameters()))\n        self.fake_score_optimizer_2 = torch.optim.AdamW(\n            fake_score_params_2,\n            lr=fake_score_lr,\n            betas=betas,\n            weight_decay=training_args.weight_decay,\n            eps=1e-8,\n        )\n        self.fake_score_lr_scheduler_2 = get_scheduler(\n            training_args.fake_score_lr_scheduler,\n            optimizer=self.fake_score_optimizer_2,\n            num_warmup_steps=training_args.lr_warmup_steps,\n            num_training_steps=training_args.max_train_steps,\n            num_cycles=training_args.lr_num_cycles,\n            power=training_args.lr_power,\n            min_lr_ratio=training_args.min_lr_ratio,\n            last_epoch=self.init_steps - 1,\n        )\n\n    logger.info(\n        \"Distillation optimizers initialized: generator and fake_score\")\n\n    self.generator_update_interval = self.training_args.generator_update_interval\n    logger.info(\n        \"Distillation pipeline initialized with generator_update_interval=%s\",\n        self.generator_update_interval)\n\n    self.denoising_step_list = torch.tensor(\n        self.training_args.pipeline_config.dmd_denoising_steps,\n        dtype=torch.long,\n        device=get_local_torch_device())\n\n    if training_args.warp_denoising_step:  # Warp the denoising step according to the scheduler time shift\n        timesteps = torch.cat((self.noise_scheduler.timesteps.cpu(),\n                               torch.tensor([0],\n                                            dtype=torch.float32))).cuda()\n        self.denoising_step_list = timesteps[1000 -\n                                             self.denoising_step_list]\n        logger.info(\"Warping denoising_step_list\")\n\n    self.denoising_step_list = self.denoising_step_list.to(\n        get_local_torch_device())\n    logger.info(\"Distillation generator model to %s denoising steps: %s\",\n                len(self.denoising_step_list), self.denoising_step_list)\n    self.num_train_timestep = self.noise_scheduler.num_train_timesteps\n\n    self.min_timestep = int(self.training_args.min_timestep_ratio *\n                            self.num_train_timestep)\n    self.max_timestep = int(self.training_args.max_timestep_ratio *\n                            self.num_train_timestep)\n\n    self.real_score_guidance_scale = self.training_args.real_score_guidance_scale\n\n    self.generator_ema: EMA_FSDP | None = None\n    self.generator_ema_2: EMA_FSDP | None = None\n    if (self.training_args.ema_decay\n            is not None) and (self.training_args.ema_decay &gt; 0.0):\n        self.generator_ema = EMA_FSDP(self.transformer,\n                                      decay=self.training_args.ema_decay)\n        logger.info(\"Initialized generator EMA with decay=%s\",\n                    self.training_args.ema_decay)\n\n        # Initialize EMA for transformer_2 if it exists\n        if self.transformer_2 is not None:\n            self.generator_ema_2 = EMA_FSDP(\n                self.transformer_2, decay=self.training_args.ema_decay)\n            logger.info(\"Initialized generator EMA_2 with decay=%s\",\n                        self.training_args.ema_decay)\n    else:\n        logger.info(\"Generator EMA disabled (ema_decay &lt;= 0.0)\")\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.DistillationPipeline.initialize_validation_pipeline","title":"fastvideo.training.DistillationPipeline.initialize_validation_pipeline  <code>abstractmethod</code>","text":"<pre><code>initialize_validation_pipeline(training_args: TrainingArgs)\n</code></pre> <p>Initialize validation pipeline - must be implemented by subclasses.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>@abstractmethod\ndef initialize_validation_pipeline(self, training_args: TrainingArgs):\n    \"\"\"Initialize validation pipeline - must be implemented by subclasses.\"\"\"\n    raise NotImplementedError(\n        \"Distillation pipelines must implement this method\")\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.DistillationPipeline.is_ema_ready","title":"fastvideo.training.DistillationPipeline.is_ema_ready","text":"<pre><code>is_ema_ready(current_step: int | None = None)\n</code></pre> <p>Check if EMA is ready for use (after ema_start_step).</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def is_ema_ready(self, current_step: int | None = None):\n    \"\"\"Check if EMA is ready for use (after ema_start_step).\"\"\"\n    if current_step is None:\n        current_step = getattr(self, 'current_trainstep', 0)\n    return (self.generator_ema is not None\n            and current_step &gt;= self.training_args.ema_start_step)\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.DistillationPipeline.load_module_from_path","title":"fastvideo.training.DistillationPipeline.load_module_from_path","text":"<pre><code>load_module_from_path(\n    model_path: str,\n    module_type: str,\n    training_args: TrainingArgs,\n)\n</code></pre> <p>Load a module from a specific path using the same loading logic as the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the model</p> required <code>module_type</code> <code>str</code> <p>Type of module to load (e.g., \"transformer\")</p> required <code>training_args</code> <code>TrainingArgs</code> <p>Training arguments</p> required <p>Returns:</p> Type Description <p>The loaded module</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def load_module_from_path(self, model_path: str, module_type: str,\n                          training_args: \"TrainingArgs\"):\n    \"\"\"\n    Load a module from a specific path using the same loading logic as the pipeline.\n\n    Args:\n        model_path: Path to the model\n        module_type: Type of module to load (e.g., \"transformer\")\n        training_args: Training arguments\n\n    Returns:\n        The loaded module\n    \"\"\"\n    logger.info(\"Loading %s from custom path: %s\", module_type, model_path)\n    # Set flag to prevent custom weight loading for teacher/critic models\n    training_args._loading_teacher_critic_model = True\n\n    try:\n        from fastvideo.models.loader.component_loader import (\n            PipelineComponentLoader)\n\n        # Download the model if it's a Hugging Face model ID\n        local_model_path = maybe_download_model(model_path)\n        logger.info(\"Model downloaded/found at: %s\", local_model_path)\n        config = verify_model_config_and_directory(local_model_path)\n\n        if module_type not in config:\n            if hasattr(self, '_extra_config_module_map'\n                       ) and module_type in self._extra_config_module_map:\n                extra_module = self._extra_config_module_map[module_type]\n                if extra_module in config:\n                    module_type = extra_module\n                    logger.info(\"Using %s for %s\", extra_module,\n                                module_type)\n                else:\n                    raise ValueError(\n                        f\"Module {module_type} not found in config at {local_model_path}\"\n                    )\n            else:\n                raise ValueError(\n                    f\"Module {module_type} not found in config at {local_model_path}\"\n                )\n\n        module_info = config[module_type]\n        if module_info is None:\n            raise ValueError(\n                f\"Module {module_type} has null value in config at {local_model_path}\"\n            )\n\n        transformers_or_diffusers, architecture = module_info\n        component_path = os.path.join(local_model_path, module_type)\n        module = PipelineComponentLoader.load_module(\n            module_name=module_type,\n            component_model_path=component_path,\n            transformers_or_diffusers=transformers_or_diffusers,\n            fastvideo_args=training_args,\n        )\n\n        logger.info(\"Successfully loaded %s from %s\", module_type,\n                    component_path)\n        return module\n    finally:\n        # Always clean up the flag\n        if hasattr(training_args, '_loading_teacher_critic_model'):\n            delattr(training_args, '_loading_teacher_critic_model')\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.DistillationPipeline.reset_ema","title":"fastvideo.training.DistillationPipeline.reset_ema","text":"<pre><code>reset_ema()\n</code></pre> <p>Reset EMA to current model weights.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def reset_ema(self):\n    \"\"\"Reset EMA to current model weights.\"\"\"\n    if self.generator_ema is not None:\n        logger.info(\"Resetting EMA to current model weights\")\n        self.generator_ema.update(self.transformer)\n        # Force update to current weights by setting decay to 0 temporarily\n        original_decay = self.generator_ema.decay\n        self.generator_ema.decay = 0.0\n        self.generator_ema.update(self.transformer)\n        self.generator_ema.decay = original_decay\n        logger.info(\"EMA reset completed\")\n    else:\n        logger.warning(\"Cannot reset EMA: EMA not initialized\")\n\n    if self.generator_ema_2 is not None:\n        logger.info(\"Resetting EMA_2 to current model weights\")\n        self.generator_ema_2.update(self.transformer_2)\n        # Force update to current weights by setting decay to 0 temporarily\n        original_decay_2 = self.generator_ema_2.decay\n        self.generator_ema_2.decay = 0.0\n        self.generator_ema_2.update(self.transformer_2)\n        self.generator_ema_2.decay = original_decay_2\n        logger.info(\"EMA_2 reset completed\")\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.DistillationPipeline.save_ema_weights","title":"fastvideo.training.DistillationPipeline.save_ema_weights","text":"<pre><code>save_ema_weights(output_dir: str, step: int)\n</code></pre> <p>Save EMA weights separately for inference purposes.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def save_ema_weights(self, output_dir: str, step: int):\n    \"\"\"Save EMA weights separately for inference purposes.\"\"\"\n    if self.generator_ema is None and self.generator_ema_2 is None:\n        logger.warning(\"Cannot save EMA weights: No EMA initialized\")\n        return\n\n    if not self.is_ema_ready():\n        logger.warning(\n            \"Cannot save EMA weights: EMA not ready yet (step &lt; ema_start_step)\"\n        )\n        return\n\n    try:\n        # Save main transformer EMA\n        if self.generator_ema is not None:\n            ema_model = self.get_ema_model_copy()\n            if ema_model is None:\n                logger.warning(\"Failed to create EMA model copy\")\n            else:\n                ema_save_dir = os.path.join(output_dir,\n                                            f\"ema_checkpoint-{step}\")\n                os.makedirs(ema_save_dir, exist_ok=True)\n\n                # save as diffusers format\n                from safetensors.torch import save_file\n\n                from fastvideo.training.training_utils import (\n                    custom_to_hf_state_dict, gather_state_dict_on_cpu_rank0)\n                cpu_state = gather_state_dict_on_cpu_rank0(ema_model,\n                                                           device=None)\n\n                if self.global_rank == 0:\n                    weight_path = os.path.join(\n                        ema_save_dir, \"diffusion_pytorch_model.safetensors\")\n                    diffusers_state_dict = custom_to_hf_state_dict(\n                        cpu_state, ema_model.reverse_param_names_mapping)\n                    save_file(diffusers_state_dict, weight_path)\n\n                    config_dict = ema_model.hf_config\n                    if \"dtype\" in config_dict:\n                        del config_dict[\"dtype\"]\n                    config_path = os.path.join(ema_save_dir, \"config.json\")\n                    with open(config_path, \"w\") as f:\n                        json.dump(config_dict, f, indent=4)\n\n                    logger.info(\"EMA weights saved to %s\", weight_path)\n\n                del ema_model\n\n        # Save transformer_2 EMA\n        if self.generator_ema_2 is not None:\n            ema_2_model = self.get_ema_2_model_copy()\n            if ema_2_model is None:\n                logger.warning(\"Failed to create EMA_2 model copy\")\n            else:\n                ema_2_save_dir = os.path.join(output_dir,\n                                              f\"ema_2_checkpoint-{step}\")\n                os.makedirs(ema_2_save_dir, exist_ok=True)\n\n                # save as diffusers format\n                from safetensors.torch import save_file\n\n                from fastvideo.training.training_utils import (\n                    custom_to_hf_state_dict, gather_state_dict_on_cpu_rank0)\n                cpu_state_2 = gather_state_dict_on_cpu_rank0(ema_2_model,\n                                                             device=None)\n\n                if self.global_rank == 0:\n                    weight_path_2 = os.path.join(\n                        ema_2_save_dir,\n                        \"diffusion_pytorch_model.safetensors\")\n                    diffusers_state_dict_2 = custom_to_hf_state_dict(\n                        cpu_state_2,\n                        ema_2_model.reverse_param_names_mapping)\n                    save_file(diffusers_state_dict_2, weight_path_2)\n\n                    config_dict_2 = ema_2_model.hf_config\n                    if \"dtype\" in config_dict_2:\n                        del config_dict_2[\"dtype\"]\n                    config_path_2 = os.path.join(ema_2_save_dir,\n                                                 \"config.json\")\n                    with open(config_path_2, \"w\") as f:\n                        json.dump(config_dict_2, f, indent=4)\n\n                    logger.info(\"EMA_2 weights saved to %s\", weight_path_2)\n\n                del ema_2_model\n\n    except Exception as e:\n        logger.error(\"Failed to save EMA weights: %s\", str(e))\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.DistillationPipeline.train","title":"fastvideo.training.DistillationPipeline.train","text":"<pre><code>train() -&gt; None\n</code></pre> <p>Main training loop with distillation-specific logging.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def train(self) -&gt; None:\n    \"\"\"Main training loop with distillation-specific logging.\"\"\"\n    assert self.training_args.seed is not None, \"seed must be set\"\n    seed = self.training_args.seed\n\n    # Set the same seed within each SP group to ensure reproducibility\n    if self.sp_world_size &gt; 1:\n        # Use the same seed for all processes within the same SP group\n        sp_group_seed = seed + (self.global_rank // self.sp_world_size)\n        set_random_seed(sp_group_seed)\n        logger.info(\"Rank %s: Using SP group seed %s\", self.global_rank,\n                    sp_group_seed)\n    else:\n        set_random_seed(seed + self.global_rank)\n\n    # Set random seeds for deterministic training\n    self.noise_random_generator = torch.Generator(device=\"cpu\").manual_seed(\n        self.seed)\n    self.noise_gen_cuda = torch.Generator(device=\"cuda\").manual_seed(\n        self.seed)\n    self.validation_random_generator = torch.Generator(\n        device=\"cpu\").manual_seed(self.seed)\n    logger.info(\"Initialized random seeds with seed: %s\", seed)\n\n    # Initialize current_trainstep for EMA ready checks\n    #TODO: check if needed\n    self.current_trainstep = self.init_steps\n\n    # Resume from checkpoint if specified (this will restore random states)\n    if self.training_args.resume_from_checkpoint:\n        self._resume_from_checkpoint()\n        logger.info(\"Resumed from checkpoint, random states restored\")\n    else:\n        logger.info(\"Starting training from scratch\")\n\n    self.train_loader_iter = iter(self.train_dataloader)\n\n    step_times: deque[float] = deque(maxlen=100)\n\n    self._log_training_info()\n    self._log_validation(self.transformer, self.training_args,\n                         self.init_steps)\n\n    progress_bar = tqdm(\n        range(0, self.training_args.max_train_steps),\n        initial=self.init_steps,\n        desc=\"Steps\",\n        disable=self.local_rank &gt; 0,\n    )\n\n    use_vsa = vsa_available and envs.FASTVIDEO_ATTENTION_BACKEND == \"VIDEO_SPARSE_ATTN\"\n    for step in range(self.init_steps + 1,\n                      self.training_args.max_train_steps + 1):\n        start_time = time.perf_counter()\n        if use_vsa:\n            vsa_sparsity = self.training_args.VSA_sparsity\n            vsa_decay_rate = self.training_args.VSA_decay_rate\n            vsa_decay_interval_steps = self.training_args.VSA_decay_interval_steps\n            if vsa_decay_interval_steps &gt; 1:\n                current_decay_times = min(step // vsa_decay_interval_steps,\n                                          vsa_sparsity // vsa_decay_rate)\n                current_vsa_sparsity = current_decay_times * vsa_decay_rate\n            else:\n                current_vsa_sparsity = vsa_sparsity\n        else:\n            current_vsa_sparsity = 0.0\n\n        training_batch = TrainingBatch()\n        self.current_trainstep = step\n        training_batch.current_vsa_sparsity = current_vsa_sparsity\n\n        if (step &gt;= self.training_args.ema_start_step) and \\\n                (self.generator_ema is None) and (self.training_args.ema_decay &gt; 0):\n            self.generator_ema = EMA_FSDP(\n                self.transformer, decay=self.training_args.ema_decay)\n            logger.info(\"Created generator EMA at step %s with decay=%s\",\n                        step, self.training_args.ema_decay)\n\n            # Create EMA for transformer_2 if it exists\n            if self.transformer_2 is not None and self.generator_ema_2 is None:\n                self.generator_ema_2 = EMA_FSDP(\n                    self.transformer_2, decay=self.training_args.ema_decay)\n                logger.info(\n                    \"Created generator EMA_2 at step %s with decay=%s\",\n                    step, self.training_args.ema_decay)\n\n        with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n            training_batch = self.train_one_step(training_batch)\n\n        total_loss = training_batch.total_loss\n        generator_loss = training_batch.generator_loss\n        fake_score_loss = training_batch.fake_score_loss\n        grad_norm = training_batch.grad_norm\n\n        step_time = time.perf_counter() - start_time\n        step_times.append(step_time)\n        avg_step_time = sum(step_times) / len(step_times)\n\n        progress_bar.set_postfix({\n            \"total_loss\":\n            f\"{total_loss:.4f}\",\n            \"generator_loss\":\n            f\"{generator_loss:.4f}\",\n            \"fake_score_loss\":\n            f\"{fake_score_loss:.4f}\",\n            \"step_time\":\n            f\"{step_time:.2f}s\",\n            \"grad_norm\":\n            grad_norm,\n            \"ema\":\n            \"\u2713\" if (self.generator_ema is not None and self.is_ema_ready())\n            else \"\u2717\",\n            \"ema2\":\n            \"\u2713\" if (self.generator_ema_2 is not None\n                    and self.is_ema_ready()) else \"\u2717\",\n        })\n        progress_bar.update(1)\n\n        if self.global_rank == 0:\n            # Prepare logging data\n            log_data = {\n                \"train_total_loss\":\n                total_loss,\n                \"train_fake_score_loss\":\n                fake_score_loss,\n                \"learning_rate\":\n                self.lr_scheduler.get_last_lr()[0],\n                \"fake_score_learning_rate\":\n                self.fake_score_lr_scheduler.get_last_lr()[0],\n                \"step_time\":\n                step_time,\n                \"avg_step_time\":\n                avg_step_time,\n                \"grad_norm\":\n                grad_norm,\n            }\n            # Only log generator loss when generator is actually trained\n            if (step % self.generator_update_interval == 0):\n                log_data[\"train_generator_loss\"] = generator_loss\n            if use_vsa:\n                log_data[\"VSA_train_sparsity\"] = current_vsa_sparsity\n\n            if self.generator_ema is not None or self.generator_ema_2 is not None:\n                log_data[\"ema_enabled\"] = self.generator_ema is not None\n                log_data[\"ema_2_enabled\"] = self.generator_ema_2 is not None\n                log_data[\"ema_decay\"] = self.training_args.ema_decay\n            else:\n                log_data[\"ema_enabled\"] = False\n                log_data[\"ema_2_enabled\"] = False\n\n            ema_stats = self.get_ema_stats()\n            log_data.update(ema_stats)\n\n            if training_batch.dmd_latent_vis_dict:\n                dmd_additional_logs = {\n                    \"generator_timestep\":\n                    training_batch.\n                    dmd_latent_vis_dict[\"generator_timestep\"].item(),\n                    \"dmd_timestep\":\n                    training_batch.dmd_latent_vis_dict[\"dmd_timestep\"].item(\n                    ),\n                }\n                log_data.update(dmd_additional_logs)\n\n            faker_score_additional_logs = {\n                \"fake_score_timestep\":\n                training_batch.\n                fake_score_latent_vis_dict[\"fake_score_timestep\"].item(),\n            }\n            log_data.update(faker_score_additional_logs)\n\n            self.tracker.log(log_data, step)\n\n        # Save training state checkpoint (for resuming training)\n        if (self.training_args.training_state_checkpointing_steps &gt; 0\n                and step %\n                self.training_args.training_state_checkpointing_steps == 0):\n            print(\"rank\", self.global_rank,\n                  \"save training state checkpoint at step\", step)\n            save_distillation_checkpoint(\n                self.transformer,\n                self.fake_score_transformer,\n                self.global_rank,\n                self.training_args.output_dir,\n                step,\n                self.optimizer,\n                self.fake_score_optimizer,\n                self.train_dataloader,\n                self.lr_scheduler,\n                self.fake_score_lr_scheduler,\n                self.noise_random_generator,\n                self.generator_ema,\n                # MoE support\n                generator_transformer_2=getattr(self, 'transformer_2',\n                                                None),\n                real_score_transformer_2=getattr(\n                    self, 'real_score_transformer_2', None),\n                fake_score_transformer_2=getattr(\n                    self, 'fake_score_transformer_2', None),\n                generator_optimizer_2=getattr(self, 'optimizer_2', None),\n                fake_score_optimizer_2=getattr(self,\n                                               'fake_score_optimizer_2',\n                                               None),\n                generator_scheduler_2=getattr(self, 'lr_scheduler_2', None),\n                fake_score_scheduler_2=getattr(self,\n                                               'fake_score_lr_scheduler_2',\n                                               None),\n                generator_ema_2=getattr(self, 'generator_ema_2', None))\n\n            if self.transformer:\n                self.transformer.train()\n            self.sp_group.barrier()\n\n        # Save weight-only checkpoint\n        if (self.training_args.weight_only_checkpointing_steps &gt; 0\n                and step %\n                self.training_args.weight_only_checkpointing_steps == 0):\n            print(\"rank\", self.global_rank,\n                  \"save weight-only checkpoint at step\", step)\n            save_distillation_checkpoint(\n                self.transformer,\n                self.fake_score_transformer,\n                self.global_rank,\n                self.training_args.output_dir,\n                f\"{step}_weight_only\",\n                only_save_generator_weight=True,\n                generator_ema=self.generator_ema,\n                # MoE support\n                generator_transformer_2=getattr(self, 'transformer_2',\n                                                None),\n                real_score_transformer_2=getattr(\n                    self, 'real_score_transformer_2', None),\n                fake_score_transformer_2=getattr(\n                    self, 'fake_score_transformer_2', None),\n                generator_optimizer_2=getattr(self, 'optimizer_2', None),\n                fake_score_optimizer_2=getattr(self,\n                                               'fake_score_optimizer_2',\n                                               None),\n                generator_scheduler_2=getattr(self, 'lr_scheduler_2', None),\n                fake_score_scheduler_2=getattr(self,\n                                               'fake_score_lr_scheduler_2',\n                                               None),\n                generator_ema_2=getattr(self, 'generator_ema_2', None))\n\n            if self.training_args.use_ema and self.is_ema_ready():\n                self.save_ema_weights(self.training_args.output_dir, step)\n\n        if self.training_args.log_validation and step % self.training_args.validation_steps == 0:\n            if self.training_args.log_visualization:\n                self.visualize_intermediate_latents(training_batch,\n                                                    self.training_args,\n                                                    step)\n            self._log_validation(self.transformer, self.training_args, step)\n\n    self.tracker.finish()\n\n    # Save final training state checkpoint\n    print(\"rank\", self.global_rank,\n          \"save final training state checkpoint at step\",\n          self.training_args.max_train_steps)\n    save_distillation_checkpoint(\n        self.transformer,\n        self.fake_score_transformer,\n        self.global_rank,\n        self.training_args.output_dir,\n        self.training_args.max_train_steps,\n        self.optimizer,\n        self.fake_score_optimizer,\n        self.train_dataloader,\n        self.lr_scheduler,\n        self.fake_score_lr_scheduler,\n        self.noise_random_generator,\n        self.generator_ema,\n        # MoE support\n        generator_transformer_2=getattr(self, 'transformer_2', None),\n        real_score_transformer_2=getattr(self, 'real_score_transformer_2',\n                                         None),\n        fake_score_transformer_2=getattr(self, 'fake_score_transformer_2',\n                                         None),\n        generator_optimizer_2=getattr(self, 'optimizer_2', None),\n        fake_score_optimizer_2=getattr(self, 'fake_score_optimizer_2',\n                                       None),\n        generator_scheduler_2=getattr(self, 'lr_scheduler_2', None),\n        fake_score_scheduler_2=getattr(self, 'fake_score_lr_scheduler_2',\n                                       None),\n        generator_ema_2=getattr(self, 'generator_ema_2', None))\n\n    if self.training_args.use_ema and self.is_ema_ready():\n        self.save_ema_weights(self.training_args.output_dir,\n                              self.training_args.max_train_steps)\n\n    if envs.FASTVIDEO_TORCH_PROFILER_DIR:\n        logger.info(\"Stopping profiler...\")\n        self.profiler_controller.stop()\n        logger.info(\"Profiler stopped.\")\n\n    if get_sp_group():\n        cleanup_dist_env_and_memory()\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.DistillationPipeline.visualize_intermediate_latents","title":"fastvideo.training.DistillationPipeline.visualize_intermediate_latents","text":"<pre><code>visualize_intermediate_latents(\n    training_batch: TrainingBatch,\n    training_args: TrainingArgs,\n    step: int,\n)\n</code></pre> <p>Add visualization data to tracker logging and save frames to disk.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def visualize_intermediate_latents(self, training_batch: TrainingBatch,\n                                   training_args: TrainingArgs, step: int):\n    \"\"\"Add visualization data to tracker logging and save frames to disk.\"\"\"\n    tracker_loss_dict: dict[str, Any] = {}\n    dmd_latents_vis_dict = training_batch.dmd_latent_vis_dict\n    fake_score_latents_vis_dict = training_batch.fake_score_latent_vis_dict\n    fake_score_log_keys = ['generator_pred_video']\n    dmd_log_keys = ['faker_score_pred_video', 'real_score_pred_video']\n\n    for latent_key in fake_score_log_keys:\n        latents = fake_score_latents_vis_dict[latent_key]\n        latents = latents.permute(0, 2, 1, 3, 4)\n\n        if isinstance(self.vae.scaling_factor, torch.Tensor):\n            latents = latents / self.vae.scaling_factor.to(\n                latents.device, latents.dtype)\n        else:\n            latents = latents / self.vae.scaling_factor\n\n        # Apply shifting if needed\n        if (hasattr(self.vae, \"shift_factor\")\n                and self.vae.shift_factor is not None):\n            if isinstance(self.vae.shift_factor, torch.Tensor):\n                latents += self.vae.shift_factor.to(latents.device,\n                                                    latents.dtype)\n            else:\n                latents += self.vae.shift_factor\n            with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n                video = self.vae.decode(latents)\n            video = (video / 2 + 0.5).clamp(0, 1)\n            video = video.cpu().float()\n            video = video.permute(0, 2, 1, 3, 4)\n            video = (video * 255).numpy().astype(np.uint8)\n            video_artifact = self.tracker.video(\n                video, fps=24, format=\"mp4\")  # change to 16 for Wan2.1\n            if video_artifact is not None:\n                tracker_loss_dict[latent_key] = video_artifact\n            # Clean up references\n            del video, latents\n\n    # Process DMD training data if available - use decode_stage instead of self.vae.decode\n    if 'generator_pred_video' in dmd_latents_vis_dict:\n        for latent_key in dmd_log_keys:\n            latents = dmd_latents_vis_dict[latent_key]\n            latents = latents.permute(0, 2, 1, 3, 4)\n            # decoded_latent = decode_stage(ForwardBatch(data_type=\"video\", latents=latents), training_args)\n            if isinstance(self.vae.scaling_factor, torch.Tensor):\n                latents = latents / self.vae.scaling_factor.to(\n                    latents.device, latents.dtype)\n            else:\n                latents = latents / self.vae.scaling_factor\n\n            # Apply shifting if needed\n            if (hasattr(self.vae, \"shift_factor\")\n                    and self.vae.shift_factor is not None):\n                if isinstance(self.vae.shift_factor, torch.Tensor):\n                    latents += self.vae.shift_factor.to(\n                        latents.device, latents.dtype)\n                else:\n                    latents += self.vae.shift_factor\n            with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n                video = self.vae.decode(latents)\n            video = (video / 2 + 0.5).clamp(0, 1)\n            video = video.cpu().float()\n            video = video.permute(0, 2, 1, 3, 4)\n            video = (video * 255).numpy().astype(np.uint8)\n            video_artifact = self.tracker.video(\n                video, fps=24, format=\"mp4\")  # change to 16 for Wan2.1\n            if video_artifact is not None:\n                tracker_loss_dict[latent_key] = video_artifact\n            # Clean up references\n            del video, latents\n\n    # Log to tracker\n    if self.global_rank == 0 and tracker_loss_dict:\n        self.tracker.log_artifacts(tracker_loss_dict, step)\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.TrainingPipeline","title":"fastvideo.training.TrainingPipeline","text":"<pre><code>TrainingPipeline(\n    model_path: str,\n    fastvideo_args: TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>LoRAPipeline</code>, <code>ABC</code></p> <p>A pipeline for training a model. All training pipelines should inherit from this class. All reusable components and code should be implemented in this class.</p> Source code in <code>fastvideo/training/training_pipeline.py</code> <pre><code>def __init__(\n        self,\n        model_path: str,\n        fastvideo_args: TrainingArgs,\n        required_config_modules: list[str] | None = None,\n        loaded_modules: dict[str, torch.nn.Module] | None = None) -&gt; None:\n    fastvideo_args.inference_mode = False\n    self.lora_training = fastvideo_args.lora_training\n    if self.lora_training and fastvideo_args.lora_rank is None:\n        raise ValueError(\"lora rank must be set when using lora training\")\n\n    set_random_seed(fastvideo_args.seed)  # for lora param init\n    super().__init__(model_path, fastvideo_args, required_config_modules,\n                     loaded_modules)  # type: ignore\n    self.tracker = DummyTracker()\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.TrainingPipeline-functions","title":"Functions","text":""},{"location":"api/fastvideo/training/#fastvideo.training.TrainingPipeline.visualize_intermediate_latents","title":"fastvideo.training.TrainingPipeline.visualize_intermediate_latents","text":"<pre><code>visualize_intermediate_latents(\n    training_batch: TrainingBatch,\n    training_args: TrainingArgs,\n    step: int,\n)\n</code></pre> <p>Add visualization data to tracker logging and save frames to disk.</p> Source code in <code>fastvideo/training/training_pipeline.py</code> <pre><code>def visualize_intermediate_latents(self, training_batch: TrainingBatch,\n                                   training_args: TrainingArgs, step: int):\n    \"\"\"Add visualization data to tracker logging and save frames to disk.\"\"\"\n    raise NotImplementedError(\n        \"Visualize intermediate latents is not implemented for training pipeline\"\n    )\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.WanTrainingPipeline","title":"fastvideo.training.WanTrainingPipeline","text":"<pre><code>WanTrainingPipeline(\n    model_path: str,\n    fastvideo_args: TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>TrainingPipeline</code></p> <p>A training pipeline for Wan.</p> Source code in <code>fastvideo/training/training_pipeline.py</code> <pre><code>def __init__(\n        self,\n        model_path: str,\n        fastvideo_args: TrainingArgs,\n        required_config_modules: list[str] | None = None,\n        loaded_modules: dict[str, torch.nn.Module] | None = None) -&gt; None:\n    fastvideo_args.inference_mode = False\n    self.lora_training = fastvideo_args.lora_training\n    if self.lora_training and fastvideo_args.lora_rank is None:\n        raise ValueError(\"lora rank must be set when using lora training\")\n\n    set_random_seed(fastvideo_args.seed)  # for lora param init\n    super().__init__(model_path, fastvideo_args, required_config_modules,\n                     loaded_modules)  # type: ignore\n    self.tracker = DummyTracker()\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.WanTrainingPipeline-functions","title":"Functions","text":""},{"location":"api/fastvideo/training/#fastvideo.training.WanTrainingPipeline.create_training_stages","title":"fastvideo.training.WanTrainingPipeline.create_training_stages","text":"<pre><code>create_training_stages(training_args: TrainingArgs)\n</code></pre> <p>May be used in future refactors.</p> Source code in <code>fastvideo/training/wan_training_pipeline.py</code> <pre><code>def create_training_stages(self, training_args: TrainingArgs):\n    \"\"\"\n    May be used in future refactors.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training-modules","title":"Modules","text":""},{"location":"api/fastvideo/training/#fastvideo.training.distillation_pipeline","title":"fastvideo.training.distillation_pipeline","text":""},{"location":"api/fastvideo/training/#fastvideo.training.distillation_pipeline-classes","title":"Classes","text":""},{"location":"api/fastvideo/training/#fastvideo.training.distillation_pipeline.DistillationPipeline","title":"fastvideo.training.distillation_pipeline.DistillationPipeline","text":"<pre><code>DistillationPipeline(\n    model_path: str,\n    fastvideo_args: TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>TrainingPipeline</code></p> <p>A distillation pipeline for training a 3 step model. Inherits from TrainingPipeline to reuse training infrastructure.</p> Source code in <code>fastvideo/training/training_pipeline.py</code> <pre><code>def __init__(\n        self,\n        model_path: str,\n        fastvideo_args: TrainingArgs,\n        required_config_modules: list[str] | None = None,\n        loaded_modules: dict[str, torch.nn.Module] | None = None) -&gt; None:\n    fastvideo_args.inference_mode = False\n    self.lora_training = fastvideo_args.lora_training\n    if self.lora_training and fastvideo_args.lora_rank is None:\n        raise ValueError(\"lora rank must be set when using lora training\")\n\n    set_random_seed(fastvideo_args.seed)  # for lora param init\n    super().__init__(model_path, fastvideo_args, required_config_modules,\n                     loaded_modules)  # type: ignore\n    self.tracker = DummyTracker()\n</code></pre> Functions\u00b6 fastvideo.training.distillation_pipeline.DistillationPipeline.apply_ema_to_model \u00b6 <pre><code>apply_ema_to_model(model)\n</code></pre> <p>Apply EMA weights to the model for validation or inference.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def apply_ema_to_model(self, model):\n    \"\"\"Apply EMA weights to the model for validation or inference.\"\"\"\n    if model is self.transformer and self.generator_ema is not None:\n        with self.generator_ema.apply_to_model(model):\n            return model\n    elif model is self.transformer_2 and self.generator_ema_2 is not None:\n        with self.generator_ema_2.apply_to_model(model):\n            return model\n    return model\n</code></pre> fastvideo.training.distillation_pipeline.DistillationPipeline.get_ema_2_model_copy \u00b6 <pre><code>get_ema_2_model_copy() -&gt; torch.nn.Module | None\n</code></pre> <p>Get a copy of the transformer_2 model with EMA weights applied.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def get_ema_2_model_copy(self) -&gt; torch.nn.Module | None:\n    \"\"\"Get a copy of the transformer_2 model with EMA weights applied.\"\"\"\n    if self.generator_ema_2 is not None and self.transformer_2 is not None:\n        ema_2_model = copy.deepcopy(self.transformer_2)\n        self.generator_ema_2.copy_to_unwrapped(ema_2_model)\n        return ema_2_model\n    return None\n</code></pre> fastvideo.training.distillation_pipeline.DistillationPipeline.get_ema_model_copy \u00b6 <pre><code>get_ema_model_copy() -&gt; torch.nn.Module | None\n</code></pre> <p>Get a copy of the model with EMA weights applied.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def get_ema_model_copy(self) -&gt; torch.nn.Module | None:\n    \"\"\"Get a copy of the model with EMA weights applied.\"\"\"\n    if self.generator_ema is not None:\n        ema_model = copy.deepcopy(self.transformer)\n        self.generator_ema.copy_to_unwrapped(ema_model)\n        return ema_model\n    return None\n</code></pre> fastvideo.training.distillation_pipeline.DistillationPipeline.get_ema_stats \u00b6 <pre><code>get_ema_stats() -&gt; dict[str, Any]\n</code></pre> <p>Get EMA statistics for monitoring.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def get_ema_stats(self) -&gt; dict[str, Any]:\n    \"\"\"Get EMA statistics for monitoring.\"\"\"\n    ema_enabled = self.generator_ema is not None\n    ema_2_enabled = self.generator_ema_2 is not None\n\n    if not ema_enabled and not ema_2_enabled:\n        return {\n            \"ema_enabled\": False,\n            \"ema_2_enabled\": False,\n            \"ema_decay\": None,\n            \"ema_start_step\": self.training_args.ema_start_step,\n            \"ema_ready\": False,\n            \"ema_2_ready\": False,\n            \"ema_step\": self.current_trainstep,\n        }\n\n    return {\n        \"ema_enabled\": ema_enabled,\n        \"ema_2_enabled\": ema_2_enabled,\n        \"ema_decay\": self.training_args.ema_decay,\n        \"ema_start_step\": self.training_args.ema_start_step,\n        \"ema_ready\": self.is_ema_ready() if ema_enabled else False,\n        \"ema_2_ready\": self.is_ema_ready() if ema_2_enabled else False,\n        \"ema_step\": self.current_trainstep,\n    }\n</code></pre> fastvideo.training.distillation_pipeline.DistillationPipeline.initialize_training_pipeline \u00b6 <pre><code>initialize_training_pipeline(training_args: TrainingArgs)\n</code></pre> <p>Initialize the distillation training pipeline with multiple models.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def initialize_training_pipeline(self, training_args: TrainingArgs):\n    \"\"\"Initialize the distillation training pipeline with multiple models.\"\"\"\n    logger.info(\"Initializing distillation pipeline...\")\n\n    super().initialize_training_pipeline(training_args)\n\n    self.noise_scheduler = self.get_module(\"scheduler\")\n    self.vae = self.get_module(\"vae\")\n    self.vae.requires_grad_(False)\n\n    self.timestep_shift = self.training_args.pipeline_config.flow_shift\n    self.noise_scheduler = FlowMatchEulerDiscreteScheduler(\n        shift=self.timestep_shift)\n\n    if self.training_args.boundary_ratio is not None:\n        self.boundary_timestep = self.training_args.boundary_ratio * self.noise_scheduler.num_train_timesteps\n    else:\n        self.boundary_timestep = None\n\n    if training_args.real_score_model_path:\n        logger.info(\"Loading real score transformer from: %s\",\n                    training_args.real_score_model_path)\n        training_args.override_transformer_cls_name = \"WanTransformer3DModel\"\n        self.real_score_transformer = self.load_module_from_path(\n            training_args.real_score_model_path, \"transformer\",\n            training_args)\n        try:\n            self.real_score_transformer_2 = self.load_module_from_path(\n                training_args.real_score_model_path, \"transformer_2\",\n                training_args)\n            logger.info(\"Loaded real score transformer_2 for MoE support\")\n        except Exception:\n            logger.info(\n                \"real score transformer_2 not found, using single transformer\"\n            )\n            self.real_score_transformer_2 = None\n    else:\n        self.real_score_transformer = self.get_module(\n            \"real_score_transformer\")\n        self.real_score_transformer_2 = self.get_module(\n            \"real_score_transformer_2\")\n\n    if training_args.fake_score_model_path:\n        logger.info(\"Loading fake score transformer from: %s\",\n                    training_args.fake_score_model_path)\n        training_args.override_transformer_cls_name = \"WanTransformer3DModel\"\n        self.fake_score_transformer = self.load_module_from_path(\n            training_args.fake_score_model_path, \"transformer\",\n            training_args)\n        try:\n            self.fake_score_transformer_2 = self.load_module_from_path(\n                training_args.fake_score_model_path, \"transformer_2\",\n                training_args)\n            logger.info(\"Loaded fake score transformer_2 for MoE support\")\n        except Exception:\n            logger.info(\n                \"fake score transformer_2 not found, using single transformer\"\n            )\n            self.fake_score_transformer_2 = None\n    else:\n        self.fake_score_transformer = self.get_module(\n            \"fake_score_transformer\")\n        self.fake_score_transformer_2 = self.get_module(\n            \"fake_score_transformer_2\")\n\n    self.real_score_transformer.requires_grad_(False)\n    self.real_score_transformer.eval()\n    if self.real_score_transformer_2 is not None:\n        self.real_score_transformer_2.requires_grad_(False)\n        self.real_score_transformer_2.eval()\n\n    # Set training modes for fake score transformers (trainable)\n    self.fake_score_transformer.requires_grad_(True)\n    self.fake_score_transformer.train()\n    if self.fake_score_transformer_2 is not None:\n        self.fake_score_transformer_2.requires_grad_(True)\n        self.fake_score_transformer_2.train()\n\n    if training_args.enable_gradient_checkpointing_type is not None:\n        self.fake_score_transformer = apply_activation_checkpointing(\n            self.fake_score_transformer,\n            checkpointing_type=training_args.\n            enable_gradient_checkpointing_type)\n        if self.fake_score_transformer_2 is not None:\n            self.fake_score_transformer_2 = apply_activation_checkpointing(\n                self.fake_score_transformer_2,\n                checkpointing_type=training_args.\n                enable_gradient_checkpointing_type)\n\n        self.real_score_transformer = apply_activation_checkpointing(\n            self.real_score_transformer,\n            checkpointing_type=training_args.\n            enable_gradient_checkpointing_type)\n        if self.real_score_transformer_2 is not None:\n            self.real_score_transformer_2 = apply_activation_checkpointing(\n                self.real_score_transformer_2,\n                checkpointing_type=training_args.\n                enable_gradient_checkpointing_type)\n\n    # Initialize optimizers\n    fake_score_params = list(\n        filter(lambda p: p.requires_grad,\n               self.fake_score_transformer.parameters()))\n\n    # Use separate learning rate for fake_score_transformer if specified\n    fake_score_lr = training_args.fake_score_learning_rate\n    if fake_score_lr == 0.0:\n        fake_score_lr = training_args.learning_rate\n\n    betas_str = training_args.fake_score_betas\n    betas = tuple(float(x.strip()) for x in betas_str.split(\",\"))\n\n    self.fake_score_optimizer = torch.optim.AdamW(\n        fake_score_params,\n        lr=fake_score_lr,\n        betas=betas,\n        weight_decay=training_args.weight_decay,\n        eps=1e-8,\n    )\n\n    self.fake_score_lr_scheduler = get_scheduler(\n        training_args.fake_score_lr_scheduler,\n        optimizer=self.fake_score_optimizer,\n        num_warmup_steps=training_args.lr_warmup_steps,\n        num_training_steps=training_args.max_train_steps,\n        num_cycles=training_args.lr_num_cycles,\n        power=training_args.lr_power,\n        min_lr_ratio=training_args.min_lr_ratio,\n        last_epoch=self.init_steps - 1,\n    )\n\n    if self.fake_score_transformer_2 is not None:\n        fake_score_params_2 = list(\n            filter(lambda p: p.requires_grad,\n                   self.fake_score_transformer_2.parameters()))\n        self.fake_score_optimizer_2 = torch.optim.AdamW(\n            fake_score_params_2,\n            lr=fake_score_lr,\n            betas=betas,\n            weight_decay=training_args.weight_decay,\n            eps=1e-8,\n        )\n        self.fake_score_lr_scheduler_2 = get_scheduler(\n            training_args.fake_score_lr_scheduler,\n            optimizer=self.fake_score_optimizer_2,\n            num_warmup_steps=training_args.lr_warmup_steps,\n            num_training_steps=training_args.max_train_steps,\n            num_cycles=training_args.lr_num_cycles,\n            power=training_args.lr_power,\n            min_lr_ratio=training_args.min_lr_ratio,\n            last_epoch=self.init_steps - 1,\n        )\n\n    logger.info(\n        \"Distillation optimizers initialized: generator and fake_score\")\n\n    self.generator_update_interval = self.training_args.generator_update_interval\n    logger.info(\n        \"Distillation pipeline initialized with generator_update_interval=%s\",\n        self.generator_update_interval)\n\n    self.denoising_step_list = torch.tensor(\n        self.training_args.pipeline_config.dmd_denoising_steps,\n        dtype=torch.long,\n        device=get_local_torch_device())\n\n    if training_args.warp_denoising_step:  # Warp the denoising step according to the scheduler time shift\n        timesteps = torch.cat((self.noise_scheduler.timesteps.cpu(),\n                               torch.tensor([0],\n                                            dtype=torch.float32))).cuda()\n        self.denoising_step_list = timesteps[1000 -\n                                             self.denoising_step_list]\n        logger.info(\"Warping denoising_step_list\")\n\n    self.denoising_step_list = self.denoising_step_list.to(\n        get_local_torch_device())\n    logger.info(\"Distillation generator model to %s denoising steps: %s\",\n                len(self.denoising_step_list), self.denoising_step_list)\n    self.num_train_timestep = self.noise_scheduler.num_train_timesteps\n\n    self.min_timestep = int(self.training_args.min_timestep_ratio *\n                            self.num_train_timestep)\n    self.max_timestep = int(self.training_args.max_timestep_ratio *\n                            self.num_train_timestep)\n\n    self.real_score_guidance_scale = self.training_args.real_score_guidance_scale\n\n    self.generator_ema: EMA_FSDP | None = None\n    self.generator_ema_2: EMA_FSDP | None = None\n    if (self.training_args.ema_decay\n            is not None) and (self.training_args.ema_decay &gt; 0.0):\n        self.generator_ema = EMA_FSDP(self.transformer,\n                                      decay=self.training_args.ema_decay)\n        logger.info(\"Initialized generator EMA with decay=%s\",\n                    self.training_args.ema_decay)\n\n        # Initialize EMA for transformer_2 if it exists\n        if self.transformer_2 is not None:\n            self.generator_ema_2 = EMA_FSDP(\n                self.transformer_2, decay=self.training_args.ema_decay)\n            logger.info(\"Initialized generator EMA_2 with decay=%s\",\n                        self.training_args.ema_decay)\n    else:\n        logger.info(\"Generator EMA disabled (ema_decay &lt;= 0.0)\")\n</code></pre> fastvideo.training.distillation_pipeline.DistillationPipeline.initialize_validation_pipeline <code>abstractmethod</code> \u00b6 <pre><code>initialize_validation_pipeline(training_args: TrainingArgs)\n</code></pre> <p>Initialize validation pipeline - must be implemented by subclasses.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>@abstractmethod\ndef initialize_validation_pipeline(self, training_args: TrainingArgs):\n    \"\"\"Initialize validation pipeline - must be implemented by subclasses.\"\"\"\n    raise NotImplementedError(\n        \"Distillation pipelines must implement this method\")\n</code></pre> fastvideo.training.distillation_pipeline.DistillationPipeline.is_ema_ready \u00b6 <pre><code>is_ema_ready(current_step: int | None = None)\n</code></pre> <p>Check if EMA is ready for use (after ema_start_step).</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def is_ema_ready(self, current_step: int | None = None):\n    \"\"\"Check if EMA is ready for use (after ema_start_step).\"\"\"\n    if current_step is None:\n        current_step = getattr(self, 'current_trainstep', 0)\n    return (self.generator_ema is not None\n            and current_step &gt;= self.training_args.ema_start_step)\n</code></pre> fastvideo.training.distillation_pipeline.DistillationPipeline.load_module_from_path \u00b6 <pre><code>load_module_from_path(\n    model_path: str,\n    module_type: str,\n    training_args: TrainingArgs,\n)\n</code></pre> <p>Load a module from a specific path using the same loading logic as the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the model</p> required <code>module_type</code> <code>str</code> <p>Type of module to load (e.g., \"transformer\")</p> required <code>training_args</code> <code>TrainingArgs</code> <p>Training arguments</p> required <p>Returns:</p> Type Description <p>The loaded module</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def load_module_from_path(self, model_path: str, module_type: str,\n                          training_args: \"TrainingArgs\"):\n    \"\"\"\n    Load a module from a specific path using the same loading logic as the pipeline.\n\n    Args:\n        model_path: Path to the model\n        module_type: Type of module to load (e.g., \"transformer\")\n        training_args: Training arguments\n\n    Returns:\n        The loaded module\n    \"\"\"\n    logger.info(\"Loading %s from custom path: %s\", module_type, model_path)\n    # Set flag to prevent custom weight loading for teacher/critic models\n    training_args._loading_teacher_critic_model = True\n\n    try:\n        from fastvideo.models.loader.component_loader import (\n            PipelineComponentLoader)\n\n        # Download the model if it's a Hugging Face model ID\n        local_model_path = maybe_download_model(model_path)\n        logger.info(\"Model downloaded/found at: %s\", local_model_path)\n        config = verify_model_config_and_directory(local_model_path)\n\n        if module_type not in config:\n            if hasattr(self, '_extra_config_module_map'\n                       ) and module_type in self._extra_config_module_map:\n                extra_module = self._extra_config_module_map[module_type]\n                if extra_module in config:\n                    module_type = extra_module\n                    logger.info(\"Using %s for %s\", extra_module,\n                                module_type)\n                else:\n                    raise ValueError(\n                        f\"Module {module_type} not found in config at {local_model_path}\"\n                    )\n            else:\n                raise ValueError(\n                    f\"Module {module_type} not found in config at {local_model_path}\"\n                )\n\n        module_info = config[module_type]\n        if module_info is None:\n            raise ValueError(\n                f\"Module {module_type} has null value in config at {local_model_path}\"\n            )\n\n        transformers_or_diffusers, architecture = module_info\n        component_path = os.path.join(local_model_path, module_type)\n        module = PipelineComponentLoader.load_module(\n            module_name=module_type,\n            component_model_path=component_path,\n            transformers_or_diffusers=transformers_or_diffusers,\n            fastvideo_args=training_args,\n        )\n\n        logger.info(\"Successfully loaded %s from %s\", module_type,\n                    component_path)\n        return module\n    finally:\n        # Always clean up the flag\n        if hasattr(training_args, '_loading_teacher_critic_model'):\n            delattr(training_args, '_loading_teacher_critic_model')\n</code></pre> fastvideo.training.distillation_pipeline.DistillationPipeline.reset_ema \u00b6 <pre><code>reset_ema()\n</code></pre> <p>Reset EMA to current model weights.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def reset_ema(self):\n    \"\"\"Reset EMA to current model weights.\"\"\"\n    if self.generator_ema is not None:\n        logger.info(\"Resetting EMA to current model weights\")\n        self.generator_ema.update(self.transformer)\n        # Force update to current weights by setting decay to 0 temporarily\n        original_decay = self.generator_ema.decay\n        self.generator_ema.decay = 0.0\n        self.generator_ema.update(self.transformer)\n        self.generator_ema.decay = original_decay\n        logger.info(\"EMA reset completed\")\n    else:\n        logger.warning(\"Cannot reset EMA: EMA not initialized\")\n\n    if self.generator_ema_2 is not None:\n        logger.info(\"Resetting EMA_2 to current model weights\")\n        self.generator_ema_2.update(self.transformer_2)\n        # Force update to current weights by setting decay to 0 temporarily\n        original_decay_2 = self.generator_ema_2.decay\n        self.generator_ema_2.decay = 0.0\n        self.generator_ema_2.update(self.transformer_2)\n        self.generator_ema_2.decay = original_decay_2\n        logger.info(\"EMA_2 reset completed\")\n</code></pre> fastvideo.training.distillation_pipeline.DistillationPipeline.save_ema_weights \u00b6 <pre><code>save_ema_weights(output_dir: str, step: int)\n</code></pre> <p>Save EMA weights separately for inference purposes.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def save_ema_weights(self, output_dir: str, step: int):\n    \"\"\"Save EMA weights separately for inference purposes.\"\"\"\n    if self.generator_ema is None and self.generator_ema_2 is None:\n        logger.warning(\"Cannot save EMA weights: No EMA initialized\")\n        return\n\n    if not self.is_ema_ready():\n        logger.warning(\n            \"Cannot save EMA weights: EMA not ready yet (step &lt; ema_start_step)\"\n        )\n        return\n\n    try:\n        # Save main transformer EMA\n        if self.generator_ema is not None:\n            ema_model = self.get_ema_model_copy()\n            if ema_model is None:\n                logger.warning(\"Failed to create EMA model copy\")\n            else:\n                ema_save_dir = os.path.join(output_dir,\n                                            f\"ema_checkpoint-{step}\")\n                os.makedirs(ema_save_dir, exist_ok=True)\n\n                # save as diffusers format\n                from safetensors.torch import save_file\n\n                from fastvideo.training.training_utils import (\n                    custom_to_hf_state_dict, gather_state_dict_on_cpu_rank0)\n                cpu_state = gather_state_dict_on_cpu_rank0(ema_model,\n                                                           device=None)\n\n                if self.global_rank == 0:\n                    weight_path = os.path.join(\n                        ema_save_dir, \"diffusion_pytorch_model.safetensors\")\n                    diffusers_state_dict = custom_to_hf_state_dict(\n                        cpu_state, ema_model.reverse_param_names_mapping)\n                    save_file(diffusers_state_dict, weight_path)\n\n                    config_dict = ema_model.hf_config\n                    if \"dtype\" in config_dict:\n                        del config_dict[\"dtype\"]\n                    config_path = os.path.join(ema_save_dir, \"config.json\")\n                    with open(config_path, \"w\") as f:\n                        json.dump(config_dict, f, indent=4)\n\n                    logger.info(\"EMA weights saved to %s\", weight_path)\n\n                del ema_model\n\n        # Save transformer_2 EMA\n        if self.generator_ema_2 is not None:\n            ema_2_model = self.get_ema_2_model_copy()\n            if ema_2_model is None:\n                logger.warning(\"Failed to create EMA_2 model copy\")\n            else:\n                ema_2_save_dir = os.path.join(output_dir,\n                                              f\"ema_2_checkpoint-{step}\")\n                os.makedirs(ema_2_save_dir, exist_ok=True)\n\n                # save as diffusers format\n                from safetensors.torch import save_file\n\n                from fastvideo.training.training_utils import (\n                    custom_to_hf_state_dict, gather_state_dict_on_cpu_rank0)\n                cpu_state_2 = gather_state_dict_on_cpu_rank0(ema_2_model,\n                                                             device=None)\n\n                if self.global_rank == 0:\n                    weight_path_2 = os.path.join(\n                        ema_2_save_dir,\n                        \"diffusion_pytorch_model.safetensors\")\n                    diffusers_state_dict_2 = custom_to_hf_state_dict(\n                        cpu_state_2,\n                        ema_2_model.reverse_param_names_mapping)\n                    save_file(diffusers_state_dict_2, weight_path_2)\n\n                    config_dict_2 = ema_2_model.hf_config\n                    if \"dtype\" in config_dict_2:\n                        del config_dict_2[\"dtype\"]\n                    config_path_2 = os.path.join(ema_2_save_dir,\n                                                 \"config.json\")\n                    with open(config_path_2, \"w\") as f:\n                        json.dump(config_dict_2, f, indent=4)\n\n                    logger.info(\"EMA_2 weights saved to %s\", weight_path_2)\n\n                del ema_2_model\n\n    except Exception as e:\n        logger.error(\"Failed to save EMA weights: %s\", str(e))\n</code></pre> fastvideo.training.distillation_pipeline.DistillationPipeline.train \u00b6 <pre><code>train() -&gt; None\n</code></pre> <p>Main training loop with distillation-specific logging.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def train(self) -&gt; None:\n    \"\"\"Main training loop with distillation-specific logging.\"\"\"\n    assert self.training_args.seed is not None, \"seed must be set\"\n    seed = self.training_args.seed\n\n    # Set the same seed within each SP group to ensure reproducibility\n    if self.sp_world_size &gt; 1:\n        # Use the same seed for all processes within the same SP group\n        sp_group_seed = seed + (self.global_rank // self.sp_world_size)\n        set_random_seed(sp_group_seed)\n        logger.info(\"Rank %s: Using SP group seed %s\", self.global_rank,\n                    sp_group_seed)\n    else:\n        set_random_seed(seed + self.global_rank)\n\n    # Set random seeds for deterministic training\n    self.noise_random_generator = torch.Generator(device=\"cpu\").manual_seed(\n        self.seed)\n    self.noise_gen_cuda = torch.Generator(device=\"cuda\").manual_seed(\n        self.seed)\n    self.validation_random_generator = torch.Generator(\n        device=\"cpu\").manual_seed(self.seed)\n    logger.info(\"Initialized random seeds with seed: %s\", seed)\n\n    # Initialize current_trainstep for EMA ready checks\n    #TODO: check if needed\n    self.current_trainstep = self.init_steps\n\n    # Resume from checkpoint if specified (this will restore random states)\n    if self.training_args.resume_from_checkpoint:\n        self._resume_from_checkpoint()\n        logger.info(\"Resumed from checkpoint, random states restored\")\n    else:\n        logger.info(\"Starting training from scratch\")\n\n    self.train_loader_iter = iter(self.train_dataloader)\n\n    step_times: deque[float] = deque(maxlen=100)\n\n    self._log_training_info()\n    self._log_validation(self.transformer, self.training_args,\n                         self.init_steps)\n\n    progress_bar = tqdm(\n        range(0, self.training_args.max_train_steps),\n        initial=self.init_steps,\n        desc=\"Steps\",\n        disable=self.local_rank &gt; 0,\n    )\n\n    use_vsa = vsa_available and envs.FASTVIDEO_ATTENTION_BACKEND == \"VIDEO_SPARSE_ATTN\"\n    for step in range(self.init_steps + 1,\n                      self.training_args.max_train_steps + 1):\n        start_time = time.perf_counter()\n        if use_vsa:\n            vsa_sparsity = self.training_args.VSA_sparsity\n            vsa_decay_rate = self.training_args.VSA_decay_rate\n            vsa_decay_interval_steps = self.training_args.VSA_decay_interval_steps\n            if vsa_decay_interval_steps &gt; 1:\n                current_decay_times = min(step // vsa_decay_interval_steps,\n                                          vsa_sparsity // vsa_decay_rate)\n                current_vsa_sparsity = current_decay_times * vsa_decay_rate\n            else:\n                current_vsa_sparsity = vsa_sparsity\n        else:\n            current_vsa_sparsity = 0.0\n\n        training_batch = TrainingBatch()\n        self.current_trainstep = step\n        training_batch.current_vsa_sparsity = current_vsa_sparsity\n\n        if (step &gt;= self.training_args.ema_start_step) and \\\n                (self.generator_ema is None) and (self.training_args.ema_decay &gt; 0):\n            self.generator_ema = EMA_FSDP(\n                self.transformer, decay=self.training_args.ema_decay)\n            logger.info(\"Created generator EMA at step %s with decay=%s\",\n                        step, self.training_args.ema_decay)\n\n            # Create EMA for transformer_2 if it exists\n            if self.transformer_2 is not None and self.generator_ema_2 is None:\n                self.generator_ema_2 = EMA_FSDP(\n                    self.transformer_2, decay=self.training_args.ema_decay)\n                logger.info(\n                    \"Created generator EMA_2 at step %s with decay=%s\",\n                    step, self.training_args.ema_decay)\n\n        with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n            training_batch = self.train_one_step(training_batch)\n\n        total_loss = training_batch.total_loss\n        generator_loss = training_batch.generator_loss\n        fake_score_loss = training_batch.fake_score_loss\n        grad_norm = training_batch.grad_norm\n\n        step_time = time.perf_counter() - start_time\n        step_times.append(step_time)\n        avg_step_time = sum(step_times) / len(step_times)\n\n        progress_bar.set_postfix({\n            \"total_loss\":\n            f\"{total_loss:.4f}\",\n            \"generator_loss\":\n            f\"{generator_loss:.4f}\",\n            \"fake_score_loss\":\n            f\"{fake_score_loss:.4f}\",\n            \"step_time\":\n            f\"{step_time:.2f}s\",\n            \"grad_norm\":\n            grad_norm,\n            \"ema\":\n            \"\u2713\" if (self.generator_ema is not None and self.is_ema_ready())\n            else \"\u2717\",\n            \"ema2\":\n            \"\u2713\" if (self.generator_ema_2 is not None\n                    and self.is_ema_ready()) else \"\u2717\",\n        })\n        progress_bar.update(1)\n\n        if self.global_rank == 0:\n            # Prepare logging data\n            log_data = {\n                \"train_total_loss\":\n                total_loss,\n                \"train_fake_score_loss\":\n                fake_score_loss,\n                \"learning_rate\":\n                self.lr_scheduler.get_last_lr()[0],\n                \"fake_score_learning_rate\":\n                self.fake_score_lr_scheduler.get_last_lr()[0],\n                \"step_time\":\n                step_time,\n                \"avg_step_time\":\n                avg_step_time,\n                \"grad_norm\":\n                grad_norm,\n            }\n            # Only log generator loss when generator is actually trained\n            if (step % self.generator_update_interval == 0):\n                log_data[\"train_generator_loss\"] = generator_loss\n            if use_vsa:\n                log_data[\"VSA_train_sparsity\"] = current_vsa_sparsity\n\n            if self.generator_ema is not None or self.generator_ema_2 is not None:\n                log_data[\"ema_enabled\"] = self.generator_ema is not None\n                log_data[\"ema_2_enabled\"] = self.generator_ema_2 is not None\n                log_data[\"ema_decay\"] = self.training_args.ema_decay\n            else:\n                log_data[\"ema_enabled\"] = False\n                log_data[\"ema_2_enabled\"] = False\n\n            ema_stats = self.get_ema_stats()\n            log_data.update(ema_stats)\n\n            if training_batch.dmd_latent_vis_dict:\n                dmd_additional_logs = {\n                    \"generator_timestep\":\n                    training_batch.\n                    dmd_latent_vis_dict[\"generator_timestep\"].item(),\n                    \"dmd_timestep\":\n                    training_batch.dmd_latent_vis_dict[\"dmd_timestep\"].item(\n                    ),\n                }\n                log_data.update(dmd_additional_logs)\n\n            faker_score_additional_logs = {\n                \"fake_score_timestep\":\n                training_batch.\n                fake_score_latent_vis_dict[\"fake_score_timestep\"].item(),\n            }\n            log_data.update(faker_score_additional_logs)\n\n            self.tracker.log(log_data, step)\n\n        # Save training state checkpoint (for resuming training)\n        if (self.training_args.training_state_checkpointing_steps &gt; 0\n                and step %\n                self.training_args.training_state_checkpointing_steps == 0):\n            print(\"rank\", self.global_rank,\n                  \"save training state checkpoint at step\", step)\n            save_distillation_checkpoint(\n                self.transformer,\n                self.fake_score_transformer,\n                self.global_rank,\n                self.training_args.output_dir,\n                step,\n                self.optimizer,\n                self.fake_score_optimizer,\n                self.train_dataloader,\n                self.lr_scheduler,\n                self.fake_score_lr_scheduler,\n                self.noise_random_generator,\n                self.generator_ema,\n                # MoE support\n                generator_transformer_2=getattr(self, 'transformer_2',\n                                                None),\n                real_score_transformer_2=getattr(\n                    self, 'real_score_transformer_2', None),\n                fake_score_transformer_2=getattr(\n                    self, 'fake_score_transformer_2', None),\n                generator_optimizer_2=getattr(self, 'optimizer_2', None),\n                fake_score_optimizer_2=getattr(self,\n                                               'fake_score_optimizer_2',\n                                               None),\n                generator_scheduler_2=getattr(self, 'lr_scheduler_2', None),\n                fake_score_scheduler_2=getattr(self,\n                                               'fake_score_lr_scheduler_2',\n                                               None),\n                generator_ema_2=getattr(self, 'generator_ema_2', None))\n\n            if self.transformer:\n                self.transformer.train()\n            self.sp_group.barrier()\n\n        # Save weight-only checkpoint\n        if (self.training_args.weight_only_checkpointing_steps &gt; 0\n                and step %\n                self.training_args.weight_only_checkpointing_steps == 0):\n            print(\"rank\", self.global_rank,\n                  \"save weight-only checkpoint at step\", step)\n            save_distillation_checkpoint(\n                self.transformer,\n                self.fake_score_transformer,\n                self.global_rank,\n                self.training_args.output_dir,\n                f\"{step}_weight_only\",\n                only_save_generator_weight=True,\n                generator_ema=self.generator_ema,\n                # MoE support\n                generator_transformer_2=getattr(self, 'transformer_2',\n                                                None),\n                real_score_transformer_2=getattr(\n                    self, 'real_score_transformer_2', None),\n                fake_score_transformer_2=getattr(\n                    self, 'fake_score_transformer_2', None),\n                generator_optimizer_2=getattr(self, 'optimizer_2', None),\n                fake_score_optimizer_2=getattr(self,\n                                               'fake_score_optimizer_2',\n                                               None),\n                generator_scheduler_2=getattr(self, 'lr_scheduler_2', None),\n                fake_score_scheduler_2=getattr(self,\n                                               'fake_score_lr_scheduler_2',\n                                               None),\n                generator_ema_2=getattr(self, 'generator_ema_2', None))\n\n            if self.training_args.use_ema and self.is_ema_ready():\n                self.save_ema_weights(self.training_args.output_dir, step)\n\n        if self.training_args.log_validation and step % self.training_args.validation_steps == 0:\n            if self.training_args.log_visualization:\n                self.visualize_intermediate_latents(training_batch,\n                                                    self.training_args,\n                                                    step)\n            self._log_validation(self.transformer, self.training_args, step)\n\n    self.tracker.finish()\n\n    # Save final training state checkpoint\n    print(\"rank\", self.global_rank,\n          \"save final training state checkpoint at step\",\n          self.training_args.max_train_steps)\n    save_distillation_checkpoint(\n        self.transformer,\n        self.fake_score_transformer,\n        self.global_rank,\n        self.training_args.output_dir,\n        self.training_args.max_train_steps,\n        self.optimizer,\n        self.fake_score_optimizer,\n        self.train_dataloader,\n        self.lr_scheduler,\n        self.fake_score_lr_scheduler,\n        self.noise_random_generator,\n        self.generator_ema,\n        # MoE support\n        generator_transformer_2=getattr(self, 'transformer_2', None),\n        real_score_transformer_2=getattr(self, 'real_score_transformer_2',\n                                         None),\n        fake_score_transformer_2=getattr(self, 'fake_score_transformer_2',\n                                         None),\n        generator_optimizer_2=getattr(self, 'optimizer_2', None),\n        fake_score_optimizer_2=getattr(self, 'fake_score_optimizer_2',\n                                       None),\n        generator_scheduler_2=getattr(self, 'lr_scheduler_2', None),\n        fake_score_scheduler_2=getattr(self, 'fake_score_lr_scheduler_2',\n                                       None),\n        generator_ema_2=getattr(self, 'generator_ema_2', None))\n\n    if self.training_args.use_ema and self.is_ema_ready():\n        self.save_ema_weights(self.training_args.output_dir,\n                              self.training_args.max_train_steps)\n\n    if envs.FASTVIDEO_TORCH_PROFILER_DIR:\n        logger.info(\"Stopping profiler...\")\n        self.profiler_controller.stop()\n        logger.info(\"Profiler stopped.\")\n\n    if get_sp_group():\n        cleanup_dist_env_and_memory()\n</code></pre> fastvideo.training.distillation_pipeline.DistillationPipeline.visualize_intermediate_latents \u00b6 <pre><code>visualize_intermediate_latents(\n    training_batch: TrainingBatch,\n    training_args: TrainingArgs,\n    step: int,\n)\n</code></pre> <p>Add visualization data to tracker logging and save frames to disk.</p> Source code in <code>fastvideo/training/distillation_pipeline.py</code> <pre><code>def visualize_intermediate_latents(self, training_batch: TrainingBatch,\n                                   training_args: TrainingArgs, step: int):\n    \"\"\"Add visualization data to tracker logging and save frames to disk.\"\"\"\n    tracker_loss_dict: dict[str, Any] = {}\n    dmd_latents_vis_dict = training_batch.dmd_latent_vis_dict\n    fake_score_latents_vis_dict = training_batch.fake_score_latent_vis_dict\n    fake_score_log_keys = ['generator_pred_video']\n    dmd_log_keys = ['faker_score_pred_video', 'real_score_pred_video']\n\n    for latent_key in fake_score_log_keys:\n        latents = fake_score_latents_vis_dict[latent_key]\n        latents = latents.permute(0, 2, 1, 3, 4)\n\n        if isinstance(self.vae.scaling_factor, torch.Tensor):\n            latents = latents / self.vae.scaling_factor.to(\n                latents.device, latents.dtype)\n        else:\n            latents = latents / self.vae.scaling_factor\n\n        # Apply shifting if needed\n        if (hasattr(self.vae, \"shift_factor\")\n                and self.vae.shift_factor is not None):\n            if isinstance(self.vae.shift_factor, torch.Tensor):\n                latents += self.vae.shift_factor.to(latents.device,\n                                                    latents.dtype)\n            else:\n                latents += self.vae.shift_factor\n            with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n                video = self.vae.decode(latents)\n            video = (video / 2 + 0.5).clamp(0, 1)\n            video = video.cpu().float()\n            video = video.permute(0, 2, 1, 3, 4)\n            video = (video * 255).numpy().astype(np.uint8)\n            video_artifact = self.tracker.video(\n                video, fps=24, format=\"mp4\")  # change to 16 for Wan2.1\n            if video_artifact is not None:\n                tracker_loss_dict[latent_key] = video_artifact\n            # Clean up references\n            del video, latents\n\n    # Process DMD training data if available - use decode_stage instead of self.vae.decode\n    if 'generator_pred_video' in dmd_latents_vis_dict:\n        for latent_key in dmd_log_keys:\n            latents = dmd_latents_vis_dict[latent_key]\n            latents = latents.permute(0, 2, 1, 3, 4)\n            # decoded_latent = decode_stage(ForwardBatch(data_type=\"video\", latents=latents), training_args)\n            if isinstance(self.vae.scaling_factor, torch.Tensor):\n                latents = latents / self.vae.scaling_factor.to(\n                    latents.device, latents.dtype)\n            else:\n                latents = latents / self.vae.scaling_factor\n\n            # Apply shifting if needed\n            if (hasattr(self.vae, \"shift_factor\")\n                    and self.vae.shift_factor is not None):\n                if isinstance(self.vae.shift_factor, torch.Tensor):\n                    latents += self.vae.shift_factor.to(\n                        latents.device, latents.dtype)\n                else:\n                    latents += self.vae.shift_factor\n            with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n                video = self.vae.decode(latents)\n            video = (video / 2 + 0.5).clamp(0, 1)\n            video = video.cpu().float()\n            video = video.permute(0, 2, 1, 3, 4)\n            video = (video * 255).numpy().astype(np.uint8)\n            video_artifact = self.tracker.video(\n                video, fps=24, format=\"mp4\")  # change to 16 for Wan2.1\n            if video_artifact is not None:\n                tracker_loss_dict[latent_key] = video_artifact\n            # Clean up references\n            del video, latents\n\n    # Log to tracker\n    if self.global_rank == 0 and tracker_loss_dict:\n        self.tracker.log_artifacts(tracker_loss_dict, step)\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.distillation_pipeline-functions","title":"Functions","text":""},{"location":"api/fastvideo/training/#fastvideo.training.ode_causal_pipeline","title":"fastvideo.training.ode_causal_pipeline","text":""},{"location":"api/fastvideo/training/#fastvideo.training.ode_causal_pipeline-classes","title":"Classes","text":""},{"location":"api/fastvideo/training/#fastvideo.training.ode_causal_pipeline.ODEInitTrainingPipeline","title":"fastvideo.training.ode_causal_pipeline.ODEInitTrainingPipeline","text":"<pre><code>ODEInitTrainingPipeline(\n    model_path: str,\n    fastvideo_args: TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>TrainingPipeline</code></p> <p>Training pipeline for ODE-init using precomputed denoising trajectories.</p> <p>Supervision: predict the next latent in the stored trajectory by - feeding current latent at timestep t into the transformer to predict noise - stepping the scheduler with the predicted noise - minimizing MSE to the stored next latent at timestep t_next</p> Source code in <code>fastvideo/training/training_pipeline.py</code> <pre><code>def __init__(\n        self,\n        model_path: str,\n        fastvideo_args: TrainingArgs,\n        required_config_modules: list[str] | None = None,\n        loaded_modules: dict[str, torch.nn.Module] | None = None) -&gt; None:\n    fastvideo_args.inference_mode = False\n    self.lora_training = fastvideo_args.lora_training\n    if self.lora_training and fastvideo_args.lora_rank is None:\n        raise ValueError(\"lora rank must be set when using lora training\")\n\n    set_random_seed(fastvideo_args.seed)  # for lora param init\n    super().__init__(model_path, fastvideo_args, required_config_modules,\n                     loaded_modules)  # type: ignore\n    self.tracker = DummyTracker()\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.ode_causal_pipeline-functions","title":"Functions","text":""},{"location":"api/fastvideo/training/#fastvideo.training.self_forcing_distillation_pipeline","title":"fastvideo.training.self_forcing_distillation_pipeline","text":""},{"location":"api/fastvideo/training/#fastvideo.training.self_forcing_distillation_pipeline-classes","title":"Classes","text":""},{"location":"api/fastvideo/training/#fastvideo.training.self_forcing_distillation_pipeline.SelfForcingDistillationPipeline","title":"fastvideo.training.self_forcing_distillation_pipeline.SelfForcingDistillationPipeline","text":"<pre><code>SelfForcingDistillationPipeline(\n    model_path: str,\n    fastvideo_args: TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>DistillationPipeline</code></p> <p>A self-forcing distillation pipeline that alternates between training the generator and critic based on the self-forcing methodology.</p> <p>This implementation follows the self-forcing approach where: 1. Generator and critic are trained in alternating steps 2. Generator loss uses DMD-style loss with the critic as fake score 3. Critic loss trains the fake score model to distinguish real vs fake</p> Source code in <code>fastvideo/training/training_pipeline.py</code> <pre><code>def __init__(\n        self,\n        model_path: str,\n        fastvideo_args: TrainingArgs,\n        required_config_modules: list[str] | None = None,\n        loaded_modules: dict[str, torch.nn.Module] | None = None) -&gt; None:\n    fastvideo_args.inference_mode = False\n    self.lora_training = fastvideo_args.lora_training\n    if self.lora_training and fastvideo_args.lora_rank is None:\n        raise ValueError(\"lora rank must be set when using lora training\")\n\n    set_random_seed(fastvideo_args.seed)  # for lora param init\n    super().__init__(model_path, fastvideo_args, required_config_modules,\n                     loaded_modules)  # type: ignore\n    self.tracker = DummyTracker()\n</code></pre> Functions\u00b6 fastvideo.training.self_forcing_distillation_pipeline.SelfForcingDistillationPipeline.critic_loss \u00b6 <pre><code>critic_loss(\n    training_batch: TrainingBatch,\n) -&gt; tuple[torch.Tensor, dict[str, Any]]\n</code></pre> <p>Compute critic loss using flow matching between noise and generator output. The critic learns to predict the flow from noise to the generator's output.</p> Source code in <code>fastvideo/training/self_forcing_distillation_pipeline.py</code> <pre><code>def critic_loss(\n        self, training_batch: TrainingBatch\n) -&gt; tuple[torch.Tensor, dict[str, Any]]:\n    \"\"\"\n    Compute critic loss using flow matching between noise and generator output.\n    The critic learns to predict the flow from noise to the generator's output.\n    \"\"\"\n    updated_batch, flow_matching_loss = self.faker_score_forward(\n        training_batch)\n    training_batch.fake_score_latent_vis_dict = updated_batch.fake_score_latent_vis_dict\n    log_dict: dict[str, Any] = {}\n\n    return flow_matching_loss, log_dict\n</code></pre> fastvideo.training.self_forcing_distillation_pipeline.SelfForcingDistillationPipeline.generate_and_sync_list \u00b6 <pre><code>generate_and_sync_list(\n    num_blocks: int,\n    num_denoising_steps: int,\n    device: device,\n) -&gt; list[int]\n</code></pre> <p>Generate and synchronize random exit flags across distributed processes.</p> Source code in <code>fastvideo/training/self_forcing_distillation_pipeline.py</code> <pre><code>def generate_and_sync_list(self, num_blocks: int, num_denoising_steps: int,\n                           device: torch.device) -&gt; list[int]:\n    \"\"\"Generate and synchronize random exit flags across distributed processes.\"\"\"\n    logger.info(\n        \"RANK: %s, enter generate_and_sync_list blocks=%s steps=%s device=%s\",\n        self.global_rank,\n        num_blocks,\n        num_denoising_steps,\n        str(device),\n        local_main_process_only=False)\n    rank = dist.get_rank() if dist.is_initialized() else 0\n\n    if rank == 0:\n        # Generate random indices\n        indices = torch.randint(low=0,\n                                high=num_denoising_steps,\n                                size=(num_blocks, ),\n                                device=device)\n        if self.last_step_only:\n            indices = torch.ones_like(indices) * (num_denoising_steps - 1)\n    else:\n        indices = torch.empty(num_blocks, dtype=torch.long, device=device)\n\n    if dist.is_initialized():\n        dist.broadcast(indices,\n                       src=0)  # Broadcast the random indices to all ranks\n    flags = indices.tolist()\n    logger.info(\n        \"RANK: %s, exit generate_and_sync_list flags_len=%s first=%s\",\n        self.global_rank,\n        len(flags),\n        flags[0] if len(flags) &gt; 0 else None,\n        local_main_process_only=False)\n    return flags\n</code></pre> fastvideo.training.self_forcing_distillation_pipeline.SelfForcingDistillationPipeline.generator_loss \u00b6 <pre><code>generator_loss(\n    training_batch: TrainingBatch,\n) -&gt; tuple[torch.Tensor, dict[str, Any]]\n</code></pre> <p>Compute generator loss using DMD-style approach. The generator tries to fool the critic (fake_score_transformer).</p> Source code in <code>fastvideo/training/self_forcing_distillation_pipeline.py</code> <pre><code>def generator_loss(\n        self, training_batch: TrainingBatch\n) -&gt; tuple[torch.Tensor, dict[str, Any]]:\n    \"\"\"\n    Compute generator loss using DMD-style approach.\n    The generator tries to fool the critic (fake_score_transformer).\n    \"\"\"\n    with set_forward_context(\n            current_timestep=training_batch.timesteps,\n            attn_metadata=training_batch.attn_metadata_vsa):\n        generator_pred_video = self._generator_multi_step_simulation_forward(\n            training_batch)\n\n    with set_forward_context(current_timestep=training_batch.timesteps,\n                             attn_metadata=training_batch.attn_metadata):\n        dmd_loss = self._dmd_forward(\n            generator_pred_video=generator_pred_video,\n            training_batch=training_batch)\n\n    log_dict = {\n        \"dmdtrain_gradient_norm\": torch.tensor(0.0, device=self.device)\n    }\n\n    return dmd_loss, log_dict\n</code></pre> fastvideo.training.self_forcing_distillation_pipeline.SelfForcingDistillationPipeline.initialize_training_pipeline \u00b6 <pre><code>initialize_training_pipeline(training_args: TrainingArgs)\n</code></pre> <p>Initialize the self-forcing training pipeline.</p> Source code in <code>fastvideo/training/self_forcing_distillation_pipeline.py</code> <pre><code>def initialize_training_pipeline(self, training_args: TrainingArgs):\n    \"\"\"Initialize the self-forcing training pipeline.\"\"\"\n    logger.info(\"Initializing self-forcing distillation pipeline...\")\n\n    self.generator_ema: EMA_FSDP | None = None\n    self.generator_ema_2: EMA_FSDP | None = None\n\n    super().initialize_training_pipeline(training_args)\n    try:\n        logger.info(\"RANK: %s, entered initialize_training_pipeline\",\n                    self.global_rank,\n                    local_main_process_only=False)\n    except Exception:\n        logger.info(\"Entered initialize_training_pipeline (rank unknown)\")\n\n    self.noise_scheduler = SelfForcingFlowMatchScheduler(\n        num_inference_steps=1000,\n        shift=5.0,\n        sigma_min=0.0,\n        extra_one_step=True,\n        training=True)\n    self.dfake_gen_update_ratio = getattr(training_args,\n                                          'dfake_gen_update_ratio', 5)\n\n    self.num_frame_per_block = getattr(training_args, 'num_frame_per_block',\n                                       3)\n    self.independent_first_frame = getattr(training_args,\n                                           'independent_first_frame', False)\n    self.same_step_across_blocks = getattr(training_args,\n                                           'same_step_across_blocks', False)\n    self.last_step_only = getattr(training_args, 'last_step_only', False)\n    self.context_noise = getattr(training_args, 'context_noise', 0)\n\n    self.kv_cache1: list[dict[str, Any]] | None = None\n    self.crossattn_cache: list[dict[str, Any]] | None = None\n\n    logger.info(\"Self-forcing generator update ratio: %s\",\n                self.dfake_gen_update_ratio)\n    logger.info(\"RANK: %s, exiting initialize_training_pipeline\",\n                self.global_rank,\n                local_main_process_only=False)\n</code></pre> fastvideo.training.self_forcing_distillation_pipeline.SelfForcingDistillationPipeline.train \u00b6 <pre><code>train() -&gt; None\n</code></pre> <p>Main training loop with self-forcing specific logging.</p> Source code in <code>fastvideo/training/self_forcing_distillation_pipeline.py</code> <pre><code>@profile_region(\"profiler_region_training_train\")\ndef train(self) -&gt; None:\n    \"\"\"Main training loop with self-forcing specific logging.\"\"\"\n    assert self.training_args.seed is not None, \"seed must be set\"\n    seed = self.training_args.seed\n\n    # Set the same seed within each SP group to ensure reproducibility\n    if self.sp_world_size &gt; 1:\n        # Use the same seed for all processes within the same SP group\n        sp_group_seed = seed + (self.global_rank // self.sp_world_size)\n        set_random_seed(sp_group_seed)\n    else:\n        set_random_seed(seed + self.global_rank)\n\n    self.noise_random_generator = torch.Generator(device=\"cpu\").manual_seed(\n        self.seed)\n    self.noise_gen_cuda = torch.Generator(device=\"cuda\").manual_seed(\n        self.seed)\n    self.validation_random_generator = torch.Generator(\n        device=\"cpu\").manual_seed(self.seed)\n    logger.info(\"Initialized random seeds with seed: %s\", seed)\n\n    self.current_trainstep = self.init_steps\n\n    if self.training_args.resume_from_checkpoint:\n        self._resume_from_checkpoint()\n        logger.info(\"Resumed from checkpoint, random states restored\")\n    else:\n        logger.info(\"Starting training from scratch\")\n\n    self.train_loader_iter = iter(self.train_dataloader)\n\n    step_times: deque[float] = deque(maxlen=100)\n\n    self._log_training_info()\n    self._log_validation(self.transformer, self.training_args,\n                         self.init_steps)\n\n    progress_bar = tqdm(\n        range(0, self.training_args.max_train_steps),\n        initial=self.init_steps,\n        desc=\"Steps\",\n        disable=self.local_rank &gt; 0,\n    )\n\n    use_vsa = vsa_available and envs.FASTVIDEO_ATTENTION_BACKEND == \"VIDEO_SPARSE_ATTN\"\n    for step in range(self.init_steps + 1,\n                      self.training_args.max_train_steps + 1):\n        start_time = time.perf_counter()\n        if use_vsa:\n            vsa_sparsity = self.training_args.VSA_sparsity\n            vsa_decay_rate = self.training_args.VSA_decay_rate\n            vsa_decay_interval_steps = self.training_args.VSA_decay_interval_steps\n            if vsa_decay_interval_steps &gt; 1:\n                current_decay_times = min(step // vsa_decay_interval_steps,\n                                          vsa_sparsity // vsa_decay_rate)\n                current_vsa_sparsity = current_decay_times * vsa_decay_rate\n            else:\n                current_vsa_sparsity = vsa_sparsity\n        else:\n            current_vsa_sparsity = 0.0\n\n        training_batch = TrainingBatch()\n        self.current_trainstep = step\n        training_batch.current_vsa_sparsity = current_vsa_sparsity\n\n        if (step &gt;= self.training_args.ema_start_step) and \\\n                (self.generator_ema is None) and (self.training_args.ema_decay &gt; 0):\n            self.generator_ema = EMA_FSDP(\n                self.transformer, decay=self.training_args.ema_decay)\n            logger.info(\"Created generator EMA at step %s with decay=%s\",\n                        step, self.training_args.ema_decay)\n\n            # Create EMA for transformer_2 if it exists\n            if self.transformer_2 is not None and self.generator_ema_2 is None:\n                self.generator_ema_2 = EMA_FSDP(\n                    self.transformer_2, decay=self.training_args.ema_decay)\n                logger.info(\n                    \"Created generator EMA_2 at step %s with decay=%s\",\n                    step, self.training_args.ema_decay)\n\n        with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n            training_batch = self.train_one_step(training_batch)\n\n        total_loss = training_batch.total_loss\n        generator_loss = training_batch.generator_loss\n        fake_score_loss = training_batch.fake_score_loss\n        grad_norm = training_batch.grad_norm\n\n        step_time = time.perf_counter() - start_time\n        step_times.append(step_time)\n        avg_step_time = sum(step_times) / len(step_times)\n\n        progress_bar.set_postfix({\n            \"total_loss\":\n            f\"{total_loss:.4f}\",\n            \"generator_loss\":\n            f\"{generator_loss:.4f}\",\n            \"fake_score_loss\":\n            f\"{fake_score_loss:.4f}\",\n            \"step_time\":\n            f\"{step_time:.2f}s\",\n            \"grad_norm\":\n            grad_norm,\n            \"ema\":\n            \"\u2713\" if (self.generator_ema is not None and self.is_ema_ready())\n            else \"\u2717\",\n            \"ema2\":\n            \"\u2713\" if (self.generator_ema_2 is not None\n                    and self.is_ema_ready()) else \"\u2717\",\n        })\n        progress_bar.update(1)\n\n        if self.global_rank == 0:\n            log_data = {\n                \"train_total_loss\":\n                total_loss,\n                \"train_fake_score_loss\":\n                fake_score_loss,\n                \"learning_rate\":\n                self.lr_scheduler.get_last_lr()[0],\n                \"fake_score_learning_rate\":\n                self.fake_score_lr_scheduler.get_last_lr()[0],\n                \"step_time\":\n                step_time,\n                \"avg_step_time\":\n                avg_step_time,\n                \"grad_norm\":\n                grad_norm,\n            }\n            if (step % self.dfake_gen_update_ratio == 0):\n                log_data[\"train_generator_loss\"] = generator_loss\n            if use_vsa:\n                log_data[\"VSA_train_sparsity\"] = current_vsa_sparsity\n\n            if self.generator_ema is not None or self.generator_ema_2 is not None:\n                log_data[\"ema_enabled\"] = self.generator_ema is not None\n                log_data[\"ema_2_enabled\"] = self.generator_ema_2 is not None\n                log_data[\"ema_decay\"] = self.training_args.ema_decay\n            else:\n                log_data[\"ema_enabled\"] = False\n                log_data[\"ema_2_enabled\"] = False\n\n            ema_stats = self.get_ema_stats()\n            log_data.update(ema_stats)\n\n            if training_batch.dmd_latent_vis_dict:\n                dmd_additional_logs = {\n                    \"generator_timestep\":\n                    training_batch.\n                    dmd_latent_vis_dict[\"generator_timestep\"].item(),\n                    \"dmd_timestep\":\n                    training_batch.dmd_latent_vis_dict[\"dmd_timestep\"].item(\n                    ),\n                }\n                log_data.update(dmd_additional_logs)\n\n            faker_score_additional_logs = {\n                \"fake_score_timestep\":\n                training_batch.\n                fake_score_latent_vis_dict[\"fake_score_timestep\"].item(),\n            }\n            log_data.update(faker_score_additional_logs)\n\n            self.tracker.log(log_data, step)\n\n            if self.training_args.log_validation and step % self.training_args.validation_steps == 0 and self.training_args.log_visualization:\n                self.visualize_intermediate_latents(training_batch,\n                                                    self.training_args,\n                                                    step)\n\n        if (self.training_args.training_state_checkpointing_steps &gt; 0\n                and step %\n                self.training_args.training_state_checkpointing_steps == 0):\n            print(\"rank\", self.global_rank,\n                  \"save training state checkpoint at step\", step)\n            save_distillation_checkpoint(\n                self.transformer,\n                self.fake_score_transformer,\n                self.global_rank,\n                self.training_args.output_dir,\n                step,\n                self.optimizer,\n                self.fake_score_optimizer,\n                self.train_dataloader,\n                self.lr_scheduler,\n                self.fake_score_lr_scheduler,\n                self.noise_random_generator,\n                self.generator_ema,\n                # MoE support\n                generator_transformer_2=getattr(self, 'transformer_2',\n                                                None),\n                real_score_transformer_2=getattr(\n                    self, 'real_score_transformer_2', None),\n                fake_score_transformer_2=getattr(\n                    self, 'fake_score_transformer_2', None),\n                generator_optimizer_2=getattr(self, 'optimizer_2', None),\n                fake_score_optimizer_2=getattr(self,\n                                               'fake_score_optimizer_2',\n                                               None),\n                generator_scheduler_2=getattr(self, 'lr_scheduler_2', None),\n                fake_score_scheduler_2=getattr(self,\n                                               'fake_score_lr_scheduler_2',\n                                               None),\n                generator_ema_2=getattr(self, 'generator_ema_2', None))\n\n            if self.transformer:\n                self.transformer.train()\n            self.sp_group.barrier()\n\n        if (self.training_args.weight_only_checkpointing_steps &gt; 0\n                and step %\n                self.training_args.weight_only_checkpointing_steps == 0):\n            print(\"rank\", self.global_rank,\n                  \"save weight-only checkpoint at step\", step)\n            save_distillation_checkpoint(\n                self.transformer,\n                self.fake_score_transformer,\n                self.global_rank,\n                self.training_args.output_dir,\n                f\"{step}_weight_only\",\n                only_save_generator_weight=True,\n                generator_ema=self.generator_ema,\n                # MoE support\n                generator_transformer_2=getattr(self, 'transformer_2',\n                                                None),\n                real_score_transformer_2=getattr(\n                    self, 'real_score_transformer_2', None),\n                fake_score_transformer_2=getattr(\n                    self, 'fake_score_transformer_2', None),\n                generator_optimizer_2=getattr(self, 'optimizer_2', None),\n                fake_score_optimizer_2=getattr(self,\n                                               'fake_score_optimizer_2',\n                                               None),\n                generator_scheduler_2=getattr(self, 'lr_scheduler_2', None),\n                fake_score_scheduler_2=getattr(self,\n                                               'fake_score_lr_scheduler_2',\n                                               None),\n                generator_ema_2=getattr(self, 'generator_ema_2', None))\n\n            if self.training_args.use_ema and self.is_ema_ready():\n                self.save_ema_weights(self.training_args.output_dir, step)\n\n        if self.training_args.log_validation and step % self.training_args.validation_steps == 0:\n            self._log_validation(self.transformer, self.training_args, step)\n\n    self.tracker.finish()\n\n    print(\"rank\", self.global_rank,\n          \"save final training state checkpoint at step\",\n          self.training_args.max_train_steps)\n    save_distillation_checkpoint(\n        self.transformer,\n        self.fake_score_transformer,\n        self.global_rank,\n        self.training_args.output_dir,\n        self.training_args.max_train_steps,\n        self.optimizer,\n        self.fake_score_optimizer,\n        self.train_dataloader,\n        self.lr_scheduler,\n        self.fake_score_lr_scheduler,\n        self.noise_random_generator,\n        self.generator_ema,\n        # MoE support\n        generator_transformer_2=getattr(self, 'transformer_2', None),\n        real_score_transformer_2=getattr(self, 'real_score_transformer_2',\n                                         None),\n        fake_score_transformer_2=getattr(self, 'fake_score_transformer_2',\n                                         None),\n        generator_optimizer_2=getattr(self, 'optimizer_2', None),\n        fake_score_optimizer_2=getattr(self, 'fake_score_optimizer_2',\n                                       None),\n        generator_scheduler_2=getattr(self, 'lr_scheduler_2', None),\n        fake_score_scheduler_2=getattr(self, 'fake_score_lr_scheduler_2',\n                                       None),\n        generator_ema_2=getattr(self, 'generator_ema_2', None))\n\n    if self.training_args.use_ema and self.is_ema_ready():\n        self.save_ema_weights(self.training_args.output_dir,\n                              self.training_args.max_train_steps)\n\n    if envs.FASTVIDEO_TORCH_PROFILER_DIR:\n        logger.info(\"Stopping profiler...\")\n        self.profiler_controller.stop()\n        logger.info(\"Profiler stopped.\")\n\n    if get_sp_group():\n        cleanup_dist_env_and_memory()\n</code></pre> fastvideo.training.self_forcing_distillation_pipeline.SelfForcingDistillationPipeline.train_one_step \u00b6 <pre><code>train_one_step(\n    training_batch: TrainingBatch,\n) -&gt; TrainingBatch\n</code></pre> <p>Self-forcing training step that alternates between generator and critic training.</p> Source code in <code>fastvideo/training/self_forcing_distillation_pipeline.py</code> <pre><code>def train_one_step(self, training_batch: TrainingBatch) -&gt; TrainingBatch:\n    \"\"\"\n    Self-forcing training step that alternates between generator and critic training.\n    \"\"\"\n    gradient_accumulation_steps = getattr(self.training_args,\n                                          'gradient_accumulation_steps', 1)\n    train_generator = (self.current_trainstep %\n                       self.dfake_gen_update_ratio == 0)\n\n    batches = []\n    for _ in range(gradient_accumulation_steps):\n        batch = self._prepare_distillation(training_batch)\n        batch = self._get_next_batch(batch)\n        batch = self._normalize_dit_input(batch)\n        batch = self._prepare_dit_inputs(batch)\n        batch = self._build_attention_metadata(batch)\n        batch.attn_metadata_vsa = copy.deepcopy(batch.attn_metadata)\n        if batch.attn_metadata is not None:\n            batch.attn_metadata.VSA_sparsity = 0.0\n        batches.append(batch)\n\n    training_batch.dmd_latent_vis_dict = {}\n    training_batch.fake_score_latent_vis_dict = {}\n\n    if train_generator:\n        logger.debug(\"Training generator at step %s\",\n                     self.current_trainstep)\n        self.optimizer.zero_grad()\n        if self.transformer_2 is not None:\n            self.optimizer_2.zero_grad()\n        total_generator_loss = 0.0\n        generator_log_dict = {}\n\n        for batch in batches:\n            # Create a new batch with detached tensors\n            batch_gen = TrainingBatch()\n            for key, value in batch.__dict__.items():\n                if isinstance(value, torch.Tensor):\n                    setattr(batch_gen, key, value.detach().clone())\n                elif isinstance(value, dict):\n                    setattr(\n                        batch_gen, key, {\n                            k:\n                            v.detach().clone() if isinstance(\n                                v, torch.Tensor) else copy.deepcopy(v)\n                            for k, v in value.items()\n                        })\n                else:\n                    setattr(batch_gen, key, copy.deepcopy(value))\n\n            generator_loss, gen_log_dict = self.generator_loss(batch_gen)\n            with set_forward_context(current_timestep=batch_gen.timesteps,\n                                     attn_metadata=batch_gen.attn_metadata):\n                (generator_loss / gradient_accumulation_steps).backward()\n            total_generator_loss += generator_loss.detach().item()\n            generator_log_dict.update(gen_log_dict)\n            # Store visualization data from generator training\n            if hasattr(batch_gen, 'dmd_latent_vis_dict'):\n                training_batch.dmd_latent_vis_dict.update(\n                    batch_gen.dmd_latent_vis_dict)\n\n        # Only clip gradients and step optimizer for the model that is currently training\n        if hasattr(\n                self, 'train_transformer_2'\n        ) and self.train_transformer_2 and self.transformer_2 is not None:\n            self._clip_model_grad_norm_(batch_gen, self.transformer_2)\n            self.optimizer_2.step()\n            self.lr_scheduler_2.step()\n        else:\n            self._clip_model_grad_norm_(batch_gen, self.transformer)\n            self.optimizer.step()\n            self.lr_scheduler.step()\n\n        if self.generator_ema is not None:\n            if hasattr(\n                    self, 'train_transformer_2'\n            ) and self.train_transformer_2 and self.transformer_2 is not None:\n                # Update EMA for transformer_2 when training it\n                if self.generator_ema_2 is not None:\n                    self.generator_ema_2.update(self.transformer_2)\n            else:\n                self.generator_ema.update(self.transformer)\n\n        avg_generator_loss = torch.tensor(total_generator_loss /\n                                          gradient_accumulation_steps,\n                                          device=self.device)\n        world_group = get_world_group()\n        world_group.all_reduce(avg_generator_loss,\n                               op=torch.distributed.ReduceOp.AVG)\n        training_batch.generator_loss = avg_generator_loss.item()\n    else:\n        training_batch.generator_loss = 0.0\n\n    logger.debug(\"Training critic at step %s\", self.current_trainstep)\n    self.fake_score_optimizer.zero_grad()\n    total_critic_loss = 0.0\n    critic_log_dict = {}\n\n    for batch in batches:\n        # Create a new batch with detached tensors\n        batch_critic = TrainingBatch()\n        for key, value in batch.__dict__.items():\n            if isinstance(value, torch.Tensor):\n                setattr(batch_critic, key, value.detach().clone())\n            elif isinstance(value, dict):\n                setattr(\n                    batch_critic, key, {\n                        k:\n                        v.detach().clone()\n                        if isinstance(v, torch.Tensor) else copy.deepcopy(v)\n                        for k, v in value.items()\n                    })\n            else:\n                setattr(batch_critic, key, copy.deepcopy(value))\n\n        critic_loss, crit_log_dict = self.critic_loss(batch_critic)\n        with set_forward_context(current_timestep=batch_critic.timesteps,\n                                 attn_metadata=batch_critic.attn_metadata):\n            (critic_loss / gradient_accumulation_steps).backward()\n        total_critic_loss += critic_loss.detach().item()\n        critic_log_dict.update(crit_log_dict)\n        # Store visualization data from critic training\n        if hasattr(batch_critic, 'fake_score_latent_vis_dict'):\n            training_batch.fake_score_latent_vis_dict.update(\n                batch_critic.fake_score_latent_vis_dict)\n\n    if self.train_fake_score_transformer_2 and self.fake_score_transformer_2 is not None:\n        self._clip_model_grad_norm_(batch_critic,\n                                    self.fake_score_transformer_2)\n        self.fake_score_optimizer_2.step()\n        self.fake_score_lr_scheduler_2.step()\n    else:\n        self._clip_model_grad_norm_(batch_critic,\n                                    self.fake_score_transformer)\n        self.fake_score_optimizer.step()\n        self.fake_score_lr_scheduler.step()\n\n    avg_critic_loss = torch.tensor(total_critic_loss /\n                                   gradient_accumulation_steps,\n                                   device=self.device)\n    world_group = get_world_group()\n    world_group.all_reduce(avg_critic_loss,\n                           op=torch.distributed.ReduceOp.AVG)\n    training_batch.fake_score_loss = avg_critic_loss.item()\n\n    training_batch.total_loss = training_batch.generator_loss + training_batch.fake_score_loss\n    return training_batch\n</code></pre> fastvideo.training.self_forcing_distillation_pipeline.SelfForcingDistillationPipeline.visualize_intermediate_latents \u00b6 <pre><code>visualize_intermediate_latents(\n    training_batch: TrainingBatch,\n    training_args: TrainingArgs,\n    step: int,\n)\n</code></pre> <p>Add visualization data to tracker logging and save frames to disk.</p> Source code in <code>fastvideo/training/self_forcing_distillation_pipeline.py</code> <pre><code>def visualize_intermediate_latents(self, training_batch: TrainingBatch,\n                                   training_args: TrainingArgs, step: int):\n    \"\"\"Add visualization data to tracker logging and save frames to disk.\"\"\"\n    tracker_loss_dict: dict[str, Any] = {}\n\n    # Debug logging\n    if hasattr(training_batch, 'dmd_latent_vis_dict'):\n        logger.info(\"DMD latent keys: %s\",\n                    list(training_batch.dmd_latent_vis_dict.keys()))\n    if hasattr(training_batch, 'fake_score_latent_vis_dict'):\n        logger.info(\"Fake score latent keys: %s\",\n                    list(training_batch.fake_score_latent_vis_dict.keys()))\n\n    # Process generator predictions if available\n    if hasattr(\n            training_batch,\n            'dmd_latent_vis_dict') and training_batch.dmd_latent_vis_dict:\n        dmd_latents_vis_dict = training_batch.dmd_latent_vis_dict\n        dmd_log_keys = [\n            'generator_pred_video', 'real_score_pred_video',\n            'faker_score_pred_video'\n        ]\n\n        for latent_key in dmd_log_keys:\n            if latent_key in dmd_latents_vis_dict:\n                logger.info(\"Processing DMD latent: %s\", latent_key)\n                latents = dmd_latents_vis_dict[latent_key]\n                if not isinstance(latents, torch.Tensor):\n                    logger.warning(\"Expected tensor for %s, got %s\",\n                                   latent_key, type(latents))\n                    continue\n\n                latents = latents.detach()\n                latents = latents.permute(0, 2, 1, 3, 4)\n\n                if isinstance(self.vae.scaling_factor, torch.Tensor):\n                    latents = latents / self.vae.scaling_factor.to(\n                        latents.device, latents.dtype)\n                else:\n                    latents = latents / self.vae.scaling_factor\n\n                if (hasattr(self.vae, \"shift_factor\")\n                        and self.vae.shift_factor is not None):\n                    if isinstance(self.vae.shift_factor, torch.Tensor):\n                        latents += self.vae.shift_factor.to(\n                            latents.device, latents.dtype)\n                    else:\n                        latents += self.vae.shift_factor\n\n                with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n                    video = self.vae.decode(latents)\n                video = (video / 2 + 0.5).clamp(0, 1)\n                video = video.cpu().float()\n                video = video.permute(0, 2, 1, 3, 4)\n                video = (video * 255).numpy().astype(np.uint8)\n                video_artifact = self.tracker.video(video,\n                                                    fps=24,\n                                                    format=\"mp4\")\n                if video_artifact is not None:\n                    tracker_loss_dict[f\"dmd_{latent_key}\"] = video_artifact\n                del video, latents\n\n    # Process critic predictions\n    if hasattr(training_batch, 'fake_score_latent_vis_dict'\n               ) and training_batch.fake_score_latent_vis_dict:\n        fake_score_latents_vis_dict = training_batch.fake_score_latent_vis_dict\n        fake_score_log_keys = ['generator_pred_video']\n\n        for latent_key in fake_score_log_keys:\n            if latent_key in fake_score_latents_vis_dict:\n                logger.info(\"Processing critic latent: %s\", latent_key)\n                latents = fake_score_latents_vis_dict[latent_key]\n                if not isinstance(latents, torch.Tensor):\n                    logger.warning(\"Expected tensor for %s, got %s\",\n                                   latent_key, type(latents))\n                    continue\n\n                latents = latents.detach()\n                latents = latents.permute(0, 2, 1, 3, 4)\n\n                if isinstance(self.vae.scaling_factor, torch.Tensor):\n                    latents = latents / self.vae.scaling_factor.to(\n                        latents.device, latents.dtype)\n                else:\n                    latents = latents / self.vae.scaling_factor\n\n                if (hasattr(self.vae, \"shift_factor\")\n                        and self.vae.shift_factor is not None):\n                    if isinstance(self.vae.shift_factor, torch.Tensor):\n                        latents += self.vae.shift_factor.to(\n                            latents.device, latents.dtype)\n                    else:\n                        latents += self.vae.shift_factor\n\n                with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n                    video = self.vae.decode(latents)\n                video = (video / 2 + 0.5).clamp(0, 1)\n                video = video.cpu().float()\n                video = video.permute(0, 2, 1, 3, 4)\n                video = (video * 255).numpy().astype(np.uint8)\n                video_artifact = self.tracker.video(video,\n                                                    fps=24,\n                                                    format=\"mp4\")\n                if video_artifact is not None:\n                    tracker_loss_dict[\n                        f\"critic_{latent_key}\"] = video_artifact\n                del video, latents\n\n    # Log metadata\n    if hasattr(\n            training_batch,\n            'dmd_latent_vis_dict') and training_batch.dmd_latent_vis_dict:\n        if \"generator_timestep\" in training_batch.dmd_latent_vis_dict:\n            tracker_loss_dict[\n                \"generator_timestep\"] = training_batch.dmd_latent_vis_dict[\n                    \"generator_timestep\"].item()\n        if \"dmd_timestep\" in training_batch.dmd_latent_vis_dict:\n            tracker_loss_dict[\n                \"dmd_timestep\"] = training_batch.dmd_latent_vis_dict[\n                    \"dmd_timestep\"].item()\n\n    if hasattr(\n            training_batch, 'fake_score_latent_vis_dict'\n    ) and training_batch.fake_score_latent_vis_dict and \"fake_score_timestep\" in training_batch.fake_score_latent_vis_dict:\n        tracker_loss_dict[\n            \"fake_score_timestep\"] = training_batch.fake_score_latent_vis_dict[\n                \"fake_score_timestep\"].item()\n\n    # Log final dict contents\n    logger.info(\"Final tracker_loss_dict keys: %s\",\n                list(tracker_loss_dict.keys()))\n\n    if self.global_rank == 0 and tracker_loss_dict:\n        self.tracker.log_artifacts(tracker_loss_dict, step)\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.self_forcing_distillation_pipeline-functions","title":"Functions","text":""},{"location":"api/fastvideo/training/#fastvideo.training.trackers","title":"fastvideo.training.trackers","text":"<p>Utilities for logging metrics and artifacts to external trackers.</p> <p>This module is inspired by the trackers implementation in https://github.com/huggingface/finetrainers and provides a minimal, shared interface that can be used across all FastVideo training pipelines.</p>"},{"location":"api/fastvideo/training/#fastvideo.training.trackers-classes","title":"Classes","text":""},{"location":"api/fastvideo/training/#fastvideo.training.trackers.BaseTracker","title":"fastvideo.training.trackers.BaseTracker","text":"<pre><code>BaseTracker()\n</code></pre> <p>Base tracker implementation.</p> <p>The default tracker stores timing information but does not emit any logs.</p> Source code in <code>fastvideo/training/trackers.py</code> <pre><code>def __init__(self) -&gt; None:\n    self._timed_metrics: dict[str, float] = {}\n</code></pre> Functions\u00b6 fastvideo.training.trackers.BaseTracker.finish \u00b6 <pre><code>finish() -&gt; None\n</code></pre> <p>Finalize the tracker session.</p> Source code in <code>fastvideo/training/trackers.py</code> <pre><code>def finish(self) -&gt; None:  # pragma: no cover - interface\n    \"\"\"Finalize the tracker session.\"\"\"\n</code></pre> fastvideo.training.trackers.BaseTracker.log \u00b6 <pre><code>log(metrics: dict[str, Any], step: int) -&gt; None\n</code></pre> <p>Log metrics for the given step.</p> Source code in <code>fastvideo/training/trackers.py</code> <pre><code>def log(self, metrics: dict[str, Any],\n        step: int) -&gt; None:  # pragma: no cover - interface\n    \"\"\"Log metrics for the given step.\"\"\"\n    # Merge timing metrics with provided metrics\n    metrics = {**self._timed_metrics, **metrics}\n    self._timed_metrics = {}\n</code></pre> fastvideo.training.trackers.BaseTracker.log_artifacts \u00b6 <pre><code>log_artifacts(artifacts: dict[str, Any], step: int) -&gt; None\n</code></pre> <p>Log artifacts such as videos or images.</p> <p>By default this is treated the same as :meth:<code>log</code>.</p> Source code in <code>fastvideo/training/trackers.py</code> <pre><code>def log_artifacts(self, artifacts: dict[str, Any], step: int) -&gt; None:\n    \"\"\"Log artifacts such as videos or images.\n\n    By default this is treated the same as :meth:`log`.\n    \"\"\"\n\n    if artifacts:\n        self.log(artifacts, step)\n</code></pre> fastvideo.training.trackers.BaseTracker.video \u00b6 <pre><code>video(\n    data: Any,\n    *,\n    caption: str | None = None,\n    fps: int | None = None,\n    format: str | None = None\n) -&gt; Any | None\n</code></pre> <p>Create a tracker specific video artifact.</p> <p>Trackers that do not support video artifacts should return <code>None</code>.</p> Source code in <code>fastvideo/training/trackers.py</code> <pre><code>def video(\n    self,\n    data: Any,\n    *,\n    caption: str | None = None,\n    fps: int | None = None,\n    format: str | None = None,\n) -&gt; Any | None:\n    \"\"\"Create a tracker specific video artifact.\n\n    Trackers that do not support video artifacts should return ``None``.\n    \"\"\"\n\n    return None\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.trackers.DummyTracker","title":"fastvideo.training.trackers.DummyTracker","text":"<pre><code>DummyTracker()\n</code></pre> <p>               Bases: <code>BaseTracker</code></p> <p>Tracker implementation used when logging is disabled.</p> Source code in <code>fastvideo/training/trackers.py</code> <pre><code>def __init__(self) -&gt; None:\n    self._timed_metrics: dict[str, float] = {}\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.trackers.SequentialTracker","title":"fastvideo.training.trackers.SequentialTracker","text":"<pre><code>SequentialTracker(trackers: Iterable[BaseTracker])\n</code></pre> <p>               Bases: <code>BaseTracker</code></p> <p>A tracker that forwards logging calls to a sequence of trackers.</p> Source code in <code>fastvideo/training/trackers.py</code> <pre><code>def __init__(self, trackers: Iterable[BaseTracker]) -&gt; None:\n    super().__init__()\n    self._trackers: list[BaseTracker] = list(trackers)\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.trackers.Timer","title":"fastvideo.training.trackers.Timer  <code>dataclass</code>","text":"<pre><code>Timer(\n    name: str,\n    _start_time: float | None = None,\n    _end_time: float | None = None,\n)\n</code></pre> <p>Simple timer utility used by the trackers.</p>"},{"location":"api/fastvideo/training/#fastvideo.training.trackers.WandbTracker","title":"fastvideo.training.trackers.WandbTracker","text":"<pre><code>WandbTracker(\n    experiment_name: str,\n    log_dir: str,\n    *,\n    config: dict[str, Any] | None = None,\n    run_name: str | None = None\n)\n</code></pre> <p>               Bases: <code>BaseTracker</code></p> <p>Tracker implementation for Weights &amp; Biases.</p> Source code in <code>fastvideo/training/trackers.py</code> <pre><code>def __init__(\n    self,\n    experiment_name: str,\n    log_dir: str,\n    *,\n    config: dict[str, Any] | None = None,\n    run_name: str | None = None,\n) -&gt; None:\n    super().__init__()\n\n    import wandb\n\n    pathlib.Path(log_dir).mkdir(parents=True, exist_ok=True)\n\n    self._wandb = wandb\n    self._run = wandb.init(\n        project=experiment_name,\n        dir=log_dir,\n        config=config,\n        name=run_name,\n    )\n    logger.info(\"Initialized Weights &amp; Biases tracker\")\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.trackers-functions","title":"Functions","text":""},{"location":"api/fastvideo/training/#fastvideo.training.trackers.initialize_trackers","title":"fastvideo.training.trackers.initialize_trackers","text":"<pre><code>initialize_trackers(\n    trackers: Iterable[str],\n    *,\n    experiment_name: str,\n    config: dict[str, Any] | None,\n    log_dir: str,\n    run_name: str | None = None\n) -&gt; BaseTracker\n</code></pre> <p>Create tracker instances based on <code>trackers</code> configuration.</p> Source code in <code>fastvideo/training/trackers.py</code> <pre><code>def initialize_trackers(\n    trackers: Iterable[str],\n    *,\n    experiment_name: str,\n    config: dict[str, Any] | None,\n    log_dir: str,\n    run_name: str | None = None,\n) -&gt; BaseTracker:\n    \"\"\"Create tracker instances based on ``trackers`` configuration.\"\"\"\n\n    tracker_names = [tracker.lower() for tracker in trackers]\n    if not tracker_names:\n        return DummyTracker()\n\n    unsupported = [\n        name for name in tracker_names if name not in SUPPORTED_TRACKERS\n    ]\n    if unsupported:\n        raise ValueError(\n            f\"Unsupported tracker(s) provided: {unsupported}. Supported trackers: {sorted(SUPPORTED_TRACKERS)}\"\n        )\n\n    tracker_instances: list[BaseTracker] = []\n    for tracker_name in tracker_names:\n        if tracker_name == Trackers.NONE.value:\n            tracker_instances.append(DummyTracker())\n        elif tracker_name == Trackers.WANDB.value:\n            tracker_instances.append(\n                WandbTracker(\n                    experiment_name,\n                    os.path.abspath(log_dir),\n                    config=config,\n                    run_name=run_name,\n                ))\n\n    if not tracker_instances:\n        return DummyTracker()\n\n    if len(tracker_instances) == 1:\n        return tracker_instances[0]\n\n    return SequentialTracker(tracker_instances)\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.training_pipeline","title":"fastvideo.training.training_pipeline","text":""},{"location":"api/fastvideo/training/#fastvideo.training.training_pipeline-classes","title":"Classes","text":""},{"location":"api/fastvideo/training/#fastvideo.training.training_pipeline.TrainingPipeline","title":"fastvideo.training.training_pipeline.TrainingPipeline","text":"<pre><code>TrainingPipeline(\n    model_path: str,\n    fastvideo_args: TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>LoRAPipeline</code>, <code>ABC</code></p> <p>A pipeline for training a model. All training pipelines should inherit from this class. All reusable components and code should be implemented in this class.</p> Source code in <code>fastvideo/training/training_pipeline.py</code> <pre><code>def __init__(\n        self,\n        model_path: str,\n        fastvideo_args: TrainingArgs,\n        required_config_modules: list[str] | None = None,\n        loaded_modules: dict[str, torch.nn.Module] | None = None) -&gt; None:\n    fastvideo_args.inference_mode = False\n    self.lora_training = fastvideo_args.lora_training\n    if self.lora_training and fastvideo_args.lora_rank is None:\n        raise ValueError(\"lora rank must be set when using lora training\")\n\n    set_random_seed(fastvideo_args.seed)  # for lora param init\n    super().__init__(model_path, fastvideo_args, required_config_modules,\n                     loaded_modules)  # type: ignore\n    self.tracker = DummyTracker()\n</code></pre> Functions\u00b6 fastvideo.training.training_pipeline.TrainingPipeline.visualize_intermediate_latents \u00b6 <pre><code>visualize_intermediate_latents(\n    training_batch: TrainingBatch,\n    training_args: TrainingArgs,\n    step: int,\n)\n</code></pre> <p>Add visualization data to tracker logging and save frames to disk.</p> Source code in <code>fastvideo/training/training_pipeline.py</code> <pre><code>def visualize_intermediate_latents(self, training_batch: TrainingBatch,\n                                   training_args: TrainingArgs, step: int):\n    \"\"\"Add visualization data to tracker logging and save frames to disk.\"\"\"\n    raise NotImplementedError(\n        \"Visualize intermediate latents is not implemented for training pipeline\"\n    )\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.training_pipeline-functions","title":"Functions","text":""},{"location":"api/fastvideo/training/#fastvideo.training.training_utils","title":"fastvideo.training.training_utils","text":""},{"location":"api/fastvideo/training/#fastvideo.training.training_utils-classes","title":"Classes","text":""},{"location":"api/fastvideo/training/#fastvideo.training.training_utils.EMA_FSDP","title":"fastvideo.training.training_utils.EMA_FSDP","text":"<pre><code>EMA_FSDP(\n    module, decay: float = 0.999, mode: str = \"local_shard\"\n)\n</code></pre> FSDP2-friendly EMA with two modes <ul> <li>mode=\"local_shard\" (default): maintain float32 CPU EMA of local parameter shards on every rank.   Provides a context manager to temporarily swap EMA weights into the live model for teacher forward.</li> <li>mode=\"rank0_full\": maintain a consolidated float32 CPU EMA of full parameters on rank 0 only   using gather_state_dict_on_cpu_rank0(). Useful for checkpoint export; not for teacher forward.</li> </ul> <p>Usage (local_shard for CM teacher):   ema = EMA_FSDP(model, decay=0.999, mode=\"local_shard\")   for step in ...:       ema.update(model)   with ema.apply_to_model(model):       with torch.no_grad():           y_teacher = model(...)</p> <p>Usage (rank0_full for export):   ema = EMA_FSDP(model, decay=0.999, mode=\"rank0_full\")   ema.update(model)   ema.state_dict()  # on rank 0</p> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def __init__(self, module, decay: float = 0.999, mode: str = \"local_shard\"):\n    self.decay = float(decay)\n    self.mode = mode\n    self.shadow: dict[str, torch.Tensor] = {}\n    self.rank = dist.get_rank() if dist.is_initialized() else 0\n    if self.mode not in {\"local_shard\", \"rank0_full\"}:\n        raise ValueError(f\"Unsupported EMA_FSDP mode: {self.mode}\")\n    self._init_shadow(module)\n</code></pre> Functions\u00b6 fastvideo.training.training_utils.EMA_FSDP.copy_to_unwrapped \u00b6 <pre><code>copy_to_unwrapped(module) -&gt; None\n</code></pre> <p>Copy EMA weights into a non-sharded (unwrapped) module. Intended for export/eval. For mode=\"rank0_full\", only rank 0 has the full EMA state.</p> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>@torch.no_grad()\ndef copy_to_unwrapped(self, module) -&gt; None:\n    \"\"\"\n    Copy EMA weights into a non-sharded (unwrapped) module. Intended for export/eval.\n    For mode=\"rank0_full\", only rank 0 has the full EMA state.\n    \"\"\"\n    if self.mode == \"rank0_full\" and self.rank != 0:\n        return\n    name_to_param = dict(module.named_parameters())\n    for n, w in self.shadow.items():\n        if n in name_to_param:\n            p = name_to_param[n]\n            p.data.copy_(w.to(dtype=p.dtype, device=p.device))\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.training_utils-functions","title":"Functions","text":""},{"location":"api/fastvideo/training/#fastvideo.training.training_utils.clip_grad_norm_","title":"fastvideo.training.training_utils.clip_grad_norm_","text":"<pre><code>clip_grad_norm_(\n    parameters: Tensor | list[Tensor],\n    max_norm: float,\n    norm_type: float = 2.0,\n    error_if_nonfinite: bool = False,\n    foreach: bool | None = None,\n    pp_mesh: DeviceMesh | None = None,\n) -&gt; torch.Tensor\n</code></pre> <p>Clip the gradient norm of parameters.</p> <p>Gradient norm clipping requires computing the gradient norm over the entire model. <code>torch.nn.utils.clip_grad_norm_</code> only computes gradient norm along DP/FSDP/TP dimensions. We need to manually reduce the gradient norm across PP stages. See https://github.com/pytorch/torchtitan/issues/596 for details.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>`torch.Tensor` or `List[torch.Tensor]`</code> <p>Tensors that will have gradients normalized.</p> required <code>max_norm</code> <code>`float`</code> <p>Maximum norm of the gradients after clipping.</p> required <code>norm_type</code> <code>`float`, defaults to `2.0`</code> <p>Type of p-norm to use. Can be <code>inf</code> for infinity norm.</p> <code>2.0</code> <code>error_if_nonfinite</code> <code>`bool`, defaults to `False`</code> <p>If <code>True</code>, an error is thrown if the total norm of the gradients from <code>parameters</code> is <code>nan</code>, <code>inf</code>, or <code>-inf</code>.</p> <code>False</code> <code>foreach</code> <code>`bool`, defaults to `None`</code> <p>Use the faster foreach-based implementation. If <code>None</code>, use the foreach implementation for CUDA and CPU native tensors and silently fall back to the slow implementation for other device types.</p> <code>None</code> <code>pp_mesh</code> <code>`torch.distributed.device_mesh.DeviceMesh`, defaults to `None`</code> <p>Pipeline parallel device mesh. If not <code>None</code>, will reduce gradient norm across PP stages.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p><code>torch.Tensor</code>: Total norm of the gradients</p> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>@torch.no_grad()\ndef clip_grad_norm_(\n    parameters: torch.Tensor | list[torch.Tensor],\n    max_norm: float,\n    norm_type: float = 2.0,\n    error_if_nonfinite: bool = False,\n    foreach: bool | None = None,\n    pp_mesh: torch.distributed.device_mesh.DeviceMesh | None = None,\n) -&gt; torch.Tensor:\n    r\"\"\"\n    Clip the gradient norm of parameters.\n\n    Gradient norm clipping requires computing the gradient norm over the entire model.\n    `torch.nn.utils.clip_grad_norm_` only computes gradient norm along DP/FSDP/TP dimensions.\n    We need to manually reduce the gradient norm across PP stages.\n    See https://github.com/pytorch/torchtitan/issues/596 for details.\n\n    Args:\n        parameters (`torch.Tensor` or `List[torch.Tensor]`):\n            Tensors that will have gradients normalized.\n        max_norm (`float`):\n            Maximum norm of the gradients after clipping.\n        norm_type (`float`, defaults to `2.0`):\n            Type of p-norm to use. Can be `inf` for infinity norm.\n        error_if_nonfinite (`bool`, defaults to `False`):\n            If `True`, an error is thrown if the total norm of the gradients from `parameters` is `nan`, `inf`, or `-inf`.\n        foreach (`bool`, defaults to `None`):\n            Use the faster foreach-based implementation. If `None`, use the foreach implementation for CUDA and CPU native tensors\n            and silently fall back to the slow implementation for other device types.\n        pp_mesh (`torch.distributed.device_mesh.DeviceMesh`, defaults to `None`):\n            Pipeline parallel device mesh. If not `None`, will reduce gradient norm across PP stages.\n\n    Returns:\n        `torch.Tensor`:\n            Total norm of the gradients\n    \"\"\"\n    grads = [p.grad for p in parameters if p.grad is not None]\n\n    # TODO(aryan): Wait for next Pytorch release to use `torch.nn.utils.get_total_norm`\n    # total_norm = torch.nn.utils.get_total_norm(grads, norm_type, error_if_nonfinite, foreach)\n    total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)\n\n    # If total_norm is a DTensor, the placements must be `torch.distributed._tensor.ops.math_ops._NormPartial`.\n    # We can simply reduce the DTensor to get the total norm in this tensor's process group\n    # and then convert it to a local tensor.\n    # It has two purposes:\n    #   1. to make sure the total norm is computed correctly when PP is used (see below)\n    #   2. to return a reduced total_norm tensor whose .item() would return the correct value\n    if isinstance(total_norm, torch.distributed.tensor.DTensor):\n        # Will reach here if any non-PP parallelism is used.\n        # If only using PP, total_norm will be a local tensor.\n        total_norm = total_norm.full_tensor()\n\n    if pp_mesh is not None:\n        raise NotImplementedError(\"Pipeline parallel is not supported\")\n        if math.isinf(norm_type):\n            dist.all_reduce(total_norm,\n                            op=dist.ReduceOp.MAX,\n                            group=pp_mesh.get_group())\n        else:\n            total_norm **= norm_type\n            dist.all_reduce(total_norm,\n                            op=dist.ReduceOp.SUM,\n                            group=pp_mesh.get_group())\n            total_norm **= 1.0 / norm_type\n\n    _clip_grads_with_norm_(parameters, max_norm, total_norm, foreach)\n    return total_norm\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.training_utils.compute_density_for_timestep_sampling","title":"fastvideo.training.training_utils.compute_density_for_timestep_sampling","text":"<pre><code>compute_density_for_timestep_sampling(\n    weighting_scheme: str,\n    batch_size: int,\n    generator,\n    logit_mean: float | None = None,\n    logit_std: float | None = None,\n    mode_scale: float | None = None,\n)\n</code></pre> <p>Compute the density for sampling the timesteps when doing SD3 training.</p> <p>Courtesy: This was contributed by Rafie Walker in https://github.com/huggingface/diffusers/pull/8528.</p> <p>SD3 paper reference: https://arxiv.org/abs/2403.03206v1.</p> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def compute_density_for_timestep_sampling(\n    weighting_scheme: str,\n    batch_size: int,\n    generator,\n    logit_mean: float | None = None,\n    logit_std: float | None = None,\n    mode_scale: float | None = None,\n):\n    \"\"\"\n    Compute the density for sampling the timesteps when doing SD3 training.\n\n    Courtesy: This was contributed by Rafie Walker in https://github.com/huggingface/diffusers/pull/8528.\n\n    SD3 paper reference: https://arxiv.org/abs/2403.03206v1.\n    \"\"\"\n    if weighting_scheme == \"logit_normal\":\n        # See 3.1 in the SD3 paper ($rf/lognorm(0.00,1.00)$).\n        u = torch.normal(\n            mean=logit_mean,\n            std=logit_std,\n            size=(batch_size, ),\n            device=\"cpu\",\n            generator=generator,\n        )\n        u = torch.nn.functional.sigmoid(u)\n    elif weighting_scheme == \"mode\":\n        u = torch.rand(size=(batch_size, ), device=\"cpu\", generator=generator)\n        u = 1 - u - mode_scale * (torch.cos(math.pi * u / 2)**2 - 1 + u)\n    else:\n        u = torch.rand(size=(batch_size, ), device=\"cpu\", generator=generator)\n    return u\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.training_utils.custom_to_hf_state_dict","title":"fastvideo.training.training_utils.custom_to_hf_state_dict","text":"<pre><code>custom_to_hf_state_dict(\n    state_dict: dict[str, Any]\n    | Iterator[tuple[str, Tensor]],\n    reverse_param_names_mapping: dict[\n        str, tuple[str, int, int]\n    ],\n) -&gt; dict[str, Any]\n</code></pre> <p>Convert fastvideo's custom model format to diffusers format using reverse_param_names_mapping.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>dict[str, Any] | Iterator[tuple[str, Tensor]]</code> <p>State dict in fastvideo's custom format</p> required <code>reverse_param_names_mapping</code> <code>dict[str, tuple[str, int, int]]</code> <p>Reverse mapping from fastvideo's custom format to diffusers format</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>State dict in diffusers format</p> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def custom_to_hf_state_dict(\n    state_dict: dict[str, Any] | Iterator[tuple[str, torch.Tensor]],\n    reverse_param_names_mapping: dict[str, tuple[str, int,\n                                                 int]]) -&gt; dict[str, Any]:\n    \"\"\"\n    Convert fastvideo's custom model format to diffusers format using reverse_param_names_mapping.\n\n    Args:\n        state_dict: State dict in fastvideo's custom format\n        reverse_param_names_mapping: Reverse mapping from fastvideo's custom format to diffusers format\n\n    Returns:\n        State dict in diffusers format\n    \"\"\"\n    assert len(\n        reverse_param_names_mapping) &gt; 0, \"reverse_param_names_mapping is empty\"\n    if isinstance(state_dict, Iterator):\n        state_dict = dict(state_dict)\n    new_state_dict = {}\n    # Group parameters that need to be split (merged parameters)\n    merge_groups: dict[str, list[tuple[str, int, int]]] = {}\n\n    # First pass: collect all merge groups\n    for training_key, (\n            diffusers_key, merge_index,\n            num_params_to_merge) in reverse_param_names_mapping.items():\n        if merge_index is not None:\n            # This is a merged parameter that needs to be split\n            if training_key not in merge_groups:\n                merge_groups[training_key] = []\n            merge_groups[training_key].append(\n                (diffusers_key, merge_index, num_params_to_merge))\n\n    # Second pass: handle merged parameters by splitting them\n    used_keys = set()\n    for training_key, splits in merge_groups.items():\n        if training_key in state_dict:\n            v = state_dict[training_key]\n            # Sort by merge_index to ensure correct order\n            splits.sort(key=lambda x: x[1])\n            total = splits[0][2]\n            split_size = v.shape[0] // total\n            split_tensors = torch.split(v, split_size, dim=0)\n\n            for diffusers_key, split_index, _ in splits:\n                new_state_dict[diffusers_key] = split_tensors[split_index]\n            used_keys.add(training_key)\n\n    # Third pass: handle regular parameters (direct mappings)\n    for training_key, v in state_dict.items():\n        if training_key in used_keys:\n            continue\n\n        if training_key in reverse_param_names_mapping:\n            diffusers_key, merge_index, _ = reverse_param_names_mapping[\n                training_key]\n            if merge_index is None:\n                # Direct mapping\n                new_state_dict[diffusers_key] = v\n        else:\n            # No mapping found, keep as is\n            new_state_dict[training_key] = v\n\n    return new_state_dict\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.training_utils.get_constant_schedule","title":"fastvideo.training.training_utils.get_constant_schedule","text":"<pre><code>get_constant_schedule(\n    optimizer: Optimizer, last_epoch: int = -1\n) -&gt; LambdaLR\n</code></pre> <p>Create a schedule with a constant learning rate, using the learning rate set in optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>[`~torch.optim.Optimizer`]</code> <p>The optimizer for which to schedule the learning rate.</p> required <code>last_epoch</code> <code>`int`, *optional*, defaults to -1</code> <p>The index of the last epoch when resuming training.</p> <code>-1</code> Return <p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def get_constant_schedule(optimizer: Optimizer,\n                          last_epoch: int = -1) -&gt; LambdaLR:\n    \"\"\"\n    Create a schedule with a constant learning rate, using the learning rate set in optimizer.\n\n    Args:\n        optimizer ([`~torch.optim.Optimizer`]):\n            The optimizer for which to schedule the learning rate.\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n\n    Return:\n        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"\n    return LambdaLR(optimizer, lambda _: 1, last_epoch=last_epoch)\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.training_utils.get_constant_schedule_with_warmup","title":"fastvideo.training.training_utils.get_constant_schedule_with_warmup","text":"<pre><code>get_constant_schedule_with_warmup(\n    optimizer: Optimizer,\n    num_warmup_steps: int,\n    last_epoch: int = -1,\n) -&gt; LambdaLR\n</code></pre> <p>Create a schedule with a constant learning rate preceded by a warmup period during which the learning rate increases linearly between 0 and the initial lr set in the optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>[`~torch.optim.Optimizer`]</code> <p>The optimizer for which to schedule the learning rate.</p> required <code>num_warmup_steps</code> <code>`int`</code> <p>The number of steps for the warmup phase.</p> required <code>last_epoch</code> <code>`int`, *optional*, defaults to -1</code> <p>The index of the last epoch when resuming training.</p> <code>-1</code> Return <p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def get_constant_schedule_with_warmup(optimizer: Optimizer,\n                                      num_warmup_steps: int,\n                                      last_epoch: int = -1) -&gt; LambdaLR:\n    \"\"\"\n    Create a schedule with a constant learning rate preceded by a warmup period during which the learning rate\n    increases linearly between 0 and the initial lr set in the optimizer.\n\n    Args:\n        optimizer ([`~torch.optim.Optimizer`]):\n            The optimizer for which to schedule the learning rate.\n        num_warmup_steps (`int`):\n            The number of steps for the warmup phase.\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n\n    Return:\n        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"\n\n    def lr_lambda(current_step: int):\n        if current_step &lt; num_warmup_steps:\n            return float(current_step) / float(max(1.0, num_warmup_steps))\n        return 1.0\n\n    return LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.training_utils.get_cosine_schedule_with_min_lr","title":"fastvideo.training.training_utils.get_cosine_schedule_with_min_lr","text":"<pre><code>get_cosine_schedule_with_min_lr(\n    optimizer: Optimizer,\n    num_warmup_steps: int,\n    num_training_steps: int,\n    min_lr_ratio: float = 0.1,\n    num_cycles: float = 0.5,\n    last_epoch: int = -1,\n) -&gt; LambdaLR\n</code></pre> <p>Create a schedule with a learning rate that decreases following the values of the cosine function between the initial lr set in the optimizer to a minimum lr (min_lr_ratio * initial_lr), after a warmup period during which  it increases linearly between 0 and the initial lr set in the optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>[`~torch.optim.Optimizer`]</code> <p>The optimizer for which to schedule the learning rate.</p> required <code>num_warmup_steps</code> <code>`int`</code> <p>The number of steps for the warmup phase.</p> required <code>num_training_steps</code> <code>`int`</code> <p>The total number of training steps.</p> required <code>min_lr_ratio</code> <code>`float`, *optional*, defaults to 0.1</code> <p>The ratio of minimum learning rate to initial learning rate.</p> <code>0.1</code> <code>num_cycles</code> <code>`float`, *optional*, defaults to 0.5</code> <p>The number of periods of the cosine function in a schedule.</p> <code>0.5</code> <code>last_epoch</code> <code>`int`, *optional*, defaults to -1</code> <p>The index of the last epoch when resuming training.</p> <code>-1</code> Return <p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def get_cosine_schedule_with_min_lr(optimizer: Optimizer,\n                                    num_warmup_steps: int,\n                                    num_training_steps: int,\n                                    min_lr_ratio: float = 0.1,\n                                    num_cycles: float = 0.5,\n                                    last_epoch: int = -1) -&gt; LambdaLR:\n    \"\"\"\n    Create a schedule with a learning rate that decreases following the values of the cosine function between the\n    initial lr set in the optimizer to a minimum lr (min_lr_ratio * initial_lr), after a warmup period during which \n    it increases linearly between 0 and the initial lr set in the optimizer.\n\n    Args:\n        optimizer ([`~torch.optim.Optimizer`]):\n            The optimizer for which to schedule the learning rate.\n        num_warmup_steps (`int`):\n            The number of steps for the warmup phase.\n        num_training_steps (`int`):\n            The total number of training steps.\n        min_lr_ratio (`float`, *optional*, defaults to 0.1):\n            The ratio of minimum learning rate to initial learning rate.\n        num_cycles (`float`, *optional*, defaults to 0.5):\n            The number of periods of the cosine function in a schedule.\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n\n    Return:\n        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"\n\n    def lr_lambda(current_step):\n        if current_step &lt; num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        progress = float(current_step - num_warmup_steps) / float(\n            max(1, num_training_steps - num_warmup_steps))\n        # Cosine decay from 1.0 to min_lr_ratio over num_cycles periods\n        # Use the same formula as standard cosine but ensure minimum is min_lr_ratio instead of 0\n        cosine_value = 0.5 * (\n            1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))\n        # Ensure the value doesn't go below min_lr_ratio\n        return max(min_lr_ratio, cosine_value)\n\n    return LambdaLR(optimizer, lr_lambda, last_epoch)\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.training_utils.get_cosine_schedule_with_warmup","title":"fastvideo.training.training_utils.get_cosine_schedule_with_warmup","text":"<pre><code>get_cosine_schedule_with_warmup(\n    optimizer: Optimizer,\n    num_warmup_steps: int,\n    num_training_steps: int,\n    num_cycles: float = 0.5,\n    last_epoch: int = -1,\n) -&gt; LambdaLR\n</code></pre> <p>Create a schedule with a learning rate that decreases following the values of the cosine function between the initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the initial lr set in the optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>[`~torch.optim.Optimizer`]</code> <p>The optimizer for which to schedule the learning rate.</p> required <code>num_warmup_steps</code> <code>`int`</code> <p>The number of steps for the warmup phase.</p> required <code>num_training_steps</code> <code>`int`</code> <p>The total number of training steps.</p> required <code>num_periods</code> <code>`float`, *optional*, defaults to 0.5</code> <p>The number of periods of the cosine function in a schedule (the default is to just decrease from the max value to 0 following a half-cosine).</p> required <code>last_epoch</code> <code>`int`, *optional*, defaults to -1</code> <p>The index of the last epoch when resuming training.</p> <code>-1</code> Return <p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def get_cosine_schedule_with_warmup(optimizer: Optimizer,\n                                    num_warmup_steps: int,\n                                    num_training_steps: int,\n                                    num_cycles: float = 0.5,\n                                    last_epoch: int = -1) -&gt; LambdaLR:\n    \"\"\"\n    Create a schedule with a learning rate that decreases following the values of the cosine function between the\n    initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the\n    initial lr set in the optimizer.\n\n    Args:\n        optimizer ([`~torch.optim.Optimizer`]):\n            The optimizer for which to schedule the learning rate.\n        num_warmup_steps (`int`):\n            The number of steps for the warmup phase.\n        num_training_steps (`int`):\n            The total number of training steps.\n        num_periods (`float`, *optional*, defaults to 0.5):\n            The number of periods of the cosine function in a schedule (the default is to just decrease from the max\n            value to 0 following a half-cosine).\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n\n    Return:\n        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"\n\n    def lr_lambda(current_step):\n        if current_step &lt; num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        progress = float(current_step - num_warmup_steps) / float(\n            max(1, num_training_steps - num_warmup_steps))\n        return max(\n            0.0, 0.5 *\n            (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))\n\n    return LambdaLR(optimizer, lr_lambda, last_epoch)\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.training_utils.get_cosine_with_hard_restarts_schedule_with_warmup","title":"fastvideo.training.training_utils.get_cosine_with_hard_restarts_schedule_with_warmup","text":"<pre><code>get_cosine_with_hard_restarts_schedule_with_warmup(\n    optimizer: Optimizer,\n    num_warmup_steps: int,\n    num_training_steps: int,\n    num_cycles: int = 1,\n    last_epoch: int = -1,\n) -&gt; LambdaLR\n</code></pre> <p>Create a schedule with a learning rate that decreases following the values of the cosine function between the initial lr set in the optimizer to 0, with several hard restarts, after a warmup period during which it increases linearly between 0 and the initial lr set in the optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>[`~torch.optim.Optimizer`]</code> <p>The optimizer for which to schedule the learning rate.</p> required <code>num_warmup_steps</code> <code>`int`</code> <p>The number of steps for the warmup phase.</p> required <code>num_training_steps</code> <code>`int`</code> <p>The total number of training steps.</p> required <code>num_cycles</code> <code>`int`, *optional*, defaults to 1</code> <p>The number of hard restarts to use.</p> <code>1</code> <code>last_epoch</code> <code>`int`, *optional*, defaults to -1</code> <p>The index of the last epoch when resuming training.</p> <code>-1</code> Return <p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def get_cosine_with_hard_restarts_schedule_with_warmup(\n        optimizer: Optimizer,\n        num_warmup_steps: int,\n        num_training_steps: int,\n        num_cycles: int = 1,\n        last_epoch: int = -1) -&gt; LambdaLR:\n    \"\"\"\n    Create a schedule with a learning rate that decreases following the values of the cosine function between the\n    initial lr set in the optimizer to 0, with several hard restarts, after a warmup period during which it increases\n    linearly between 0 and the initial lr set in the optimizer.\n\n    Args:\n        optimizer ([`~torch.optim.Optimizer`]):\n            The optimizer for which to schedule the learning rate.\n        num_warmup_steps (`int`):\n            The number of steps for the warmup phase.\n        num_training_steps (`int`):\n            The total number of training steps.\n        num_cycles (`int`, *optional*, defaults to 1):\n            The number of hard restarts to use.\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n\n    Return:\n        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"\n\n    def lr_lambda(current_step):\n        if current_step &lt; num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        progress = float(current_step - num_warmup_steps) / float(\n            max(1, num_training_steps - num_warmup_steps))\n        if progress &gt;= 1.0:\n            return 0.0\n        return max(\n            0.0, 0.5 * (1.0 + math.cos(math.pi *\n                                       ((float(num_cycles) * progress) % 1.0))))\n\n    return LambdaLR(optimizer, lr_lambda, last_epoch)\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.training_utils.get_linear_schedule_with_warmup","title":"fastvideo.training.training_utils.get_linear_schedule_with_warmup","text":"<pre><code>get_linear_schedule_with_warmup(\n    optimizer: Optimizer,\n    num_warmup_steps: int,\n    num_training_steps: int,\n    last_epoch: int = -1,\n) -&gt; LambdaLR\n</code></pre> <p>Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>[`~torch.optim.Optimizer`]</code> <p>The optimizer for which to schedule the learning rate.</p> required <code>num_warmup_steps</code> <code>`int`</code> <p>The number of steps for the warmup phase.</p> required <code>num_training_steps</code> <code>`int`</code> <p>The total number of training steps.</p> required <code>last_epoch</code> <code>`int`, *optional*, defaults to -1</code> <p>The index of the last epoch when resuming training.</p> <code>-1</code> Return <p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def get_linear_schedule_with_warmup(optimizer: Optimizer,\n                                    num_warmup_steps: int,\n                                    num_training_steps: int,\n                                    last_epoch: int = -1) -&gt; LambdaLR:\n    \"\"\"\n    Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after\n    a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.\n\n    Args:\n        optimizer ([`~torch.optim.Optimizer`]):\n            The optimizer for which to schedule the learning rate.\n        num_warmup_steps (`int`):\n            The number of steps for the warmup phase.\n        num_training_steps (`int`):\n            The total number of training steps.\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n\n    Return:\n        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"\n\n    def lr_lambda(current_step: int):\n        if current_step &lt; num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        return max(\n            0.0,\n            float(num_training_steps - current_step) /\n            float(max(1, num_training_steps - num_warmup_steps)))\n\n    return LambdaLR(optimizer, lr_lambda, last_epoch)\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.training_utils.get_piecewise_constant_schedule","title":"fastvideo.training.training_utils.get_piecewise_constant_schedule","text":"<pre><code>get_piecewise_constant_schedule(\n    optimizer: Optimizer,\n    step_rules: str,\n    last_epoch: int = -1,\n) -&gt; LambdaLR\n</code></pre> <p>Create a schedule with a constant learning rate, using the learning rate set in optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>[`~torch.optim.Optimizer`]</code> <p>The optimizer for which to schedule the learning rate.</p> required <code>step_rules</code> <code>`string`</code> <p>The rules for the learning rate. ex: rule_steps=\"1:10,0.1:20,0.01:30,0.005\" it means that the learning rate if multiple 1 for the first 10 steps, multiple 0.1 for the next 20 steps, multiple 0.01 for the next 30 steps and multiple 0.005 for the other steps.</p> required <code>last_epoch</code> <code>`int`, *optional*, defaults to -1</code> <p>The index of the last epoch when resuming training.</p> <code>-1</code> Return <p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def get_piecewise_constant_schedule(optimizer: Optimizer,\n                                    step_rules: str,\n                                    last_epoch: int = -1) -&gt; LambdaLR:\n    \"\"\"\n    Create a schedule with a constant learning rate, using the learning rate set in optimizer.\n\n    Args:\n        optimizer ([`~torch.optim.Optimizer`]):\n            The optimizer for which to schedule the learning rate.\n        step_rules (`string`):\n            The rules for the learning rate. ex: rule_steps=\"1:10,0.1:20,0.01:30,0.005\" it means that the learning rate\n            if multiple 1 for the first 10 steps, multiple 0.1 for the next 20 steps, multiple 0.01 for the next 30\n            steps and multiple 0.005 for the other steps.\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n\n    Return:\n        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"\n\n    rules_dict = {}\n    rule_list = step_rules.split(\",\")\n    for rule_str in rule_list[:-1]:\n        value_str, steps_str = rule_str.split(\":\")\n        steps = int(steps_str)\n        value = float(value_str)\n        rules_dict[steps] = value\n    last_lr_multiple = float(rule_list[-1])\n\n    def create_rules_function(\n            rules_dict: dict,\n            last_lr_multiple: float) -&gt; Callable[[int], float]:\n\n        def rule_func(steps: int) -&gt; float:\n            for step_threshold, lr_multiple in sorted(rules_dict.items()):\n                if steps &lt; step_threshold:\n                    return lr_multiple\n            return last_lr_multiple\n\n        return rule_func\n\n    rules_func = create_rules_function(rules_dict, last_lr_multiple)\n\n    return LambdaLR(optimizer, rules_func, last_epoch=last_epoch)\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.training_utils.get_polynomial_decay_schedule_with_warmup","title":"fastvideo.training.training_utils.get_polynomial_decay_schedule_with_warmup","text":"<pre><code>get_polynomial_decay_schedule_with_warmup(\n    optimizer: Optimizer,\n    num_warmup_steps: int,\n    num_training_steps: int,\n    lr_end: float = 1e-07,\n    power: float = 1.0,\n    last_epoch: int = -1,\n) -&gt; LambdaLR\n</code></pre> <p>Create a schedule with a learning rate that decreases as a polynomial decay from the initial lr set in the optimizer to end lr defined by lr_end, after a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>[`~torch.optim.Optimizer`]</code> <p>The optimizer for which to schedule the learning rate.</p> required <code>num_warmup_steps</code> <code>`int`</code> <p>The number of steps for the warmup phase.</p> required <code>num_training_steps</code> <code>`int`</code> <p>The total number of training steps.</p> required <code>lr_end</code> <code>`float`, *optional*, defaults to 1e-7</code> <p>The end LR.</p> <code>1e-07</code> <code>power</code> <code>`float`, *optional*, defaults to 1.0</code> <p>Power factor.</p> <code>1.0</code> <code>last_epoch</code> <code>`int`, *optional*, defaults to -1</code> <p>The index of the last epoch when resuming training.</p> <code>-1</code> <p>Note: power defaults to 1.0 as in the fairseq implementation, which in turn is based on the original BERT implementation at https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37</p> Return <p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def get_polynomial_decay_schedule_with_warmup(\n    optimizer: Optimizer,\n    num_warmup_steps: int,\n    num_training_steps: int,\n    lr_end: float = 1e-7,\n    power: float = 1.0,\n    last_epoch: int = -1,\n) -&gt; LambdaLR:\n    \"\"\"\n    Create a schedule with a learning rate that decreases as a polynomial decay from the initial lr set in the\n    optimizer to end lr defined by *lr_end*, after a warmup period during which it increases linearly from 0 to the\n    initial lr set in the optimizer.\n\n    Args:\n        optimizer ([`~torch.optim.Optimizer`]):\n            The optimizer for which to schedule the learning rate.\n        num_warmup_steps (`int`):\n            The number of steps for the warmup phase.\n        num_training_steps (`int`):\n            The total number of training steps.\n        lr_end (`float`, *optional*, defaults to 1e-7):\n            The end LR.\n        power (`float`, *optional*, defaults to 1.0):\n            Power factor.\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n\n    Note: *power* defaults to 1.0 as in the fairseq implementation, which in turn is based on the original BERT\n    implementation at\n    https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37\n\n    Return:\n        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n\n    \"\"\"\n\n    lr_init = optimizer.defaults[\"lr\"]\n    if not (lr_init &gt; lr_end):\n        raise ValueError(\n            f\"lr_end ({lr_end}) must be smaller than initial lr ({lr_init})\")\n\n    def lr_lambda(current_step: int):\n        if current_step &lt; num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        elif current_step &gt; num_training_steps:\n            return lr_end / lr_init  # as LambdaLR multiplies by lr_init\n        else:\n            lr_range = lr_init - lr_end\n            decay_steps = num_training_steps - num_warmup_steps\n            pct_remaining = 1 - (current_step - num_warmup_steps) / decay_steps\n            decay = lr_range * pct_remaining**power + lr_end\n            return decay / lr_init  # as LambdaLR multiplies by lr_init\n\n    return LambdaLR(optimizer, lr_lambda, last_epoch)\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.training_utils.get_scheduler","title":"fastvideo.training.training_utils.get_scheduler","text":"<pre><code>get_scheduler(\n    name: str | SchedulerType,\n    optimizer: Optimizer,\n    step_rules: str | None = None,\n    num_warmup_steps: int | None = None,\n    num_training_steps: int | None = None,\n    num_cycles: int = 1,\n    power: float = 1.0,\n    min_lr_ratio: float = 0.1,\n    last_epoch: int = -1,\n) -&gt; LambdaLR\n</code></pre> <p>Unified API to get any scheduler from its name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>`str` or `SchedulerType`</code> <p>The name of the scheduler to use.</p> required <code>optimizer</code> <code>`torch.optim.Optimizer`</code> <p>The optimizer that will be used during training.</p> required <code>step_rules</code> <code>`str`, *optional*</code> <p>A string representing the step rules to use. This is only used by the <code>PIECEWISE_CONSTANT</code> scheduler.</p> <code>None</code> <code>num_warmup_steps</code> <code>`int`, *optional*</code> <p>The number of warmup steps to do. This is not required by all schedulers (hence the argument being optional), the function will raise an error if it's unset and the scheduler type requires it.</p> <code>None</code> <code>num_training_steps</code> <code>`int``, *optional*</code> <p>The number of training steps to do. This is not required by all schedulers (hence the argument being optional), the function will raise an error if it's unset and the scheduler type requires it.</p> <code>None</code> <code>num_cycles</code> <code>`int`, *optional*</code> <p>The number of hard restarts used in <code>COSINE_WITH_RESTARTS</code> scheduler.</p> <code>1</code> <code>power</code> <code>`float`, *optional*, defaults to 1.0</code> <p>Power factor. See <code>POLYNOMIAL</code> scheduler</p> <code>1.0</code> <code>min_lr_ratio</code> <code>`float`, *optional*, defaults to 0.1</code> <p>The ratio of minimum learning rate to initial learning rate. Used in <code>COSINE_WITH_MIN_LR</code> scheduler.</p> <code>0.1</code> <code>last_epoch</code> <code>`int`, *optional*, defaults to -1</code> <p>The index of the last epoch when resuming training.</p> <code>-1</code> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def get_scheduler(\n    name: str | SchedulerType,\n    optimizer: Optimizer,\n    step_rules: str | None = None,\n    num_warmup_steps: int | None = None,\n    num_training_steps: int | None = None,\n    num_cycles: int = 1,\n    power: float = 1.0,\n    min_lr_ratio: float = 0.1,\n    last_epoch: int = -1,\n) -&gt; LambdaLR:\n    \"\"\"\n    Unified API to get any scheduler from its name.\n\n    Args:\n        name (`str` or `SchedulerType`):\n            The name of the scheduler to use.\n        optimizer (`torch.optim.Optimizer`):\n            The optimizer that will be used during training.\n        step_rules (`str`, *optional*):\n            A string representing the step rules to use. This is only used by the `PIECEWISE_CONSTANT` scheduler.\n        num_warmup_steps (`int`, *optional*):\n            The number of warmup steps to do. This is not required by all schedulers (hence the argument being\n            optional), the function will raise an error if it's unset and the scheduler type requires it.\n        num_training_steps (`int``, *optional*):\n            The number of training steps to do. This is not required by all schedulers (hence the argument being\n            optional), the function will raise an error if it's unset and the scheduler type requires it.\n        num_cycles (`int`, *optional*):\n            The number of hard restarts used in `COSINE_WITH_RESTARTS` scheduler.\n        power (`float`, *optional*, defaults to 1.0):\n            Power factor. See `POLYNOMIAL` scheduler\n        min_lr_ratio (`float`, *optional*, defaults to 0.1):\n            The ratio of minimum learning rate to initial learning rate. Used in `COSINE_WITH_MIN_LR` scheduler.\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n    \"\"\"\n    name = SchedulerType(name)\n    schedule_func = TYPE_TO_SCHEDULER_FUNCTION[name]\n    if name == SchedulerType.CONSTANT:\n        return schedule_func(optimizer, last_epoch=last_epoch)\n\n    if name == SchedulerType.PIECEWISE_CONSTANT:\n        return schedule_func(optimizer,\n                             step_rules=step_rules,\n                             last_epoch=last_epoch)\n\n    # All other schedulers require `num_warmup_steps`\n    if num_warmup_steps is None:\n        raise ValueError(\n            f\"{name} requires `num_warmup_steps`, please provide that argument.\"\n        )\n\n    if name == SchedulerType.CONSTANT_WITH_WARMUP:\n        return schedule_func(optimizer,\n                             num_warmup_steps=num_warmup_steps,\n                             last_epoch=last_epoch)\n\n    # All other schedulers require `num_training_steps`\n    if num_training_steps is None:\n        raise ValueError(\n            f\"{name} requires `num_training_steps`, please provide that argument.\"\n        )\n\n    if name == SchedulerType.COSINE_WITH_RESTARTS:\n        return schedule_func(\n            optimizer,\n            num_warmup_steps=num_warmup_steps,\n            num_training_steps=num_training_steps,\n            num_cycles=num_cycles,\n            last_epoch=last_epoch,\n        )\n\n    if name == SchedulerType.POLYNOMIAL:\n        return schedule_func(\n            optimizer,\n            num_warmup_steps=num_warmup_steps,\n            num_training_steps=num_training_steps,\n            power=power,\n            last_epoch=last_epoch,\n        )\n\n    if name == SchedulerType.COSINE_WITH_MIN_LR:\n        return schedule_func(\n            optimizer,\n            num_warmup_steps=num_warmup_steps,\n            num_training_steps=num_training_steps,\n            min_lr_ratio=min_lr_ratio,\n            last_epoch=last_epoch,\n        )\n\n    return schedule_func(optimizer,\n                         num_warmup_steps=num_warmup_steps,\n                         num_training_steps=num_training_steps,\n                         last_epoch=last_epoch)\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.training_utils.load_checkpoint","title":"fastvideo.training.training_utils.load_checkpoint","text":"<pre><code>load_checkpoint(\n    transformer,\n    rank,\n    checkpoint_path,\n    optimizer=None,\n    dataloader=None,\n    scheduler=None,\n    noise_generator=None,\n) -&gt; int\n</code></pre> <p>Load checkpoint following finetrainer's distributed checkpoint approach. Returns the step number from which training should resume.</p> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def load_checkpoint(transformer,\n                    rank,\n                    checkpoint_path,\n                    optimizer=None,\n                    dataloader=None,\n                    scheduler=None,\n                    noise_generator=None) -&gt; int:\n    \"\"\"\n    Load checkpoint following finetrainer's distributed checkpoint approach.\n    Returns the step number from which training should resume.\n    \"\"\"\n    if not os.path.exists(checkpoint_path):\n        logger.warning(\"Checkpoint path %s does not exist\", checkpoint_path)\n        return 0\n\n    # Extract step number from checkpoint path\n    step = int(os.path.basename(checkpoint_path).split('-')[-1])\n\n    if rank == 0:\n        logger.info(\"Loading checkpoint from step %s\", step)\n\n    dcp_dir = os.path.join(checkpoint_path, \"distributed_checkpoint\")\n\n    if not os.path.exists(dcp_dir):\n        logger.warning(\"Distributed checkpoint directory %s does not exist\",\n                       dcp_dir)\n        return 0\n\n    states = {\n        \"model\": ModelWrapper(transformer),\n        \"random_state\": RandomStateWrapper(noise_generator),\n    }\n\n    if optimizer is not None:\n        states[\"optimizer\"] = OptimizerWrapper(transformer, optimizer)\n\n    if dataloader is not None:\n        states[\"dataloader\"] = dataloader\n\n    if scheduler is not None:\n        states[\"scheduler\"] = SchedulerWrapper(scheduler)\n\n    logger.info(\"rank: %s, loading distributed checkpoint from %s\",\n                rank,\n                dcp_dir,\n                local_main_process_only=False)\n\n    begin_time = time.perf_counter()\n    dcp.load(states, checkpoint_id=dcp_dir)\n    end_time = time.perf_counter()\n\n    logger.info(\"rank: %s, distributed checkpoint loaded in %.2f seconds\",\n                rank,\n                end_time - begin_time,\n                local_main_process_only=False)\n    logger.info(\"--&gt; checkpoint loaded from step %s\", step)\n\n    return step\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.training_utils.load_distillation_checkpoint","title":"fastvideo.training.training_utils.load_distillation_checkpoint","text":"<pre><code>load_distillation_checkpoint(\n    generator_transformer,\n    fake_score_transformer,\n    rank,\n    checkpoint_path,\n    generator_optimizer=None,\n    fake_score_optimizer=None,\n    dataloader=None,\n    generator_scheduler=None,\n    fake_score_scheduler=None,\n    noise_generator=None,\n    generator_ema=None,\n    generator_transformer_2=None,\n    real_score_transformer_2=None,\n    fake_score_transformer_2=None,\n    generator_optimizer_2=None,\n    fake_score_optimizer_2=None,\n    generator_scheduler_2=None,\n    fake_score_scheduler_2=None,\n    generator_ema_2=None,\n) -&gt; int\n</code></pre> <p>Load distillation checkpoint with both generator and fake_score models. Supports MoE (Mixture of Experts) models with transformer_2 variants. Returns the step number from which training should resume.</p> <p>Parameters:</p> Name Type Description Default <code>generator_transformer</code> <p>Main generator transformer model</p> required <code>fake_score_transformer</code> <p>Main fake score transformer model</p> required <code>generator_transformer_2</code> <p>Secondary generator transformer for MoE (optional)</p> <code>None</code> <code>real_score_transformer_2</code> <p>Secondary real score transformer for MoE (optional)</p> <code>None</code> <code>fake_score_transformer_2</code> <p>Secondary fake score transformer for MoE (optional)</p> <code>None</code> <code>generator_optimizer_2</code> <p>Optimizer for generator_transformer_2 (optional)</p> <code>None</code> <code>fake_score_optimizer_2</code> <p>Optimizer for fake_score_transformer_2 (optional)</p> <code>None</code> <code>generator_scheduler_2</code> <p>Scheduler for generator_transformer_2 (optional)</p> <code>None</code> <code>fake_score_scheduler_2</code> <p>Scheduler for fake_score_transformer_2 (optional)</p> <code>None</code> <code>generator_ema_2</code> <p>EMA for generator_transformer_2 (optional)</p> <code>None</code> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def load_distillation_checkpoint(\n        generator_transformer,\n        fake_score_transformer,\n        rank,\n        checkpoint_path,\n        generator_optimizer=None,\n        fake_score_optimizer=None,\n        dataloader=None,\n        generator_scheduler=None,\n        fake_score_scheduler=None,\n        noise_generator=None,\n        generator_ema=None,\n        # MoE support\n        generator_transformer_2=None,\n        real_score_transformer_2=None,\n        fake_score_transformer_2=None,\n        generator_optimizer_2=None,\n        fake_score_optimizer_2=None,\n        generator_scheduler_2=None,\n        fake_score_scheduler_2=None,\n        generator_ema_2=None) -&gt; int:\n    \"\"\"\n    Load distillation checkpoint with both generator and fake_score models.\n    Supports MoE (Mixture of Experts) models with transformer_2 variants.\n    Returns the step number from which training should resume.\n\n    Args:\n        generator_transformer: Main generator transformer model\n        fake_score_transformer: Main fake score transformer model\n        generator_transformer_2: Secondary generator transformer for MoE (optional)\n        real_score_transformer_2: Secondary real score transformer for MoE (optional)\n        fake_score_transformer_2: Secondary fake score transformer for MoE (optional)\n        generator_optimizer_2: Optimizer for generator_transformer_2 (optional)\n        fake_score_optimizer_2: Optimizer for fake_score_transformer_2 (optional)\n        generator_scheduler_2: Scheduler for generator_transformer_2 (optional)\n        fake_score_scheduler_2: Scheduler for fake_score_transformer_2 (optional)\n        generator_ema_2: EMA for generator_transformer_2 (optional)\n    \"\"\"\n    if not os.path.exists(checkpoint_path):\n        logger.warning(\"Distillation checkpoint path %s does not exist\",\n                       checkpoint_path)\n        return 0\n\n    # Extract step number from checkpoint path\n    step = int(os.path.basename(checkpoint_path).split('-')[-1])\n\n    if rank == 0:\n        logger.info(\"Loading distillation checkpoint from step %s\", step)\n\n    # Load generator distributed checkpoint\n    generator_dcp_dir = os.path.join(checkpoint_path, \"distributed_checkpoint\",\n                                     \"generator\")\n    if not os.path.exists(generator_dcp_dir):\n        logger.warning(\n            \"Generator distributed checkpoint directory %s does not exist\",\n            generator_dcp_dir)\n        return 0\n\n    generator_states = {\n        \"model\": ModelWrapper(generator_transformer),\n    }\n\n    if generator_optimizer is not None:\n        generator_states[\"optimizer\"] = OptimizerWrapper(\n            generator_transformer, generator_optimizer)\n\n    if dataloader is not None:\n        generator_states[\"dataloader\"] = dataloader\n\n    if generator_scheduler is not None:\n        generator_states[\"scheduler\"] = SchedulerWrapper(generator_scheduler)\n\n    logger.info(\"rank: %s, loading generator distributed checkpoint from %s\",\n                rank,\n                generator_dcp_dir,\n                local_main_process_only=False)\n\n    begin_time = time.perf_counter()\n    dcp.load(generator_states, checkpoint_id=generator_dcp_dir)\n    end_time = time.perf_counter()\n\n    logger.info(\n        \"rank: %s, generator distributed checkpoint loaded in %.2f seconds\",\n        rank,\n        end_time - begin_time,\n        local_main_process_only=False)\n\n    # Load EMA state if available and generator_ema is provided\n    if generator_ema is not None:\n        try:\n            ema_state = generator_states.get(\"ema\")\n            if ema_state is not None:\n                generator_ema.load_state_dict(ema_state)\n                logger.info(\"rank: %s, generator EMA state loaded successfully\",\n                            rank)\n            else:\n                logger.info(\"rank: %s, no EMA state found in checkpoint\", rank)\n        except Exception as e:\n            logger.warning(\"rank: %s, failed to load EMA state: %s\", rank,\n                           str(e))\n\n    # Load generator_2 distributed checkpoint (MoE support)\n    if generator_transformer_2 is not None:\n        generator_2_dcp_dir = os.path.join(checkpoint_path,\n                                           \"distributed_checkpoint\",\n                                           \"generator_2\")\n        if os.path.exists(generator_2_dcp_dir):\n            generator_2_states = {\n                \"model\": ModelWrapper(generator_transformer_2),\n            }\n\n            if generator_optimizer_2 is not None:\n                generator_2_states[\"optimizer\"] = OptimizerWrapper(\n                    generator_transformer_2, generator_optimizer_2)\n\n            if dataloader is not None:\n                generator_2_states[\"dataloader\"] = dataloader\n\n            if generator_scheduler_2 is not None:\n                generator_2_states[\"scheduler\"] = SchedulerWrapper(\n                    generator_scheduler_2)\n\n            logger.info(\n                \"rank: %s, loading generator_2 distributed checkpoint from %s\",\n                rank,\n                generator_2_dcp_dir,\n                local_main_process_only=False)\n\n            begin_time = time.perf_counter()\n            dcp.load(generator_2_states, checkpoint_id=generator_2_dcp_dir)\n            end_time = time.perf_counter()\n\n            logger.info(\n                \"rank: %s, generator_2 distributed checkpoint loaded in %.2f seconds\",\n                rank,\n                end_time - begin_time,\n                local_main_process_only=False)\n\n            # Load EMA_2 state if available and generator_ema_2 is provided\n            if generator_ema_2 is not None:\n                try:\n                    ema_2_state = generator_2_states.get(\"ema\")\n                    if ema_2_state is not None:\n                        generator_ema_2.load_state_dict(ema_2_state)\n                        logger.info(\n                            \"rank: %s, generator_2 EMA state loaded successfully\",\n                            rank)\n                    else:\n                        logger.info(\n                            \"rank: %s, no EMA_2 state found in checkpoint\",\n                            rank)\n                except Exception as e:\n                    logger.warning(\"rank: %s, failed to load EMA_2 state: %s\",\n                                   rank, str(e))\n        else:\n            logger.info(\"rank: %s, generator_2 checkpoint not found, skipping\",\n                        rank)\n\n    # Load critic distributed checkpoint\n    critic_dcp_dir = os.path.join(checkpoint_path, \"distributed_checkpoint\",\n                                  \"critic\")\n    if not os.path.exists(critic_dcp_dir):\n        logger.warning(\n            \"Critic distributed checkpoint directory %s does not exist\",\n            critic_dcp_dir)\n        return 0\n\n    critic_states = {\n        \"model\": ModelWrapper(fake_score_transformer),\n    }\n\n    if fake_score_optimizer is not None:\n        critic_states[\"optimizer\"] = OptimizerWrapper(fake_score_transformer,\n                                                      fake_score_optimizer)\n\n    if dataloader is not None:\n        critic_states[\"dataloader\"] = dataloader\n\n    if fake_score_scheduler is not None:\n        critic_states[\"scheduler\"] = SchedulerWrapper(fake_score_scheduler)\n\n    logger.info(\"rank: %s, loading critic distributed checkpoint from %s\",\n                rank,\n                critic_dcp_dir,\n                local_main_process_only=False)\n\n    begin_time = time.perf_counter()\n    dcp.load(critic_states, checkpoint_id=critic_dcp_dir)\n    end_time = time.perf_counter()\n\n    logger.info(\n        \"rank: %s, critic distributed checkpoint loaded in %.2f seconds\",\n        rank,\n        end_time - begin_time,\n        local_main_process_only=False)\n\n    # Load critic_2 distributed checkpoint (MoE support)\n    if fake_score_transformer_2 is not None:\n        critic_2_dcp_dir = os.path.join(checkpoint_path,\n                                        \"distributed_checkpoint\", \"critic_2\")\n        if os.path.exists(critic_2_dcp_dir):\n            critic_2_states = {\n                \"model\": ModelWrapper(fake_score_transformer_2),\n            }\n\n            if fake_score_optimizer_2 is not None:\n                critic_2_states[\"optimizer\"] = OptimizerWrapper(\n                    fake_score_transformer_2, fake_score_optimizer_2)\n\n            if dataloader is not None:\n                critic_2_states[\"dataloader\"] = dataloader\n\n            if fake_score_scheduler_2 is not None:\n                critic_2_states[\"scheduler\"] = SchedulerWrapper(\n                    fake_score_scheduler_2)\n\n            logger.info(\n                \"rank: %s, loading critic_2 distributed checkpoint from %s\",\n                rank,\n                critic_2_dcp_dir,\n                local_main_process_only=False)\n\n            begin_time = time.perf_counter()\n            dcp.load(critic_2_states, checkpoint_id=critic_2_dcp_dir)\n            end_time = time.perf_counter()\n\n            logger.info(\n                \"rank: %s, critic_2 distributed checkpoint loaded in %.2f seconds\",\n                rank,\n                end_time - begin_time,\n                local_main_process_only=False)\n        else:\n            logger.info(\"rank: %s, critic_2 checkpoint not found, skipping\",\n                        rank)\n\n    # Load real_score_2 distributed checkpoint (MoE support)\n    if real_score_transformer_2 is not None:\n        real_score_2_dcp_dir = os.path.join(checkpoint_path,\n                                            \"distributed_checkpoint\",\n                                            \"real_score_2\")\n        if os.path.exists(real_score_2_dcp_dir):\n            real_score_2_states = {\n                \"model\": ModelWrapper(real_score_transformer_2),\n            }\n\n            if dataloader is not None:\n                real_score_2_states[\"dataloader\"] = dataloader\n\n            logger.info(\n                \"rank: %s, loading real_score_2 distributed checkpoint from %s\",\n                rank,\n                real_score_2_dcp_dir,\n                local_main_process_only=False)\n\n            begin_time = time.perf_counter()\n            dcp.load(real_score_2_states, checkpoint_id=real_score_2_dcp_dir)\n            end_time = time.perf_counter()\n\n            logger.info(\n                \"rank: %s, real_score_2 distributed checkpoint loaded in %.2f seconds\",\n                rank,\n                end_time - begin_time,\n                local_main_process_only=False)\n        else:\n            logger.info(\"rank: %s, real_score_2 checkpoint not found, skipping\",\n                        rank)\n\n    # Load shared random state\n    shared_dcp_dir = os.path.join(checkpoint_path, \"distributed_checkpoint\",\n                                  \"shared\")\n    if not os.path.exists(shared_dcp_dir):\n        logger.warning(\"Shared random state directory %s does not exist\",\n                       shared_dcp_dir)\n        return 0\n\n    shared_states = {\n        \"random_state\": RandomStateWrapper(noise_generator),\n    }\n\n    begin_time = time.perf_counter()\n    dcp.load(shared_states, checkpoint_id=shared_dcp_dir)\n    end_time = time.perf_counter()\n\n    logger.info(\"rank: %s, shared random state loaded in %.2f seconds\",\n                rank,\n                end_time - begin_time,\n                local_main_process_only=False)\n    logger.info(\"--&gt; distillation checkpoint loaded from step %s\", step)\n    return step\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.training_utils.save_checkpoint","title":"fastvideo.training.training_utils.save_checkpoint","text":"<pre><code>save_checkpoint(\n    transformer,\n    rank,\n    output_dir,\n    step,\n    optimizer=None,\n    dataloader=None,\n    scheduler=None,\n    noise_generator=None,\n) -&gt; None\n</code></pre> <p>Save checkpoint following finetrainer's distributed checkpoint approach. Saves both distributed checkpoint and consolidated model weights.</p> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def save_checkpoint(transformer,\n                    rank,\n                    output_dir,\n                    step,\n                    optimizer=None,\n                    dataloader=None,\n                    scheduler=None,\n                    noise_generator=None) -&gt; None:\n    \"\"\"\n    Save checkpoint following finetrainer's distributed checkpoint approach.\n    Saves both distributed checkpoint and consolidated model weights.\n    \"\"\"\n    save_dir = os.path.join(output_dir, f\"checkpoint-{step}\")\n    os.makedirs(save_dir, exist_ok=True)\n\n    states = {\n        \"model\": ModelWrapper(transformer),\n        \"random_state\": RandomStateWrapper(noise_generator),\n    }\n\n    if optimizer is not None:\n        states[\"optimizer\"] = OptimizerWrapper(transformer, optimizer)\n\n    if dataloader is not None:\n        states[\"dataloader\"] = dataloader\n\n    if scheduler is not None:\n        states[\"scheduler\"] = SchedulerWrapper(scheduler)\n    dcp_dir = os.path.join(save_dir, \"distributed_checkpoint\")\n    logger.info(\"rank: %s, saving distributed checkpoint to %s\",\n                rank,\n                dcp_dir,\n                local_main_process_only=False)\n\n    begin_time = time.perf_counter()\n    dcp.save(states, checkpoint_id=dcp_dir)\n    end_time = time.perf_counter()\n\n    logger.info(\"rank: %s, distributed checkpoint saved in %.2f seconds\",\n                rank,\n                end_time - begin_time,\n                local_main_process_only=False)\n\n    cpu_state = gather_state_dict_on_cpu_rank0(transformer, device=None)\n    if rank == 0:\n        # Save model weights (consolidated)\n        transformer_save_dir = os.path.join(save_dir, \"transformer\")\n        os.makedirs(transformer_save_dir, exist_ok=True)\n        weight_path = os.path.join(transformer_save_dir,\n                                   \"diffusion_pytorch_model.safetensors\")\n        logger.info(\"rank: %s, saving consolidated checkpoint to %s\",\n                    rank,\n                    weight_path,\n                    local_main_process_only=False)\n\n        # Convert training format to diffusers format and save\n        diffusers_state_dict = custom_to_hf_state_dict(\n            cpu_state, transformer.reverse_param_names_mapping)\n        save_file(diffusers_state_dict, weight_path)\n\n        logger.info(\"rank: %s, consolidated checkpoint saved to %s\",\n                    rank,\n                    weight_path,\n                    local_main_process_only=False)\n\n        # Save model config\n        config_dict = transformer.hf_config\n        if \"dtype\" in config_dict:\n            del config_dict[\"dtype\"]  # TODO\n        config_path = os.path.join(transformer_save_dir, \"config.json\")\n        # save dict as json\n        with open(config_path, \"w\") as f:\n            json.dump(config_dict, f, indent=4)\n        logger.info(\"--&gt; checkpoint saved at step %s to %s\", step, weight_path)\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.training_utils.save_distillation_checkpoint","title":"fastvideo.training.training_utils.save_distillation_checkpoint","text":"<pre><code>save_distillation_checkpoint(\n    generator_transformer,\n    fake_score_transformer,\n    rank,\n    output_dir,\n    step,\n    generator_optimizer=None,\n    fake_score_optimizer=None,\n    dataloader=None,\n    generator_scheduler=None,\n    fake_score_scheduler=None,\n    noise_generator=None,\n    generator_ema=None,\n    only_save_generator_weight=False,\n    generator_transformer_2=None,\n    real_score_transformer_2=None,\n    fake_score_transformer_2=None,\n    generator_optimizer_2=None,\n    fake_score_optimizer_2=None,\n    generator_scheduler_2=None,\n    fake_score_scheduler_2=None,\n    generator_ema_2=None,\n) -&gt; None\n</code></pre> <p>Save distillation checkpoint with both generator and fake_score models. Supports MoE (Mixture of Experts) models with transformer_2 variants. Saves both distributed checkpoint and consolidated model weights. Only saves the generator model for inference (consolidated weights).</p> <p>Parameters:</p> Name Type Description Default <code>generator_transformer</code> <p>Main generator transformer model</p> required <code>fake_score_transformer</code> <p>Main fake score transformer model</p> required <code>only_save_generator_weight</code> <p>If True, only save the generator model weights for inference                        without saving distributed checkpoint for training resume.</p> <code>False</code> <code>generator_transformer_2</code> <p>Secondary generator transformer for MoE (optional)</p> <code>None</code> <code>real_score_transformer_2</code> <p>Secondary real score transformer for MoE (optional) </p> <code>None</code> <code>fake_score_transformer_2</code> <p>Secondary fake score transformer for MoE (optional)</p> <code>None</code> <code>generator_optimizer_2</code> <p>Optimizer for generator_transformer_2 (optional)</p> <code>None</code> <code>fake_score_optimizer_2</code> <p>Optimizer for fake_score_transformer_2 (optional)</p> <code>None</code> <code>generator_scheduler_2</code> <p>Scheduler for generator_transformer_2 (optional)</p> <code>None</code> <code>fake_score_scheduler_2</code> <p>Scheduler for fake_score_transformer_2 (optional)</p> <code>None</code> <code>generator_ema_2</code> <p>EMA for generator_transformer_2 (optional)</p> <code>None</code> Source code in <code>fastvideo/training/training_utils.py</code> <pre><code>def save_distillation_checkpoint(\n        generator_transformer,\n        fake_score_transformer,\n        rank,\n        output_dir,\n        step,\n        generator_optimizer=None,\n        fake_score_optimizer=None,\n        dataloader=None,\n        generator_scheduler=None,\n        fake_score_scheduler=None,\n        noise_generator=None,\n        generator_ema=None,\n        only_save_generator_weight=False,\n        # MoE support\n        generator_transformer_2=None,\n        real_score_transformer_2=None,\n        fake_score_transformer_2=None,\n        generator_optimizer_2=None,\n        fake_score_optimizer_2=None,\n        generator_scheduler_2=None,\n        fake_score_scheduler_2=None,\n        generator_ema_2=None) -&gt; None:\n    \"\"\"\n    Save distillation checkpoint with both generator and fake_score models.\n    Supports MoE (Mixture of Experts) models with transformer_2 variants.\n    Saves both distributed checkpoint and consolidated model weights.\n    Only saves the generator model for inference (consolidated weights).\n\n    Args:\n        generator_transformer: Main generator transformer model\n        fake_score_transformer: Main fake score transformer model\n        only_save_generator_weight: If True, only save the generator model weights for inference\n                                   without saving distributed checkpoint for training resume.\n        generator_transformer_2: Secondary generator transformer for MoE (optional)\n        real_score_transformer_2: Secondary real score transformer for MoE (optional) \n        fake_score_transformer_2: Secondary fake score transformer for MoE (optional)\n        generator_optimizer_2: Optimizer for generator_transformer_2 (optional)\n        fake_score_optimizer_2: Optimizer for fake_score_transformer_2 (optional)\n        generator_scheduler_2: Scheduler for generator_transformer_2 (optional)\n        fake_score_scheduler_2: Scheduler for fake_score_transformer_2 (optional)\n        generator_ema_2: EMA for generator_transformer_2 (optional)\n    \"\"\"\n    save_dir = os.path.join(output_dir, f\"checkpoint-{step}\")\n    os.makedirs(save_dir, exist_ok=True)\n\n    # Create directories for models\n    inference_save_dir = os.path.join(save_dir,\n                                      \"generator_inference_transformer\")\n\n    # Only save distributed checkpoint if not only saving generator weight\n    if not only_save_generator_weight:\n        # Save generator distributed checkpoint\n        generator_states = {\n            \"model\": ModelWrapper(generator_transformer),\n        }\n        if generator_optimizer is not None:\n            generator_states[\"optimizer\"] = OptimizerWrapper(\n                generator_transformer, generator_optimizer)\n        if dataloader is not None:\n            generator_states[\"dataloader\"] = dataloader\n        if generator_scheduler is not None:\n            generator_states[\"scheduler\"] = SchedulerWrapper(\n                generator_scheduler)\n        if generator_ema is not None:\n            generator_states[\"ema\"] = generator_ema.state_dict()\n\n        generator_dcp_dir = os.path.join(save_dir, \"distributed_checkpoint\",\n                                         \"generator\")\n        logger.info(\"rank: %s, saving generator distributed checkpoint to %s\",\n                    rank,\n                    generator_dcp_dir,\n                    local_main_process_only=False)\n\n        begin_time = time.perf_counter()\n        dcp.save(generator_states, checkpoint_id=generator_dcp_dir)\n        end_time = time.perf_counter()\n\n        logger.info(\n            \"rank: %s, generator distributed checkpoint saved in %.2f seconds\",\n            rank,\n            end_time - begin_time,\n            local_main_process_only=False)\n\n        # Save generator_2 distributed checkpoint (MoE support)\n        if generator_transformer_2 is not None:\n            generator_2_states = {\n                \"model\": ModelWrapper(generator_transformer_2),\n            }\n            if generator_optimizer_2 is not None:\n                generator_2_states[\"optimizer\"] = OptimizerWrapper(\n                    generator_transformer_2, generator_optimizer_2)\n            if dataloader is not None:\n                generator_2_states[\"dataloader\"] = dataloader\n            if generator_scheduler_2 is not None:\n                generator_2_states[\"scheduler\"] = SchedulerWrapper(\n                    generator_scheduler_2)\n            if generator_ema_2 is not None:\n                generator_2_states[\"ema\"] = generator_ema_2.state_dict()\n\n            generator_2_dcp_dir = os.path.join(save_dir,\n                                               \"distributed_checkpoint\",\n                                               \"generator_2\")\n            logger.info(\n                \"rank: %s, saving generator_2 distributed checkpoint to %s\",\n                rank,\n                generator_2_dcp_dir,\n                local_main_process_only=False)\n\n            begin_time = time.perf_counter()\n            dcp.save(generator_2_states, checkpoint_id=generator_2_dcp_dir)\n            end_time = time.perf_counter()\n\n            logger.info(\n                \"rank: %s, generator_2 distributed checkpoint saved in %.2f seconds\",\n                rank,\n                end_time - begin_time,\n                local_main_process_only=False)\n\n        # Save critic distributed checkpoint\n        critic_states = {\n            \"model\": ModelWrapper(fake_score_transformer),\n        }\n        if fake_score_optimizer is not None:\n            critic_states[\"optimizer\"] = OptimizerWrapper(\n                fake_score_transformer, fake_score_optimizer)\n        if dataloader is not None:\n            critic_states[\"dataloader\"] = dataloader\n        if fake_score_scheduler is not None:\n            critic_states[\"scheduler\"] = SchedulerWrapper(fake_score_scheduler)\n\n        critic_dcp_dir = os.path.join(save_dir, \"distributed_checkpoint\",\n                                      \"critic\")\n        logger.info(\"rank: %s, saving critic distributed checkpoint to %s\",\n                    rank,\n                    critic_dcp_dir,\n                    local_main_process_only=False)\n\n        begin_time = time.perf_counter()\n        dcp.save(critic_states, checkpoint_id=critic_dcp_dir)\n        end_time = time.perf_counter()\n\n        logger.info(\n            \"rank: %s, critic distributed checkpoint saved in %.2f seconds\",\n            rank,\n            end_time - begin_time,\n            local_main_process_only=False)\n\n        # Save critic_2 distributed checkpoint (MoE support)\n        if fake_score_transformer_2 is not None:\n            critic_2_states = {\n                \"model\": ModelWrapper(fake_score_transformer_2),\n            }\n            if fake_score_optimizer_2 is not None:\n                critic_2_states[\"optimizer\"] = OptimizerWrapper(\n                    fake_score_transformer_2, fake_score_optimizer_2)\n            if dataloader is not None:\n                critic_2_states[\"dataloader\"] = dataloader\n            if fake_score_scheduler_2 is not None:\n                critic_2_states[\"scheduler\"] = SchedulerWrapper(\n                    fake_score_scheduler_2)\n\n            critic_2_dcp_dir = os.path.join(save_dir, \"distributed_checkpoint\",\n                                            \"critic_2\")\n            logger.info(\n                \"rank: %s, saving critic_2 distributed checkpoint to %s\",\n                rank,\n                critic_2_dcp_dir,\n                local_main_process_only=False)\n\n            begin_time = time.perf_counter()\n            dcp.save(critic_2_states, checkpoint_id=critic_2_dcp_dir)\n            end_time = time.perf_counter()\n\n            logger.info(\n                \"rank: %s, critic_2 distributed checkpoint saved in %.2f seconds\",\n                rank,\n                end_time - begin_time,\n                local_main_process_only=False)\n\n        # Save real_score_transformer_2 distributed checkpoint (MoE support)\n        if real_score_transformer_2 is not None:\n            real_score_2_states = {\n                \"model\": ModelWrapper(real_score_transformer_2),\n            }\n            # Note: real_score_transformer_2 typically doesn't have optimizer/scheduler\n            # since it's used for inference only, but we include dataloader for consistency\n            if dataloader is not None:\n                real_score_2_states[\"dataloader\"] = dataloader\n\n            real_score_2_dcp_dir = os.path.join(save_dir,\n                                                \"distributed_checkpoint\",\n                                                \"real_score_2\")\n            logger.info(\n                \"rank: %s, saving real_score_2 distributed checkpoint to %s\",\n                rank,\n                real_score_2_dcp_dir,\n                local_main_process_only=False)\n\n            begin_time = time.perf_counter()\n            dcp.save(real_score_2_states, checkpoint_id=real_score_2_dcp_dir)\n            end_time = time.perf_counter()\n\n            logger.info(\n                \"rank: %s, real_score_2 distributed checkpoint saved in %.2f seconds\",\n                rank,\n                end_time - begin_time,\n                local_main_process_only=False)\n\n        # Save shared random state separately\n        shared_states = {\n            \"random_state\": RandomStateWrapper(noise_generator),\n        }\n        shared_dcp_dir = os.path.join(save_dir, \"distributed_checkpoint\",\n                                      \"shared\")\n\n        dcp.save(shared_states, checkpoint_id=shared_dcp_dir)\n\n    else:\n        logger.info(\n            \"rank: %s, skipping distributed checkpoint save (only_save_generator_weight=True)\",\n            rank,\n            local_main_process_only=False)\n\n    # Save generator model weights (consolidated) for inference\n    cpu_state = gather_state_dict_on_cpu_rank0(generator_transformer,\n                                               device=None)\n\n    if rank == 0:\n        # Save generator model weights (consolidated) for inference\n        os.makedirs(inference_save_dir, exist_ok=True)\n        weight_path = os.path.join(inference_save_dir,\n                                   \"diffusion_pytorch_model.safetensors\")\n        logger.info(\n            \"rank: %s, saving consolidated generator inference checkpoint to %s\",\n            rank,\n            weight_path,\n            local_main_process_only=False)\n\n        # Convert training format to diffusers format and save\n        diffusers_state_dict = custom_to_hf_state_dict(\n            cpu_state, generator_transformer.reverse_param_names_mapping)\n        save_file(diffusers_state_dict, weight_path)\n\n        logger.info(\n            \"rank: %s, consolidated generator inference checkpoint saved to %s\",\n            rank,\n            weight_path,\n            local_main_process_only=False)\n\n        # Save model config\n        config_dict = generator_transformer.hf_config\n        if \"dtype\" in config_dict:\n            del config_dict[\"dtype\"]  # TODO\n        config_path = os.path.join(inference_save_dir, \"config.json\")\n        # save dict as json\n        with open(config_path, \"w\") as f:\n            json.dump(config_dict, f, indent=4)\n        logger.info(\"--&gt; distillation checkpoint saved at step %s to %s\", step,\n                    weight_path)\n\n        # Save generator_2 model weights (consolidated) for inference (MoE support)\n        if generator_transformer_2 is not None:\n            inference_save_dir_2 = os.path.join(\n                save_dir, \"generator_2_inference_transformer\")\n            cpu_state_2 = gather_state_dict_on_cpu_rank0(\n                generator_transformer_2, device=None)\n\n            if rank == 0:\n                os.makedirs(inference_save_dir_2, exist_ok=True)\n                weight_path_2 = os.path.join(\n                    inference_save_dir_2, \"diffusion_pytorch_model.safetensors\")\n                logger.info(\n                    \"rank: %s, saving consolidated generator_2 inference checkpoint to %s\",\n                    rank,\n                    weight_path_2,\n                    local_main_process_only=False)\n\n                # Convert training format to diffusers format and save\n                diffusers_state_dict_2 = custom_to_hf_state_dict(\n                    cpu_state_2,\n                    generator_transformer_2.reverse_param_names_mapping)\n                save_file(diffusers_state_dict_2, weight_path_2)\n\n                logger.info(\n                    \"rank: %s, consolidated generator_2 inference checkpoint saved to %s\",\n                    rank,\n                    weight_path_2,\n                    local_main_process_only=False)\n\n                # Save model config\n                config_dict_2 = generator_transformer_2.hf_config\n                if \"dtype\" in config_dict_2:\n                    del config_dict_2[\"dtype\"]  # TODO\n                config_path_2 = os.path.join(inference_save_dir_2,\n                                             \"config.json\")\n                with open(config_path_2, \"w\") as f:\n                    json.dump(config_dict_2, f, indent=4)\n                logger.info(\n                    \"--&gt; generator_2 distillation checkpoint saved at step %s to %s\",\n                    step, weight_path_2)\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.wan_distillation_pipeline","title":"fastvideo.training.wan_distillation_pipeline","text":""},{"location":"api/fastvideo/training/#fastvideo.training.wan_distillation_pipeline-classes","title":"Classes","text":""},{"location":"api/fastvideo/training/#fastvideo.training.wan_distillation_pipeline.WanDistillationPipeline","title":"fastvideo.training.wan_distillation_pipeline.WanDistillationPipeline","text":"<pre><code>WanDistillationPipeline(\n    model_path: str,\n    fastvideo_args: TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>DistillationPipeline</code></p> <p>A distillation pipeline for Wan that uses a single transformer model. The main transformer serves as the student model, and copies are made for teacher and critic.</p> Source code in <code>fastvideo/training/training_pipeline.py</code> <pre><code>def __init__(\n        self,\n        model_path: str,\n        fastvideo_args: TrainingArgs,\n        required_config_modules: list[str] | None = None,\n        loaded_modules: dict[str, torch.nn.Module] | None = None) -&gt; None:\n    fastvideo_args.inference_mode = False\n    self.lora_training = fastvideo_args.lora_training\n    if self.lora_training and fastvideo_args.lora_rank is None:\n        raise ValueError(\"lora rank must be set when using lora training\")\n\n    set_random_seed(fastvideo_args.seed)  # for lora param init\n    super().__init__(model_path, fastvideo_args, required_config_modules,\n                     loaded_modules)  # type: ignore\n    self.tracker = DummyTracker()\n</code></pre> Functions\u00b6 fastvideo.training.wan_distillation_pipeline.WanDistillationPipeline.create_training_stages \u00b6 <pre><code>create_training_stages(training_args: TrainingArgs)\n</code></pre> <p>May be used in future refactors.</p> Source code in <code>fastvideo/training/wan_distillation_pipeline.py</code> <pre><code>def create_training_stages(self, training_args: TrainingArgs):\n    \"\"\"\n    May be used in future refactors.\n    \"\"\"\n    pass\n</code></pre> fastvideo.training.wan_distillation_pipeline.WanDistillationPipeline.initialize_pipeline \u00b6 <pre><code>initialize_pipeline(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Initialize Wan-specific scheduler.</p> Source code in <code>fastvideo/training/wan_distillation_pipeline.py</code> <pre><code>def initialize_pipeline(self, fastvideo_args: FastVideoArgs):\n    \"\"\"Initialize Wan-specific scheduler.\"\"\"\n    self.modules[\"scheduler\"] = FlowMatchEulerDiscreteScheduler(\n        shift=fastvideo_args.pipeline_config.flow_shift)\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.wan_distillation_pipeline-functions","title":"Functions","text":""},{"location":"api/fastvideo/training/#fastvideo.training.wan_i2v_distillation_pipeline","title":"fastvideo.training.wan_i2v_distillation_pipeline","text":""},{"location":"api/fastvideo/training/#fastvideo.training.wan_i2v_distillation_pipeline-classes","title":"Classes","text":""},{"location":"api/fastvideo/training/#fastvideo.training.wan_i2v_distillation_pipeline.WanI2VDistillationPipeline","title":"fastvideo.training.wan_i2v_distillation_pipeline.WanI2VDistillationPipeline","text":"<pre><code>WanI2VDistillationPipeline(\n    model_path: str,\n    fastvideo_args: TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>DistillationPipeline</code></p> <p>A distillation pipeline for Wan that uses a single transformer model. The main transformer serves as the student model, and copies are made for teacher and critic.</p> Source code in <code>fastvideo/training/training_pipeline.py</code> <pre><code>def __init__(\n        self,\n        model_path: str,\n        fastvideo_args: TrainingArgs,\n        required_config_modules: list[str] | None = None,\n        loaded_modules: dict[str, torch.nn.Module] | None = None) -&gt; None:\n    fastvideo_args.inference_mode = False\n    self.lora_training = fastvideo_args.lora_training\n    if self.lora_training and fastvideo_args.lora_rank is None:\n        raise ValueError(\"lora rank must be set when using lora training\")\n\n    set_random_seed(fastvideo_args.seed)  # for lora param init\n    super().__init__(model_path, fastvideo_args, required_config_modules,\n                     loaded_modules)  # type: ignore\n    self.tracker = DummyTracker()\n</code></pre> Functions\u00b6 fastvideo.training.wan_i2v_distillation_pipeline.WanI2VDistillationPipeline.create_training_stages \u00b6 <pre><code>create_training_stages(training_args: TrainingArgs)\n</code></pre> <p>May be used in future refactors.</p> Source code in <code>fastvideo/training/wan_i2v_distillation_pipeline.py</code> <pre><code>def create_training_stages(self, training_args: TrainingArgs):\n    \"\"\"\n    May be used in future refactors.\n    \"\"\"\n    pass\n</code></pre> fastvideo.training.wan_i2v_distillation_pipeline.WanI2VDistillationPipeline.initialize_pipeline \u00b6 <pre><code>initialize_pipeline(fastvideo_args: FastVideoArgs)\n</code></pre> <p>Initialize Wan-specific scheduler.</p> Source code in <code>fastvideo/training/wan_i2v_distillation_pipeline.py</code> <pre><code>def initialize_pipeline(self, fastvideo_args: FastVideoArgs):\n    \"\"\"Initialize Wan-specific scheduler.\"\"\"\n    self.modules[\"scheduler\"] = FlowMatchEulerDiscreteScheduler(\n        shift=fastvideo_args.pipeline_config.flow_shift)\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.wan_i2v_distillation_pipeline-functions","title":"Functions","text":""},{"location":"api/fastvideo/training/#fastvideo.training.wan_i2v_training_pipeline","title":"fastvideo.training.wan_i2v_training_pipeline","text":""},{"location":"api/fastvideo/training/#fastvideo.training.wan_i2v_training_pipeline-classes","title":"Classes","text":""},{"location":"api/fastvideo/training/#fastvideo.training.wan_i2v_training_pipeline.WanI2VTrainingPipeline","title":"fastvideo.training.wan_i2v_training_pipeline.WanI2VTrainingPipeline","text":"<pre><code>WanI2VTrainingPipeline(\n    model_path: str,\n    fastvideo_args: TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>TrainingPipeline</code></p> <p>A training pipeline for Wan.</p> Source code in <code>fastvideo/training/training_pipeline.py</code> <pre><code>def __init__(\n        self,\n        model_path: str,\n        fastvideo_args: TrainingArgs,\n        required_config_modules: list[str] | None = None,\n        loaded_modules: dict[str, torch.nn.Module] | None = None) -&gt; None:\n    fastvideo_args.inference_mode = False\n    self.lora_training = fastvideo_args.lora_training\n    if self.lora_training and fastvideo_args.lora_rank is None:\n        raise ValueError(\"lora rank must be set when using lora training\")\n\n    set_random_seed(fastvideo_args.seed)  # for lora param init\n    super().__init__(model_path, fastvideo_args, required_config_modules,\n                     loaded_modules)  # type: ignore\n    self.tracker = DummyTracker()\n</code></pre> Functions\u00b6 fastvideo.training.wan_i2v_training_pipeline.WanI2VTrainingPipeline.create_training_stages \u00b6 <pre><code>create_training_stages(training_args: TrainingArgs)\n</code></pre> <p>May be used in future refactors.</p> Source code in <code>fastvideo/training/wan_i2v_training_pipeline.py</code> <pre><code>def create_training_stages(self, training_args: TrainingArgs):\n    \"\"\"\n    May be used in future refactors.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.wan_i2v_training_pipeline-functions","title":"Functions","text":""},{"location":"api/fastvideo/training/#fastvideo.training.wan_self_forcing_distillation_pipeline","title":"fastvideo.training.wan_self_forcing_distillation_pipeline","text":""},{"location":"api/fastvideo/training/#fastvideo.training.wan_self_forcing_distillation_pipeline-classes","title":"Classes","text":""},{"location":"api/fastvideo/training/#fastvideo.training.wan_self_forcing_distillation_pipeline.WanSelfForcingDistillationPipeline","title":"fastvideo.training.wan_self_forcing_distillation_pipeline.WanSelfForcingDistillationPipeline","text":"<pre><code>WanSelfForcingDistillationPipeline(\n    model_path: str,\n    fastvideo_args: TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>SelfForcingDistillationPipeline</code></p> <p>A self-forcing distillation pipeline for Wan that uses the self-forcing methodology with DMD for video generation.</p> Source code in <code>fastvideo/training/training_pipeline.py</code> <pre><code>def __init__(\n        self,\n        model_path: str,\n        fastvideo_args: TrainingArgs,\n        required_config_modules: list[str] | None = None,\n        loaded_modules: dict[str, torch.nn.Module] | None = None) -&gt; None:\n    fastvideo_args.inference_mode = False\n    self.lora_training = fastvideo_args.lora_training\n    if self.lora_training and fastvideo_args.lora_rank is None:\n        raise ValueError(\"lora rank must be set when using lora training\")\n\n    set_random_seed(fastvideo_args.seed)  # for lora param init\n    super().__init__(model_path, fastvideo_args, required_config_modules,\n                     loaded_modules)  # type: ignore\n    self.tracker = DummyTracker()\n</code></pre> Functions\u00b6 fastvideo.training.wan_self_forcing_distillation_pipeline.WanSelfForcingDistillationPipeline.create_training_stages \u00b6 <pre><code>create_training_stages(training_args: TrainingArgs)\n</code></pre> <p>May be used in future refactors.</p> Source code in <code>fastvideo/training/wan_self_forcing_distillation_pipeline.py</code> <pre><code>def create_training_stages(self, training_args: TrainingArgs):\n    \"\"\"\n    May be used in future refactors.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.wan_self_forcing_distillation_pipeline-functions","title":"Functions","text":""},{"location":"api/fastvideo/training/#fastvideo.training.wan_training_pipeline","title":"fastvideo.training.wan_training_pipeline","text":""},{"location":"api/fastvideo/training/#fastvideo.training.wan_training_pipeline-classes","title":"Classes","text":""},{"location":"api/fastvideo/training/#fastvideo.training.wan_training_pipeline.WanTrainingPipeline","title":"fastvideo.training.wan_training_pipeline.WanTrainingPipeline","text":"<pre><code>WanTrainingPipeline(\n    model_path: str,\n    fastvideo_args: TrainingArgs,\n    required_config_modules: list[str] | None = None,\n    loaded_modules: dict[str, Module] | None = None,\n)\n</code></pre> <p>               Bases: <code>TrainingPipeline</code></p> <p>A training pipeline for Wan.</p> Source code in <code>fastvideo/training/training_pipeline.py</code> <pre><code>def __init__(\n        self,\n        model_path: str,\n        fastvideo_args: TrainingArgs,\n        required_config_modules: list[str] | None = None,\n        loaded_modules: dict[str, torch.nn.Module] | None = None) -&gt; None:\n    fastvideo_args.inference_mode = False\n    self.lora_training = fastvideo_args.lora_training\n    if self.lora_training and fastvideo_args.lora_rank is None:\n        raise ValueError(\"lora rank must be set when using lora training\")\n\n    set_random_seed(fastvideo_args.seed)  # for lora param init\n    super().__init__(model_path, fastvideo_args, required_config_modules,\n                     loaded_modules)  # type: ignore\n    self.tracker = DummyTracker()\n</code></pre> Functions\u00b6 fastvideo.training.wan_training_pipeline.WanTrainingPipeline.create_training_stages \u00b6 <pre><code>create_training_stages(training_args: TrainingArgs)\n</code></pre> <p>May be used in future refactors.</p> Source code in <code>fastvideo/training/wan_training_pipeline.py</code> <pre><code>def create_training_stages(self, training_args: TrainingArgs):\n    \"\"\"\n    May be used in future refactors.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fastvideo/training/#fastvideo.training.wan_training_pipeline-functions","title":"Functions","text":""},{"location":"api/fastvideo/utils/","title":"utils","text":""},{"location":"api/fastvideo/utils/#fastvideo.utils","title":"utils","text":""},{"location":"api/fastvideo/utils/#fastvideo.utils-classes","title":"Classes","text":""},{"location":"api/fastvideo/utils/#fastvideo.utils.FlexibleArgumentParser","title":"fastvideo.utils.FlexibleArgumentParser","text":"<pre><code>FlexibleArgumentParser(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ArgumentParser</code></p> <p>ArgumentParser that allows both underscore and dash in names.</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    # Set the default 'formatter_class' to SortedHelpFormatter\n    if 'formatter_class' not in kwargs:\n        kwargs['formatter_class'] = SortedHelpFormatter\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"api/fastvideo/utils/#fastvideo.utils.SortedHelpFormatter","title":"fastvideo.utils.SortedHelpFormatter","text":"<p>               Bases: <code>HelpFormatter</code></p> <p>SortedHelpFormatter that sorts arguments by their option strings.</p>"},{"location":"api/fastvideo/utils/#fastvideo.utils-functions","title":"Functions","text":""},{"location":"api/fastvideo/utils/#fastvideo.utils.align_to","title":"fastvideo.utils.align_to","text":"<pre><code>align_to(value: int, alignment: int) -&gt; int\n</code></pre> <p>align height, width according to alignment</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int</code> <p>height or width</p> required <code>alignment</code> <code>int</code> <p>target alignment factor</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>the aligned value</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def align_to(value: int, alignment: int) -&gt; int:\n    \"\"\"align height, width according to alignment\n\n    Args:\n        value (int): height or width\n        alignment (int): target alignment factor\n\n    Returns:\n        int: the aligned value\n    \"\"\"\n    return int(math.ceil(value / alignment) * alignment)\n</code></pre>"},{"location":"api/fastvideo/utils/#fastvideo.utils.cuda_is_initialized","title":"fastvideo.utils.cuda_is_initialized","text":"<pre><code>cuda_is_initialized() -&gt; bool\n</code></pre> <p>Check if CUDA is initialized.</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def cuda_is_initialized() -&gt; bool:\n    \"\"\"Check if CUDA is initialized.\"\"\"\n    if not torch.cuda._is_compiled():\n        return False\n    return torch.cuda.is_initialized()\n</code></pre>"},{"location":"api/fastvideo/utils/#fastvideo.utils.current_stream","title":"fastvideo.utils.current_stream","text":"<pre><code>current_stream() -&gt; torch.cuda.Stream | None\n</code></pre> <p>replace <code>torch.cuda.current_stream()</code> with <code>fastvideo.utils.current_stream()</code>. it turns out that <code>torch.cuda.current_stream()</code> is quite expensive, as it will construct a new stream object at each call. here we patch <code>torch.cuda.set_stream</code> to keep track of the current stream directly, so that we can avoid calling <code>torch.cuda.current_stream()</code>.</p> <p>the underlying hypothesis is that we do not call <code>torch._C._cuda_setStream</code> from C/C++ code.</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def current_stream() -&gt; torch.cuda.Stream | None:\n    \"\"\"\n    replace `torch.cuda.current_stream()` with `fastvideo.utils.current_stream()`.\n    it turns out that `torch.cuda.current_stream()` is quite expensive,\n    as it will construct a new stream object at each call.\n    here we patch `torch.cuda.set_stream` to keep track of the current stream\n    directly, so that we can avoid calling `torch.cuda.current_stream()`.\n\n    the underlying hypothesis is that we do not call `torch._C._cuda_setStream`\n    from C/C++ code.\n    \"\"\"\n    from fastvideo.platforms import current_platform\n\n    # For non-CUDA platforms, return None\n    if not current_platform.is_cuda_alike():\n        return None\n\n    global _current_stream\n    if _current_stream is None:\n        # when this function is called before any stream is set,\n        # we return the default stream.\n        # On ROCm using the default 0 stream in combination with RCCL\n        # is hurting performance. Therefore creating a dedicated stream\n        # per process\n        _current_stream = torch.cuda.Stream() if current_platform.is_rocm(\n        ) else torch.cuda.current_stream()\n    return _current_stream\n</code></pre>"},{"location":"api/fastvideo/utils/#fastvideo.utils.decorate_logs","title":"fastvideo.utils.decorate_logs","text":"<pre><code>decorate_logs(process_name: str | None = None) -&gt; None\n</code></pre> <p>Adds a process-specific prefix to each line of output written to stdout and stderr.</p> <p>Parameters:</p> Name Type Description Default <code>process_name</code> <code>str | None</code> <p>Optional; the name of the process to use in the prefix. If not provided, the current process name from the multiprocessing context is used.</p> <code>None</code> Source code in <code>fastvideo/utils.py</code> <pre><code>def decorate_logs(process_name: str | None = None) -&gt; None:\n    \"\"\"\n    Adds a process-specific prefix to each line of output written to stdout and\n    stderr.\n\n    Args:\n        process_name: Optional; the name of the process to use in the prefix.\n            If not provided, the current process name from the multiprocessing\n            context is used.\n    \"\"\"\n    if process_name is None:\n        process_name = get_mp_context().current_process().name\n    pid = os.getpid()\n    _add_prefix(sys.stdout, process_name, pid)\n    _add_prefix(sys.stderr, process_name, pid)\n</code></pre>"},{"location":"api/fastvideo/utils/#fastvideo.utils.dict_to_3d_list","title":"fastvideo.utils.dict_to_3d_list","text":"<pre><code>dict_to_3d_list(\n    mask_strategy: dict[str, Any] | None = None,\n    t_max: int | None = None,\n    l_max: int | None = None,\n    h_max: int | None = None,\n) -&gt; list[list[list[torch.Tensor | None]]]\n</code></pre> <p>Convert a dictionary of mask indices to a 3D list of tensors. Args:     mask_strategy: keys are \"t_l_h\", values are torch.Tensor masks.     t_max, l_max, h_max: if provided (all three), force the output shape to (t_max, l_max, h_max).                         If all three are None, infer shape from the data.</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def dict_to_3d_list(\n    mask_strategy: dict[str, Any] | None = None,\n    t_max: int | None = None,\n    l_max: int | None = None,\n    h_max: int | None = None,\n) -&gt; list[list[list[torch.Tensor | None]]]:\n    \"\"\"\n    Convert a dictionary of mask indices to a 3D list of tensors.\n    Args:\n        mask_strategy: keys are \"t_l_h\", values are torch.Tensor masks.\n        t_max, l_max, h_max: if provided (all three), force the output shape to (t_max, l_max, h_max).\n                            If all three are None, infer shape from the data.\n    \"\"\"\n    # Case 1: no data, but fixed shape requested\n    if mask_strategy is None:\n        assert t_max is not None and l_max is not None and h_max is not None, (\n            \"If mask_strategy is None, you must provide t_max, l_max, and h_max\"\n        )\n        return [[[None for _ in range(h_max)] for _ in range(l_max)]\n                for _ in range(t_max)]\n\n    # Parse all keys into integer tuples\n    indices = [tuple(map(int, key.split(\"_\"))) for key in mask_strategy]\n\n    # Decide on dimensions\n    if t_max is None and l_max is None and h_max is None:\n        # fully dynamic: infer from data\n        max_timesteps_idx = max(t for t, _, _ in indices) + 1\n        max_layer_idx = max(l for _, l, _ in indices) + 1  # noqa: E741\n        max_head_idx = max(h for _, _, h in indices) + 1\n    else:\n        # require all three to be provided\n        assert t_max is not None and l_max is not None and h_max is not None, (\n            \"Either supply none of (t_max, l_max, h_max) to infer dimensions, \"\n            \"or supply all three to fix the shape.\")\n        max_timesteps_idx = t_max\n        max_layer_idx = l_max\n        max_head_idx = h_max\n\n    # Preallocate\n    result = [[[None for _ in range(max_head_idx)]\n               for _ in range(max_layer_idx)] for _ in range(max_timesteps_idx)]\n\n    # Fill in, skipping any out-of-bounds entries\n    for key, value in mask_strategy.items():\n        t, l, h = map(int, key.split(\"_\"))  # noqa: E741\n        if 0 &lt;= t &lt; max_timesteps_idx and 0 &lt;= l &lt; max_layer_idx and 0 &lt;= h &lt; max_head_idx:\n            result[t][l][h] = value\n        # else: silently ignore any key that doesn't fit\n\n    return result\n</code></pre>"},{"location":"api/fastvideo/utils/#fastvideo.utils.find_hccl_library","title":"fastvideo.utils.find_hccl_library","text":"<pre><code>find_hccl_library() -&gt; str\n</code></pre> <p>We either use the library file specified by the <code>HCCL_SO_PATH</code> environment variable, or we find the library file brought by PyTorch. After importing <code>torch</code>, <code>libhccl.so</code> can be found by <code>ctypes</code> automatically.</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def find_hccl_library() -&gt; str:\n    \"\"\"\n    We either use the library file specified by the `HCCL_SO_PATH`\n    environment variable, or we find the library file brought by PyTorch.\n    After importing `torch`, `libhccl.so` can be\n    found by `ctypes` automatically.\n    \"\"\"\n    so_file = envs.HCCL_SO_PATH\n\n    # manually load the nccl library\n    if so_file:\n        logger.info(\"Found hccl from environment variable HCCL_SO_PATH=%s\",\n                    so_file)\n    else:\n        if torch.version.cann is not None:  # codespell:ignore cann\n            so_file = \"libhccl.so\"\n        else:\n            raise ValueError(\"HCCL only supports Ascend NPU backends.\")\n        logger.info(\"Found hccl from library %s\", so_file)\n    return so_file\n</code></pre>"},{"location":"api/fastvideo/utils/#fastvideo.utils.find_nccl_library","title":"fastvideo.utils.find_nccl_library","text":"<pre><code>find_nccl_library() -&gt; str\n</code></pre> <p>We either use the library file specified by the <code>FASTVIDEO_NCCL_SO_PATH</code> environment variable, or we find the library file brought by PyTorch. After importing <code>torch</code>, <code>libnccl.so.2</code> or <code>librccl.so.1</code> can be found by <code>ctypes</code> automatically.</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def find_nccl_library() -&gt; str:\n    \"\"\"\n    We either use the library file specified by the `FASTVIDEO_NCCL_SO_PATH`\n    environment variable, or we find the library file brought by PyTorch.\n    After importing `torch`, `libnccl.so.2` or `librccl.so.1` can be\n    found by `ctypes` automatically.\n    \"\"\"\n    so_file = envs.FASTVIDEO_NCCL_SO_PATH\n\n    # manually load the nccl library\n    if so_file:\n        logger.info(\n            \"Found nccl from environment variable FASTVIDEO_NCCL_SO_PATH=%s\",\n            so_file)\n    else:\n        if torch.version.cuda is not None:\n            so_file = \"libnccl.so.2\"\n        elif torch.version.hip is not None:\n            so_file = \"librccl.so.1\"\n        else:\n            raise ValueError(\"NCCL only supports CUDA and ROCm backends.\")\n        logger.info(\"Found nccl from library %s\", so_file)\n    return str(so_file)\n</code></pre>"},{"location":"api/fastvideo/utils/#fastvideo.utils.get_compute_dtype","title":"fastvideo.utils.get_compute_dtype","text":"<pre><code>get_compute_dtype() -&gt; torch.dtype\n</code></pre> <p>Get the current compute dtype from mixed precision policy.</p> <p>Returns:</p> Type Description <code>dtype</code> <p>torch.dtype: The compute dtype to use, defaults to get_default_dtype() if no policy set</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def get_compute_dtype() -&gt; torch.dtype:\n    \"\"\"Get the current compute dtype from mixed precision policy.\n\n    Returns:\n        torch.dtype: The compute dtype to use, defaults to get_default_dtype() if no policy set\n    \"\"\"\n    if not hasattr(_mixed_precision_state, 'state'):\n        return torch.get_default_dtype()\n    else:\n        state = get_mixed_precision_state()\n        return state.param_dtype\n</code></pre>"},{"location":"api/fastvideo/utils/#fastvideo.utils.get_mixed_precision_state","title":"fastvideo.utils.get_mixed_precision_state","text":"<pre><code>get_mixed_precision_state() -&gt; MixedPrecisionState\n</code></pre> <p>Get the current mixed precision state.</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def get_mixed_precision_state() -&gt; MixedPrecisionState:\n    \"\"\"Get the current mixed precision state.\"\"\"\n    if not hasattr(_mixed_precision_state, 'state'):\n        raise ValueError(\"Mixed precision state not set\")\n    return cast(MixedPrecisionState, _mixed_precision_state.state)\n</code></pre>"},{"location":"api/fastvideo/utils/#fastvideo.utils.get_mp_context","title":"fastvideo.utils.get_mp_context","text":"<pre><code>get_mp_context() -&gt; BaseContext\n</code></pre> <p>Get a multiprocessing context with a particular method (spawn or fork). By default we follow the value of the FASTVIDEO_WORKER_MULTIPROC_METHOD to determine the multiprocessing method (default is fork). However, under certain conditions, we may enforce spawn and override the value of FASTVIDEO_WORKER_MULTIPROC_METHOD.</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def get_mp_context() -&gt; BaseContext:\n    \"\"\"Get a multiprocessing context with a particular method (spawn or fork).\n    By default we follow the value of the FASTVIDEO_WORKER_MULTIPROC_METHOD to\n    determine the multiprocessing method (default is fork). However, under\n    certain conditions, we may enforce spawn and override the value of\n    FASTVIDEO_WORKER_MULTIPROC_METHOD.\n    \"\"\"\n    force_spawn()\n    mp_method = envs.FASTVIDEO_WORKER_MULTIPROC_METHOD\n    return multiprocessing.get_context(mp_method)\n</code></pre>"},{"location":"api/fastvideo/utils/#fastvideo.utils.import_pynvml","title":"fastvideo.utils.import_pynvml","text":"<pre><code>import_pynvml()\n</code></pre> <p>Historical comments:</p> <p>libnvml.so is the library behind nvidia-smi, and pynvml is a Python wrapper around it. We use it to get GPU status without initializing CUDA context in the current process. Historically, there are two packages that provide pynvml: - <code>nvidia-ml-py</code> (https://pypi.org/project/nvidia-ml-py/): The official     wrapper. It is a dependency of FastVideo, and is installed when users     install FastVideo. It provides a Python module named <code>pynvml</code>. - <code>pynvml</code> (https://pypi.org/project/pynvml/): An unofficial wrapper.     Prior to version 12.0, it also provides a Python module <code>pynvml</code>,     and therefore conflicts with the official one which is a standalone Python file.     This causes errors when both of them are installed.     Starting from version 12.0, it migrates to a new module     named <code>pynvml_utils</code> to avoid the conflict. It is so confusing that many packages in the community use the unofficial one by mistake, and we have to handle this case. For example, <code>nvcr.io/nvidia/pytorch:24.12-py3</code> uses the unofficial one, and it will cause errors, see the issue https://github.com/vllm-project/vllm/issues/12847 for example. After all the troubles, we decide to copy the official <code>pynvml</code> module to our codebase, and use it directly.</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def import_pynvml():\n    \"\"\"\n    Historical comments:\n\n    libnvml.so is the library behind nvidia-smi, and\n    pynvml is a Python wrapper around it. We use it to get GPU\n    status without initializing CUDA context in the current process.\n    Historically, there are two packages that provide pynvml:\n    - `nvidia-ml-py` (https://pypi.org/project/nvidia-ml-py/): The official\n        wrapper. It is a dependency of FastVideo, and is installed when users\n        install FastVideo. It provides a Python module named `pynvml`.\n    - `pynvml` (https://pypi.org/project/pynvml/): An unofficial wrapper.\n        Prior to version 12.0, it also provides a Python module `pynvml`,\n        and therefore conflicts with the official one which is a standalone Python file.\n        This causes errors when both of them are installed.\n        Starting from version 12.0, it migrates to a new module\n        named `pynvml_utils` to avoid the conflict.\n    It is so confusing that many packages in the community use the\n    unofficial one by mistake, and we have to handle this case.\n    For example, `nvcr.io/nvidia/pytorch:24.12-py3` uses the unofficial\n    one, and it will cause errors, see the issue\n    https://github.com/vllm-project/vllm/issues/12847 for example.\n    After all the troubles, we decide to copy the official `pynvml`\n    module to our codebase, and use it directly.\n    \"\"\"\n    import fastvideo.third_party.pynvml as pynvml\n    return pynvml\n</code></pre>"},{"location":"api/fastvideo/utils/#fastvideo.utils.log_torch_cuda_memory","title":"fastvideo.utils.log_torch_cuda_memory","text":"<pre><code>log_torch_cuda_memory(\n    tag: str | None = None,\n    *,\n    log_fn: Callable[[str], None] | None = None,\n    log_file_path: str\n    | PathLike[str]\n    | None = \"memory_trace.txt\"\n) -&gt; None\n</code></pre> <p>Log CUDA memory statistics via logger and append to a trace file.</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def log_torch_cuda_memory(\n        tag: str | None = None,\n        *,\n        log_fn: Callable[[str], None] | None = None,\n        log_file_path: str | os.PathLike[str] | None = \"memory_trace.txt\"\n) -&gt; None:\n    \"\"\"Log CUDA memory statistics via logger and append to a trace file.\"\"\"\n\n    log_fn = log_fn or logger.info\n    prefix = f\"[{tag}] \" if tag else \"\"\n\n    if not torch.cuda.is_available():\n        message = f\"{prefix}CUDA not available on this host.\"\n        log_fn(message)\n        _append_to_memory_trace(message, log_file_path)\n        return\n\n    try:\n        device_index = torch.cuda.current_device()\n        device_name = torch.cuda.get_device_name(device_index)\n        allocated = torch.cuda.memory_allocated(device_index)\n        reserved = torch.cuda.memory_reserved(device_index)\n        max_allocated = torch.cuda.max_memory_allocated(device_index)\n        max_reserved = torch.cuda.max_memory_reserved(device_index)\n        free_mem, total_mem = torch.cuda.mem_get_info(device_index)\n    except Exception as exc:  # noqa: BLE001\n        message = f\"{prefix}Unable to query CUDA memory stats: {exc}\"\n        log_fn(message)\n        _append_to_memory_trace(message, log_file_path)\n        return\n\n    used_mem = total_mem - free_mem\n\n    stats = [\n        f\"device={device_name} (index={device_index})\",\n        f\"allocated={_format_bytes(allocated)}\",\n        f\"reserved={_format_bytes(reserved)}\",\n        f\"max_allocated={_format_bytes(max_allocated)}\",\n        f\"max_reserved={_format_bytes(max_reserved)}\",\n        f\"used={_format_bytes(used_mem)}\",\n        f\"free={_format_bytes(free_mem)}\",\n        f\"total={_format_bytes(total_mem)}\",\n    ]\n\n    message = f\"{prefix}CUDA memory stats: {' | '.join(stats)}\"\n    log_fn(message)\n    _append_to_memory_trace(message, log_file_path)\n</code></pre>"},{"location":"api/fastvideo/utils/#fastvideo.utils.maybe_download_lora","title":"fastvideo.utils.maybe_download_lora","text":"<pre><code>maybe_download_lora(\n    model_name_or_path: str,\n    local_dir: str | None = None,\n    download: bool = True,\n) -&gt; str\n</code></pre> <p>Check if the model path is a Hugging Face Hub model ID and download it if needed. Args:     model_name_or_path: Local path or Hugging Face Hub model ID     local_dir: Local directory to save the model     download: Whether to download the model from Hugging Face Hub</p> <p>Returns:</p> Type Description <code>str</code> <p>Local path to the model</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def maybe_download_lora(model_name_or_path: str,\n                        local_dir: str | None = None,\n                        download: bool = True) -&gt; str:\n    \"\"\"\n    Check if the model path is a Hugging Face Hub model ID and download it if needed.\n    Args:\n        model_name_or_path: Local path or Hugging Face Hub model ID\n        local_dir: Local directory to save the model\n        download: Whether to download the model from Hugging Face Hub\n\n    Returns:\n        Local path to the model\n    \"\"\"\n\n    local_path = maybe_download_model(model_name_or_path, local_dir, download)\n    weight_name = _best_guess_weight_name(model_name_or_path,\n                                          file_extension=\".safetensors\")\n    return os.path.join(local_path, weight_name)\n</code></pre>"},{"location":"api/fastvideo/utils/#fastvideo.utils.maybe_download_model","title":"fastvideo.utils.maybe_download_model","text":"<pre><code>maybe_download_model(\n    model_name_or_path: str,\n    local_dir: str | None = None,\n    download: bool = True,\n) -&gt; str\n</code></pre> <p>Check if the model path is a Hugging Face Hub model ID and download it if needed.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>Local path or Hugging Face Hub model ID</p> required <code>local_dir</code> <code>str | None</code> <p>Local directory to save the model</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download the model from Hugging Face Hub</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Local path to the model</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def maybe_download_model(model_name_or_path: str,\n                         local_dir: str | None = None,\n                         download: bool = True) -&gt; str:\n    \"\"\"\n    Check if the model path is a Hugging Face Hub model ID and download it if needed.\n\n    Args:\n        model_name_or_path: Local path or Hugging Face Hub model ID\n        local_dir: Local directory to save the model\n        download: Whether to download the model from Hugging Face Hub\n\n    Returns:\n        Local path to the model\n    \"\"\"\n\n    # If the path exists locally, return it\n    if os.path.exists(model_name_or_path):\n        logger.info(\"Model already exists locally at %s\", model_name_or_path)\n        return model_name_or_path\n\n    # Otherwise, assume it's a HF Hub model ID and try to download it\n    try:\n        logger.info(\"Downloading model snapshot from HF Hub for %s...\",\n                    model_name_or_path)\n        with get_lock(model_name_or_path):\n            local_path = snapshot_download(\n                repo_id=model_name_or_path,\n                ignore_patterns=[\"*.onnx\", \"*.msgpack\"],\n                local_dir=local_dir)\n        logger.info(\"Downloaded model to %s\", local_path)\n        return str(local_path)\n    except Exception as e:\n        raise ValueError(\n            f\"Could not find model at {model_name_or_path} and failed to download from HF Hub: {e}\"\n        ) from e\n</code></pre>"},{"location":"api/fastvideo/utils/#fastvideo.utils.maybe_download_model_index","title":"fastvideo.utils.maybe_download_model_index","text":"<pre><code>maybe_download_model_index(\n    model_name_or_path: str,\n) -&gt; dict[str, Any]\n</code></pre> <p>Download and extract just the model_index.json for a Hugging Face model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>Path or HF Hub model ID</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The parsed model_index.json as a dictionary</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def maybe_download_model_index(model_name_or_path: str) -&gt; dict[str, Any]:\n    \"\"\"\n    Download and extract just the model_index.json for a Hugging Face model.\n\n    Args:\n        model_name_or_path: Path or HF Hub model ID\n\n    Returns:\n        The parsed model_index.json as a dictionary\n    \"\"\"\n    import tempfile\n\n    from huggingface_hub import hf_hub_download\n\n    # If it's a local path, verify it directly\n    if os.path.exists(model_name_or_path):\n        return verify_model_config_and_directory(model_name_or_path)\n\n    # For remote models, download just the model_index.json\n    try:\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            # Download just the model_index.json file\n            model_index_path = hf_hub_download(repo_id=model_name_or_path,\n                                               filename=\"model_index.json\",\n                                               local_dir=tmp_dir)\n\n            # Load the model_index.json\n            with open(model_index_path) as f:\n                config: dict[str, Any] = json.load(f)\n\n            # Verify it has the required fields\n            if \"_class_name\" not in config:\n                raise ValueError(\n                    f\"model_index.json for {model_name_or_path} does not contain _class_name field\"\n                )\n\n            if \"_diffusers_version\" not in config:\n                raise ValueError(\n                    f\"model_index.json for {model_name_or_path} does not contain _diffusers_version field\"\n                )\n\n            # Add the pipeline name for downstream use\n            config[\"pipeline_name\"] = config[\"_class_name\"]\n\n            logger.info(\"Downloaded model_index.json for %s, pipeline: %s\",\n                        model_name_or_path, config[\"_class_name\"])\n            return config\n\n    except Exception as e:\n        raise ValueError(\n            f\"Failed to download or parse model_index.json for {model_name_or_path}: {e}\"\n        ) from e\n</code></pre>"},{"location":"api/fastvideo/utils/#fastvideo.utils.resolve_obj_by_qualname","title":"fastvideo.utils.resolve_obj_by_qualname","text":"<pre><code>resolve_obj_by_qualname(qualname: str) -&gt; Any\n</code></pre> <p>Resolve an object by its fully qualified name.</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def resolve_obj_by_qualname(qualname: str) -&gt; Any:\n    \"\"\"\n    Resolve an object by its fully qualified name.\n    \"\"\"\n    module_name, obj_name = qualname.rsplit(\".\", 1)\n    module = importlib.import_module(module_name)\n    return getattr(module, obj_name)\n</code></pre>"},{"location":"api/fastvideo/utils/#fastvideo.utils.run_method","title":"fastvideo.utils.run_method","text":"<pre><code>run_method(\n    obj: Any,\n    method: str | bytes | Callable,\n    args: tuple[Any],\n    kwargs: dict[str, Any],\n) -&gt; Any\n</code></pre> <p>Run a method of an object with the given arguments and keyword arguments. If the method is string, it will be converted to a method using getattr. If the method is serialized bytes and will be deserialized using cloudpickle. If the method is a callable, it will be called directly.</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def run_method(obj: Any, method: str | bytes | Callable, args: tuple[Any],\n               kwargs: dict[str, Any]) -&gt; Any:\n    \"\"\"\n    Run a method of an object with the given arguments and keyword arguments.\n    If the method is string, it will be converted to a method using getattr.\n    If the method is serialized bytes and will be deserialized using\n    cloudpickle.\n    If the method is a callable, it will be called directly.\n    \"\"\"\n    if isinstance(method, bytes):\n        func = partial(cloudpickle.loads(method), obj)\n    elif isinstance(method, str):\n        try:\n            func = getattr(obj, method)\n        except AttributeError:\n            raise NotImplementedError(f\"Method {method!r} is not\"\n                                      \" implemented.\") from None\n    else:\n        func = partial(method, obj)  # type: ignore\n    return func(*args, **kwargs)\n</code></pre>"},{"location":"api/fastvideo/utils/#fastvideo.utils.set_mixed_precision_policy","title":"fastvideo.utils.set_mixed_precision_policy","text":"<pre><code>set_mixed_precision_policy(\n    param_dtype: dtype,\n    reduce_dtype: dtype,\n    output_dtype: dtype | None = None,\n    mp_policy: MixedPrecisionPolicy | None = None,\n)\n</code></pre> <p>Set mixed precision policy globally.</p> <p>Parameters:</p> Name Type Description Default <code>param_dtype</code> <code>dtype</code> <p>Parameter dtype used for training</p> required <code>reduce_dtype</code> <code>dtype</code> <p>Reduction dtype used for gradients</p> required <code>output_dtype</code> <code>dtype | None</code> <p>Optional output dtype</p> <code>None</code> Source code in <code>fastvideo/utils.py</code> <pre><code>def set_mixed_precision_policy(\n    param_dtype: torch.dtype,\n    reduce_dtype: torch.dtype,\n    output_dtype: torch.dtype | None = None,\n    mp_policy: MixedPrecisionPolicy | None = None,\n):\n    \"\"\"Set mixed precision policy globally.\n\n    Args:\n        param_dtype: Parameter dtype used for training\n        reduce_dtype: Reduction dtype used for gradients\n        output_dtype: Optional output dtype\n    \"\"\"\n    state = MixedPrecisionState(\n        param_dtype=param_dtype,\n        reduce_dtype=reduce_dtype,\n        output_dtype=output_dtype,\n        mp_policy=mp_policy,\n    )\n    _mixed_precision_state.state = state\n</code></pre>"},{"location":"api/fastvideo/utils/#fastvideo.utils.verify_model_config_and_directory","title":"fastvideo.utils.verify_model_config_and_directory","text":"<pre><code>verify_model_config_and_directory(\n    model_path: str,\n) -&gt; dict[str, Any]\n</code></pre> <p>Verify that the model directory contains a valid diffusers configuration.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the model directory</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The loaded model configuration as a dictionary</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def verify_model_config_and_directory(model_path: str) -&gt; dict[str, Any]:\n    \"\"\"\n    Verify that the model directory contains a valid diffusers configuration.\n\n    Args:\n        model_path: Path to the model directory\n\n    Returns:\n        The loaded model configuration as a dictionary\n    \"\"\"\n\n    # Check for model_index.json which is required for diffusers models\n    config_path = os.path.join(model_path, \"model_index.json\")\n    if not os.path.exists(config_path):\n        raise ValueError(\n            f\"Model directory {model_path} does not contain model_index.json. \"\n            \"Only Hugging Face diffusers format is supported.\")\n\n    # Check for transformer and vae directories\n    transformer_dir = os.path.join(model_path, \"transformer\")\n    vae_dir = os.path.join(model_path, \"vae\")\n\n    if not os.path.exists(transformer_dir):\n        raise ValueError(\n            f\"Model directory {model_path} does not contain a transformer/ directory.\"\n        )\n\n    if not os.path.exists(vae_dir):\n        raise ValueError(\n            f\"Model directory {model_path} does not contain a vae/ directory.\")\n\n    # Load the config\n    with open(config_path) as f:\n        config = json.load(f)\n\n    # Verify diffusers version exists\n    if \"_diffusers_version\" not in config:\n        raise ValueError(\"model_index.json does not contain _diffusers_version\")\n\n    logger.info(\"Diffusers version: %s\", config[\"_diffusers_version\"])\n    return cast(dict[str, Any], config)\n</code></pre>"},{"location":"api/fastvideo/utils/#fastvideo.utils.warn_for_unimplemented_methods","title":"fastvideo.utils.warn_for_unimplemented_methods","text":"<pre><code>warn_for_unimplemented_methods(cls: type[T]) -&gt; type[T]\n</code></pre> <p>A replacement for <code>abc.ABC</code>. When we use <code>abc.ABC</code>, subclasses will fail to instantiate if they do not implement all abstract methods. Here, we only require <code>raise NotImplementedError</code> in the base class, and log a warning if the method is not implemented in the subclass.</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def warn_for_unimplemented_methods(cls: type[T]) -&gt; type[T]:\n    \"\"\"\n    A replacement for `abc.ABC`.\n    When we use `abc.ABC`, subclasses will fail to instantiate\n    if they do not implement all abstract methods.\n    Here, we only require `raise NotImplementedError` in the\n    base class, and log a warning if the method is not implemented\n    in the subclass.\n    \"\"\"\n\n    original_init = cls.__init__\n\n    def find_unimplemented_methods(self: object):\n        unimplemented_methods = []\n        for attr_name in dir(self):\n            # bypass inner method\n            if attr_name.startswith('_'):\n                continue\n\n            try:\n                attr = getattr(self, attr_name)\n                # get the func of callable method\n                if callable(attr):\n                    attr_func = attr.__func__\n            except AttributeError:\n                continue\n            src = inspect.getsource(attr_func)\n            if \"NotImplementedError\" in src:\n                unimplemented_methods.append(attr_name)\n        if unimplemented_methods:\n            method_names = ','.join(unimplemented_methods)\n            msg = (f\"Methods {method_names} not implemented in {self}\")\n            logger.warning(msg)\n\n    @wraps(original_init)\n    def wrapped_init(self, *args, **kwargs) -&gt; None:\n        original_init(self, *args, **kwargs)\n        find_unimplemented_methods(self)\n\n    type.__setattr__(cls, '__init__', wrapped_init)\n    return cls\n</code></pre>"},{"location":"api/fastvideo/utils/#fastvideo.utils.xpu_is_initialized","title":"fastvideo.utils.xpu_is_initialized","text":"<pre><code>xpu_is_initialized() -&gt; bool\n</code></pre> <p>Check if XPU is initialized.</p> Source code in <code>fastvideo/utils.py</code> <pre><code>def xpu_is_initialized() -&gt; bool:\n    \"\"\"Check if XPU is initialized.\"\"\"\n    if not torch.xpu._is_compiled():\n        return False\n    return torch.xpu.is_initialized()\n</code></pre>"},{"location":"api/fastvideo/version/","title":"version","text":""},{"location":"api/fastvideo/version/#fastvideo.version","title":"version","text":""},{"location":"api/fastvideo/worker/","title":"worker","text":""},{"location":"api/fastvideo/worker/#fastvideo.worker","title":"worker","text":""},{"location":"api/fastvideo/worker/#fastvideo.worker-classes","title":"Classes","text":""},{"location":"api/fastvideo/worker/#fastvideo.worker.Executor","title":"fastvideo.worker.Executor","text":"<pre><code>Executor(fastvideo_args: FastVideoArgs)\n</code></pre> <p>               Bases: <code>ABC</code></p> Source code in <code>fastvideo/worker/executor.py</code> <pre><code>def __init__(self, fastvideo_args: FastVideoArgs):\n    self.fastvideo_args = fastvideo_args\n\n    self._init_executor()\n</code></pre>"},{"location":"api/fastvideo/worker/#fastvideo.worker.Executor-functions","title":"Functions","text":""},{"location":"api/fastvideo/worker/#fastvideo.worker.Executor.collective_rpc","title":"fastvideo.worker.Executor.collective_rpc  <code>abstractmethod</code>","text":"<pre><code>collective_rpc(\n    method: str | Callable[..., _R],\n    timeout: float | None = None,\n    args: tuple = (),\n    kwargs: dict[str, Any] | None = None,\n) -&gt; list[_R]\n</code></pre> <p>Execute an RPC call on all workers.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str | Callable[..., _R]</code> <p>Name of the worker method to execute, or a callable that is serialized and sent to all workers to execute.</p> <p>If the method is a callable, it should accept an additional <code>self</code> argument, in addition to the arguments passed in <code>args</code> and <code>kwargs</code>. The <code>self</code> argument will be the worker object.</p> required <code>timeout</code> <code>float | None</code> <p>Maximum time in seconds to wait for execution. Raises a :exc:<code>TimeoutError</code> on timeout. <code>None</code> means wait indefinitely.</p> <code>None</code> <code>args</code> <code>tuple</code> <p>Positional arguments to pass to the worker method.</p> <code>()</code> <code>kwargs</code> <code>dict[str, Any] | None</code> <p>Keyword arguments to pass to the worker method.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[_R]</code> <p>A list containing the results from each worker.</p> Note <p>It is recommended to use this API to only pass control messages, and set up data-plane communication to pass data.</p> Source code in <code>fastvideo/worker/executor.py</code> <pre><code>@abstractmethod\ndef collective_rpc(self,\n                   method: str | Callable[..., _R],\n                   timeout: float | None = None,\n                   args: tuple = (),\n                   kwargs: dict[str, Any] | None = None) -&gt; list[_R]:\n    \"\"\"\n    Execute an RPC call on all workers.\n\n    Args:\n        method: Name of the worker method to execute, or a callable that\n            is serialized and sent to all workers to execute.\n\n            If the method is a callable, it should accept an additional\n            `self` argument, in addition to the arguments passed in `args`\n            and `kwargs`. The `self` argument will be the worker object.\n        timeout: Maximum time in seconds to wait for execution. Raises a\n            :exc:`TimeoutError` on timeout. `None` means wait indefinitely.\n        args: Positional arguments to pass to the worker method.\n        kwargs: Keyword arguments to pass to the worker method.\n\n    Returns:\n        A list containing the results from each worker.\n\n    Note:\n        It is recommended to use this API to only pass control messages,\n        and set up data-plane communication to pass data.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fastvideo/worker/#fastvideo.worker.Executor.merge_lora_weights","title":"fastvideo.worker.Executor.merge_lora_weights  <code>abstractmethod</code>","text":"<pre><code>merge_lora_weights() -&gt; None\n</code></pre> <p>Merge the LoRA weights for the workers.</p> Source code in <code>fastvideo/worker/executor.py</code> <pre><code>@abstractmethod\ndef merge_lora_weights(self) -&gt; None:\n    \"\"\"\n    Merge the LoRA weights for the workers.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fastvideo/worker/#fastvideo.worker.Executor.set_lora_adapter","title":"fastvideo.worker.Executor.set_lora_adapter  <code>abstractmethod</code>","text":"<pre><code>set_lora_adapter(\n    lora_nickname: str, lora_path: str | None = None\n) -&gt; None\n</code></pre> <p>Set the LoRA adapter for the workers.</p> Source code in <code>fastvideo/worker/executor.py</code> <pre><code>@abstractmethod\ndef set_lora_adapter(self,\n                     lora_nickname: str,\n                     lora_path: str | None = None) -&gt; None:\n    \"\"\"\n    Set the LoRA adapter for the workers.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fastvideo/worker/#fastvideo.worker.Executor.shutdown","title":"fastvideo.worker.Executor.shutdown  <code>abstractmethod</code>","text":"<pre><code>shutdown() -&gt; None\n</code></pre> <p>Shutdown the executor.</p> Source code in <code>fastvideo/worker/executor.py</code> <pre><code>@abstractmethod\ndef shutdown(self) -&gt; None:\n    \"\"\"\n    Shutdown the executor.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fastvideo/worker/#fastvideo.worker.Executor.unmerge_lora_weights","title":"fastvideo.worker.Executor.unmerge_lora_weights  <code>abstractmethod</code>","text":"<pre><code>unmerge_lora_weights() -&gt; None\n</code></pre> <p>Unmerge the LoRA weights for the workers.</p> Source code in <code>fastvideo/worker/executor.py</code> <pre><code>@abstractmethod\ndef unmerge_lora_weights(self) -&gt; None:\n    \"\"\"\n    Unmerge the LoRA weights for the workers.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fastvideo/worker/#fastvideo.worker.MultiprocExecutor","title":"fastvideo.worker.MultiprocExecutor","text":"<pre><code>MultiprocExecutor(fastvideo_args: FastVideoArgs)\n</code></pre> <p>               Bases: <code>Executor</code></p> Source code in <code>fastvideo/worker/executor.py</code> <pre><code>def __init__(self, fastvideo_args: FastVideoArgs):\n    self.fastvideo_args = fastvideo_args\n\n    self._init_executor()\n</code></pre>"},{"location":"api/fastvideo/worker/#fastvideo.worker.MultiprocExecutor-functions","title":"Functions","text":""},{"location":"api/fastvideo/worker/#fastvideo.worker.MultiprocExecutor.__del__","title":"fastvideo.worker.MultiprocExecutor.__del__","text":"<pre><code>__del__()\n</code></pre> <p>Ensure cleanup on garbage collection</p> Source code in <code>fastvideo/worker/multiproc_executor.py</code> <pre><code>def __del__(self):\n    \"\"\"Ensure cleanup on garbage collection\"\"\"\n    self.shutdown()\n</code></pre>"},{"location":"api/fastvideo/worker/#fastvideo.worker.MultiprocExecutor.__enter__","title":"fastvideo.worker.MultiprocExecutor.__enter__","text":"<pre><code>__enter__()\n</code></pre> <p>Support for context manager protocol</p> Source code in <code>fastvideo/worker/multiproc_executor.py</code> <pre><code>def __enter__(self):\n    \"\"\"Support for context manager protocol\"\"\"\n    return self\n</code></pre>"},{"location":"api/fastvideo/worker/#fastvideo.worker.MultiprocExecutor.__exit__","title":"fastvideo.worker.MultiprocExecutor.__exit__","text":"<pre><code>__exit__(exc_type, exc_val, exc_tb)\n</code></pre> <p>Ensure cleanup when exiting context</p> Source code in <code>fastvideo/worker/multiproc_executor.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Ensure cleanup when exiting context\"\"\"\n    self.shutdown()\n</code></pre>"},{"location":"api/fastvideo/worker/#fastvideo.worker.MultiprocExecutor.shutdown","title":"fastvideo.worker.MultiprocExecutor.shutdown","text":"<pre><code>shutdown() -&gt; None\n</code></pre> <p>Properly shut down the executor and its workers</p> Source code in <code>fastvideo/worker/multiproc_executor.py</code> <pre><code>def shutdown(self) -&gt; None:\n    \"\"\"Properly shut down the executor and its workers\"\"\"\n    if hasattr(self, 'shutting_down') and self.shutting_down:\n        return  # Prevent multiple shutdown calls\n\n    logger.info(\"Shutting down MultiprocExecutor...\")\n    self.shutting_down = True\n\n    # First try gentle termination\n    try:\n        # Send termination message to all workers\n        for worker in self.workers:\n            with contextlib.suppress(Exception):\n                worker.pipe.send({\n                    \"method\": \"shutdown\",\n                    \"args\": (),\n                    \"kwargs\": {}\n                })\n\n        # Give workers some time to exit gracefully\n        start_time = time.perf_counter()\n        while time.perf_counter() - start_time &lt; 5.0:  # 5 seconds timeout\n            if all(not worker.proc.is_alive() for worker in self.workers):\n                break\n            time.sleep(0.1)\n\n        # Force terminate any remaining workers\n        for worker in self.workers:\n            if worker.proc.is_alive():\n                worker.proc.terminate()\n\n        # Final timeout for terminate\n        start_time = time.perf_counter()\n        while time.perf_counter() - start_time &lt; 2.0:  # 2 seconds timeout\n            if all(not worker.proc.is_alive() for worker in self.workers):\n                break\n            time.sleep(0.1)\n\n        # Kill if still alive\n        for worker in self.workers:\n            if worker.proc.is_alive():\n                worker.proc.kill()\n            worker.proc.join(timeout=1.0)\n\n    except Exception as e:\n        logger.error(\"Error during shutdown: %s\", e)\n        # Last resort, try to kill all workers\n        for worker in self.workers:\n            with contextlib.suppress(Exception):\n                if worker.proc.is_alive():\n                    worker.proc.kill()\n\n    # Clean up pipes\n    for worker in self.workers:\n        with contextlib.suppress(Exception):\n            worker.pipe.close()\n\n    self.workers = []\n    logger.info(\"MultiprocExecutor shutdown complete\")\n</code></pre>"},{"location":"api/fastvideo/worker/#fastvideo.worker-functions","title":"Functions","text":""},{"location":"api/fastvideo/worker/#fastvideo.worker.initialize_ray_cluster","title":"fastvideo.worker.initialize_ray_cluster","text":"<pre><code>initialize_ray_cluster(\n    fastvideo_args: FastVideoArgs,\n    ray_address: str | None = None,\n)\n</code></pre> <p>Initialize the distributed cluster with Ray.</p> <p>it will connect to the Ray cluster and create a placement group for the workers, which includes the specification of the resources for each distributed worker.</p> <p>Parameters:</p> Name Type Description Default <code>parallel_config</code> <p>The configurations for parallel execution.</p> required <code>ray_address</code> <code>str | None</code> <p>The address of the Ray cluster. If None, uses the default Ray cluster address.</p> <code>None</code> Source code in <code>fastvideo/worker/ray_utils.py</code> <pre><code>def initialize_ray_cluster(\n    fastvideo_args: FastVideoArgs,\n    ray_address: str | None = None,\n):\n    \"\"\"Initialize the distributed cluster with Ray.\n\n    it will connect to the Ray cluster and create a placement group\n    for the workers, which includes the specification of the resources\n    for each distributed worker.\n\n    Args:\n        parallel_config: The configurations for parallel execution.\n        ray_address: The address of the Ray cluster. If None, uses\n            the default Ray cluster address.\n    \"\"\"\n    assert_ray_available()\n    from fastvideo.platforms import current_platform\n\n    if ray.is_initialized():\n        logger.info(\"Ray is already initialized. Skipping Ray initialization.\")\n    elif current_platform.is_rocm() or current_platform.is_xpu():\n        # Try to connect existing ray instance and create a new one if not found\n        try:\n            ray.init(\"auto\")\n        except ConnectionError:\n            logger.warning(\n                \"No existing RAY instance detected. \"\n                \"A new instance will be launched with current node resources.\")\n            ray.init(address=ray_address,\n                     num_gpus=fastvideo_args.num_gpus,\n                     runtime_env=fastvideo_args.ray_runtime_env)\n    else:\n        ray.init(address=ray_address,\n                 runtime_env=fastvideo_args.ray_runtime_env)\n\n    device_str = current_platform.ray_device_key\n    if not device_str:\n        raise ValueError(\n            f\"current platform {current_platform.device_name} does not \"\n            \"support ray.\")\n\n    # Create or get the placement group for worker processes\n    if fastvideo_args.ray_placement_group:\n        current_placement_group = fastvideo_args.ray_placement_group\n    else:\n        current_placement_group = ray.util.get_current_placement_group()\n\n    if current_placement_group:\n        logger.info(\"Using the existing placement group\")\n\n        # We are in a placement group\n        bundles = current_placement_group.bundle_specs\n        # Verify that we can use the placement group.\n        device_bundles = 0\n        for bundle in bundles:\n            bundle_devices = bundle.get(device_str, 0)\n            if bundle_devices &gt; 1:\n                raise ValueError(\n                    \"Placement group bundle cannot have more than 1 \"\n                    f\"{device_str}.\")\n            if bundle_devices:\n                device_bundles += 1\n        if fastvideo_args.num_gpus &gt; device_bundles:\n            raise ValueError(\n                f\"The number of required {device_str}s exceeds the total \"\n                f\"number of available {device_str}s in the placement group. \"\n                f\"Required number of devices: {fastvideo_args.num_gpus}. \"\n                f\"Total number of devices: {device_bundles}.\")\n    else:\n        logger.info(\"No current placement group found. \"\n                    \"Creating a new placement group.\")\n        num_devices_in_cluster = ray.cluster_resources().get(device_str, 0)\n        # Log a warning message and delay resource allocation failure response.\n        # Avoid immediate rejection to allow user-initiated placement group\n        # created and wait cluster to be ready\n        if fastvideo_args.num_gpus &gt; num_devices_in_cluster:\n            logger.warning(\n                \"The number of required %ss exceeds the total \"\n                \"number of available %ss in the placement group.\", device_str,\n                device_str)\n        # Create a new placement group\n        placement_group_specs: list[dict[str, float]] = ([{\n            device_str: 1.0\n        } for _ in range(fastvideo_args.num_gpus)])\n\n        # FastVideo engine is also a worker to execute model with an accelerator,\n        # so it requires to have the device in a current node. Check if\n        # the current node has at least one device.\n        current_ip = get_ip()\n        current_node_id = ray.get_runtime_context().get_node_id()\n        current_node_resource = available_resources_per_node()[current_node_id]\n        if current_node_resource.get(device_str, 0) &lt; 1:\n            raise ValueError(\n                f\"Current node has no {device_str} available. \"\n                f\"{current_node_resource=}. FastVideo engine cannot start without \"\n                f\"{device_str}. Make sure you have at least 1 {device_str} \"\n                f\"available in a node {current_node_id=} {current_ip=}.\")\n        # This way, at least bundle is required to be created in a current\n        # node.\n        placement_group_specs[0][f\"node:{current_ip}\"] = 0.001\n\n        # By default, Ray packs resources as much as possible.\n        current_placement_group = ray.util.placement_group(\n            placement_group_specs, strategy=\"PACK\")\n        _wait_until_pg_ready(current_placement_group)\n\n    assert current_placement_group is not None\n    _verify_bundles(current_placement_group, fastvideo_args, device_str)\n    # Set the placement group in the fastvideo args\n    fastvideo_args.ray_placement_group = current_placement_group\n</code></pre>"},{"location":"api/fastvideo/worker/#fastvideo.worker-modules","title":"Modules","text":""},{"location":"api/fastvideo/worker/#fastvideo.worker.executor","title":"fastvideo.worker.executor","text":""},{"location":"api/fastvideo/worker/#fastvideo.worker.executor-classes","title":"Classes","text":""},{"location":"api/fastvideo/worker/#fastvideo.worker.executor.Executor","title":"fastvideo.worker.executor.Executor","text":"<pre><code>Executor(fastvideo_args: FastVideoArgs)\n</code></pre> <p>               Bases: <code>ABC</code></p> Source code in <code>fastvideo/worker/executor.py</code> <pre><code>def __init__(self, fastvideo_args: FastVideoArgs):\n    self.fastvideo_args = fastvideo_args\n\n    self._init_executor()\n</code></pre> Functions\u00b6 fastvideo.worker.executor.Executor.collective_rpc <code>abstractmethod</code> \u00b6 <pre><code>collective_rpc(\n    method: str | Callable[..., _R],\n    timeout: float | None = None,\n    args: tuple = (),\n    kwargs: dict[str, Any] | None = None,\n) -&gt; list[_R]\n</code></pre> <p>Execute an RPC call on all workers.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str | Callable[..., _R]</code> <p>Name of the worker method to execute, or a callable that is serialized and sent to all workers to execute.</p> <p>If the method is a callable, it should accept an additional <code>self</code> argument, in addition to the arguments passed in <code>args</code> and <code>kwargs</code>. The <code>self</code> argument will be the worker object.</p> required <code>timeout</code> <code>float | None</code> <p>Maximum time in seconds to wait for execution. Raises a :exc:<code>TimeoutError</code> on timeout. <code>None</code> means wait indefinitely.</p> <code>None</code> <code>args</code> <code>tuple</code> <p>Positional arguments to pass to the worker method.</p> <code>()</code> <code>kwargs</code> <code>dict[str, Any] | None</code> <p>Keyword arguments to pass to the worker method.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[_R]</code> <p>A list containing the results from each worker.</p> Note <p>It is recommended to use this API to only pass control messages, and set up data-plane communication to pass data.</p> Source code in <code>fastvideo/worker/executor.py</code> <pre><code>@abstractmethod\ndef collective_rpc(self,\n                   method: str | Callable[..., _R],\n                   timeout: float | None = None,\n                   args: tuple = (),\n                   kwargs: dict[str, Any] | None = None) -&gt; list[_R]:\n    \"\"\"\n    Execute an RPC call on all workers.\n\n    Args:\n        method: Name of the worker method to execute, or a callable that\n            is serialized and sent to all workers to execute.\n\n            If the method is a callable, it should accept an additional\n            `self` argument, in addition to the arguments passed in `args`\n            and `kwargs`. The `self` argument will be the worker object.\n        timeout: Maximum time in seconds to wait for execution. Raises a\n            :exc:`TimeoutError` on timeout. `None` means wait indefinitely.\n        args: Positional arguments to pass to the worker method.\n        kwargs: Keyword arguments to pass to the worker method.\n\n    Returns:\n        A list containing the results from each worker.\n\n    Note:\n        It is recommended to use this API to only pass control messages,\n        and set up data-plane communication to pass data.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.worker.executor.Executor.merge_lora_weights <code>abstractmethod</code> \u00b6 <pre><code>merge_lora_weights() -&gt; None\n</code></pre> <p>Merge the LoRA weights for the workers.</p> Source code in <code>fastvideo/worker/executor.py</code> <pre><code>@abstractmethod\ndef merge_lora_weights(self) -&gt; None:\n    \"\"\"\n    Merge the LoRA weights for the workers.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.worker.executor.Executor.set_lora_adapter <code>abstractmethod</code> \u00b6 <pre><code>set_lora_adapter(\n    lora_nickname: str, lora_path: str | None = None\n) -&gt; None\n</code></pre> <p>Set the LoRA adapter for the workers.</p> Source code in <code>fastvideo/worker/executor.py</code> <pre><code>@abstractmethod\ndef set_lora_adapter(self,\n                     lora_nickname: str,\n                     lora_path: str | None = None) -&gt; None:\n    \"\"\"\n    Set the LoRA adapter for the workers.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.worker.executor.Executor.shutdown <code>abstractmethod</code> \u00b6 <pre><code>shutdown() -&gt; None\n</code></pre> <p>Shutdown the executor.</p> Source code in <code>fastvideo/worker/executor.py</code> <pre><code>@abstractmethod\ndef shutdown(self) -&gt; None:\n    \"\"\"\n    Shutdown the executor.\n    \"\"\"\n    raise NotImplementedError\n</code></pre> fastvideo.worker.executor.Executor.unmerge_lora_weights <code>abstractmethod</code> \u00b6 <pre><code>unmerge_lora_weights() -&gt; None\n</code></pre> <p>Unmerge the LoRA weights for the workers.</p> Source code in <code>fastvideo/worker/executor.py</code> <pre><code>@abstractmethod\ndef unmerge_lora_weights(self) -&gt; None:\n    \"\"\"\n    Unmerge the LoRA weights for the workers.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/fastvideo/worker/#fastvideo.worker.executor-functions","title":"Functions","text":""},{"location":"api/fastvideo/worker/#fastvideo.worker.gpu_worker","title":"fastvideo.worker.gpu_worker","text":""},{"location":"api/fastvideo/worker/#fastvideo.worker.gpu_worker-classes","title":"Classes","text":""},{"location":"api/fastvideo/worker/#fastvideo.worker.gpu_worker.Worker","title":"fastvideo.worker.gpu_worker.Worker","text":"<pre><code>Worker(\n    fastvideo_args: FastVideoArgs,\n    local_rank: int,\n    rank: int,\n    distributed_init_method: str,\n)\n</code></pre> Source code in <code>fastvideo/worker/gpu_worker.py</code> <pre><code>def __init__(self, fastvideo_args: FastVideoArgs, local_rank: int,\n             rank: int, distributed_init_method: str):\n    self.fastvideo_args = fastvideo_args\n    self.local_rank = local_rank\n    self.rank = rank\n    self.distributed_init_method = distributed_init_method\n</code></pre> Functions\u00b6 fastvideo.worker.gpu_worker.Worker.init_device \u00b6 <pre><code>init_device() -&gt; None\n</code></pre> <p>Initialize the device for the worker.</p> Source code in <code>fastvideo/worker/gpu_worker.py</code> <pre><code>def init_device(self) -&gt; None:\n    \"\"\"Initialize the device for the worker.\"\"\"\n\n    # torch.distributed.all_reduce does not free the input tensor until\n    # the synchronization point. This causes the memory usage to grow\n    # as the number of all_reduce calls increases. This env var disables\n    # this behavior.\n    # Related issue:\n    # https://discuss.pytorch.org/t/cuda-allocation-lifetime-for-inputs-to-distributed-all-reduce/191573\n    os.environ[\"TORCH_NCCL_AVOID_RECORD_STREAMS\"] = \"1\"\n    # This env var set by Ray causes exceptions with graph building.\n    os.environ.pop(\"NCCL_ASYNC_ERROR_HANDLING\", None)\n\n    # Platform-agnostic device initialization\n    self.device = get_local_torch_device()\n\n    from fastvideo.platforms import current_platform\n\n    # _check_if_gpu_supports_dtype(self.model_config.dtype)\n    if current_platform.is_cuda_alike():\n        self.init_gpu_memory = torch.cuda.mem_get_info()[0]\n    else:\n        # For MPS, we can't get memory info the same way\n        self.init_gpu_memory = 0\n\n    if self.fastvideo_args.distributed_executor_backend == \"mp\":\n        os.environ[\"LOCAL_RANK\"] = str(self.local_rank)\n    os.environ[\"RANK\"] = str(self.rank)\n    os.environ[\"WORLD_SIZE\"] = str(self.fastvideo_args.num_gpus)\n\n    # Initialize the distributed environment.\n    maybe_init_distributed_environment_and_model_parallel(\n        self.fastvideo_args.tp_size, self.fastvideo_args.sp_size,\n        self.distributed_init_method)\n\n    self.pipeline = build_pipeline(self.fastvideo_args)\n</code></pre> fastvideo.worker.gpu_worker.Worker.shutdown \u00b6 <pre><code>shutdown() -&gt; dict[str, Any]\n</code></pre> <p>Gracefully shut down the worker process</p> Source code in <code>fastvideo/worker/gpu_worker.py</code> <pre><code>def shutdown(self) -&gt; dict[str, Any]:\n    \"\"\"Gracefully shut down the worker process\"\"\"\n    logger.info(\"Worker %d shutting down...\",\n                self.rank,\n                local_main_process_only=False)\n    # Clean up resources\n    if hasattr(self, 'pipeline') and self.pipeline is not None:\n        # Clean up pipeline resources if needed\n        pass\n\n    # Destroy the distributed environment\n    cleanup_dist_env_and_memory(shutdown_ray=False)\n\n    logger.info(\"Worker %d shutdown complete\",\n                self.rank,\n                local_main_process_only=False)\n    return {\"status\": \"shutdown_complete\"}\n</code></pre>"},{"location":"api/fastvideo/worker/#fastvideo.worker.gpu_worker-functions","title":"Functions","text":""},{"location":"api/fastvideo/worker/#fastvideo.worker.multiproc_executor","title":"fastvideo.worker.multiproc_executor","text":""},{"location":"api/fastvideo/worker/#fastvideo.worker.multiproc_executor-classes","title":"Classes","text":""},{"location":"api/fastvideo/worker/#fastvideo.worker.multiproc_executor.MultiprocExecutor","title":"fastvideo.worker.multiproc_executor.MultiprocExecutor","text":"<pre><code>MultiprocExecutor(fastvideo_args: FastVideoArgs)\n</code></pre> <p>               Bases: <code>Executor</code></p> Source code in <code>fastvideo/worker/executor.py</code> <pre><code>def __init__(self, fastvideo_args: FastVideoArgs):\n    self.fastvideo_args = fastvideo_args\n\n    self._init_executor()\n</code></pre> Functions\u00b6 fastvideo.worker.multiproc_executor.MultiprocExecutor.__del__ \u00b6 <pre><code>__del__()\n</code></pre> <p>Ensure cleanup on garbage collection</p> Source code in <code>fastvideo/worker/multiproc_executor.py</code> <pre><code>def __del__(self):\n    \"\"\"Ensure cleanup on garbage collection\"\"\"\n    self.shutdown()\n</code></pre> fastvideo.worker.multiproc_executor.MultiprocExecutor.__enter__ \u00b6 <pre><code>__enter__()\n</code></pre> <p>Support for context manager protocol</p> Source code in <code>fastvideo/worker/multiproc_executor.py</code> <pre><code>def __enter__(self):\n    \"\"\"Support for context manager protocol\"\"\"\n    return self\n</code></pre> fastvideo.worker.multiproc_executor.MultiprocExecutor.__exit__ \u00b6 <pre><code>__exit__(exc_type, exc_val, exc_tb)\n</code></pre> <p>Ensure cleanup when exiting context</p> Source code in <code>fastvideo/worker/multiproc_executor.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Ensure cleanup when exiting context\"\"\"\n    self.shutdown()\n</code></pre> fastvideo.worker.multiproc_executor.MultiprocExecutor.shutdown \u00b6 <pre><code>shutdown() -&gt; None\n</code></pre> <p>Properly shut down the executor and its workers</p> Source code in <code>fastvideo/worker/multiproc_executor.py</code> <pre><code>def shutdown(self) -&gt; None:\n    \"\"\"Properly shut down the executor and its workers\"\"\"\n    if hasattr(self, 'shutting_down') and self.shutting_down:\n        return  # Prevent multiple shutdown calls\n\n    logger.info(\"Shutting down MultiprocExecutor...\")\n    self.shutting_down = True\n\n    # First try gentle termination\n    try:\n        # Send termination message to all workers\n        for worker in self.workers:\n            with contextlib.suppress(Exception):\n                worker.pipe.send({\n                    \"method\": \"shutdown\",\n                    \"args\": (),\n                    \"kwargs\": {}\n                })\n\n        # Give workers some time to exit gracefully\n        start_time = time.perf_counter()\n        while time.perf_counter() - start_time &lt; 5.0:  # 5 seconds timeout\n            if all(not worker.proc.is_alive() for worker in self.workers):\n                break\n            time.sleep(0.1)\n\n        # Force terminate any remaining workers\n        for worker in self.workers:\n            if worker.proc.is_alive():\n                worker.proc.terminate()\n\n        # Final timeout for terminate\n        start_time = time.perf_counter()\n        while time.perf_counter() - start_time &lt; 2.0:  # 2 seconds timeout\n            if all(not worker.proc.is_alive() for worker in self.workers):\n                break\n            time.sleep(0.1)\n\n        # Kill if still alive\n        for worker in self.workers:\n            if worker.proc.is_alive():\n                worker.proc.kill()\n            worker.proc.join(timeout=1.0)\n\n    except Exception as e:\n        logger.error(\"Error during shutdown: %s\", e)\n        # Last resort, try to kill all workers\n        for worker in self.workers:\n            with contextlib.suppress(Exception):\n                if worker.proc.is_alive():\n                    worker.proc.kill()\n\n    # Clean up pipes\n    for worker in self.workers:\n        with contextlib.suppress(Exception):\n            worker.pipe.close()\n\n    self.workers = []\n    logger.info(\"MultiprocExecutor shutdown complete\")\n</code></pre>"},{"location":"api/fastvideo/worker/#fastvideo.worker.multiproc_executor.UnreadyWorkerProcHandle","title":"fastvideo.worker.multiproc_executor.UnreadyWorkerProcHandle  <code>dataclass</code>","text":"<pre><code>UnreadyWorkerProcHandle(\n    proc: BaseProcess,\n    rank: int,\n    pipe: Connection,\n    ready_pipe: Connection,\n)\n</code></pre> <p>WorkerProcess handle before READY.</p>"},{"location":"api/fastvideo/worker/#fastvideo.worker.multiproc_executor.WorkerMultiprocProc","title":"fastvideo.worker.multiproc_executor.WorkerMultiprocProc","text":"<pre><code>WorkerMultiprocProc(\n    fastvideo_args: FastVideoArgs,\n    local_rank: int,\n    rank: int,\n    distributed_init_method: str,\n    pipe: Connection,\n)\n</code></pre> <p>Adapter that runs one Worker in busy loop.</p> Source code in <code>fastvideo/worker/multiproc_executor.py</code> <pre><code>def __init__(\n    self,\n    fastvideo_args: FastVideoArgs,\n    local_rank: int,\n    rank: int,\n    distributed_init_method: str,\n    pipe: Connection,\n):\n    self.rank = rank\n    self.pipe = pipe\n    wrapper = WorkerWrapperBase(fastvideo_args=fastvideo_args,\n                                rpc_rank=rank)\n\n    all_kwargs: list[dict] = [{} for _ in range(fastvideo_args.num_gpus)]\n    all_kwargs[rank] = {\n        \"fastvideo_args\": fastvideo_args,\n        \"local_rank\": local_rank,\n        \"rank\": rank,\n        \"distributed_init_method\": distributed_init_method,\n    }\n    wrapper.init_worker(all_kwargs)\n    self.worker = wrapper\n\n    # Initialize device\n    self.worker.init_device()\n\n    # Set process title and log prefix\n    self.setup_proc_title_and_log_prefix()\n</code></pre> Functions\u00b6 fastvideo.worker.multiproc_executor.WorkerMultiprocProc.worker_busy_loop \u00b6 <pre><code>worker_busy_loop() -&gt; None\n</code></pre> <p>Main busy loop for Multiprocessing Workers</p> Source code in <code>fastvideo/worker/multiproc_executor.py</code> <pre><code>def worker_busy_loop(self) -&gt; None:\n    \"\"\"Main busy loop for Multiprocessing Workers\"\"\"\n    while True:\n        logger.info(\"Worker %d starting event loop...\", self.rank)\n        try:\n            rpc_call = self.pipe.recv()\n            method = rpc_call.get(\"method\")\n            args = rpc_call.get(\"args\", ())\n            kwargs = rpc_call.get(\"kwargs\", {})\n\n            if isinstance(method, str):\n                if method == \"shutdown\":\n                    response = self.shutdown()\n                    with contextlib.suppress(Exception):\n                        self.pipe.send(response)\n                    break\n                if method == 'execute_forward':\n                    forward_batch = kwargs['forward_batch']\n                    fastvideo_args = kwargs['fastvideo_args']\n                    output_batch = self.worker.execute_forward(\n                        forward_batch, fastvideo_args)\n                    logging_info = None\n                    if envs.FASTVIDEO_STAGE_LOGGING:\n                        logging_info = output_batch.logging_info\n                    self.pipe.send({\n                        \"output_batch\": output_batch.output.cpu(),\n                        \"logging_info\": logging_info\n                    })\n            else:\n                result = self.worker.execute_method(method, *args, **kwargs)\n                self.pipe.send(result)\n        except KeyboardInterrupt:\n            logger.error(\n                \"Worker %d in loop received KeyboardInterrupt, aborting forward pass\",\n                self.rank)\n            try:\n                self.pipe.send(\n                    {\"error\": \"Operation aborted by KeyboardInterrupt\"})\n                logger.info(\"Worker %d sent error response after interrupt\",\n                            self.rank)\n            except Exception as e:\n                logger.error(\"Worker %d failed to send error response: %s\",\n                             self.rank, str(e))\n            continue\n</code></pre> fastvideo.worker.multiproc_executor.WorkerMultiprocProc.worker_main <code>staticmethod</code> \u00b6 <pre><code>worker_main(*args, **kwargs)\n</code></pre> <p>Worker initialization and execution loops. This runs a background process</p> Source code in <code>fastvideo/worker/multiproc_executor.py</code> <pre><code>@staticmethod\ndef worker_main(*args, **kwargs):\n    \"\"\" Worker initialization and execution loops.\n    This runs a background process \"\"\"\n\n    # Signal handler used for graceful termination.\n    # SystemExit exception is only raised once to allow this and worker\n    # processes to terminate without error\n    shutdown_requested = False\n\n    def signal_handler(signum, frame):\n        nonlocal shutdown_requested\n        if not shutdown_requested:\n            shutdown_requested = True\n            raise SystemExit()\n\n    # Either SIGTERM or SIGINT will terminate the worker\n    signal.signal(signal.SIGTERM, signal_handler)\n    signal.signal(signal.SIGINT, signal_handler)\n    kill_itself_when_parent_died()\n    faulthandler.enable()\n    parent_process = psutil.Process().parent()\n\n    worker = None\n    ready_pipe = kwargs.pop(\"ready_pipe\")\n    rank = kwargs.get(\"rank\")\n\n    try:\n        worker = WorkerMultiprocProc(*args, **kwargs)\n\n        # Send READY once we know everything is loaded\n        ready_pipe.send({\n            \"status\": WorkerMultiprocProc.READY_STR,\n        })\n\n        ready_pipe.close()\n        ready_pipe = None\n\n        worker.worker_busy_loop()\n\n    except Exception:\n        if ready_pipe is not None:\n            logger.exception(\"WorkerMultiprocProc failed to start.\")\n        else:\n            logger.exception(\"WorkerMultiprocProc failed.\")\n\n        # The parent sends a SIGTERM to all worker processes if\n        # any worker dies. Set this value so we don't re-throw\n        # SystemExit() to avoid zmq exceptions in __del__.\n        shutdown_requested = True\n        traceback = get_exception_traceback()\n        logger.error(\"Worker %d hit an exception: %s\", rank, traceback)\n        parent_process.send_signal(signal.SIGQUIT)\n\n    finally:\n        if ready_pipe is not None:\n            ready_pipe.close()\n        # Clean up once worker exits busy loop\n        if worker is not None:\n            worker.shutdown()\n</code></pre>"},{"location":"api/fastvideo/worker/#fastvideo.worker.multiproc_executor-functions","title":"Functions","text":""},{"location":"api/fastvideo/worker/#fastvideo.worker.multiproc_executor.set_multiproc_executor_envs","title":"fastvideo.worker.multiproc_executor.set_multiproc_executor_envs","text":"<pre><code>set_multiproc_executor_envs() -&gt; None\n</code></pre> <p>Set up environment variables that should be used when there are workers in a multiprocessing environment. This should be called by the parent  process before worker processes are created</p> Source code in <code>fastvideo/worker/multiproc_executor.py</code> <pre><code>def set_multiproc_executor_envs() -&gt; None:\n    \"\"\" Set up environment variables that should be used when there are workers\n    in a multiprocessing environment. This should be called by the parent \n    process before worker processes are created\"\"\"\n\n    force_spawn()\n</code></pre>"},{"location":"api/fastvideo/worker/#fastvideo.worker.ray_distributed_executor","title":"fastvideo.worker.ray_distributed_executor","text":""},{"location":"api/fastvideo/worker/#fastvideo.worker.ray_distributed_executor-classes","title":"Classes","text":""},{"location":"api/fastvideo/worker/#fastvideo.worker.ray_distributed_executor.RayDistributedExecutor","title":"fastvideo.worker.ray_distributed_executor.RayDistributedExecutor","text":"<pre><code>RayDistributedExecutor(fastvideo_args: FastVideoArgs)\n</code></pre> <p>               Bases: <code>Executor</code></p> <p>Ray-based distributed executor</p> Source code in <code>fastvideo/worker/executor.py</code> <pre><code>def __init__(self, fastvideo_args: FastVideoArgs):\n    self.fastvideo_args = fastvideo_args\n\n    self._init_executor()\n</code></pre>"},{"location":"api/fastvideo/worker/#fastvideo.worker.ray_distributed_executor.RayWorkerMetaData","title":"fastvideo.worker.ray_distributed_executor.RayWorkerMetaData  <code>dataclass</code>","text":"<pre><code>RayWorkerMetaData(\n    worker: ActorHandle,\n    created_rank: int,\n    adjusted_rank: int = -1,\n    ip: str = \"\",\n)\n</code></pre> <p>Metadata for a Ray worker. The order of ray worker creation can be random, and we need to reset the rank after creating all workers.</p>"},{"location":"api/fastvideo/worker/#fastvideo.worker.ray_distributed_executor-functions","title":"Functions","text":""},{"location":"api/fastvideo/worker/#fastvideo.worker.ray_env","title":"fastvideo.worker.ray_env","text":""},{"location":"api/fastvideo/worker/#fastvideo.worker.ray_env-functions","title":"Functions","text":""},{"location":"api/fastvideo/worker/#fastvideo.worker.ray_env.get_env_vars_to_copy","title":"fastvideo.worker.ray_env.get_env_vars_to_copy","text":"<pre><code>get_env_vars_to_copy(\n    exclude_vars: set[str] | None = None,\n    additional_vars: set[str] | None = None,\n    destination: str | None = None,\n) -&gt; set[str]\n</code></pre> <p>Get the environment variables to copy to downstream Ray actors.</p> <p>Example use cases: - Copy environment variables from RayDistributedExecutor to Ray workers. - Copy environment variables from RayDPClient to Ray DPEngineCoreActor.</p> <p>Parameters:</p> Name Type Description Default <code>exclude_vars</code> <code>set[str] | None</code> <p>A set of FastVideo defined environment variables to exclude from copying.</p> <code>None</code> <code>additional_vars</code> <code>set[str] | None</code> <p>A set of additional environment variables to copy. If a variable is in both exclude_vars and additional_vars, it will be excluded.</p> <code>None</code> <code>destination</code> <code>str | None</code> <p>The destination of the environment variables.</p> <code>None</code> <p>Returns:     A set of environment variables to copy.</p> Source code in <code>fastvideo/worker/ray_env.py</code> <pre><code>def get_env_vars_to_copy(\n    exclude_vars: set[str] | None = None,\n    additional_vars: set[str] | None = None,\n    destination: str | None = None,\n) -&gt; set[str]:\n    \"\"\"\n    Get the environment variables to copy to downstream Ray actors.\n\n    Example use cases:\n    - Copy environment variables from RayDistributedExecutor to Ray workers.\n    - Copy environment variables from RayDPClient to Ray DPEngineCoreActor.\n\n    Args:\n        exclude_vars: A set of FastVideo defined environment variables to exclude\n            from copying.\n        additional_vars: A set of additional environment variables to copy.\n            If a variable is in both exclude_vars and additional_vars, it will\n            be excluded.\n        destination: The destination of the environment variables.\n    Returns:\n        A set of environment variables to copy.\n    \"\"\"\n    exclude_vars = exclude_vars or set()\n    additional_vars = additional_vars or set()\n\n    env_vars_to_copy = {\n        v\n        for v in set(envs.environment_variables).union(additional_vars)\n        if v not in exclude_vars and v not in RAY_NON_CARRY_OVER_ENV_VARS\n    }\n\n    to_destination = \" to \" + destination if destination is not None else \"\"\n\n    logger.info(\n        \"RAY_NON_CARRY_OVER_ENV_VARS from config: %s\",\n        RAY_NON_CARRY_OVER_ENV_VARS,\n    )\n    logger.info(\n        \"Copying the following environment variables%s: %s\",\n        to_destination,\n        [v for v in env_vars_to_copy if v in os.environ],\n    )\n    logger.info(\n        \"If certain env vars should NOT be copied, add them to %s file\",\n        RAY_NON_CARRY_OVER_ENV_VARS_FILE,\n    )\n\n    return env_vars_to_copy\n</code></pre>"},{"location":"api/fastvideo/worker/#fastvideo.worker.ray_utils","title":"fastvideo.worker.ray_utils","text":""},{"location":"api/fastvideo/worker/#fastvideo.worker.ray_utils-classes","title":"Classes","text":""},{"location":"api/fastvideo/worker/#fastvideo.worker.ray_utils.RayWorkerWrapper","title":"fastvideo.worker.ray_utils.RayWorkerWrapper","text":"<pre><code>RayWorkerWrapper(*args, **kwargs)\n</code></pre> <p>               Bases: <code>WorkerWrapperBase</code></p> <p>Ray wrapper for fastvideo.worker.Worker, allowing Worker to be lazily initialized after Ray sets CUDA_VISIBLE_DEVICES.</p> Source code in <code>fastvideo/worker/ray_utils.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"api/fastvideo/worker/#fastvideo.worker.ray_utils-functions","title":"Functions","text":""},{"location":"api/fastvideo/worker/#fastvideo.worker.ray_utils.assert_ray_available","title":"fastvideo.worker.ray_utils.assert_ray_available","text":"<pre><code>assert_ray_available() -&gt; None\n</code></pre> <p>Raise an exception if Ray is not available.</p> Source code in <code>fastvideo/worker/ray_utils.py</code> <pre><code>def assert_ray_available() -&gt; None:\n    \"\"\"Raise an exception if Ray is not available.\"\"\"\n    if ray is None:\n        raise ValueError(f\"Failed to import Ray: {ray_import_err}.\"\n                         \"Please install Ray with `pip install ray`.\")\n</code></pre>"},{"location":"api/fastvideo/worker/#fastvideo.worker.ray_utils.initialize_ray_cluster","title":"fastvideo.worker.ray_utils.initialize_ray_cluster","text":"<pre><code>initialize_ray_cluster(\n    fastvideo_args: FastVideoArgs,\n    ray_address: str | None = None,\n)\n</code></pre> <p>Initialize the distributed cluster with Ray.</p> <p>it will connect to the Ray cluster and create a placement group for the workers, which includes the specification of the resources for each distributed worker.</p> <p>Parameters:</p> Name Type Description Default <code>parallel_config</code> <p>The configurations for parallel execution.</p> required <code>ray_address</code> <code>str | None</code> <p>The address of the Ray cluster. If None, uses the default Ray cluster address.</p> <code>None</code> Source code in <code>fastvideo/worker/ray_utils.py</code> <pre><code>def initialize_ray_cluster(\n    fastvideo_args: FastVideoArgs,\n    ray_address: str | None = None,\n):\n    \"\"\"Initialize the distributed cluster with Ray.\n\n    it will connect to the Ray cluster and create a placement group\n    for the workers, which includes the specification of the resources\n    for each distributed worker.\n\n    Args:\n        parallel_config: The configurations for parallel execution.\n        ray_address: The address of the Ray cluster. If None, uses\n            the default Ray cluster address.\n    \"\"\"\n    assert_ray_available()\n    from fastvideo.platforms import current_platform\n\n    if ray.is_initialized():\n        logger.info(\"Ray is already initialized. Skipping Ray initialization.\")\n    elif current_platform.is_rocm() or current_platform.is_xpu():\n        # Try to connect existing ray instance and create a new one if not found\n        try:\n            ray.init(\"auto\")\n        except ConnectionError:\n            logger.warning(\n                \"No existing RAY instance detected. \"\n                \"A new instance will be launched with current node resources.\")\n            ray.init(address=ray_address,\n                     num_gpus=fastvideo_args.num_gpus,\n                     runtime_env=fastvideo_args.ray_runtime_env)\n    else:\n        ray.init(address=ray_address,\n                 runtime_env=fastvideo_args.ray_runtime_env)\n\n    device_str = current_platform.ray_device_key\n    if not device_str:\n        raise ValueError(\n            f\"current platform {current_platform.device_name} does not \"\n            \"support ray.\")\n\n    # Create or get the placement group for worker processes\n    if fastvideo_args.ray_placement_group:\n        current_placement_group = fastvideo_args.ray_placement_group\n    else:\n        current_placement_group = ray.util.get_current_placement_group()\n\n    if current_placement_group:\n        logger.info(\"Using the existing placement group\")\n\n        # We are in a placement group\n        bundles = current_placement_group.bundle_specs\n        # Verify that we can use the placement group.\n        device_bundles = 0\n        for bundle in bundles:\n            bundle_devices = bundle.get(device_str, 0)\n            if bundle_devices &gt; 1:\n                raise ValueError(\n                    \"Placement group bundle cannot have more than 1 \"\n                    f\"{device_str}.\")\n            if bundle_devices:\n                device_bundles += 1\n        if fastvideo_args.num_gpus &gt; device_bundles:\n            raise ValueError(\n                f\"The number of required {device_str}s exceeds the total \"\n                f\"number of available {device_str}s in the placement group. \"\n                f\"Required number of devices: {fastvideo_args.num_gpus}. \"\n                f\"Total number of devices: {device_bundles}.\")\n    else:\n        logger.info(\"No current placement group found. \"\n                    \"Creating a new placement group.\")\n        num_devices_in_cluster = ray.cluster_resources().get(device_str, 0)\n        # Log a warning message and delay resource allocation failure response.\n        # Avoid immediate rejection to allow user-initiated placement group\n        # created and wait cluster to be ready\n        if fastvideo_args.num_gpus &gt; num_devices_in_cluster:\n            logger.warning(\n                \"The number of required %ss exceeds the total \"\n                \"number of available %ss in the placement group.\", device_str,\n                device_str)\n        # Create a new placement group\n        placement_group_specs: list[dict[str, float]] = ([{\n            device_str: 1.0\n        } for _ in range(fastvideo_args.num_gpus)])\n\n        # FastVideo engine is also a worker to execute model with an accelerator,\n        # so it requires to have the device in a current node. Check if\n        # the current node has at least one device.\n        current_ip = get_ip()\n        current_node_id = ray.get_runtime_context().get_node_id()\n        current_node_resource = available_resources_per_node()[current_node_id]\n        if current_node_resource.get(device_str, 0) &lt; 1:\n            raise ValueError(\n                f\"Current node has no {device_str} available. \"\n                f\"{current_node_resource=}. FastVideo engine cannot start without \"\n                f\"{device_str}. Make sure you have at least 1 {device_str} \"\n                f\"available in a node {current_node_id=} {current_ip=}.\")\n        # This way, at least bundle is required to be created in a current\n        # node.\n        placement_group_specs[0][f\"node:{current_ip}\"] = 0.001\n\n        # By default, Ray packs resources as much as possible.\n        current_placement_group = ray.util.placement_group(\n            placement_group_specs, strategy=\"PACK\")\n        _wait_until_pg_ready(current_placement_group)\n\n    assert current_placement_group is not None\n    _verify_bundles(current_placement_group, fastvideo_args, device_str)\n    # Set the placement group in the fastvideo args\n    fastvideo_args.ray_placement_group = current_placement_group\n</code></pre>"},{"location":"api/fastvideo/worker/#fastvideo.worker.ray_utils.is_in_ray_actor","title":"fastvideo.worker.ray_utils.is_in_ray_actor","text":"<pre><code>is_in_ray_actor()\n</code></pre> <p>Check if we are in a Ray actor.</p> Source code in <code>fastvideo/worker/ray_utils.py</code> <pre><code>def is_in_ray_actor():\n    \"\"\"Check if we are in a Ray actor.\"\"\"\n\n    try:\n        import ray\n        return (ray.is_initialized()\n                and ray.get_runtime_context().get_actor_id() is not None)\n    except ImportError:\n        return False\n</code></pre>"},{"location":"api/fastvideo/worker/#fastvideo.worker.worker_base","title":"fastvideo.worker.worker_base","text":""},{"location":"api/fastvideo/worker/#fastvideo.worker.worker_base-classes","title":"Classes","text":""},{"location":"api/fastvideo/worker/#fastvideo.worker.worker_base.WorkerWrapperBase","title":"fastvideo.worker.worker_base.WorkerWrapperBase","text":"<pre><code>WorkerWrapperBase(\n    fastvideo_args: FastVideoArgs, rpc_rank: int = 0\n)\n</code></pre> <p>This class represents one process in an executor/engine. It is responsible for lazily initializing the worker and handling the worker's lifecycle. We first instantiate the WorkerWrapper, which remembers the worker module and class name. Then, when we call <code>update_environment_variables</code>, and the real initialization happens in <code>init_worker</code>.</p> <p>Initialize the worker wrapper with the given fastvideo_args and rpc_rank. Note: rpc_rank is the rank of the worker in the executor. In most cases, it is also the rank of the worker in the distributed group. However, when multiple executors work together, they can be different. e.g. in the case of SPMD-style offline inference with TP=2, users can launch 2 engines/executors, each with only 1 worker. All workers have rpc_rank=0, but they have different ranks in the TP group.</p> Source code in <code>fastvideo/worker/worker_base.py</code> <pre><code>def __init__(\n    self,\n    fastvideo_args: FastVideoArgs,\n    rpc_rank: int = 0,\n) -&gt; None:\n    \"\"\"\n    Initialize the worker wrapper with the given fastvideo_args and rpc_rank.\n    Note: rpc_rank is the rank of the worker in the executor. In most cases,\n    it is also the rank of the worker in the distributed group. However,\n    when multiple executors work together, they can be different.\n    e.g. in the case of SPMD-style offline inference with TP=2,\n    users can launch 2 engines/executors, each with only 1 worker.\n    All workers have rpc_rank=0, but they have different ranks in the TP\n    group.\n    \"\"\"\n    self.rpc_rank = rpc_rank\n    self.worker: Worker | None = None\n    self.fastvideo_args: FastVideoArgs | None = None\n</code></pre> Functions\u00b6 fastvideo.worker.worker_base.WorkerWrapperBase.adjust_rank \u00b6 <pre><code>adjust_rank(rank_mapping: dict[int, int]) -&gt; None\n</code></pre> <p>Adjust the rpc_rank based on the given mapping. It is only used during the initialization of the executor, to adjust the rpc_rank of workers after we create all workers.</p> Source code in <code>fastvideo/worker/worker_base.py</code> <pre><code>def adjust_rank(self, rank_mapping: dict[int, int]) -&gt; None:\n    \"\"\"\n    Adjust the rpc_rank based on the given mapping.\n    It is only used during the initialization of the executor,\n    to adjust the rpc_rank of workers after we create all workers.\n    \"\"\"\n    if self.rpc_rank in rank_mapping:\n        self.rpc_rank = rank_mapping[self.rpc_rank]\n</code></pre> fastvideo.worker.worker_base.WorkerWrapperBase.init_worker \u00b6 <pre><code>init_worker(all_kwargs: list[dict[str, Any]]) -&gt; None\n</code></pre> <p>Here we inject some common logic before initializing the worker. Arguments are passed to the worker class constructor.</p> Source code in <code>fastvideo/worker/worker_base.py</code> <pre><code>def init_worker(self, all_kwargs: list[dict[str, Any]]) -&gt; None:\n    \"\"\"\n    Here we inject some common logic before initializing the worker.\n    Arguments are passed to the worker class constructor.\n    \"\"\"\n    kwargs = all_kwargs[self.rpc_rank]\n    self.fastvideo_args = kwargs.get(\"fastvideo_args\")\n    assert self.fastvideo_args is not None, (\n        \"fastvideo_args is required to initialize the worker\")\n\n    self.worker = Worker(**kwargs)\n    assert self.worker is not None\n</code></pre>"},{"location":"api/fastvideo/worker/#fastvideo.worker.worker_base-functions","title":"Functions","text":""},{"location":"api/fastvideo/workflow/","title":"workflow","text":""},{"location":"api/fastvideo/workflow/#fastvideo.workflow","title":"workflow","text":""},{"location":"api/fastvideo/workflow/#fastvideo.workflow-modules","title":"Modules","text":""},{"location":"api/fastvideo/workflow/#fastvideo.workflow.preprocess","title":"fastvideo.workflow.preprocess","text":""},{"location":"api/fastvideo/workflow/#fastvideo.workflow.preprocess-modules","title":"Modules","text":""},{"location":"api/fastvideo/workflow/#fastvideo.workflow.preprocess.components","title":"fastvideo.workflow.preprocess.components","text":"Classes\u00b6 fastvideo.workflow.preprocess.components.ParquetDatasetSaver \u00b6 <pre><code>ParquetDatasetSaver(\n    flush_frequency: int,\n    samples_per_file: int,\n    schema: Schema,\n    record_creator: Callable[..., list[dict[str, Any]]],\n)\n</code></pre> <p>Component for saving and writing Parquet datasets using shared parquet_io.</p> Source code in <code>fastvideo/workflow/preprocess/components.py</code> <pre><code>def __init__(self, flush_frequency: int, samples_per_file: int,\n             schema: pa.Schema,\n             record_creator: Callable[..., list[dict[str, Any]]]):\n    self.flush_frequency = flush_frequency\n    self.samples_per_file = samples_per_file\n    self.schema = schema\n    self.create_records_from_batch = record_creator\n    self.num_processed_samples: int = 0\n    self._writer: ParquetDatasetWriter | None = None\n</code></pre> Functions\u00b6 fastvideo.workflow.preprocess.components.ParquetDatasetSaver.clean_up \u00b6 <pre><code>clean_up() -&gt; None\n</code></pre> <p>Clean up all tables</p> Source code in <code>fastvideo/workflow/preprocess/components.py</code> <pre><code>def clean_up(self) -&gt; None:\n    \"\"\"Clean up all tables\"\"\"\n    self.flush_tables(write_remainder=True)\n    self._writer = None\n    self.num_processed_samples = 0\n    gc.collect()\n</code></pre> fastvideo.workflow.preprocess.components.ParquetDatasetSaver.flush_tables \u00b6 <pre><code>flush_tables(write_remainder: bool = False)\n</code></pre> <p>Flush buffered records to disk.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <p>Directory where parquet files are written. Kept for API symmetry (writer already configured with this path).</p> required <code>write_remainder</code> <code>bool</code> <p>If True, also write any leftover rows smaller than <code>samples_per_file</code> as a final small file. Useful for the last flush.</p> <code>False</code> Source code in <code>fastvideo/workflow/preprocess/components.py</code> <pre><code>def flush_tables(self, write_remainder: bool = False):\n    \"\"\"Flush buffered records to disk.\n\n    Args:\n        output_dir: Directory where parquet files are written. Kept for API\n            symmetry (writer already configured with this path).\n        write_remainder: If True, also write any leftover rows smaller than\n            ``samples_per_file`` as a final small file. Useful for the last flush.\n    \"\"\"\n    if self._writer is None:\n        return\n    _ = self._writer.flush(write_remainder=write_remainder)\n    # Reset processed sample count modulo samples_per_file\n    remainder = self.num_processed_samples % self.samples_per_file\n    self.num_processed_samples = 0 if write_remainder else remainder\n</code></pre> fastvideo.workflow.preprocess.components.ParquetDatasetSaver.save_and_write_parquet_batch \u00b6 <pre><code>save_and_write_parquet_batch(\n    batch: PreprocessBatch,\n    output_dir: str,\n    extra_features: dict[str, Any] | None = None,\n) -&gt; None\n</code></pre> <p>Save and write Parquet dataset batch</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>PreprocessBatch</code> <p>PreprocessBatch containing video and metadata information</p> required <code>output_dir</code> <code>str</code> <p>Output directory</p> required <code>extra_features</code> <code>dict[str, Any] | None</code> <p>Extra features</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>Number of processed samples</p> Source code in <code>fastvideo/workflow/preprocess/components.py</code> <pre><code>def save_and_write_parquet_batch(\n        self,\n        batch: PreprocessBatch,\n        output_dir: str,\n        extra_features: dict[str, Any] | None = None) -&gt; None:\n    \"\"\"\n    Save and write Parquet dataset batch\n\n    Args:\n        batch: PreprocessBatch containing video and metadata information\n        output_dir: Output directory\n        extra_features: Extra features\n\n    Returns:\n        Number of processed samples\n    \"\"\"\n    assert isinstance(batch.latents, torch.Tensor)\n    assert isinstance(batch.prompt_embeds, list)\n    assert isinstance(batch.prompt_attention_mask, list)\n\n    # Process non-padded embeddings (if needed)\n    if batch.prompt_attention_mask is not None:\n        batch.prompt_embeds = self._process_non_padded_embeddings(\n            batch.prompt_embeds[0], batch.prompt_attention_mask[0])\n    else:\n        raise ValueError(\"prompt_attention_mask is None\")\n\n    # Prepare batch data for Parquet dataset\n    batch_data: list[dict[str, Any]] = []\n\n    for key in dataclasses.fields(batch):\n        value = getattr(batch, key.name)\n        if isinstance(value, list):\n            for idx in range(len(value)):\n                if isinstance(value[idx], torch.Tensor):\n                    value[idx] = value[idx].cpu().numpy()\n        elif isinstance(value, torch.Tensor):\n            value = value.cpu().numpy()\n            setattr(batch, key.name, value)\n\n    # Create record for Parquet dataset\n    records = self.create_records_from_batch(batch)\n    batch_data.extend(records)\n\n    if batch_data:\n        self.num_processed_samples += len(batch_data)\n        table = records_to_table(batch_data, self.schema)\n        if self._writer is None:\n            os.makedirs(output_dir, exist_ok=True)\n            self._writer = ParquetDatasetWriter(\n                out_dir=output_dir, samples_per_file=self.samples_per_file)\n        self._writer.append_table(table)\n        logger.debug(\"Collected batch with %s samples\", len(table))\n\n    # If flush is needed\n    if self.num_processed_samples &gt;= self.flush_frequency:\n        self.flush_tables()\n</code></pre> fastvideo.workflow.preprocess.components.PreprocessingDataValidator \u00b6 <pre><code>PreprocessingDataValidator(\n    max_height: int = 1024,\n    max_width: int = 1024,\n    max_h_div_w_ratio: float = 17 / 16,\n    min_h_div_w_ratio: float = 8 / 16,\n    num_frames: int = 16,\n    train_fps: int = 24,\n    speed_factor: float = 1.0,\n    video_length_tolerance_range: float = 5.0,\n    drop_short_ratio: float = 0.0,\n    hw_aspect_threshold: float = 1.5,\n)\n</code></pre> Source code in <code>fastvideo/workflow/preprocess/components.py</code> <pre><code>def __init__(self,\n             max_height: int = 1024,\n             max_width: int = 1024,\n             max_h_div_w_ratio: float = 17 / 16,\n             min_h_div_w_ratio: float = 8 / 16,\n             num_frames: int = 16,\n             train_fps: int = 24,\n             speed_factor: float = 1.0,\n             video_length_tolerance_range: float = 5.0,\n             drop_short_ratio: float = 0.0,\n             hw_aspect_threshold: float = 1.5):\n    self.max_height = max_height\n    self.max_width = max_width\n    self.max_h_div_w_ratio = max_h_div_w_ratio\n    self.min_h_div_w_ratio = min_h_div_w_ratio\n    self.num_frames = num_frames\n    self.train_fps = train_fps\n    self.speed_factor = speed_factor\n    self.video_length_tolerance_range = video_length_tolerance_range\n    self.drop_short_ratio = drop_short_ratio\n    self.hw_aspect_threshold = hw_aspect_threshold\n    self.validators: dict[str, Callable[[dict[str, Any]], bool]] = {}\n    self.filter_counts: dict[str, int] = {}\n\n    self.num_items_before_filtering = 0\n    self.num_items_after_filtering = 0\n\n    self.register_validators()\n</code></pre> Functions\u00b6 fastvideo.workflow.preprocess.components.PreprocessingDataValidator.__call__ \u00b6 <pre><code>__call__(batch: dict[str, Any]) -&gt; bool\n</code></pre> <p>Validate whether the preprocessing data batch is valid.</p> Source code in <code>fastvideo/workflow/preprocess/components.py</code> <pre><code>def __call__(self, batch: dict[str, Any]) -&gt; bool:\n    \"\"\"\n    Validate whether the preprocessing data batch is valid.\n    \"\"\"\n    self.num_items_before_filtering += 1\n\n    for name, validator in self.validators.items():\n        if not validator(batch):\n            self.filter_counts[name] += 1\n            return False\n\n    self.num_items_after_filtering += 1\n    return True\n</code></pre> Functions\u00b6"},{"location":"api/fastvideo/workflow/#fastvideo.workflow.workflow_base","title":"fastvideo.workflow.workflow_base","text":""},{"location":"api/fastvideo/workflow/#fastvideo.workflow.workflow_base-classes","title":"Classes","text":""},{"location":"api/fastvideo/workflow/#fastvideo.workflow.workflow_base.WorkflowBase","title":"fastvideo.workflow.workflow_base.WorkflowBase","text":"<pre><code>WorkflowBase(fastvideo_args: FastVideoArgs)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for defining video processing workflows.</p> <p>A workflow serves as the top-level orchestrator that coordinates multiple pipelines and components to accomplish a specific video processing task. The workflow pattern provides several key benefits:</p> <ol> <li> <p>Separation of Concerns: Workflows separate high-level orchestration logic    from low-level processing implementations in pipelines.</p> </li> <li> <p>Modularity: Different workflows can be created for different execution modes    (preprocess, inference, etc.) while sharing common pipeline components.</p> </li> <li> <p>Configuration Management: Workflows manage the configuration and initialization    of multiple related pipelines and components in a centralized manner.</p> </li> <li> <p>Environment Setup: Workflows handle system-level setup and resource    allocation before pipeline execution begins.</p> </li> <li> <p>Lifecycle Management: Workflows control the complete lifecycle from    initialization through execution to cleanup.</p> </li> </ol> <p>The workflow acts as a factory and coordinator, creating the appropriate pipelines based on configuration, setting up the execution environment, and orchestrating the overall processing flow.</p> <p>Initialize the workflow with configuration arguments.</p> <p>Parameters:</p> Name Type Description Default <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>Configuration object containing all parameters           needed for workflow and pipeline setup.</p> required Source code in <code>fastvideo/workflow/workflow_base.py</code> <pre><code>def __init__(self, fastvideo_args: FastVideoArgs):\n    \"\"\"\n    Initialize the workflow with configuration arguments.\n\n    Args:\n        fastvideo_args: Configuration object containing all parameters\n                      needed for workflow and pipeline setup.\n    \"\"\"\n    self.fastvideo_args = fastvideo_args\n\n    # TODO: pipeline_config should be: dict[str, PipelineConfig]\n    # pipeline_type should be included in the PipelineConfig\n    # pipeline_config[pipeline_name] = (pipeline_type, fastvideo_args)\n    self._pipeline_configs: dict[str, tuple[PipelineType,\n                                            FastVideoArgs]] = {}\n    self._pipelines: dict[str, ComposedPipelineBase] = {}\n    self._components: dict[str, Any] = {}\n    self.register_pipelines()\n    self.register_components()\n\n    self.prepare_system_environment()\n    self.load_pipelines()\n</code></pre> Functions\u00b6 fastvideo.workflow.workflow_base.WorkflowBase.add_component \u00b6 <pre><code>add_component(component_name: str, component: Any) -&gt; None\n</code></pre> <p>Register a component instance with the workflow.</p> <p>Components are auxiliary objects that may be shared across pipelines or used for workflow-level functionality (e.g., databases, caches, external services).</p> <p>Parameters:</p> Name Type Description Default <code>component_name</code> <code>str</code> <p>Unique identifier for the component.</p> required <code>component</code> <code>Any</code> <p>The component instance to register.</p> required Source code in <code>fastvideo/workflow/workflow_base.py</code> <pre><code>def add_component(self, component_name: str, component: Any) -&gt; None:\n    \"\"\"\n    Register a component instance with the workflow.\n\n    Components are auxiliary objects that may be shared across pipelines\n    or used for workflow-level functionality (e.g., databases, caches,\n    external services).\n\n    Args:\n        component_name: Unique identifier for the component.\n        component: The component instance to register.\n    \"\"\"\n    self._components[component_name] = component\n    setattr(self, component_name, component)\n</code></pre> fastvideo.workflow.workflow_base.WorkflowBase.add_pipeline_config \u00b6 <pre><code>add_pipeline_config(\n    pipeline_name: str,\n    pipeline_config: tuple[PipelineType, FastVideoArgs],\n) -&gt; None\n</code></pre> <p>Register a pipeline configuration for later instantiation.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_name</code> <code>str</code> <p>Unique identifier for the pipeline.</p> required <code>pipeline_config</code> <code>tuple[PipelineType, FastVideoArgs]</code> <p>Tuple containing the pipeline type and            configuration arguments.</p> required Source code in <code>fastvideo/workflow/workflow_base.py</code> <pre><code>def add_pipeline_config(\n        self, pipeline_name: str,\n        pipeline_config: tuple[PipelineType, FastVideoArgs]) -&gt; None:\n    \"\"\"\n    Register a pipeline configuration for later instantiation.\n\n    Args:\n        pipeline_name: Unique identifier for the pipeline.\n        pipeline_config: Tuple containing the pipeline type and\n                       configuration arguments.\n    \"\"\"\n    self._pipeline_configs[pipeline_name] = pipeline_config\n</code></pre> fastvideo.workflow.workflow_base.WorkflowBase.get_component \u00b6 <pre><code>get_component(component_name: str) -&gt; Any\n</code></pre> <p>Retrieve a registered component by name.</p> <p>Parameters:</p> Name Type Description Default <code>component_name</code> <code>str</code> <p>The name of the component to retrieve.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The component instance.</p> Source code in <code>fastvideo/workflow/workflow_base.py</code> <pre><code>def get_component(self, component_name: str) -&gt; Any:\n    \"\"\"\n    Retrieve a registered component by name.\n\n    Args:\n        component_name: The name of the component to retrieve.\n\n    Returns:\n        The component instance.\n    \"\"\"\n    return self._components[component_name]\n</code></pre> fastvideo.workflow.workflow_base.WorkflowBase.get_workflow_cls <code>classmethod</code> \u00b6 <pre><code>get_workflow_cls(\n    fastvideo_args: FastVideoArgs,\n) -&gt; Optional[WorkflowBase]\n</code></pre> <p>Factory method to get the appropriate workflow class based on execution mode.</p> <p>This method acts as a workflow factory, returning the appropriate workflow class implementation based on the specified execution mode in the configuration arguments.</p> <p>Parameters:</p> Name Type Description Default <code>fastvideo_args</code> <code>FastVideoArgs</code> <p>Configuration object containing the execution mode           and other parameters.</p> required <p>Returns:</p> Type Description <code>Optional[WorkflowBase]</code> <p>The appropriate workflow class for the specified execution mode,</p> <code>Optional[WorkflowBase]</code> <p>or None if no workflow is available for the given mode.</p> Source code in <code>fastvideo/workflow/workflow_base.py</code> <pre><code>@classmethod\ndef get_workflow_cls(\n        cls, fastvideo_args: FastVideoArgs) -&gt; Optional[\"WorkflowBase\"]:\n    \"\"\"\n    Factory method to get the appropriate workflow class based on execution mode.\n\n    This method acts as a workflow factory, returning the appropriate\n    workflow class implementation based on the specified execution mode\n    in the configuration arguments.\n\n    Args:\n        fastvideo_args: Configuration object containing the execution mode\n                      and other parameters.\n\n    Returns:\n        The appropriate workflow class for the specified execution mode,\n        or None if no workflow is available for the given mode.\n    \"\"\"\n    if fastvideo_args.mode == ExecutionMode.PREPROCESS:\n        from fastvideo.workflow.preprocess.preprocess_workflow import (\n            PreprocessWorkflow)\n        return PreprocessWorkflow.get_workflow_cls(fastvideo_args)\n    else:\n        raise ValueError(\n            f\"Execution mode: {fastvideo_args.mode} is not supported in workflow.\"\n        )\n</code></pre> fastvideo.workflow.workflow_base.WorkflowBase.load_pipelines \u00b6 <pre><code>load_pipelines() -&gt; None\n</code></pre> <p>Create and initialize all registered pipelines.</p> <p>This method instantiates pipeline objects from their configurations and makes them available as both dictionary entries and instance attributes for convenient access.</p> Source code in <code>fastvideo/workflow/workflow_base.py</code> <pre><code>def load_pipelines(self) -&gt; None:\n    \"\"\"\n    Create and initialize all registered pipelines.\n\n    This method instantiates pipeline objects from their configurations\n    and makes them available as both dictionary entries and instance\n    attributes for convenient access.\n    \"\"\"\n    for pipeline_name, pipeline_config in self._pipeline_configs.items():\n        pipeline_type, fastvideo_args = pipeline_config\n        pipeline = build_pipeline(fastvideo_args, pipeline_type)\n        self._pipelines[pipeline_name] = pipeline\n        setattr(self, pipeline_name, pipeline)\n</code></pre> fastvideo.workflow.workflow_base.WorkflowBase.prepare_system_environment <code>abstractmethod</code> \u00b6 <pre><code>prepare_system_environment() -&gt; None\n</code></pre> <p>Prepare the system environment for workflow execution.</p> <p>Subclasses must implement this method to handle any system-level setup required before pipeline execution (e.g., GPU initialization, temporary directories, resource allocation).</p> Source code in <code>fastvideo/workflow/workflow_base.py</code> <pre><code>@abstractmethod\ndef prepare_system_environment(self) -&gt; None:\n    \"\"\"\n    Prepare the system environment for workflow execution.\n\n    Subclasses must implement this method to handle any system-level\n    setup required before pipeline execution (e.g., GPU initialization,\n    temporary directories, resource allocation).\n    \"\"\"\n    pass\n</code></pre> fastvideo.workflow.workflow_base.WorkflowBase.register_components <code>abstractmethod</code> \u00b6 <pre><code>register_components() -&gt; None\n</code></pre> <p>Register workflow-specific components.</p> <p>Subclasses must implement this method to register any components needed for their specific workflow (e.g., databases, external APIs, shared resources).</p> Source code in <code>fastvideo/workflow/workflow_base.py</code> <pre><code>@abstractmethod\ndef register_components(self) -&gt; None:\n    \"\"\"\n    Register workflow-specific components.\n\n    Subclasses must implement this method to register any components\n    needed for their specific workflow (e.g., databases, external APIs,\n    shared resources).\n    \"\"\"\n    pass\n</code></pre> fastvideo.workflow.workflow_base.WorkflowBase.register_pipelines <code>abstractmethod</code> \u00b6 <pre><code>register_pipelines() -&gt; None\n</code></pre> <p>Register workflow-specific pipelines.</p> <p>Subclasses must implement this method to define which pipelines are needed for their specific workflow and how they should be configured.</p> Source code in <code>fastvideo/workflow/workflow_base.py</code> <pre><code>@abstractmethod\ndef register_pipelines(self) -&gt; None:\n    \"\"\"\n    Register workflow-specific pipelines.\n\n    Subclasses must implement this method to define which pipelines\n    are needed for their specific workflow and how they should be\n    configured.\n    \"\"\"\n    pass\n</code></pre> fastvideo.workflow.workflow_base.WorkflowBase.run <code>abstractmethod</code> \u00b6 <pre><code>run()\n</code></pre> <p>Execute the main workflow logic.</p> <p>Subclasses must implement this method to define the specific execution flow for their workflow, coordinating the registered pipelines and components to accomplish the desired task.</p> Source code in <code>fastvideo/workflow/workflow_base.py</code> <pre><code>@abstractmethod\ndef run(self):\n    \"\"\"\n    Execute the main workflow logic.\n\n    Subclasses must implement this method to define the specific\n    execution flow for their workflow, coordinating the registered\n    pipelines and components to accomplish the desired task.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/fastvideo/workflow/#fastvideo.workflow.workflow_base-functions","title":"Functions","text":""},{"location":"contributing/overview/","title":"\ud83d\udee0\ufe0f Contributing to FastVideo","text":"<p>Thank you for your interest in contributing to FastVideo. We want to make the process as smooth for you as possible and this is a guide to help get you started!</p> <p>Our community is open to everyone and welcomes any contributions no matter how large or small.</p>"},{"location":"contributing/overview/#developer-environment","title":"Developer Environment:","text":"<p>Do make sure you have CUDA 12.4 installed and supported. FastVideo currently only support Linux and CUDA GPUs, but we hope to support other platforms in the future.</p> <p>We recommend using a fresh Python 3.10 Conda environment to develop FastVideo:</p> <p>Install Miniconda:</p> <pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh\nsource ~/.bashrc\n</code></pre> <p>Create and activate a Conda environment for FastVideo:</p> <pre><code>conda create -n fastvideo python=3.12 -y\nconda activate fastvideo\n</code></pre> <p>Install <code>uv</code> (optional, but recommended):</p> <p>From instructions on uv:</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n# or \nwget -qO- https://astral.sh/uv/install.sh | sh\n</code></pre> <p>Clone the FastVideo repository and go to the FastVideo directory:</p> <pre><code>git clone https://github.com/hao-ai-lab/FastVideo.git &amp;&amp; cd FastVideo\n</code></pre> <p>Now you can install FastVideo and setup git hooks for running linting. By using <code>pre-commit</code>, the linters will run and have to pass before you'll be able to make a commit.</p> <pre><code>uv pip install -e .[dev]\n\n# Can also install flash-attn (optional)\nuv pip install flash-attn --no-build-isolation \n\n# Linting, formatting and static type checking\npre-commit install --hook-type pre-commit --hook-type commit-msg\n\n# You can manually run pre-commit with\npre-commit run --all-files\n\n# Unit tests\npytest tests/\n</code></pre> <p>If you are on a Hopper GPU, you should also install FA3 for much better performance:</p> <pre><code>git clone https://github.com/Dao-AILab/flash-attention.git &amp;&amp; cd flash-attention/hopper\n\n# make sure you have ninja installed\nuv pip install ninja\n\npython setup.py install\n</code></pre>"},{"location":"contributing/profiling/","title":"Profiling FastVideo","text":"<p>!!! warning     Profiling is only intended for FastVideo developers and maintainers to understand the proportion of time spent in different parts of the codebase. FastVideo end-users should never turn on profiling as it will significantly slow down the inference.</p>"},{"location":"contributing/profiling/#profiling-with-pytorch","title":"Profiling with PyTorch","text":"<p>FastVideo exposes a process-wide torch profiler that you can enable via environment variables. Set <code>FASTVIDEO_TORCH_PROFILER_DIR</code> to an absolute directory path to start collecting traces, and specify the regions you want recorded with <code>FASTVIDEO_TORCH_PROFILE_REGIONS</code>:</p> <pre><code>FASTVIDEO_TORCH_PROFILER_DIR=/mnt/traces/fastvideo \\\nFASTVIDEO_TORCH_PROFILE_REGIONS=\"profiler_region_model_loading,profiler_region_training_step\"\n</code></pre> <p>All profiled regions must be registered in <code>fastvideo.profiler</code>; the current list includes:</p> <ul> <li><code>profiler_region_model_loading</code> \u2014 pipeline/module loading</li> <li><code>profiler_region_inference_pre_denoising</code></li> <li><code>profiler_region_inference_denoising</code></li> <li><code>profiler_region_inference_post_denoising</code></li> <li><code>profiler_region_training_checkpoint_saving</code></li> <li><code>profiler_region_training_dit</code></li> <li><code>profiler_region_training_validation</code></li> <li><code>profiler_region_training_epoch</code></li> <li><code>profiler_region_training_step</code></li> <li><code>profiler_region_training_backward</code></li> <li><code>profiler_region_training_optimizer</code></li> <li><code>profiler_region_distillation_teacher_forward</code></li> <li><code>profiler_region_distillation_student_forward</code></li> <li><code>profiler_region_distillation_loss</code></li> <li><code>profiler_region_distillation_update</code></li> </ul> <p>While profiling is enabled, FastVideo records additional annotations:</p> <ul> <li><code>fastvideo.region::&lt;name&gt;</code> spans are emitted when entering a region.</li> <li><code>fastvideo.profiler.enable_collection</code> / <code>fastvideo.profiler.disable_collection</code> events mark when torch profiler collection is toggled on or off.</li> </ul> <p>Only one profiler instance is created per process; subsequent pipelines reuse the same controller. If you set <code>FASTVIDEO_TORCH_PROFILE_REGIONS</code> incorrectly (e.g. misspelled name), FastVideo logs a warning and ignores that entry.</p> <p>Additional knobs:</p> <ul> <li><code>FASTVIDEO_TORCH_PROFILER_RECORD_SHAPES</code></li> <li><code>FASTVIDEO_TORCH_PROFILER_WITH_PROFILE_MEMORY</code></li> <li><code>FASTVIDEO_TORCH_PROFILER_WITH_STACK</code></li> <li><code>FASTVIDEO_TORCH_PROFILER_WITH_FLOPS</code></li> </ul> <p>Traces can be visualized using https://ui.perfetto.dev/.</p>"},{"location":"contributing/profiling/#best-practices","title":"Best Practices","text":"<ul> <li>Keep the profiled step count small; traces can be large and slow down job shutdown while the profiler flushes data.</li> <li>After profiling, clean up trace directories to avoid filling disks.</li> <li>When adding new regions, register them in <code>fastvideo.profiler</code> and wrap the corresponding code block with <code>with self.profiler_controller.region(\"your_region\"):</code> or the <code>@profile_region</code> decorator.</li> </ul>"},{"location":"contributing/developer_env/","title":"Index","text":"<p>(developer-env)</p>"},{"location":"contributing/developer_env/#developer-environment","title":"\ud83e\uddf0 Developer Environment","text":"<p>Accelerate your FastVideo development workflow by leveraging Docker images and cloud GPUs for efficient experimentation and reproducible environments.</p>"},{"location":"contributing/developer_env/docker/","title":"\ud83d\udc33 Using the FastVideo Docker Image","text":"<p>If you prefer a containerized development environment or want to avoid managing dependencies manually, you can use our prebuilt Docker image:</p> <p>Images: <code>ghcr.io/hao-ai-lab/fastvideo/fastvideo-dev:py3.12-latest</code></p>"},{"location":"contributing/developer_env/docker/#starting-the-container","title":"Starting the container","text":"<pre><code>docker run --gpus all -it ghcr.io/hao-ai-lab/fastvideo/fastvideo-dev:py3.12-latest\n</code></pre> <p>This will:</p> <ul> <li>Start the container with GPU access  </li> <li>Drop you into a shell with the <code>fastvideo-dev</code> Conda environment preconfigured</li> </ul>"},{"location":"contributing/developer_env/docker/#using-the-container","title":"Using the container","text":"<pre><code># Conda environment should already be active\n# FastVideo package installed in editable mode\n\n# Pull the latest changes from remote\ncd /FastVideo\ngit pull\n\n# Run linters and tests\npre-commit run --all-files\npytest tests/\n</code></pre>"},{"location":"contributing/developer_env/runpod/","title":"\ud83d\udce6 Developing FastVideo on RunPod","text":"<p>You can easily use the FastVideo Docker image as a custom container on RunPod for development or experimentation.</p>"},{"location":"contributing/developer_env/runpod/#creating-a-new-pod","title":"Creating a new pod","text":"<p>Choose a GPU that supports CUDA 12.8</p> <p>Pick 1 or 2 L40S GPU(s)</p> <p></p> <p>When creating your pod template, use this image:</p> <pre><code>ghcr.io/hao-ai-lab/fastvideo/fastvideo-dev:py3.12-latest\n</code></pre> <p>Paste Container Start Command to support SSH (RunPod Docs):</p> <pre><code>bash -c \"apt update;DEBIAN_FRONTEND=noninteractive apt-get install openssh-server -y;mkdir -p ~/.ssh;cd $_;chmod 700 ~/.ssh;echo \\\"$PUBLIC_KEY\\\" &gt;&gt; authorized_keys;chmod 700 authorized_keys;service ssh start;sleep infinity\"\n</code></pre> <p></p> <p>After deploying, the pod will take a few minutes to pull the image and start the SSH service.</p> <p></p>"},{"location":"contributing/developer_env/runpod/#working-with-the-pod","title":"Working with the pod","text":"<p>After SSH'ing into your pod, you'll find the <code>fastvideo-dev</code> Conda environment already activated.</p> <p>To pull in the latest changes from the GitHub repo:</p> <pre><code>cd /FastVideo\ngit pull\n</code></pre> <p><code>If you have a persistent volume and want to keep your code changes, you can move /FastVideo to /workspace/FastVideo, or simply clone the repository there.</code></p> <p>Run your development workflows as usual:</p> <pre><code># Run linters\npre-commit run --all-files\n\n# Run tests\npytest tests/\n</code></pre>"},{"location":"design/overview/","title":"\ud83d\udd0d FastVideo Overview","text":"<p>This document outlines FastVideo's architecture for developers interested in framework internals or contributions. It serves as an onboarding guide for new contributors by providing an overview of the most important directories and files within the <code>fastvideo/</code> codebase.</p>"},{"location":"design/overview/#table-of-contents-directory-structure-and-files","title":"Table of Contents - Directory Structure and Files","text":"<ul> <li><code>fastvideo/pipelines/</code> - Core diffusion pipeline components</li> <li><code>fastvideo/models/</code> - Model implementations</li> <li><code>dits/</code> - Transformer-based diffusion models</li> <li><code>vaes/</code> - Variational autoencoders</li> <li><code>encoders/</code> - Text and image encoders</li> <li><code>schedulers/</code> - Diffusion schedulers</li> <li><code>fastvideo/attention/</code> - Optimized attention implementations</li> <li><code>fastvideo/distributed/</code> - Distributed computing utilities</li> <li><code>fastvideo/layers/</code> - Custom neural network layers</li> <li><code>fastvideo/platforms/</code> - Hardware platform abstractions</li> <li><code>fastvideo/worker/</code> - Multi-GPU process management</li> <li><code>fastvideo/fastvideo_args.py</code> - Argument handling</li> <li><code>fastvideo/forward_context.py</code> - Forward pass context management</li> <li><code>fastvideo/utils.py</code> - Utility functions</li> <li><code>fastvideo/logger.py</code> - Logging infrastructure</li> </ul>"},{"location":"design/overview/#core-architecture","title":"Core Architecture","text":"<p>FastVideo separates model components from execution logic with these principles: - Component Isolation: Models (encoders, VAEs, transformers) are isolated from execution (pipelines, stages, distributed processing) - Modular Design: Components can be independently replaced - Distributed Execution: Supports various parallelism strategies (Tensor, Sequence) - Custom Attention Backends: Components can support and use different Attention implementations - Pipeline Abstraction: Consistent interface across diffusion models</p>"},{"location":"design/overview/#fastvideoargs","title":"FastVideoArgs","text":"<p>The <code>FastVideoArgs</code> class in <code>fastvideo/fastvideo_args.py</code> serves as the central configuration system for FastVideo. It contains all parameters needed to control model loading, inference configuration, performance optimization settings, and more.</p> <p>Key features include: - Command-line Interface: Automatic conversion between CLI arguments and dataclass fields - Configuration Groups: Organized by functional areas (model loading, video params, optimization settings) - Context Management: Global access to current settings via <code>get_current_fastvideo_args()</code> - Parameter Validation: Ensures valid combinations of settings</p> <p>Common configuration areas: - Model paths and loading options: <code>model_path</code>, <code>trust_remote_code</code>, <code>revision</code> - Distributed execution settings: <code>num_gpus</code>, <code>tp_size</code>, <code>sp_size</code> - Video generation parameters: <code>height</code>, <code>width</code>, <code>num_frames</code>, <code>num_inference_steps</code> - Precision settings: Control computation precision for different components</p> <p>Example usage:</p> <pre><code># Load arguments from command line\nfastvideo_args = prepare_fastvideo_args(sys.argv[1:])\n\n# Access parameters\nmodel = load_model(fastvideo_args.model_path)\n\n# Set as global context\nwith set_current_fastvideo_args(fastvideo_args):\n    # Code that requires access to these arguments\n    result = generate_video()\n</code></pre>"},{"location":"design/overview/#pipeline-system","title":"Pipeline System","text":""},{"location":"design/overview/#composedpipelinebase","title":"<code>ComposedPipelineBase</code>","text":"<p>This foundational class provides:</p> <ul> <li>Model Loading: Automatically loads components from HuggingFace-Diffusers-compatible model directories</li> <li>Stage Management: Creates and orchestrates processing stages</li> <li>Data Flow Coordination: Ensures proper state flow between stages</li> </ul> <pre><code>class MyCustomPipeline(ComposedPipelineBase):\n    _required_config_modules = [\n        \"text_encoder\", \"tokenizer\", \"vae\", \"transformer\", \"scheduler\"\n    ]\n\n    def initialize_pipeline(self, fastvideo_args: FastVideoArgs):\n        # Pipeline-specific initialization\n        pass\n\n    def create_pipeline_stages(self, fastvideo_args: FastVideoArgs):\n        self.add_stage(\"input_validation_stage\", InputValidationStage())\n        self.add_stage(\"text_encoding_stage\", CLIPTextEncodingStage(\n            text_encoder=self.get_module(\"text_encoder\"),\n            tokenizer=self.get_module(\"tokenizer\")\n        ))\n        # Additional stages...\n</code></pre>"},{"location":"design/overview/#pipeline-stages","title":"Pipeline Stages","text":"<p>Each stage handles a specific diffusion process component: - Input Validation: Parameter verification - Text Encoding: CLIP, LLaMA, or T5-based encoding - Image Encoding: Image input processing - Timestep &amp; Latent Preparation: Setup for diffusion - Denoising: Core diffusion loop - Decoding: Latent-to-pixel conversion</p> <p>Each stage implements a standard interface:</p> <pre><code>def forward(self, batch: ForwardBatch, fastvideo_args: FastVideoArgs) -&gt; ForwardBatch:\n    # Process batch and update state\n    return batch\n</code></pre>"},{"location":"design/overview/#forwardbatch","title":"ForwardBatch","text":"<p>Defined in <code>fastvideo/pipelines/pipeline_batch_info.py</code>, <code>ForwardBatch</code> encapsulates the data payload passed between pipeline stages. It typically holds:</p> <ul> <li>Input Data: Prompts, images, generation parameters</li> <li>Intermediate State: Embeddings, latents, timesteps, accumulated during stage execution</li> <li>Output Storage: Generated results and metadata</li> <li>Configuration: Sampling parameters, precision settings</li> </ul> <p>This structure facilitates clear state transitions between stages.</p>"},{"location":"design/overview/#model-components","title":"Model Components","text":"<p>The <code>fastvideo/models/</code> directory contains implementations of the core neural network models used in video diffusion:</p>"},{"location":"design/overview/#transformer-models","title":"Transformer Models","text":"<p>Transformer networks perform the actual denoising during diffusion:</p> <ul> <li>Location: <code>fastvideo/models/dits/</code></li> <li>Examples:</li> <li><code>WanTransformer3DModel</code></li> <li><code>HunyuanVideoTransformer3DModel</code></li> </ul> <p>Features include: - Text/image conditioning - Standardized interface for model-specific optimizations</p> <pre><code>def forward(\n    self, \n    latents,                    # [B, T, C, H, W]\n    encoder_hidden_states,      # Text embeddings\n    timestep,                   # Current diffusion timestep\n    encoder_hidden_states_image=None,  # Optional image embeddings\n    **kwargs\n):\n    # Perform denoising computation\n    return noise_pred  # Predicted noise residual\n</code></pre>"},{"location":"design/overview/#vae-variational-auto-encoder","title":"VAE (Variational Auto-Encoder)","text":"<p>VAEs handle conversion between pixel space and latent space:</p> <ul> <li>Location: <code>fastvideo/models/vaes/</code></li> <li>Examples:</li> <li><code>AutoencoderKLWan</code></li> <li><code>AutoencoderKLHunyuanVideo</code></li> </ul> <p>These models compress image/video data to a more efficient latent representation (typically 4x-8x smaller in each dimension).</p> <p>FastVideo's VAE implementations include: - Efficient video batch processing - Memory optimization - Optional tiling for large frames - Distributed weight support</p>"},{"location":"design/overview/#text-and-image-encoders","title":"Text and Image Encoders","text":"<p>Encoders process conditioning inputs into embeddings:</p> <ul> <li>Location: <code>fastvideo/models/encoders/</code></li> <li>Text Encoders:</li> <li><code>CLIPTextModel</code></li> <li><code>LlamaModel</code></li> <li><code>UMT5EncoderModel</code></li> <li>Image Encoders:</li> <li><code>CLIPVisionModel</code></li> </ul> <p>FastVideo implements optimizations such as: - Vocab parallelism for distributed processing - Caching for common prompts - Precision-tuned computation</p>"},{"location":"design/overview/#schedulers","title":"Schedulers","text":"<p>Schedulers manage the diffusion sampling process:</p> <ul> <li>Location: <code>fastvideo/models/schedulers/</code></li> <li>Examples:</li> <li><code>UniPCMultistepScheduler</code></li> <li><code>FlowMatchEulerDiscreteScheduler</code></li> </ul> <p>These components control: - Diffusion timestep sequences - Noise prediction to latent update conversions - Quality/speed trade-offs</p> <pre><code>def step(\n    self, \n    model_output: torch.Tensor,\n    timestep: torch.LongTensor,\n    sample: torch.Tensor,\n    **kwargs\n) -&gt; torch.Tensor:\n    # Process model output and update latents\n    # Return updated latents\n    return prev_sample\n</code></pre>"},{"location":"design/overview/#optimized-attention","title":"Optimized Attention","text":"<p>The <code>fastvideo/attention/</code> directory contains optimized attention implementations crucial for efficient video diffusion:</p>"},{"location":"design/overview/#attention-backends","title":"Attention Backends","text":"<p>Multiple implementations with automatic selection: - FLASH_ATTN: Optimized for supporting hardware - TORCH_SDPA: Built-in PyTorch scaled dot-product attention - SLIDING_TILE_ATTN: For very long sequences</p> <pre><code># Configure available attention backends for this layer\nself.attn = LocalAttention(\n    num_heads=num_heads,\n    head_size=head_dim,\n    causal=False,\n    supported_attention_backends=(_Backend.FLASH_ATTN, _Backend.TORCH_SDPA)\n)\n\n# Override via environment variable\n# export FASTVIDEO_ATTENTION_BACKEND=FLASH_ATTN\n</code></pre>"},{"location":"design/overview/#attention-patterns","title":"Attention Patterns","text":"<p>Supports various patterns with memory optimization techniques: - Cross/Self/Temporal/Global-Local Attention - Chunking, progressive computation, optimized masking</p>"},{"location":"design/overview/#distributed-processing","title":"Distributed Processing","text":"<p>The <code>fastvideo/distributed/</code> directory contains implementations for distributed model execution:</p>"},{"location":"design/overview/#tensor-parallelism","title":"Tensor Parallelism","text":"<p>Tensor parallelism splits model weights across devices:</p> <ul> <li>Implementation: Through <code>RowParallelLinear</code> and <code>ColumnParallelLinear</code> layers</li> <li>Use cases: Will be used by encoder models as their sequence lengths are shorter and enables efficient sharding.</li> </ul> <pre><code># Tensor-parallel layers in a transformer block\nfrom fastvideo.layers.linear import ColumnParallelLinear, RowParallelLinear\n\n# Split along output dimension\nself.qkv_proj = ColumnParallelLinear(\n    input_size=hidden_size,\n    output_size=3 * hidden_size,\n    bias=True,\n    gather_output=False\n)\n\n# Split along input dimension\nself.out_proj = RowParallelLinear(\n    input_size=hidden_size,\n    output_size=hidden_size,\n    bias=True,\n    input_is_parallel=True\n)\n</code></pre>"},{"location":"design/overview/#sequence-parallelism","title":"Sequence Parallelism","text":"<p>Sequence parallelism splits sequences across devices:</p> <ul> <li>Implementation: Through <code>DistributedAttention</code> and sequence splitting</li> <li>Use cases: Long video sequences or high-resolution processing. Used by DiT models.</li> </ul> <pre><code># Distributed attention for long sequences\nfrom fastvideo.attention import DistributedAttention\n\nself.attn = DistributedAttention(\n    num_heads=num_heads,\n    head_size=head_dim,\n    causal=False,\n    supported_attention_backends=(_Backend.SLIDING_TILE_ATTN, _Backend.FLASH_ATTN)\n)\n</code></pre>"},{"location":"design/overview/#communication-primitives","title":"Communication Primitives","text":"<p>Efficient distributed operations via AllGather, AllReduce, and synchronization mechanisms.</p> <p>Efficient communication primitives minimize distributed overhead:</p> <ul> <li>Sequence-Parallel AllGather: Collects sequence chunks</li> <li>Tensor-Parallel AllReduce: Combines partial results</li> <li>Distributed Synchronization: Coordinates execution</li> </ul>"},{"location":"design/overview/#forward-context-management","title":"Forward Context Management","text":""},{"location":"design/overview/#forwardcontext","title":"ForwardContext","text":"<p>Defined in <code>fastvideo/forward_context.py</code>, <code>ForwardContext</code> manages execution-specific state within a forward pass, particularly for low-level optimizations. It is accessed via <code>get_forward_context()</code>.</p> <ul> <li>Attention Metadata: Configuration for optimized attention kernels (<code>attn_metadata</code>)</li> <li>Profiling Data: Potential hooks for performance metrics collection</li> </ul> <p>This context-based approach enables: - Dynamic optimization based on execution state (e.g., attention backend selection) - Step-specific customizations within model components</p> <p>Usage example:</p> <pre><code>with set_forward_context(current_timestep, attn_metadata, fastvideo_args):\n    # During this forward pass, components can access context\n    # through get_forward_context()\n    output = model(inputs)\n</code></pre>"},{"location":"design/overview/#executor-and-worker-system","title":"Executor and Worker System","text":"<p>The <code>fastvideo/worker/</code> directory contains the distributed execution framework:</p>"},{"location":"design/overview/#executor-abstraction","title":"Executor Abstraction","text":"<p>FastVideo implements a flexible execution model for distributed processing:</p> <ul> <li>Executor Base Class: An abstract base class defining the interface for all executors</li> <li>MultiProcExecutor: Primary implementation that spawns and manages worker processes</li> <li>GPU Workers: Handle actual model execution on individual GPUs</li> </ul> <p>The MultiProcExecutor implementation: 1. Spawns worker processes for each GPU 2. Establishes communication channels via pipes 3. Coordinates distributed operations across workers 4. Handles graceful startup and shutdown of the process group</p> <p>Each GPU worker: 1. Initializes the distributed environment 2. Builds the pipeline for the specified model 3. Executes requested operations on its assigned GPU 4. Manages local resources and communicates results back to the executor</p> <p>This design allows FastVideo to efficiently utilize multiple GPUs while providing a simple, unified interface for model execution.</p>"},{"location":"design/overview/#platforms","title":"Platforms","text":"<p>The <code>fastvideo/platforms/</code> directory provides hardware platform abstractions that enable FastVideo to run efficiently on different hardware configurations:</p>"},{"location":"design/overview/#platform-abstraction","title":"Platform Abstraction","text":"<p>FastVideo's platform abstraction layer enables: - Hardware Detection: Automatic detection of available hardware - Backend Selection: Appropriate selection of compute kernels - Memory Management: Efficient utilization of hardware-specific memory features</p> <p>The primary components include: - Platform Interface: Defines the common API for all platform implementations - CUDA Platform: Optimized implementation for NVIDIA GPUs - Backend Enum: Used throughout the codebase for feature selection</p> <p>Usage example:</p> <pre><code>from fastvideo.platforms import current_platform, _Backend\n\n# Check hardware capabilities\nif current_platform.supports_backend(_Backend.FLASH_ATTN):\n    # Use FlashAttention implementation\nelse:\n    # Fall back to standard implementation\n</code></pre> <p>The platform system is designed to be extensible for future hardware targets.</p>"},{"location":"design/overview/#logger","title":"Logger","text":"<p>See PR</p> <p>TODO: (help wanted) Add an environment variable that disables process-aware logging.</p>"},{"location":"design/overview/#contributing-to-fastvideo","title":"Contributing to FastVideo","text":"<p>If you're a new contributor, here are some common areas to explore:</p> <ol> <li>Adding a new model: Implement new model types in the appropriate subdirectory of <code>fastvideo/models/</code></li> <li>Optimizing performance: Look at attention implementations or memory management</li> <li>Adding a new pipeline: Create a new pipeline subclass in <code>fastvideo/pipelines/</code></li> <li>Hardware support: Extend the <code>platforms</code> module for new hardware targets</li> </ol> <p>When adding code, follow these practices: - Use type hints for better code readability - Add appropriate docstrings - Maintain the separation between model components and execution logic - Follow existing patterns for distributed processing</p>"},{"location":"distillation/data_preprocess/","title":"\ud83e\uddf1 Data Preprocess for Distillation","text":"<p>For distillation, we use the same data preprocessing pipeline as training. Please refer to the Training Data Preprocess for general preprocessing steps.</p>"},{"location":"distillation/data_preprocess/#distillation-specific-datasets","title":"Distillation-Specific Datasets","text":""},{"location":"distillation/data_preprocess/#fastvideo-480p-synthetic-wan-dataset","title":"FastVideo 480P Synthetic Wan Dataset","text":"<p>For Wan2.1 T2V distillation, we use the FastVideo 480P Synthetic Wan dataset (FastVideo/Wan-Syn_77x448x832_600k) which contains 600k synthetic latents.</p> <pre><code># Download the preprocessed dataset\npython scripts/huggingface/download_hf.py \\\n    --repo_id \"FastVideo/Wan-Syn_77x448x832_600k\" \\\n    --local_dir \"FastVideo/Wan-Syn_77x448x832_600k\" \\\n    --repo_type \"dataset\"\n</code></pre>"},{"location":"distillation/data_preprocess/#crush-smol-dataset","title":"Crush Smol Dataset","text":"<p>For Wan2.2 TI2V distillation, we use the crush_smol dataset which includes both raw videos and preprocessed latents.</p> <pre><code># Download dataset\npython scripts/huggingface/download_hf.py \\\n    --repo_id=FastVideo/mini_i2v_dataset \\\n    --local_dir=data/mini_i2v_dataset \\\n    --repo_type=dataset\n</code></pre>"},{"location":"distillation/data_preprocess/#preprocessing-for-distillation","title":"Preprocessing for Distillation","text":"<p>The preprocessing steps are identical to training. Run the appropriate preprocessing script based on your model:</p> <pre><code># For Wan2.1 T2V\nbash scripts/preprocess/v1_preprocess_wan_data_t2v\n\n# For Wan2.2 TI2V  \nbash examples/distill/Wan2.2-TI2V-5B-Diffusers/crush_smol/preprocess_wan_data_ti2v_5b.sh\n</code></pre>"},{"location":"distillation/dmd/","title":"\ud83c\udfaf Distillation","text":"<p>We introduce a new finetuning strategy - Sparse-distill, which jointly integrates DMD and VSA in a single training process. This approach combines the benefits of both distillation to shorten diffusion steps and sparse attention to reduce attention computations, enabling much faster video generation.</p>"},{"location":"distillation/dmd/#model-overview","title":"\ud83d\udcca Model Overview","text":"<p>We provide two distilled models:</p> <ul> <li>FastWan2.1-T2V-1.3B-Diffusers: 3-step inference, up to 16 FPS on H100 GPU</li> <li>FastWan2.1-T2V-14B-480P-Diffusers: 3-step inference, up to 60x speed up at 480P, 90x speed up at 720P for denoising loop</li> <li>FastWan2.2-TI2V-5B-FullAttn-Diffusers: 3-step inference, up to 50x speed up at 720P for denoising loop</li> </ul> <p>Both models are trained on 61\u00d7448\u00d7832 resolution but support generating videos with any resolution (1.3B  model mainly support 480P, 14B model support 480P and 720P, quality may degrade for different resolutions).</p>"},{"location":"distillation/dmd/#inference","title":"\u2699\ufe0f Inference","text":"<p>First install VSA. Set <code>MODEL_BASE</code> to your own model path and run:</p> <pre><code>bash scripts/inference/v1_inference_wan_dmd.sh\n</code></pre>"},{"location":"distillation/dmd/#dataset","title":"\ud83d\uddc2\ufe0f Dataset","text":"<p>We use the FastVideo 480P Synthetic Wan dataset (FastVideo/Wan-Syn_77x448x832_600k) for distillation, which contains 600k synthetic latents.</p>"},{"location":"distillation/dmd/#download-dataset","title":"Download Dataset","text":"<pre><code># Download the preprocessed dataset\npython scripts/huggingface/download_hf.py \\\n    --repo_id \"FastVideo/Wan-Syn_77x448x832_600k\" \\\n    --local_dir \"FastVideo/Wan-Syn_77x448x832_600k\" \\\n    --repo_type \"dataset\"\n</code></pre>"},{"location":"distillation/dmd/#training-scripts","title":"\ud83d\ude80 Training Scripts","text":""},{"location":"distillation/dmd/#wan21-13b-model-sparse-distill","title":"Wan2.1 1.3B Model Sparse-Distill","text":"<p>For the 1.3B model, we use 4 nodes with 32 H200 GPUs (8 GPUs per node):</p> <pre><code># Multi-node training (8 nodes, 64 GPUs total)\nsbatch examples/distill/Wan2.1-T2V/Wan-Syn-Data-480P/distill_dmd_VSA_t2v_1.3B.slurm\n</code></pre> <p>Key Configuration: - Global batch size: 64 - Gradient accumulation steps: 2 - Learning rate: 1e-5 - VSA attention sparsity: 0.8 - Training steps: 4000 (~12 hours)</p>"},{"location":"distillation/dmd/#wan21-14b-model-sparse-distill","title":"Wan2.1 14B Model Sparse-Distill","text":"<p>For the 14B model, we use 8 nodes with 64 H200 GPUs (8 GPUs per node):</p> <pre><code># Multi-node training (8 nodes, 64 GPUs total)\nsbatch examples/distill/Wan2.1-T2V/Wan-Syn-Data-480P/distill_dmd_VSA_t2v_14B.slurm\n</code></pre> <p>Key Configuration: - Global batch size: 64 - Sequence parallel size: 4 - Gradient accumulation steps: 4 - Learning rate: 1e-5 - VSA attention sparsity: 0.9 - Training steps: 3000 (~52 hours) - HSDP shard dim: 8</p>"},{"location":"distillation/dmd/#wan22-5b-model-sparse-distill","title":"Wan2.2 5B Model Sparse-Distill","text":"<p>For the 5B model, we use 8 nodes with 64 H200 GPUs (8 GPUs per node):</p> <pre><code># Multi-node training (8 nodes, 64 GPUs total)\nsbatch examples/distill/Wan2.2-TI2V-5B-Diffusers/Data-free/distill_dmd_t2v_5B.sh \n</code></pre> <p>Key Configuration: - Global batch size: 64 - Sequence parallel size: 1 - Gradient accumulation steps: 1 - Learning rate: 2e-5 - Training steps: 3000 (~12 hours) - HSDP shard dim: 1</p>"},{"location":"getting_started/installation/","title":"\ud83d\udd27 Installation","text":"<p>FastVideo supports the following hardware platforms:</p> <ul> <li>NVIDIA CUDA</li> <li>Apple silicon</li> </ul>"},{"location":"getting_started/installation/#quick-installation","title":"Quick Installation","text":""},{"location":"getting_started/installation/#using-pip","title":"Using pip","text":"<pre><code>pip install fastvideo\n</code></pre>"},{"location":"getting_started/installation/#using-conda","title":"Using conda","text":"<pre><code>conda install -c conda-forge fastvideo\n</code></pre>"},{"location":"getting_started/installation/#from-source","title":"From source","text":"<pre><code>git clone https://github.com/hao-ai-lab/FastVideo.git\ncd FastVideo\npip install -e .\n</code></pre>"},{"location":"getting_started/installation/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>NVIDIA GPUs: CUDA 11.8+ with compute capability 7.0+</li> <li>Apple Silicon: macOS 12.0+ with M1/M2/M3 chips</li> <li>CPU: x86_64 architecture (for CPU-only inference)</li> </ul>"},{"location":"getting_started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Get started with your first video generation</li> <li>Configuration - Learn about configuration options</li> <li>Examples - Explore example scripts and notebooks</li> </ul>"},{"location":"getting_started/quick_start/","title":"\ud83d\ude80 Quick Start","text":"<p>Get up and running with FastVideo in minutes!</p>"},{"location":"getting_started/quick_start/#installation","title":"Installation","text":"<p>First, install FastVideo:</p> <pre><code>pip install fastvideo\n</code></pre>"},{"location":"getting_started/quick_start/#basic-usage","title":"Basic Usage","text":""},{"location":"getting_started/quick_start/#text-to-video-generation","title":"Text-to-Video Generation","text":"<pre><code>from fastvideo import FastVideoPipeline\n\n# Initialize the pipeline\npipe = FastVideoPipeline.from_pretrained(\"wan2.1-t2v-1.3B\")\n\n# Generate a video\nprompt = \"A cat playing with a ball of yarn\"\nvideo = pipe(prompt, num_frames=16, height=512, width=512)\n\n# Save the video\nvideo.save(\"output.mp4\")\n</code></pre>"},{"location":"getting_started/quick_start/#image-to-video-generation","title":"Image-to-Video Generation","text":"<pre><code>from fastvideo import FastVideoPipeline\nfrom PIL import Image\n\n# Load an image\nimage = Image.open(\"input.jpg\")\n\n# Initialize the pipeline\npipe = FastVideoPipeline.from_pretrained(\"wan2.1-i2v-14B-480p\")\n\n# Generate a video from the image\nvideo = pipe(image, num_frames=16, height=480, width=480)\n\n# Save the video\nvideo.save(\"output.mp4\")\n</code></pre>"},{"location":"getting_started/quick_start/#next-steps","title":"Next Steps","text":"<ul> <li>Installation Guide - Detailed installation instructions</li> <li>Configuration - Learn about configuration options</li> <li>Examples - Explore more examples</li> <li>Optimizations - Performance optimization tips</li> </ul>"},{"location":"getting_started/v1_api/","title":"V1 API","text":"<p>FastVideo's V1 API provides a streamlined interface for video generation tasks with powerful customization options. This page documents the primary components of the API.</p>"},{"location":"getting_started/v1_api/#video-generator","title":"Video Generator","text":"<p>This class will be the primary Python API for generating videos and images.</p> <pre><code>    fastvideo.VideoGenerator\n</code></pre> <p>`````{py:class} VideoGenerator(fastvideo_args: fastvideo.fastvideo_args.FastVideoArgs, executor_class: type[fastvideo.worker.executor.Executor], log_stats: bool) :canonical: fastvideo.entrypoints.video_generator.VideoGenerator</p> <p>```{autodoc2-docstring} fastvideo.entrypoints.video_generator.VideoGenerator :parser: docs.source.autodoc2_docstring_parser <pre><code>`VideoGenerator.from_pretrained()` should be the primary way of creating a new video generator.\n\n````{py:method} from_pretrained(model_path: str, device: typing.Optional[str] = None, torch_dtype: typing.Optional[torch.dtype] = None, pipeline_config: typing.Optional[typing.Union[str | fastvideo.configs.pipelines.PipelineConfig]] = None, **kwargs) -&gt; fastvideo.entrypoints.video_generator.VideoGenerator\n:canonical: fastvideo.entrypoints.video_generator.VideoGenerator.from_pretrained\n:classmethod:\n\n```{autodoc2-docstring} fastvideo.entrypoints.video_generator.VideoGenerator.from_pretrained\n:parser: docs.source.autodoc2_docstring_parser\n</code></pre></p>"},{"location":"getting_started/v1_api/#configuring-fastvideo","title":"Configuring FastVideo","text":"<p>The follow two classes <code>PipelineConfig</code> and <code>SamplingParam</code> are used to configure initialization and sampling parameters, respectively.</p>"},{"location":"getting_started/v1_api/#pipelineconfig","title":"PipelineConfig","text":"<pre><code>    fastvideo.PipelineConfig\n</code></pre> <p>`````{py:class} PipelineConfig :canonical: fastvideo.configs.pipelines.base.PipelineConfig</p> <p>```{autodoc2-docstring} fastvideo.configs.pipelines.base.PipelineConfig :parser: docs.source.autodoc2_docstring_parser <pre><code>````{py:method} from_pretrained(model_path: str) -&gt; fastvideo.configs.pipelines.base.PipelineConfig\n:canonical: fastvideo.configs.pipelines.base.PipelineConfig.from_pretrained\n:classmethod:\n\n```{autodoc2-docstring} fastvideo.configs.pipelines.base.PipelineConfig.from_pretrained\n:parser: docs.source.autodoc2_docstring_parser\n</code></pre></p> <p>````{py:method} dump_to_json(file_path: str) :canonical: fastvideo.configs.pipelines.base.PipelineConfig.dump_to_json</p> <p>```{autodoc2-docstring} fastvideo.configs.pipelines.base.PipelineConfig.dump_to_json :parser: docs.source.autodoc2_docstring_parser <pre><code>### SamplingParam\n\n```{autodoc2-summary}\n    fastvideo.SamplingParam\n</code></pre></p> <p>`````{py:class} SamplingParam :canonical: fastvideo.configs.sample.base.SamplingParam</p> <p>```{autodoc2-docstring} fastvideo.configs.sample.base.SamplingParam :parser: docs.source.autodoc2_docstring_parser <pre><code>````{py:method} from_pretrained(model_path: str) -&gt; fastvideo.configs.sample.base.SamplingParam\n:canonical: fastvideo.configs.sample.base.SamplingParam.from_pretrained\n:classmethod:\n\n```{autodoc2-docstring} fastvideo.configs.sample.base.SamplingParam.from_pretrained\n:parser: docs.source.autodoc2_docstring_parser\n</code></pre></p>"},{"location":"getting_started/installation/gpu/","title":"NVIDIA GPU","text":"<p>Instructions to install FastVideo for NVIDIA CUDA GPUs.</p>"},{"location":"getting_started/installation/gpu/#requirements","title":"Requirements","text":"<ul> <li>OS: Linux or Windows WSL</li> <li>Python: 3.10-3.12</li> <li>CUDA 12.8</li> <li>At least 1 NVIDIA GPU</li> </ul>"},{"location":"getting_started/installation/gpu/#set-up-using-python","title":"Set up using Python","text":""},{"location":"getting_started/installation/gpu/#create-a-new-python-environment","title":"Create a new Python environment","text":""},{"location":"getting_started/installation/gpu/#conda","title":"Conda","text":"<p>You can create a new python environment using Conda</p>"},{"location":"getting_started/installation/gpu/#1-install-miniconda-if-not-already-installed","title":"1. Install Miniconda (if not already installed)","text":"<pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh\nsource ~/.bashrc\n</code></pre>"},{"location":"getting_started/installation/gpu/#2-create-and-activate-a-conda-environment-for-fastvideo","title":"2. Create and activate a Conda environment for FastVideo","text":"<pre><code># (Recommended) Create a new conda environment.\nconda create -n fastvideo python=3.12 -y\nconda activate fastvideo\n</code></pre>"},{"location":"getting_started/installation/gpu/#uv","title":"uv","text":"<p>Or you can create a new Python environment using uv, a very fast Python environment manager. Please follow the documentation to install <code>uv</code>. After installing <code>uv</code>, you can create a new Python environment using the following command:</p> <pre><code># (Recommended) Create a new uv environment. Use `--seed` to install `pip` and `setuptools` in the environment.\nuv venv --python 3.12 --seed\nsource .venv/bin/activate\n</code></pre>"},{"location":"getting_started/installation/gpu/#installation","title":"Installation","text":"<pre><code>pip install fastvideo\n\n# or if you are using uv\nuv pip install fastvideo\n</code></pre> <p>Also optionally install flash-attn:</p> <pre><code>pip install flash-attn --no-build-isolation\n</code></pre>"},{"location":"getting_started/installation/gpu/#installation-from-source","title":"Installation from Source","text":""},{"location":"getting_started/installation/gpu/#1-clone-the-fastvideo-repository","title":"1. Clone the FastVideo repository","text":"<pre><code>git clone https://github.com/hao-ai-lab/FastVideo.git &amp;&amp; cd FastVideo\n</code></pre>"},{"location":"getting_started/installation/gpu/#2-install-fastvideo","title":"2. Install FastVideo","text":"<p>Basic installation:</p> <pre><code>pip install -e .\n\n# or if you are using uv\nuv pip install -e .\n</code></pre>"},{"location":"getting_started/installation/gpu/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"getting_started/installation/gpu/#flash-attention","title":"Flash Attention","text":"<pre><code>pip install flash-attn --no-build-isolation\n</code></pre>"},{"location":"getting_started/installation/gpu/#set-up-using-docker","title":"Set up using Docker","text":"<p>We also have prebuilt docker images with FastVideo dependencies pre-installed: Docker Images</p>"},{"location":"getting_started/installation/gpu/#development-environment-setup","title":"Development Environment Setup","text":"<p>If you're planning to contribute to FastVideo please see the following page: Contributor Guide</p>"},{"location":"getting_started/installation/gpu/#hardware-requirements","title":"Hardware Requirements","text":""},{"location":"getting_started/installation/gpu/#for-basic-inference","title":"For Basic Inference","text":"<ul> <li>NVIDIA GPU with CUDA 12.8 support</li> </ul>"},{"location":"getting_started/installation/gpu/#for-lora-finetuning","title":"For Lora Finetuning","text":"<ul> <li>40GB GPU memory each for 2 GPUs with lora</li> <li>30GB GPU memory each for 2 GPUs with CPU offload and lora</li> </ul>"},{"location":"getting_started/installation/gpu/#for-full-finetuningdistillation","title":"For Full Finetuning/Distillation","text":"<ul> <li>Multiple high-memory GPUs recommended (e.g., H100)</li> </ul>"},{"location":"getting_started/installation/gpu/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues during installation, please open an issue on our GitHub repository.</p> <p>You can also join our Slack community for additional support.</p>"},{"location":"getting_started/installation/mps/","title":"MPS (Apple Silicon)","text":"<p>Instructions to install FastVideo for Apple Silicon.</p>"},{"location":"getting_started/installation/mps/#requirements","title":"Requirements","text":"<ul> <li>OS: MacOS</li> <li>Python: 3.12.4</li> </ul>"},{"location":"getting_started/installation/mps/#set-up-using-python","title":"Set up using Python","text":""},{"location":"getting_started/installation/mps/#create-a-new-python-environment","title":"Create a new Python environment","text":""},{"location":"getting_started/installation/mps/#conda","title":"Conda","text":"<p>You can create a new python environment using Conda</p>"},{"location":"getting_started/installation/mps/#1-install-miniconda-if-not-already-installed","title":"1. Install Miniconda (if not already installed)","text":"<pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh\nbash Miniconda3-latest-MacOSX-arm64.sh\nsource ~/.zshrc\n</code></pre>"},{"location":"getting_started/installation/mps/#2-create-and-activate-a-conda-environment-for-fastvideo","title":"2. Create and activate a Conda environment for FastVideo","text":"<pre><code># (Recommended) Create a new conda environment.\nconda create -n fastvideo python=3.12.4 -y\nconda activate fastvideo\n</code></pre>"},{"location":"getting_started/installation/mps/#uv","title":"uv","text":"<p>Or you can create a new Python environment using uv, a very fast Python environment manager. Please follow the documentation to install <code>uv</code>. After installing <code>uv</code>, you can create a new Python environment using the following command:</p> <pre><code># (Recommended) Create a new uv environment. Use `--seed` to install `pip` and `setuptools` in the environment.\nuv venv --python 3.12 --seed\nsource .venv/bin/activate\n</code></pre>"},{"location":"getting_started/installation/mps/#dependencies","title":"Dependencies","text":"<pre><code>brew install ffmpeg\n</code></pre>"},{"location":"getting_started/installation/mps/#installation","title":"Installation","text":"<pre><code>pip install fastvideo\n\n# or if you are using uv\nuv pip install fastvideo\n</code></pre>"},{"location":"getting_started/installation/mps/#installation-from-source","title":"Installation from Source","text":""},{"location":"getting_started/installation/mps/#1-clone-the-fastvideo-repository","title":"1. Clone the FastVideo repository","text":"<pre><code>git clone https://github.com/hao-ai-lab/FastVideo.git &amp;&amp; cd FastVideo\n</code></pre>"},{"location":"getting_started/installation/mps/#2-install-fastvideo","title":"2. Install FastVideo","text":"<p>Basic installation:</p> <pre><code>pip install -e .\n\n# or if you are using uv\nuv pip install -e .\n</code></pre>"},{"location":"getting_started/installation/mps/#development-environment-setup","title":"Development Environment Setup","text":"<p>If you're planning to contribute to FastVideo please see the following page: Contributor Guide</p>"},{"location":"getting_started/installation/mps/#hardware-requirements","title":"Hardware Requirements","text":""},{"location":"getting_started/installation/mps/#for-basic-inference","title":"For Basic Inference","text":"<ul> <li>Mac M1, M2, M3, or M4 (at least 32 GB RAM is preferable for high quality video generation)</li> </ul>"},{"location":"getting_started/installation/mps/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues during installation, please open an issue on our GitHub repository.</p> <p>You can also join our Slack community for additional support.</p>"},{"location":"inference/add_pipeline/","title":"\ud83c\udfd7\ufe0f Adding a New Pipeline","text":"<p>This guide explains how to implement a custom diffusion pipeline in FastVideo, leveraging the framework's modular architecture for high-performance video generation.</p>"},{"location":"inference/add_pipeline/#implementation-process-overview","title":"Implementation Process Overview","text":"<ol> <li>Port Required Modules - Identify and implement necessary model components</li> <li>Create Directory Structure - Set up pipeline files and folders</li> <li>Implement Pipeline Class - Build the pipeline using existing or custom stages</li> <li>Register Your Pipeline - Make it discoverable by the framework</li> <li>Configure Your Pipeline - (Coming soon)</li> </ol> <p>Need help? Join our Slack community.</p>"},{"location":"inference/add_pipeline/#step-1-pipeline-modules","title":"Step 1: Pipeline Modules","text":""},{"location":"inference/add_pipeline/#identifying-required-modules","title":"Identifying Required Modules","text":"<p>FastVideo uses the Hugging Face Diffusers format for model organization:</p> <ol> <li>Examine the <code>model_index.json</code> in the HF model repository:</li> </ol> <pre><code>{\n    \"_class_name\": \"WanImageToVideoPipeline\",\n    \"_diffusers_version\": \"0.33.0.dev0\",\n    \"image_encoder\": [\"transformers\", \"CLIPVisionModelWithProjection\"],\n    \"image_processor\": [\"transformers\", \"CLIPImageProcessor\"],\n    \"scheduler\": [\"diffusers\", \"UniPCMultistepScheduler\"],\n    \"text_encoder\": [\"transformers\", \"UMT5EncoderModel\"],\n    \"tokenizer\": [\"transformers\", \"T5TokenizerFast\"],\n    \"transformer\": [\"diffusers\", \"WanTransformer3DModel\"],\n    \"vae\": [\"diffusers\", \"AutoencoderKLWan\"]\n}\n</code></pre> <ol> <li>For each component:</li> <li>Note the originating library (<code>transformers</code> or <code>diffusers</code>)</li> <li>Identify the class name</li> <li> <p>Check if it's already available in FastVideo</p> </li> <li> <p>Review config files in each component's directory for architecture details</p> </li> </ol>"},{"location":"inference/add_pipeline/#implementing-modules","title":"Implementing Modules","text":"<p>Place new modules in the appropriate directories: - Encoders: <code>fastvideo/models/encoders/</code> - VAEs: <code>fastvideo/models/vaes/</code> - Transformer models: <code>fastvideo/models/dits/</code> - Schedulers: <code>fastvideo/models/schedulers/</code></p>"},{"location":"inference/add_pipeline/#adapting-model-layers","title":"Adapting Model Layers","text":""},{"location":"inference/add_pipeline/#layer-replacements","title":"Layer Replacements","text":"<p>Replace standard PyTorch layers with FastVideo optimized versions: - nn.LayerNorm \u2192 fastvideo.layers.layernorm.RMSNorm - Embedding layers \u2192 fastvideo.layers.vocab_parallel_embedding modules - Activation functions \u2192 versions from fastvideo.layers.activation</p>"},{"location":"inference/add_pipeline/#distributed-linear-layers","title":"Distributed Linear Layers","text":"<p>Use appropriate parallel layers for distribution:</p> <pre><code># Output dimension parallelism\nfrom fastvideo.layers.linear import ColumnParallelLinear\nself.q_proj = ColumnParallelLinear(\n    input_size=hidden_size,\n    output_size=head_size * num_heads,\n    bias=bias,\n    gather_output=False\n)\n\n# Fused QKV projection\nfrom fastvideo.layers.linear import QKVParallelLinear\nself.qkv_proj = QKVParallelLinear(\n    hidden_size=hidden_size,\n    head_size=attention_head_dim,\n    total_num_heads=num_attention_heads,\n    bias=True\n)\n\n# Input dimension parallelism\nfrom fastvideo.layers.linear import RowParallelLinear\nself.out_proj = RowParallelLinear(\n    input_size=head_size * num_heads,\n    output_size=hidden_size,\n    bias=bias,\n    input_is_parallel=True\n)\n</code></pre>"},{"location":"inference/add_pipeline/#attention-layers","title":"Attention Layers","text":"<p>Replace standard attention with FastVideo's optimized attention:</p> <pre><code># Local attention patterns\nfrom fastvideo.attention import LocalAttention\nfrom fastvideo.attention.backends.abstract import _Backend\nself.attn = LocalAttention(\n    num_heads=num_heads,\n    head_size=head_dim,\n    dropout_rate=0.0,\n    softmax_scale=None,\n    causal=False,\n    supported_attention_backends=(_Backend.FLASH_ATTN, _Backend.TORCH_SDPA)\n)\n\n# Distributed attention for long sequences\nfrom fastvideo.attention import DistributedAttention\nself.attn = DistributedAttention(\n    num_heads=num_heads,\n    head_size=head_dim,\n    dropout_rate=0.0,\n    softmax_scale=None,\n    causal=False,\n    supported_attention_backends=(_Backend.SLIDING_TILE_ATTN, _Backend.FLASH_ATTN, _Backend.TORCH_SDPA)\n)\n</code></pre>"},{"location":"inference/add_pipeline/#define-supported-backend-selection","title":"Define supported backend selection","text":"<pre><code>   _supported_attention_backends = (_Backend.FLASH_ATTN, _Backend.TORCH_SDPA)\n</code></pre>"},{"location":"inference/add_pipeline/#registering-models","title":"Registering Models","text":"<p>Register implemented modules in the model registry:</p> <pre><code># In fastvideo/models/registry.py\n_TEXT_TO_VIDEO_DIT_MODELS = {\n    \"YourTransformerModel\": (\"dits\", \"yourmodule\", \"YourTransformerClass\"),\n}\n\n_VAE_MODELS = {\n    \"YourVAEModel\": (\"vaes\", \"yourvae\", \"YourVAEClass\"),\n}\n</code></pre>"},{"location":"inference/add_pipeline/#step-2-directory-structure","title":"Step 2: Directory Structure","text":"<p>Create a new directory for your pipeline:</p> <pre><code>fastvideo/pipelines/\n\u251c\u2500\u2500 your_pipeline/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 your_pipeline.py\n</code></pre>"},{"location":"inference/add_pipeline/#step-3-implement-pipeline-class","title":"Step 3: Implement Pipeline Class","text":"<p>Pipelines are composed of stages, each handling a specific part of the diffusion process:</p> <ul> <li>InputValidationStage: Validates input parameters</li> <li>Text Encoding Stages: Handle text encoding (CLIP/Llama/T5)</li> <li>CLIPImageEncodingStage: Processes image inputs</li> <li>TimestepPreparationStage: Prepares diffusion timesteps</li> <li>LatentPreparationStage: Manages latent representations</li> <li>ConditioningStage: Processes conditioning inputs</li> <li>DenoisingStage: Performs denoising diffusion</li> <li>DecodingStage: Converts latents to pixels</li> </ul>"},{"location":"inference/add_pipeline/#creating-your-pipeline","title":"Creating Your Pipeline","text":"<pre><code>from fastvideo.pipelines.composed_pipeline_base import ComposedPipelineBase\nfrom fastvideo.pipelines.stages import (\n    InputValidationStage, CLIPTextEncodingStage, TimestepPreparationStage,\n    LatentPreparationStage, DenoisingStage, DecodingStage\n)\nfrom fastvideo.fastvideo_args import FastVideoArgs\nfrom fastvideo.pipelines.pipeline_batch_info import ForwardBatch\nimport torch\n\nclass MyCustomPipeline(ComposedPipelineBase):\n    \"\"\"Custom diffusion pipeline implementation.\"\"\"\n\n    # Define required model components from model_index.json\n    _required_config_modules = [\n        \"text_encoder\", \"tokenizer\", \"vae\", \"transformer\", \"scheduler\"\n    ]\n\n    @property\n    def required_config_modules(self) -&gt; List[str]:\n        return self._required_config_modules\n\n    def initialize_pipeline(self, fastvideo_args: FastVideoArgs):\n        \"\"\"Initialize pipeline-specific components.\"\"\"\n        pass\n\n    def create_pipeline_stages(self, fastvideo_args: FastVideoArgs):\n        \"\"\"Set up pipeline stages with proper dependency injection.\"\"\"\n        self.add_stage(\n            stage_name=\"input_validation_stage\",\n            stage=InputValidationStage()\n        )\n\n        self.add_stage(\n            stage_name=\"prompt_encoding_stage\",\n            stage=CLIPTextEncodingStage(\n                text_encoder=self.get_module(\"text_encoder\"),\n                tokenizer=self.get_module(\"tokenizer\")\n            )\n        )\n\n        self.add_stage(\n            stage_name=\"timestep_preparation_stage\",\n            stage=TimestepPreparationStage(\n                scheduler=self.get_module(\"scheduler\")\n            )\n        )\n\n        self.add_stage(\n            stage_name=\"latent_preparation_stage\",\n            stage=LatentPreparationStage(\n                scheduler=self.get_module(\"scheduler\"),\n                vae=self.get_module(\"vae\")\n            )\n        )\n\n        self.add_stage(\n            stage_name=\"denoising_stage\",\n            stage=DenoisingStage(\n                transformer=self.get_module(\"transformer\"),\n                scheduler=self.get_module(\"scheduler\")\n            )\n        )\n\n        self.add_stage(\n            stage_name=\"decoding_stage\",\n            stage=DecodingStage(\n                vae=self.get_module(\"vae\")\n            )\n        )\n\n# Register the pipeline class\nEntryClass = MyCustomPipeline\n</code></pre>"},{"location":"inference/add_pipeline/#creating-custom-stages-optional","title":"Creating Custom Stages (Optional)","text":"<p>If existing stages don't meet your needs, create custom ones:</p> <pre><code>from fastvideo.pipelines.stages.base import PipelineStage\n\nclass MyCustomStage(PipelineStage):\n    \"\"\"Custom processing stage for the pipeline.\"\"\"\n\n    def __init__(self, custom_module, other_param=None):\n        super().__init__()\n        self.custom_module = custom_module\n        self.other_param = other_param\n\n    def forward(self, batch: ForwardBatch, fastvideo_args: FastVideoArgs) -&gt; ForwardBatch:\n        # Access input data\n        input_data = batch.some_attribute\n\n        # Validate inputs\n        if input_data is None:\n            raise ValueError(\"Required input is missing\")\n\n        # Process with your module\n        result = self.custom_module(input_data)\n\n        # Update batch with results\n        batch.some_output = result\n\n        return batch\n</code></pre> <p>Add your custom stage to the pipeline:</p> <pre><code>self.add_stage(\n    stage_name=\"my_custom_stage\",\n    stage=MyCustomStage(\n        custom_module=self.get_module(\"custom_module\"),\n        other_param=\"some_value\"\n    )\n)\n</code></pre>"},{"location":"inference/add_pipeline/#stage-design-principles","title":"Stage Design Principles","text":"<ol> <li>Single Responsibility: Focus on one specific task</li> <li>Functional Pattern: Receive and return a <code>ForwardBatch</code> object</li> <li>Dependency Injection: Pass dependencies through constructor</li> <li>Input Validation: Validate inputs for clear error messages</li> </ol>"},{"location":"inference/add_pipeline/#step-4-register-your-pipeline","title":"Step 4: Register Your Pipeline","text":"<p>Define <code>EntryClass</code> at the end of your pipeline file:</p> <pre><code># Single pipeline class\nEntryClass = MyCustomPipeline\n\n# Or multiple pipeline classes\nEntryClass = [MyCustomPipeline, MyOtherPipeline]\n</code></pre> <p>The registry will automatically: 1. Scan all packages under <code>fastvideo/pipelines/</code> 2. Look for <code>EntryClass</code> variables 3. Register pipelines using their class names as identifiers</p>"},{"location":"inference/add_pipeline/#best-practices","title":"Best Practices","text":"<ul> <li>Reuse Existing Components: Leverage built-in stages and modules</li> <li>Follow Module Organization: Place new modules in appropriate directories</li> <li>Match Model Patterns: Follow existing code patterns and conventions</li> </ul>"},{"location":"inference/cli/","title":"FastVideo CLI Inference","text":"<p>The FastVideo CLI provides a quick way to access the FastVideo inference pipeline for video generation. For more advanced usage, see the Python interface here.</p>"},{"location":"inference/cli/#basic-usage","title":"Basic Usage","text":"<p>The basic command to generate a video is:</p> <pre><code>fastvideo generate --model-path {MODEL_PATH} --prompt {PROMPT}\n</code></pre>"},{"location":"inference/cli/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>--model-path {MODEL_PATH}</code>: Path to the model or model ID</li> <li><code>--prompt {PROMPT}</code>: Text description for the video you want to generate</li> </ul>"},{"location":"inference/cli/#common-arguments","title":"Common Arguments","text":"<p>To see all the options, you can use the <code>--help</code> flag:</p> <pre><code>fastvideo generate --help\n</code></pre>"},{"location":"inference/cli/#hardware-configuration","title":"Hardware Configuration","text":"<ul> <li><code>--num-gpus {NUM_GPUS}</code>: Number of GPUs to use</li> <li><code>--tp-size {TP_SIZE}</code>: Tensor parallelism size (only for the encoder, should not be larger than 1 if text encoder offload is enabled, as layerwise offload + prefetch is faster)</li> <li><code>--sp-size {SP_SIZE}</code>: Sequence parallelism size (Typically should match the number of GPUs)</li> </ul>"},{"location":"inference/cli/#video-configuration","title":"Video Configuration","text":"<ul> <li><code>--height {HEIGHT}</code>: Height of the generated video</li> <li><code>--width {WIDTH}</code>: Width of the generated video</li> <li><code>--num-frames {NUM_FRAMES}</code>: Number of frames to generate</li> <li><code>--fps {FPS}</code>: Frames per second for the saved video</li> </ul>"},{"location":"inference/cli/#generation-parameters","title":"Generation Parameters","text":"<ul> <li><code>--num-inference-steps {STEPS}</code>: Number of denoising steps</li> <li><code>--negative-prompt {PROMPT}</code>: Negative prompt to guide generation away from certain concepts</li> <li><code>--seed {SEED}</code>: Random seed for reproducible generation</li> </ul>"},{"location":"inference/cli/#output-options","title":"Output Options","text":"<ul> <li><code>--output-path {PATH}</code>: Directory to save the generated video</li> <li><code>--save-video</code>: Whether to save the video to disk</li> <li><code>--return-frames</code>: Whether to return the raw frames</li> </ul>"},{"location":"inference/cli/#using-configuration-files","title":"Using Configuration Files","text":"<p>Instead of specifying all parameters on the command line, you can use a configuration file:</p> <pre><code>fastvideo generate --config {CONFIG_FILE_PATH}\n</code></pre> <p>The config file should be in JSON or YAML format with the same parameter names as the CLI options. Command-line arguments will take precedence over settings in the configuration file, allowing you to override specific values while keeping the rest from the config file.</p> <p>Example configuration file (config.json):</p> <pre><code>{\n    \"model_path\": \"FastVideo/FastHunyuan-diffusers\",\n    \"prompt\": \"A beautiful woman in a red dress walking down a street\",\n    \"output_path\": \"outputs/\",\n    \"num_gpus\": 2,\n    \"sp_size\": 2,\n    \"tp_size\": 1,\n    \"num_frames\": 45,\n    \"height\": 720,\n    \"width\": 1280,\n    \"num_inference_steps\": 6,\n    \"seed\": 1024,\n    \"fps\": 24,\n    \"precision\": \"bf16\",\n    \"vae_precision\": \"fp16\",\n    \"vae_tiling\": true,\n    \"vae_sp\": true,\n    \"vae_config\": {\n        \"load_encoder\": false,\n        \"load_decoder\": true,\n        \"tile_sample_min_height\": 256,\n        \"tile_sample_min_width\": 256\n    },\n    \"text_encoder_precisions\": [\n        \"fp16\",\n        \"fp16\"\n    ],\n    \"mask_strategy_file_path\": null,\n    \"enable_torch_compile\": false\n}\n</code></pre> <p>Or using YAML format (config.yaml):</p> <pre><code>model_path: \"FastVideo/FastHunyuan-diffusers\"\nprompt: \"A beautiful woman in a red dress walking down a street\"\noutput_path: \"outputs/\"\nnum_gpus: 2\nsp_size: 2\ntp_size: 1\nnum_frames: 45\nheight: 720\nwidth: 1280\nnum_inference_steps: 6\nseed: 1024\nfps: 24\nprecision: \"bf16\"\nvae_precision: \"fp16\"\nvae_tiling: true\nvae_sp: true\nvae_config:\n  load_encoder: false\n  load_decoder: true\n  tile_sample_min_height: 256\n  tile_sample_min_width: 256\ntext_encoder_precisions:\n  - \"fp16\"\n  - \"fp16\"\nmask_strategy_file_path: null\nenable_torch_compile: false\n</code></pre>"},{"location":"inference/cli/#examples","title":"Examples","text":"<p>Generating a simple video:</p> <pre><code>fastvideo generate --model-path FastVideo/FastHunyuan-diffusers --prompt \"A cat playing with a ball of yarn\" --num-frames 45 --height 720 --width 1280 --num-inference-steps 6 --seed 1024 --output-path outputs/\n</code></pre> <p>Using a negative prompt to avoid certain elements:</p> <pre><code>fastvideo generate --model-path FastVideo/FastHunyuan-diffusers --prompt \"A beautiful forest landscape\" --negative-prompt \"people, buildings, roads\"\n</code></pre> <p>Combining command line arguments and a configuration file:</p> <pre><code>fastvideo generate --config config.json --prompt \"A capybara lounging in a hammock\"\n</code></pre>"},{"location":"inference/cli/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If you encounter CUDA out-of-memory errors, try reducing the video dimensions or number of frames, or the number of inference steps.</li> <li>For reproducible results, set the same seed value between runs.</li> </ul>"},{"location":"inference/comfyui/","title":"FastVideo + ComfyUI","text":"<p>FastVideo provides a custom node suite for ComfyUI.</p> <p>See this README for instructions.</p>"},{"location":"inference/configuration/","title":"Configuration","text":""},{"location":"inference/configuration/#multi-gpu-setup","title":"Multi-GPU Setup","text":"<p>FastVideo automatically distributes the generation process when multiple GPUs are specified:</p> <pre><code># Will use 4 GPUs in parallel for faster generation\ngenerator = VideoGenerator.from_pretrained(\n    \"Wan-AI/Wan2.1-T2V-1.3B-Diffusers\",\n    num_gpus=4,\n)\n</code></pre>"},{"location":"inference/configuration/#customizing-generation","title":"Customizing Generation","text":"<ul> <li><code>PipelineConfig</code>: Initialization time parameters</li> <li><code>SamplingParam</code>: Generation time parameters</li> </ul> <p>You can customize various parameters when generating videos using the <code>PipelineConfig</code> and <code>SamplingParam</code> class:</p> <pre><code>from fastvideo import VideoGenerator, SamplingParam, PipelineConfig\n\ndef main():\n    model_name = \"Wan-AI/Wan2.1-T2V-1.3B-Diffusers\"\n    config = PipelineConfig.from_pretrained(model_name)\n    config.vae_precision = \"fp16\"\n    config.dit_cpu_offload = True\n\n    # Create the generator\n    generator = VideoGenerator.from_pretrained(\n        model_name,\n        num_gpus=1,\n        pipeline_config=config\n    )\n\n    # Create and customize sampling parameters\n    sampling_param = SamplingParam.from_pretrained(\"Wan-AI/Wan2.1-T2V-1.3B-Diffusers\")\n\n    # How many frames to generate\n    sampling_param.num_frames = 45\n\n    # Video resolution (width, height)\n    sampling_param.width = 1024\n    sampling_param.height = 576\n\n    # How many steps we denoise the video (higher = better quality, slower generation)\n    sampling_param.num_inference_steps = 30\n\n    # How strongly the video conforms to the prompt (higher = more faithful to prompt)\n    sampling_param.guidance_scale = 7.5\n\n    # Random seed for reproducibility\n    sampling_param.seed = 42  # Optional, leave unset for random results\n\n    # Generate video with custom parameters\n    prompt = \"A beautiful sunset over a calm ocean, with gentle waves.\"\n    video = generator.generate_video(\n        prompt, \n        sampling_param=sampling_param, \n        output_path=\"my_videos/\",  # Controls where videos are saved\n        return_frames=True,  # Also return frames from this call (defaults to False)\n        save_video=True\n    )\n\n    # If return_frames=True, video contains the generated frames as a NumPy array\n    print(f\"Generated {len(video)} frames\")\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"inference/configuration/#performance-optimization","title":"Performance Optimization","text":"<p>For configuring optimizations, please see our optimizations guide</p>"},{"location":"inference/inference_quick_start/","title":"Inference Quick Start","text":"<p>This page contains step-by-step instructions to get you quickly started with video generation using FastVideo.</p>"},{"location":"inference/inference_quick_start/#requirements","title":"Requirements","text":"<ul> <li>OS: Linux (Tested on Ubuntu 22.04+)</li> <li>Python: 3.10-3.12</li> <li>CUDA: 12.8</li> <li>GPU: At least one NVIDIA GPU</li> </ul>"},{"location":"inference/inference_quick_start/#installation","title":"Installation","text":"<p>We recommend using an environment manager such as <code>Conda</code> to create a clean environment:</p> <pre><code># Create and activate a new conda environment\nconda create -n fastvideo python=3.12\nconda activate fastvideo\n\n# Install FastVideo\npip install fastvideo\n</code></pre> <p>For advanced installation options, see the Installation Guide.</p>"},{"location":"inference/inference_quick_start/#generating-your-first-video","title":"Generating Your First Video","text":"<p>Here's a minimal example to generate a video using the default settings. Create a file called <code>example.py</code> with the following code:</p> <pre><code>from fastvideo import VideoGenerator\n\ndef main():\n    # Create a video generator with a pre-trained model\n    generator = VideoGenerator.from_pretrained(\n        \"Wan-AI/Wan2.1-T2V-1.3B-Diffusers\",\n        num_gpus=1,  # Adjust based on your hardware\n    )\n\n    # Define a prompt for your video\n    prompt = \"A curious raccoon peers through a vibrant field of yellow sunflowers, its eyes wide with interest.\"\n\n    # Generate the video\n    video = generator.generate_video(\n        prompt,\n        return_frames=True,  # Also return frames from this call (defaults to False)\n        output_path=\"my_videos/\",  # Controls where videos are saved\n        save_video=True\n    )\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>Run the script with:</p> <pre><code>python example.py\n</code></pre> <p>The generated video will be saved in the current directory under <code>my_videos/</code> </p> <p>More inference example scripts can be found in <code>scripts/inference/</code></p>"},{"location":"inference/inference_quick_start/#available-models","title":"Available Models","text":"<p>Please see the support matrix for the list of supported models and their available optimizations.</p>"},{"location":"inference/inference_quick_start/#image-to-video-generation","title":"Image-to-Video Generation","text":"<p>You can generate a video starting from an initial image:</p> <pre><code>from fastvideo import VideoGenerator, SamplingParam\n\ndef main():\n    # Create the generator\n    model_name = \"Wan-AI/Wan2.1-I2V-14B-480P-Diffusers\"\n    generator = VideoGenerator.from_pretrained(model_name, num_gpus=1)\n\n    # Set up parameters with an initial image\n    sampling_param = SamplingParam.from_pretrained(model_name)\n    sampling_param.image_path = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/astronaut.jpg\"\n    sampling_param.num_frames = 107\n\n    # Generate video based on the image\n    prompt = \"A photograph coming to life with gentle movement\"\n    generator.generate_video(prompt, sampling_param=sampling_param,\n                             output_path=\"my_videos/\",\n                             save_video=True)\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"inference/inference_quick_start/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues and their solutions:</p>"},{"location":"inference/inference_quick_start/#out-of-memory-errors","title":"Out of Memory Errors","text":"<p>If you encounter CUDA out of memory errors: - Reduce <code>num_frames</code> or video resolution - Enable memory optimization with <code>enable_model_cpu_offload</code> - Try a smaller model or use distilled versions - Use <code>num_gpus</code> &gt; 1 if multiple GPUs are available</p>"},{"location":"inference/inference_quick_start/#slow-generation","title":"Slow Generation","text":"<p>To speed up generation: - Reduce <code>num_inference_steps</code> (20-30 is usually sufficient) - Use half precision (<code>fp16</code>) for the VAE - Use multiple GPUs if available</p>"},{"location":"inference/inference_quick_start/#unexpected-results","title":"Unexpected Results","text":"<p>If the generated video doesn't match your prompt: - Try increasing <code>guidance_scale</code> (7.0-9.0 works well) - Make your prompt more detailed and specific - Experiment with different random seeds - Try a different model</p>"},{"location":"inference/inference_quick_start/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Advanced Inference Configurations</li> <li>Learn about using Optimizations</li> <li>See Examples for more usage scenarios</li> <li>Join our Community Discord.</li> <li>Join our Community Slack.</li> </ul>"},{"location":"inference/optimizations/","title":"Optimizations","text":"<p>This page describes the various options for speeding up generation times in FastVideo.</p>"},{"location":"inference/optimizations/#table-of-contents","title":"Table of Contents","text":"<ul> <li> <p>Optimized Attention Backends</p> </li> <li> <p>Flash Attention</p> </li> <li>Sliding Tile Attention</li> <li>Sage Attention</li> <li> <p>Sage Attention 3</p> </li> <li> <p>Caching Techniques</p> </li> <li>TeaCache</li> </ul>"},{"location":"inference/optimizations/#attention-backends","title":"Attention Backends","text":""},{"location":"inference/optimizations/#available-backends","title":"Available Backends","text":"<ul> <li>Torch SDPA: <code>FASTVIDEO_ATTENTION_BACKEND=TORCH_SDPA</code></li> <li>Flash Attention 2 and 3: <code>FASTVIDEO_ATTENTION_BACKEND=FLASH_ATTN</code></li> <li>Sliding Tile Attention: <code>FASTVIDEO_ATTENTION_BACKEND=SLIDING_TILE_ATTN</code></li> <li>Video Sparse Attention: <code>FASTVIDEO_ATTENTION_BACKEND=VIDEO_SPARSE_ATTN</code></li> <li>Sage Attention: <code>FASTVIDEO_ATTENTION_BACKEND=SAGE_ATTN</code></li> <li>Sage Attention 3: <code>FASTVIDEO_ATTENTION_BACKEND=SAGE_ATTN_THREE</code></li> </ul>"},{"location":"inference/optimizations/#configuring-backends","title":"Configuring Backends","text":"<p>There are two ways to configure the attention backend in FastVideo.</p>"},{"location":"inference/optimizations/#1-in-python","title":"1. In Python","text":"<p>In python, set the <code>FASTVIDEO_ATTENTION_BACKEND</code> environment variable before instantiating <code>VideoGenerator</code> like this:</p> <pre><code>os.environ[\"FASTVIDEO_ATTENTION_BACKEND\"] = \"SLIDING_TILE_ATTN\"\n</code></pre>"},{"location":"inference/optimizations/#2-in-cli","title":"2. In CLI","text":"<p>You can also set the environment variable on the command line:</p> <pre><code>FASTVIDEO_ATTENTION_BACKEND=SAGE_ATTN python example.py\n</code></pre>"},{"location":"inference/optimizations/#flash-attention","title":"Flash Attention","text":"<p><code>FLASH_ATTN</code></p> <p>We recommend always installing Flash Attention 2:</p> <pre><code>pip install flash-attn==2.7.4.post1 --no-build-isolation\n</code></pre> <p>And if using a Hopper+ GPU (ie H100), installing Flash Attention 3 by compiling it from source (takes about 10 minutes for me):</p> <pre><code>git clone https://github.com/Dao-AILab/flash-attention.git &amp;&amp; cd flash-attention\n\ncd hopper\npip install ninja\npython setup.py install\n</code></pre>"},{"location":"inference/optimizations/#sliding-tile-attention","title":"Sliding Tile Attention","text":"<p><code>SLIDING_TILE_ATTN</code></p> <pre><code>pip install st_attn==0.0.4\n</code></pre> <p>Please see this page for more installation instructions.</p>"},{"location":"inference/optimizations/#video-sparse-attention","title":"Video Sparse Attention","text":"<p><code>VIDEO_SPARSE_ATTN</code></p> <pre><code>git submodule update --init --recursive\npython setup_vsa.py install\n</code></pre> <p>Please see this page for more installation instructions.</p>"},{"location":"inference/optimizations/#sage-attention","title":"Sage Attention","text":"<p><code>SAGE_ATTN</code></p> <p>To use SageAttention 2.1.1, please compile from source:</p> <pre><code>git clone https://github.com/thu-ml/SageAttention.git\ncd sageattention\npython setup.py install  # or pip install -e .\n</code></pre>"},{"location":"inference/optimizations/#sage-attention-3","title":"Sage Attention 3","text":"<p><code>SAGE_ATTN_THREE</code></p> <p>SageAttention 3 is an advanced attention mechanism that leverages FP4 quantization and Blackwell GPU Tensor Cores for significant performance improvements.</p>"},{"location":"inference/optimizations/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>RTX5090</li> </ul>"},{"location":"inference/optimizations/#installation","title":"Installation","text":"<p>Note that Sage Attention 3 requires <code>python&gt;=3.13</code>, <code>torch&gt;=2.8.0</code>, <code>CUDA &gt;=12.8</code>. If you are using <code>uv</code> and using <code>torch==2.8.0</code> make sure that <code>sentencepiece==0.2.1</code> in the pyproject.toml file.</p> <p>To use Sage Attention 3 in FastVideo, first get access to the SageAttention3 code, then move <code>sageattn/</code> and <code>setup.py</code> to the directory <code>fastvideo/attention/backends</code>, then install from using:</p> <pre><code>python setup.py install\n</code></pre>"},{"location":"inference/optimizations/#teacache","title":"Teacache","text":"<p>TeaCache is an optimization technique supported in FastVideo that can significantly speed up video generation by skipping redundant calculations across diffusion steps. This guide explains how to enable and configure TeaCache for optimal performance in FastVideo.</p>"},{"location":"inference/optimizations/#what-is-teacache","title":"What is TeaCache?","text":"<p>See the official TeaCache repo and their paper for more details.</p>"},{"location":"inference/optimizations/#how-to-enable-teacache","title":"How to Enable TeaCache","text":"<p>Enabling TeaCache is straightforward - simply add the <code>enable_teacache=True</code> parameter to your <code>generate_video()</code> call:</p> <pre><code># ... previous code\ngenerator.generate_video(\n    prompt=\"Your prompt here\",\n    sampling_param=params,\n    enable_teacache=True\n)\n# more code ...\n</code></pre>"},{"location":"inference/optimizations/#complete-example","title":"Complete Example","text":"<p>At the bottom is a complete example of using TeaCache for faster video generation. You can run it using the following command:</p> <pre><code>python examples/inference/optimizations/teacache_example.py\n</code></pre>"},{"location":"inference/optimizations/#advanced-configuration","title":"Advanced Configuration","text":"<p>While TeaCache works well with default settings, you can fine-tune its behavior by adjusting the threshold value:</p> <ol> <li>Lower threshold values (e.g., 0.1) will result in more skipped calculations and faster generation with slightly more potential for quality degradation</li> <li>Higher threshold values (e.g., 0.15-0.23) will skip fewer calculations but maintain quality closer to the original</li> </ol> <p>Note that the optimal threshold depends on your specific model and content.</p>"},{"location":"inference/optimizations/#benchmarking-different-optimizations","title":"Benchmarking different optimizations","text":"<p>To benchmark the performance improvement, try generating the same video with and without TeaCache enabled and compare the generation times:</p> <pre><code># Without TeaCache\nstart_time = time.perf_counter()\ngenerator.generate_video(prompt=\"Your prompt\", enable_teacache=False)\nstandard_time = time.perf_counter() - start_time\n\n# With TeaCache\nstart_time = time.perf_counter()\ngenerator.generate_video(prompt=\"Your prompt\", enable_teacache=True)\nteacache_time = time.perf_counter() - start_time\n\nprint(f\"Standard generation: {standard_time:.2f} seconds\")\nprint(f\"TeaCache generation: {teacache_time:.2f} seconds\")\nprint(f\"Speedup: {standard_time/teacache_time:.2f}x\")\n</code></pre> <p>Note: If you want to benchmark different attention backends, you'll need to reinstantiate <code>VideoGenerator</code>.</p>"},{"location":"inference/support_matrix/","title":"Compatibility Matrix","text":"<p>The table below shows every supported model and optimizations supported for them.</p> <p>The symbols used have the following meanings:</p> <ul> <li>\u2705 = Full compatibility</li> <li>\u274c = No compatibility</li> <li>\u2b55 = Does not apply to this model</li> </ul>"},{"location":"inference/support_matrix/#models-x-optimization","title":"Models x Optimization","text":"<p>The <code>HuggingFace Model ID</code> can be directly pass to <code>from_pretrained()</code> methods and FastVideo will use the optimal default parameters when initializing and generating videos.</p> <p>Note: Wan2.2 TI2V 5B has some quality issues when performing I2V generation. We are working on fixing this issue.</p>"},{"location":"inference/support_matrix/#special-requirements","title":"Special requirements","text":""},{"location":"inference/support_matrix/#stepvideo-t2v","title":"StepVideo T2V","text":"<ul> <li>The self-attention in text-encoder (step_llm) only supports CUDA capabilities sm_80 sm_86 and sm_90</li> </ul>"},{"location":"inference/support_matrix/#sliding-tile-attention","title":"Sliding Tile Attention","text":"<ul> <li>Currently only Hopper GPUs (H100s) are supported.</li> </ul>"},{"location":"inference/examples/basic/","title":"Basic","text":"<p>Source ."},{"location":"inference/examples/basic/#example-materials","title":"Example materials","text":"<p>{literalinclude} ../../../../examples/inference/basic/basic.py ::: :::</p> <p>{literalinclude} ../../../../examples/inference/basic/basic_dmd.py ::: :::</p> <p>{literalinclude} ../../../../examples/inference/basic/basic_mps.py ::: :::</p> <p>{literalinclude} ../../../../examples/inference/basic/basic_ray.py ::: :::</p> <p>{literalinclude} ../../../../examples/inference/basic/basic_self_forcing_causal.py ::: :::</p> <p>{literalinclude} ../../../../examples/inference/basic/basic_wan2_2.py ::: :::</p> <p>{literalinclude} ../../../../examples/inference/basic/basic_wan2_2_Fun.py ::: :::</p> <p>{literalinclude} ../../../../examples/inference/basic/basic_wan2_2_i2v.py ::: :::</p> <p>{literalinclude} ../../../../examples/inference/basic/basic_wan2_2_ti2v.py ::: :::</p>"},{"location":"inference/examples/examples_inference_index/","title":"\ud83d\ude80 Examples","text":"<p>Inference examples demonstrate how to use FastVideo inference. We recommend starting with basic.md.</p>"},{"location":"inference/examples/optimizations/","title":"Optimizations","text":"<p>Source ."},{"location":"inference/examples/optimizations/#example-materials","title":"Example materials","text":"<p>{literalinclude} ../../../../examples/inference/optimizations/attention_example.py ::: :::</p> <p>{literalinclude} ../../../../examples/inference/optimizations/teacache_example.py ::: :::</p>"},{"location":"inference/examples/sta_mask_search/","title":"STA Mask Search","text":"<p>Source ."},{"location":"inference/examples/sta_mask_search/#example-materials","title":"Example materials","text":"<p>{literalinclude} ../../../../examples/inference/sta_mask_search/inference_wan_sta.sh ::: :::</p> <p>{literalinclude} ../../../../examples/inference/sta_mask_search/wan_example.py ::: :::</p>"},{"location":"sliding_tile_attention/demo/","title":"\ud83d\udd0d Demo","text":"<p>This is is a demo for 2D STA with window size (6,6) operating on a (10, 10) image.</p>      Your browser does not support the video tag.    <p>You can run STA using the following command:</p> <pre><code>bash scripts/inference/v1_inference_wan_STA.sh\n</code></pre>"},{"location":"sliding_tile_attention/installation/","title":"\ud83d\udd27 Installation","text":"<p>You can install the Sliding Tile Attention package using</p> <pre><code>pip install st_attn\n</code></pre>"},{"location":"sliding_tile_attention/installation/#building-from-source","title":"Building from Source","text":"<p>We test our code on Pytorch 2.5.0 and CUDA&gt;=12.4. Currently we only have implementation on H100. First, install C++20 for ThunderKittens:</p> <pre><code>sudo apt update\nsudo apt install gcc-11 g++-11\n\nsudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-11 100 --slave /usr/bin/g++ g++ /usr/bin/g++-11\n\nsudo apt update\nsudo apt install clang-11\n</code></pre> <p>Set up CUDA environment (if using CUDA 12.4):</p> <pre><code>export CUDA_HOME=/usr/local/cuda-12.4\nexport PATH=${CUDA_HOME}/bin:${PATH} \nexport LD_LIBRARY_PATH=${CUDA_HOME}/lib64:$LD_LIBRARY_PATH\n</code></pre> <p>Install STA:</p> <pre><code>cd csrc/attn/sliding_tile_attn/\ngit submodule update --init --recursive\npython setup.py install\n</code></pre>"},{"location":"sliding_tile_attention/installation/#test","title":"\ud83e\uddea Test","text":"<pre><code>python csrc/attn/tests/test_sta.py\n</code></pre>"},{"location":"sliding_tile_attention/installation/#usage","title":"\ud83d\udccb Usage","text":"<pre><code>from st_attn import sliding_tile_attention\n# assuming video size (T, H, W) = (30, 48, 80), text tokens = 256 with padding. \n# q, k, v: [batch_size, num_heads, seq_length, head_dim], seq_length = T*H*W + 256\n# a tile is a cube of size (6, 8, 8)\n# window_size in tiles: [(window_t, window_h, window_w), (..)...]. For example, window size (3, 3, 3) means a query can attend to (3x6, 3x8, 3x8) = (18, 24, 24) tokens out of the total 30x48x80 video.\n# text_length: int ranging from 0 to 256\n# If your attention contains text token (Hunyuan)\nout = sliding_tile_attention(q, k, v, window_size, text_length)\n# If your attention does not contain text token (StepVideo)\nout = sliding_tile_attention(q, k, v, window_size, 0, False)\n</code></pre>"},{"location":"sliding_tile_attention/installation/#inference","title":"\ud83d\ude80Inference","text":"<pre><code>bash scripts/inference/v1_inference_wan_STA.sh\n</code></pre>"},{"location":"training/data_preprocess/","title":"\ud83e\uddf1 Data Preprocess","text":"<p>To save GPU memory, we precompute text embeddings and VAE latents to eliminate the need to load the text encoder and VAE during training.</p> <p>We provide a sample dataset to help you get started. Download the source media using the following command:</p> <pre><code>python scripts/huggingface/download_hf.py --repo_id=FastVideo/mini_i2v_dataset --local_dir=data/mini_i2v_dataset --repo_type=dataset\n</code></pre> <p>The folder <code>crush-smol_raw/</code> contains raw videos and captions for testing preprocessing, while <code>crush-smol_preprocessed/</code> contains latents prepared for testing training.</p> <p>To preprocess the dataset for fine-tuning or distillation, run:</p> <pre><code>bash scripts/preprocess/v1_preprocess_wan_data_t2v # for wan\n</code></pre>"},{"location":"training/data_preprocess/#process-your-own-dataset","title":"Process your own dataset","text":"<p>If you wish to create your own dataset for finetuning or distillation, please refer <code>mini_i2v_dataset/crush-smol_raw/</code> to structure you video dataset in the following format:</p> <pre><code>path_to_your_dataset_folder/\n\u251c\u2500\u2500 videos/\n\u2502   \u251c\u2500\u2500 0.mp4\n\u2502   \u251c\u2500\u2500 1.mp4\n\u251c\u2500\u2500 videos.txt\n\u2514\u2500\u2500 prompt.txt\n</code></pre> <p>To geranate the <code>videos2caption.json</code> and <code>merge.txt</code>, run</p> <pre><code>python scripts/dataset_preparation/prepare_json_file.py --data_folder mini_i2v_dataset/crush-smol_raw/ --output your_output_folder\n</code></pre> <p>Adjust the <code>DATA_MERGE_PATH</code> and <code>OUTPUT_DIR</code> in <code>scripts/preprocess/v1_preprocess_****.sh</code> accordingly and run:</p> <pre><code>bash scripts/preprocess/v1_preprocess_****.sh\n</code></pre> <p>The preprocessed data will be put into the <code>OUTPUT_DIR</code> and the <code>videos2caption.json</code> can be used in finetune and distill scripts.</p>"},{"location":"training/finetune/","title":"\ud83e\udde0 Finetune","text":""},{"location":"training/finetune/#full-finetune","title":"\u26a1 Full Finetune","text":"<p>Ensure your data is prepared and preprocessed in the format specified in data_preprocess.md. For convenience, we also provide a mochi preprocessed Black Myth Wukong data that can be downloaded directly:</p> <pre><code>python scripts/huggingface/download_hf.py --repo_id=FastVideo/Mochi-Black-Myth --local_dir=data/Mochi-Black-Myth --repo_type=dataset\n</code></pre> <p>Download the original model weights as specified in the Distillation Section:</p> <p>Then you can run the finetune with:</p> <pre><code>bash scripts/finetune/finetune_mochi.sh # for mochi\n</code></pre> <p>Note that for finetuning, we did not tune the hyperparameters in the provided script.</p>"},{"location":"training/finetune/#finetune-with-vsa","title":"\u26a1 Finetune with VSA","text":"<p>Follow data_preprocess.md to get parquet files for preproccessed latent, and then run:</p> <pre><code>bash scripts/finetune/finetune_v1_VSA.sh\n</code></pre>"},{"location":"training/finetune/#lora-finetune","title":"\u26a1 Lora Finetune","text":"<p>Hunyuan supports Lora fine-tuning of videos up to 720p. Demos and prompts of Black-Myth-Wukong can be found in here. You can download the Lora weight through:</p> <pre><code>python scripts/huggingface/download_hf.py --repo_id=FastVideo/Hunyuan-Black-Myth-Wukong-lora-weight --local_dir=data/Hunyuan-Black-Myth-Wukong-lora-weight --repo_type=model\n</code></pre>"},{"location":"training/finetune/#minimum-hardware-requirement","title":"Minimum Hardware Requirement","text":"<ul> <li>40 GB GPU memory each for 2 GPUs with lora.</li> <li>30 GB GPU memory each for 2 GPUs with CPU offload and lora.</li> </ul> <p>Currently, both Mochi and Hunyuan models support Lora finetuning through diffusers. To generate personalized videos from your own dataset, you'll need to follow three main steps: dataset preparation, finetuning, and inference.</p>"},{"location":"training/finetune/#dataset-preparation","title":"Dataset Preparation","text":"<p>We provide scripts to better help you get started to train on your own characters! You can run this to organize your dataset to get the videos2caption.json before preprocess. Specify your video folder and corresponding caption folder (caption files should be .txt files and have the same name with its video):</p> <pre><code>python scripts/dataset_preparation/prepare_json_file.py --video_dir data/input_videos/ --prompt_dir data/captions/ --output_path data/output_folder/videos2caption.json --verbose\n</code></pre> <p>Also, we provide script to resize your videos:</p> <pre><code>python scripts/data_preprocess/resize_videos.py\n</code></pre>"},{"location":"training/finetune/#finetuning","title":"Finetuning","text":"<p>After basic dataset preparation and preprocess, you can start to finetune your model using Lora:</p> <pre><code>bash scripts/finetune/finetune_hunyuan_hf_lora.sh\n</code></pre>"},{"location":"training/finetune/#inference","title":"Inference","text":"<p>For inference with Lora checkpoint, you can run the following scripts with additional parameter <code>--lora_checkpoint_dir</code>:</p> <pre><code>bash scripts/inference/inference_hunyuan_hf.sh\n</code></pre> <p>We also provide scripts for Mochi in the same directory.</p>"},{"location":"training/finetune/#finetune-with-both-image-and-video","title":"Finetune with Both Image and Video","text":"<p>Our codebase support finetuning with both image and video.</p> <pre><code>bash scripts/finetune/finetune_hunyuan.sh\nbash scripts/finetune/finetune_mochi_lora_mix.sh\n</code></pre> <p>For Image-Video Mixture Fine-tuning, make sure to enable the <code>--group_frame</code> option in your script.</p>"},{"location":"training/examples/Wan2.1-Fun-1.3B-InP/","title":"Wan2.1-Fun-1.3B-InP","text":"<p>Examples for the Wan2.1-Fun-1.3B-InP model.</p>"},{"location":"training/examples/Wan2.1-Fun-1.3B-InP_crush_smol/","title":"Wan2.1 Fun 1.3B InP crush smol","text":"<p>Source ."},{"location":"training/examples/Wan2.1-Fun-1.3B-InP_crush_smol/#example-materials","title":"Example materials","text":"<p>{literalinclude} ../../../../examples/training/finetune/Wan2.1-Fun-1.3B-InP/crush_smol/download_dataset.sh ::: :::</p> <p>{literalinclude} ../../../../examples/training/finetune/Wan2.1-Fun-1.3B-InP/crush_smol/finetune_i2v.sh ::: :::</p> <p>{literalinclude} ../../../../examples/training/finetune/Wan2.1-Fun-1.3B-InP/crush_smol/finetune_i2v.slurm ::: :::</p> <p>{literalinclude} ../../../../examples/training/finetune/Wan2.1-Fun-1.3B-InP/crush_smol/preprocess_wan_data_i2v.sh ::: :::</p> <p>{literalinclude} ../../../../examples/training/finetune/Wan2.1-Fun-1.3B-InP/crush_smol/validation.json ::: :::</p> <p>{literalinclude} ../../../../examples/training/finetune/Wan2.1-Fun-1.3B-InP/crush_smol/validation_dataset/yYcK4nANZz4-Scene-027.mp4 ::: :::</p> <p>{literalinclude} ../../../../examples/training/finetune/Wan2.1-Fun-1.3B-InP/crush_smol/validation_dataset/yYcK4nANZz4-Scene-030.mp4 ::: :::</p> <p>{literalinclude} ../../../../examples/training/finetune/Wan2.1-Fun-1.3B-InP/crush_smol/validation_dataset/yYcK4nANZz4-Scene-034.mp4 ::: :::</p>"},{"location":"training/examples/Wan2.1-VSA/","title":"Wan2.1-VSA","text":"<p>Examples for the Wan2.1-VSA model.</p>"},{"location":"training/examples/Wan2.1-VSA_Wan-Syn-Data/","title":"Wan2.1 VSA Wan Syn Data","text":"<p>Source ."},{"location":"training/examples/Wan2.1-VSA_Wan-Syn-Data/#example-materials","title":"Example materials","text":"<p>{literalinclude} ../../../../examples/training/finetune/Wan2.1-VSA/Wan-Syn-Data/I2V-14B-VSA.slurm ::: :::</p> <p>{literalinclude} ../../../../examples/training/finetune/Wan2.1-VSA/Wan-Syn-Data/T2V-1.3B-VSA.slurm ::: :::</p> <p>{literalinclude} ../../../../examples/training/finetune/Wan2.1-VSA/Wan-Syn-Data/T2V-14B-VSA.slurm ::: :::</p> <p>{literalinclude} ../../../../examples/training/finetune/Wan2.1-VSA/Wan-Syn-Data/download_dataset.sh ::: :::</p> <p>{literalinclude} ../../../../examples/training/finetune/Wan2.1-VSA/Wan-Syn-Data/validation_64.json ::: :::</p>"},{"location":"training/examples/examples_training_index/","title":"\ud83d\ude80 Examples","text":"<p>Training examples demonstrate how to use FastVideo training.</p>"},{"location":"training/examples/finetune/","title":"Finetune","text":"<p>Examples using finetune.</p>"},{"location":"training/examples/wan_i2v_14B_480p/","title":"Wan I2V 14B 480P","text":"<p>Examples for the wan_i2v_14B_480p model.</p>"},{"location":"training/examples/wan_i2v_14B_480p_crush_smol/","title":"wan i2v 14B 480p crush smol","text":"<p>Source ."},{"location":"training/examples/wan_i2v_14B_480p_crush_smol/#example-materials","title":"Example materials","text":"<p>{literalinclude} ../../../../examples/training/finetune/wan_i2v_14B_480p/crush_smol/download_dataset.sh ::: :::</p> <p>{literalinclude} ../../../../examples/training/finetune/wan_i2v_14B_480p/crush_smol/finetune_i2v.sh ::: :::</p> <p>{literalinclude} ../../../../examples/training/finetune/wan_i2v_14B_480p/crush_smol/finetune_i2v.slurm ::: :::</p> <p>{literalinclude} ../../../../examples/training/finetune/wan_i2v_14B_480p/crush_smol/finetune_i2v_lora.sh ::: :::</p> <p>{literalinclude} ../../../../examples/training/finetune/wan_i2v_14B_480p/crush_smol/preprocess_wan_data_i2v.sh ::: :::</p> <p>{literalinclude} ../../../../examples/training/finetune/wan_i2v_14B_480p/crush_smol/preprocess_wan_data_i2v_new.sh ::: :::</p> <p>{literalinclude} ../../../../examples/training/finetune/wan_i2v_14B_480p/crush_smol/validation.json ::: :::</p>"},{"location":"training/examples/wan_t2v_1.3B/","title":"Wan T2V 1.3B","text":"<p>Examples for the wan_t2v_1.3B model.</p>"},{"location":"training/examples/wan_t2v_1.3B_crush_smol/","title":"wan t2v 1.3B crush smol","text":"<p>Source ."},{"location":"training/examples/wan_t2v_1.3B_crush_smol/#example-materials","title":"Example materials","text":"<p>{literalinclude} ../../../../examples/training/finetune/wan_t2v_1.3B/crush_smol/download_dataset.sh ::: :::</p> <p>{literalinclude} ../../../../examples/training/finetune/wan_t2v_1.3B/crush_smol/finetune_t2v.sh ::: :::</p> <p>{literalinclude} ../../../../examples/training/finetune/wan_t2v_1.3B/crush_smol/finetune_t2v.slurm ::: :::</p> <p>{literalinclude} ../../../../examples/training/finetune/wan_t2v_1.3B/crush_smol/finetune_t2v_lora.sh ::: :::</p> <p>{literalinclude} ../../../../examples/training/finetune/wan_t2v_1.3B/crush_smol/preprocess_wan_data_t2v.sh ::: :::</p> <p>{literalinclude} ../../../../examples/training/finetune/wan_t2v_1.3B/crush_smol/preprocess_wan_data_t2v_new.sh ::: :::</p> <p>{literalinclude} ../../../../examples/training/finetune/wan_t2v_1.3B/crush_smol/validation.json ::: :::</p>"},{"location":"video_sparse_attention/installation/","title":"\ud83d\udd27 Installation","text":"<p>You can install the Video Sparse Attention package using</p> <pre><code>pip install vsa\n</code></pre>"},{"location":"video_sparse_attention/installation/#building-from-source","title":"Building from Source","text":"<p>We support H100 (via ThunderKittens) and any other GPU (via Triton) for VSA.</p> <p>First, install C++20 for ThunderKittens (if using H100):</p> <pre><code>sudo apt update\nsudo apt install gcc-11 g++-11\n\nsudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-11 100 --slave /usr/bin/g++ g++ /usr/bin/g++-11\n\nsudo apt update\nsudo apt install clang-11\n</code></pre> <p>Set up CUDA environment (if using CUDA 12.8):</p> <pre><code>export CUDA_HOME=/usr/local/cuda-12.8\nexport PATH=${CUDA_HOME}/bin:${PATH} \nexport LD_LIBRARY_PATH=${CUDA_HOME}/lib64:$LD_LIBRARY_PATH\n</code></pre> <p>Install VSA:</p> <pre><code>cd csrc/attn/video_sparse_attn/\ngit submodule update --init --recursive\npython setup.py install\n</code></pre>"},{"location":"video_sparse_attention/installation/#test","title":"\ud83e\uddea Test","text":"<pre><code>python csrc/attn/tests/test_vsa.py\n</code></pre>"},{"location":"video_sparse_attention/installation/#usage","title":"\ud83d\udccb Usage","text":"<pre><code>from vsa import video_sparse_attn\n\n# q, k, v: [batch_size, num_heads, seq_len, head_dim]\n# variable_block_sizes: [num_blocks] - number of valid tokens in each block\n# topk: int - number of top-k blocks to attend to\n# block_size: int or tuple of 3 ints - size of each block (default: 64 tokens)\n# compress_attn_weight: optional weight for compressed attention branch\n\noutput = video_sparse_attn(q, k, v, variable_block_sizes, topk, block_size, compress_attn_weight)\n</code></pre>"},{"location":"video_sparse_attention/installation/#inference","title":"\ud83d\ude80Inference","text":"<pre><code>bash scripts/inference/v1_inference_wan_VSA.sh\n</code></pre>"}]}